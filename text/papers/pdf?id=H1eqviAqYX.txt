Under review as a conference paper at ICLR 2019
WHY DO NEURAL RESPONSE GENERATION MODELS PREFER UNIVERSAL REPLIES?
Anonymous authors Paper under double-blind review
ABSTRACT
Recent advances in neural Sequence-to-Sequence (Seq2Seq) models reveal a purely data-driven approach to the response generation task. Despite its diverse variants and applications, the existing Seq2Seq models are prone to producing short and generic replies, which blocks such neural network architectures from being utilized in practical open-domain response generation tasks. In this research, we analyze this critical issue from the perspective of the optimization goal of models and the specific characteristics of human-to-human conversational corpora. Our analysis is conducted by decomposing the goal of Neural Response Generation (NRG) into the optimizations of word selection and ordering. It can be derived from the decomposing that Seq2Seq based NRG models naturally tend to select common words to compose responses, and ignore the semantic of queries in word ordering. On the basis of the analysis, we propose a max-marginal ranking regularization term to avoid Seq2Seq models from producing the generic and uninformative responses. The empirical experiments on benchmarks with several metrics have validated our analysis and proposed methodology.
1 INTRODUCTION
Past years have witnessed the dramatic progress on the application of generative sequential models (also noted as seq2seq learning (Sutskever et al., 2014; Bahdanau et al., 2015)) on Neural Response Generation (NRG) fields (Vinyals & Le, 2015; Serban et al., 2017a). Seq2seq model has been proved to be capable of directly generating reply given an open domain query (Li et al., 2016c; Xing et al., 2017). Both relevant words or phrases are automatically selected, and smoothness and fluency of responses are guaranteed through the end-to-end learning. Moreover, abundant impressive humanto-machine conversation cases have been presented in many previous studies (Serban et al., 2016; Shang et al., 2015; Shao et al., 2017).
Despite these promising results, current Sequence-to-Sequence (Seq2Seq) architectures for response generation are still far from steadily generating relevant and coherent replies. The essential issue identified by many studies is the Universal Replies: the model tends to generate short and general replies which contain limited information, such as "That's great!", "I don't know", etc. (Li et al., 2016b;d; Mou et al., 2016; Xing et al., 2017). Intuitively, this problem was attributed to the vast coverage of common replies in the training set and insufficient guiding knowledge in the models' response generation step (Mou et al., 2016; Shao et al., 2017). Hence, current efforts mainly focus on introducing external information to the model (Mou et al., 2016; Xing et al., 2017), and encouraging the model to generate diverse responses in searching space via variational beam search strategies during inference (Shao et al., 2017; Li et al., 2016b;d).
Nevertheless, most historical analyses over the issue are empirical and lack of statistical evidence. Therefore, in this paper, we conduct an in-depth investigation on the performance of seq2seq models on the NRG task. In our inspections on the existing dialog corpora, it is shown that those repeatedly appeared replies have two essential traits: 1) Most of them are composed of highly frequent words; 2) They cover a large portion of responses in the dialog corpora. Above characters of universal replies deviate the NRG from other successful applications of sea2seq model such as translation, and lead to the misrecognition of those common replies as grammatically corrected patterns by models. Therefore common replies are unconsciously preferred by current generative NRG models.
1

Under review as a conference paper at ICLR 2019

Table 1: Replies and translated version of an example which reveal the different source-target sentence distribution for dialog and translation.

Query I would add Metropolis to the list.

Replies

I love this film so much. Me too, it is a beautiful film. This movie has beautiful background art. Fritz is really a good director, I like his film. Is "Metropolis" based on a book?
Brigitte cooling off on the set of Metropolis.

Translate J'ajouterais Metropolis a` la liste. Je voudrais ajouter Metropolis a` la liste.

To discuss the influences from the specific distributed corpus, we decompose the target sequence's probability into two parts and analyze the probability respectively.
To break down the mentioned characteristics of dialog corpora in the model training step, we propose a ranking-oriented regularization term to prune the scores of those irrelevant replies. Experimental results reveal that the model with such regularization can produce better results and avoid generating ambiguous responses. Also, case studies show that the issue of generic response is alleviated that these common responses are ranked relatively lower than more appropriate answers.
The main contributions of this paper are concluded as follows: 1) We analyze the loss function of Seq2seq models on NRG task and conclude several critical reasons that the NRG models prefer universal replies; 2) Based on the analysis, a max-marginal ranking regularization is presented to help the model converge to informative responses.
2 ANALYSIS OF SEQ2SEQ MODELS FOR NRG
Different from significant advances in machine translation (Bahdanau et al., 2015) and abstractive summarization (Rush et al., 2015; Nallapati et al., 2016), it remains challenging to apply Seq2Seq models in practical response generation. One widely accepted issue within current models is that Seq2Seq architectures are inclined to produce common and unrelated replies, even when the quality of training data is significantly improved and different Seq2Seq variants are proposed. The primary reason for this phenomenon lies in the fact that the semantic constraint from query to the possible responses is naturally weak, since the responses to a given query are not required to be semantically equivalent. In contrast, the references in machine translation or summarization are usually restricted to be equivalent to each other semantically or even lexically. Especially, for machine translation, words that appear in the target language should satisfy word level mapping from the source sentence, so the learned word alignment function could ensure the model to generate suitable translated words. Different from learning the semantic alignments between languages in NMT, in NRG the replies can be diversified as they only need to satisfy the causality with the given queries. Moreover, given a query, the sequential model is optimized to learn the shared information among all replies, thus the model is more likely to choose those high-frequent common replies, which is also mentioned in Ritter et al. (2011).
Taking the case in Table 1 for example, the topic of this query is about movie. It can be observed that the replies shown in the table are semantically diversified: the first two replies are related to the opinion of the respondent toward the movie, while the rest replies are about the director, content, and origin of the movie. By contrast, the two valid translations in French are very similar regarding their semantics, which can be attributed to the fixed word-level mapping between query and targets.
2.1 PROBLEM DECOMPOSITION
The sequence-mapping problem in NRG can be decomposed into two independent sub-learning problems: 1) Target word selection, in which a query is summarized and translated into the semantic space of responses, and then a set of target words is selected to represent the meaning; 2) Word ordering, in which a grammatical coherent reply is generated based on the candidate word set (Vinyals
2

Under review as a conference paper at ICLR 2019

Probability
0.07 0.05 0.03 0.01

mebteraouptfiioflloiiunsalsm lltaoshnvietgsie
backdcbigorrrrbimeobgoehmafgllocaeouoitrisituthllovtihaooeonslncokmhttifiaoirffkrdtddtosyemeeezghoes

Figure 1: Response Unigram probability distribution in Table 1.

et al., 2016). The word selection and ordering of the target sequence are jointly learned which can also be reflected in the model's loss function by two possible factored phases:

log p(y|x) = log p(S(y)|x) + log p(y|S(y), x)

(1)

where x stands for the given query and y is the corresponding response with n words. Besides, S(y) = {w1, · · · , wn|wi  y, i  [1, n]} represents all predicted words without sequential order, so p(S(y)|x) is referred as the probability of the target word selection. Meanwhile, p(y|S(y), x)
indicates the probability of word ordering given this group of possible words. Thus, the objective can be redescribed from maximizing the probability of the ground truth response y under query x to
maximizing these two joint probabilities simultaneously.

After the above interpretation, we will further discuss the impact of the implicative constriction from two separated probabilities in Eq. 1, which results in the potential failure of models in learning conversational patterns.

2.2 TARGET WORD SELECTION PROBABILITY

Assuming we have a set of K ground truth replies: {y1, · · · , yK} for a given query x, the variational upper bound of the target word selection probability can be derived using Jensen's Inequality (Boyd
& Vandenberghe, 2004):

KK

log p(S(yk)|x) = log

p(w|x)

k k wS(yk)

= log p(w|x)
wKk S(yk)

(2)

 log

p(w|x)

wKk S(yk)

where kKS(yk) denotes all words that appear in the entire response set. So optimizing the first segment is proportional to maximizing the last conditional probabilities, and the optimal strategy is to assign probabilities according to the frequency of words in these K responses. Such strategy
adopted by Seq2Seq can be verified by the long-tailed distribution of words in Fig. 1, in which
only few common words are assigned with preferred high probabilities. Given that, during the
inference, the best strategy is to employ more frequently occurring words rather than rare ones such
as "background," "art," and "director" in Table 1.

Furthermore, assuming that each response contains a fixed number of T words, we can find that the probability of each response for x is inversely proportional to K:

p(w|x) = E(w|x) × T  1

K×T

K

wKk S(yk)

(3)

where E(w|x) denotes the mean frequency of words appeared in these K replies, which is 1.32 for the cases in Table 1. In general, the mean frequency is around 1 owing to the long-tailed Unigram distribution which satisfies Zipf's law (Zipf, 1935). In other words, the target word selection

3

Under review as a conference paper at ICLR 2019

probability is limited by K, so queries with more diverse answers are more challenging to learn. Meanwhile, it is difficult to obtain good predictions for lower-informational queries, as they contain more possible responses which are somewhat equivalent to a larger K (Li et al., 2016a).
Nonetheless, the translation task requires word-level mappings as they are well-aligned in the semantic space, therefore source and target sentences are semantically equivalent. So that, translated candidates are confined to K  1. Thus the upper bound can be approximated as the full probability.

2.3 WORD ORDERING PROBABILITY

2.3.1 LEMMAS

Before discussing the word ordering probability, we present four lemmas and corresponding proofs. Moreover, all these lemmas are only available for the response generation task except Lemma 1.

According to the Zipf's law (Zipf, 1935), the frequency of any word is inversely proportional to its rank in the frequency table, such that the probability p(wi) = C/i, where C  0.1,   1, and i is
the frequency rank of the word wi. Then, denoting the vocabulary size as V and the total number of
query-response pairs as N , we can formulate two characteristics of a universal reply y as follows:

1) A response is universal if it consists of only top-t ranked words. For any word w in such response, p(w)  1/(10t) according to the Zipf's law.

2) The amount of possible queries M of y is directly proportional to size of query-response pairs N , noted as 1 M  N .

To simplify, we suppose t > 1000 to cover most universal replies. The frequency of the response not belonging to the universal replies is 1. Then we can derive the following lemmas.

Lemma 1 p(S(y)|y) = 1, p(S(y), y) = p(y), p(x, y, S(y)) = p(x, y).

Proof. Lemma 1 describes the obvious fact that the event "the word set of the response equals to S(y)" must happen when the event "y stands for the response" is established.

Lemma 2 p(x|yur) = 1, where 1 > 0 and is sufficiently small, and p(x|yur) is a universal reply.

Proof. Based on the second character of the universal reply and the fact that N is a very large number

for any large scaled datasets, Lemma 2

is established as:

p(x|yur )

=

1 M



1 N

=

1

Lemma 3 stands for

the

ii-pth(yuiunri|vSer(sya)l)reply1a, npd(yyjojo|Sis(yth)e)

= j-th

2, where 2 > 0 and is sufficiently small, yiur non-universal grammatical replies, meanwhile,

S(yiur)  S(y) and S(yjo)  S(y)

Proof. According to the following inequation

t1 it

>

t+1 1

1 x

dx

=

ln(t +

1),

we

can

get

the

conclusion that the probability of a chosen word belonging to the most frequent t words is large than

0.1  ln(t + 1) > 0.69. Since y contains T words, there is at least T ln(t + 1) words belonging to

the top-t ranked on average according to the binomial distribution.

We suppose m responses are universal replies among the n possible responses when their words are constrained by S(y). Besides, the proportion of m can be computed as:

m T ln(t+1) =
n
i=1

CTi

T j=1

CTj



1 ln(t + 1)
10

2T - =

T i=T

ln(t+1)

CTi

2T



1 ln(t + 1)
10

1 > ln(t + 1)
20

> 0.34

4

(4)

Under review as a conference paper at ICLR 2019

where C donates the combination. Since n/m is not a very large number, the total probability of these m replies can be deducted as:

p(yiur|S(y)) =
i

m i

f

(yiur

)

m i

f

(yiur

)

+

n-m i

f

(Yio)

M m = M  m + 1  (n - m)

M =
M + n/m - 1

M >
M +3

(5)

where f (y) donates the frequency of a response y in the corpus. According to the Eq. 5 and the fact

that M can be

 N is a very large number for any established. Apparently, for any

practical large-scale datasets, the other candidate response yjo, its

i p(yiur|S(y))  1 probability satisfies

p(yjo|S(y)) < 1 - i p(yiur|S(y)) = 2.

Lemma 4 Assuming each informative query has K ground-truth replies and the query-response

pairs are extracted from a multi-turn conversational corpus, a reply y not belonging to universal

replies

has

K

unique

queries,

noted

as

p(x|y)

=

1 K

.

Proof. Most query-response pairs are extracted from a practical large-scale multi-turn conversational corpus, so that any response always works as the post in another pair. That is, y also appears K times as it also has K replies. Therefore, there also exist K unique posts for y.

2.3.2 DISCUSSION

On the basis of Lemma 1, the word ordering probability could be deducted as:

log

p(y|S (y),

x)

=

log

p(S(y)|y) p(y) p(x|y, S(y)) p(S(y)) p(x|S(y))

p(y) p(x|y, S(y)) = log1 + log p(S(y)) + log p(x|S(y))

p(y, S(y))

p(x, y, S(y)) p(S(y))

= log p(S(y)) + log p(y, S(y)) p(x, S(y))

=

log

p(y|S (y))

+

log

p(x, p(y)

y) p(S p(x, S

(y)) (y))

=

log

p(y|S (y))

+

log

p(x|y) p(x|S (y))

= log p(y|S(y)) + log

p(x|y)

i p(x|yi)p(yi|S(y))

(6)

All the possible yi satisfying S(yi)  S(y) can be divided into three categories: ground-truth reply y, universal replies yur and other replies yo. From above, we can get the following direct proportion according to the Lemma 2 and Lemma 3,

p(x|yi )p(yi |S (y))
i
= p(x|y)p(y|S(y)) + p(x|yiur)p(yiur|S(y)) + p(x|yio)p(yio|S(y))
ii
 p(x|y)p(y|S(y)) + 1 + 2

(7)

On the basis of Eq. 7 and Lemma 4, for any reply y not belonging to universal replies, the Eq. 6 can be further deducted as:

5

Under review as a conference paper at ICLR 2019

log

p(y|S

(y),

x)



log

p(y|S

(y))

+

log

p(x|y) p(x|y)p(y|S (y))

+



log

p(y|S (y)) p(y|S(y)) +

1 K

(8)

where = 1 + 2 > 0, which is also a sufficiently small positive value. Thus, optimizing the word ordering probability for the non-universal replies is partially equivalent to maximizing p(y|S(y)).

In fact the term p(y|S(y)) is the language model probability and it is irrelevant with the query

x (Maning et al., 2009). In the sequential models, it is performed as t p(yt|y1:t-1, S(y)), in other words the sequences are generated based only on previously outputted words. This equation

indicates that optimizing the mainly seeks the grammatical competence based on the selected words.

2.4 BRIEF SUMMARY
In conclusion, the insufficient constraint of the target words' cross-entropy loss in NRG is the primary reason that hinders seq2seq models from exploring presumable parameters. This situation is mainly caused by the particular distribution of NRG corpus, since there exist many universal replies composed of high-frequent words in corpus. Consequently, the model tends to promotes such universal replies, regardless of the given query.

3 MAX-MARGINAL RANKING REGULARIZATION

As discussed above, various responses corresponding to the same query appearing in the training data leads to the undesired preference of NRG on universal replies, so an intuitive solution is removing the multiple replies and just keeping one-to-one pairs. However, filtering the training dataset in large scale raises the difficulty of model training. Besides, naively removing the multiple replies is detrimental to the reply diversity, which is important in NRG task. As shown in Table 1, an ideal chatbot agent is prospected to provide all listed replies and build a connection with some keywords such as `film', `background', `director' and `book', rather than other commonly appeared words like `I', `him', `a' and `really'.

Thus, under this assumption, we propose a max-marginal ranking loss to emphasize the queries' impact on these less common but relevant words. During training, as it becomes a necessity to constrain the learned feature space and reinforce related replies with more discriminative information, we classify the candidate responses into two categories: positive (i.e., highly related) and negative (i.e., irrelevant) answers. A training instance is re-constructed as a triplet (x, y, y-), where a tuple (x, y) is the original query-response pair and noise y- is uniformly sampled from all of the responses in the training data. Given that, the model's cost function is reconstructed as:

 = log p(y|x) +  max{0, log p(y|x) - log p(y-|x) - }

(9)

where log p(y|x) denotes the cross-entropy loss between the model's prediction and ground truth sequences, and the second part encourages the separation between the irrelevant responses and related replies. Moreover, the hyper-parameter  defines the penalty for the seq2seq loss, it offers a degree of freedom to control the importance of the max-marginal between the positive and negative instances. The model is trained in the same setting as the conventional model when  = 0.

The gradient of  is computed using the sub-gradient method, as the second term is nondifferentiable but convex (Agarwal & Collins, 2010). Supposing log p(y|x) - log p(y-|x)  , the gradient of the composed loss function can be formalized as:

  = log p(y|x), If log p(y|x) - log p(y-|x) > , then the gradient should be written as:

(10)

  = (1 + )log p(y|x) - log p(y-|x).

(11)

The underlying motivation of our proposed loss function is based on three considerations: 1) Universal replies are more likely to be sampled from a statistical perspective, so adding a negative term would directly ease the weight of these generic responses, and the ranking regularization can penalize those irrelevant responses; 2) Positive and negative sentences overall share a same set of generic words, which suggests that the loss optimization should pay more attention on those different words rather than generic ones; 3) Only differentiable loss can solely be served as the model's

6

Under review as a conference paper at ICLR 2019

Table 2: Dataset statistics. For multiple replies, the three values represent the percentages of queries with one, two, and more than two responses, respectively. For the out of vocabulary (OOV) columns, the number in front of "/" denotes the percentage rate of the query, and the other one denotes replies.

# train # valid # test

QA Pairs Unique Replies Multi Replies(%) OOV (%) Vocab Size

5,982,868 315,136 315,136 4,499,176 298,723 287,312 70/24/6 97/2/1 96/3/1
.90/.90 .92/.93 .91/.92 29241/27859

optimization goal for the sequence generation model. Furthermore, the newly proposed loss aims to penalize frequent words and irrelevant candidates, rather than repudiating the literal expression included in negative samples. Consequently, based on these considerations, we propose this term as a regularization to constrain the search space of parameters instead of the stand-alone loss function.
4 EXPERIMENTAL STUDIES
4.1 EXPERIMENTAL SETUPS
4.1.1 DATASET DESCRIPTION
The dataset used in this study contained almost ten million query and response pairs collected from a popular Chinese social media site: Douban Group Chat1. All case studies used in this paper were extracted from this dataset and translated into English.
For easier training and better efficiency, the maximal lengths of queries and replies were set to 30 and 50 respectively. In all of our experiments, our dataset was split into the training, validation and test sets, with detailed statistical characterization given in Table 2. Thirty percent of queries had more than one responses, and each answer appeared about 1.33 times in the training dataset, which is consistent with our hypothesis in the analysis section.
4.1.2 BASELINE MODELS
To validate the performance of the proposed model, the following baselines were considered:
· S2SA: The basic seq2seq model with attention mechanism (Bahdanau et al., 2015) at the target output side.
· S2SA + MMI: The best performing model in Li et al. (2016b) with the length norm based on the same S2SA.
· Ranking-Reg: The seq2seq model with proposed ranking regularization and attention. In this model, negative samples were uniformly sampled from the corpus, and the process was repeated 4 times for every positive case. The averaged negative loss was calculated as the probability of universal replies.
· Ranking-Reg + MMI: Ranking-Reg with MMI during inference procedure.
4.1.3 EVALUATION METRICS
The quality of response was measured using both numeric metrics and human annotators. Firstly, Word Perplexity (PPL) was used to determine the semantic context of phrase-level utterance (Serban et al., 2016). Secondly, ROGUE score Lin (2004), which evaluates the extent of overlapping words between the ground-truth results and the predicted answers, was also adopted in our research. Thirdly, we employed the widely used diversity measurements Distinct-1 and Distinct-2 to evaluate the number of distinct Unigrams and Bigrams of the generated responses (Li et al., 2016b).
Furthermore, we recruited three highly educated human annotators to cross verify the quality of generated responses. We randomly sampled 100 queries and generated 10 replies for each query
1https://www.douban.com/group/explore
7

Under review as a conference paper at ICLR 2019

Table 3: Summarized results of testing set with metrics: Human Label, ROGUE-1, ROGUE-L, Distinct-1, Distinct-2 and PPL.

Methods

Human Label 01 2

ROUGE

Distinct

ROUGE-1 ROUGE-L 1 2

PPL

S2SA S2SA + MMI Rank-Reg Rank-Reg + MMI

52.46% 51.88% 48.20% 47.40%

20.52% 19.92% 15.38% 18.75%

27.02% 28.20% 36.42% 33.85%

4.97% 3.96% 3.45% 3.43%

3.13% 2.77% 2.55% 2.63%

.129 .285 110.0 .140 .312 110.0 .163 .358 85.6 .167 .345 85.6

using different models, with beam size set to 10. The labeled results were categorized into three degree (Xing et al., 2017; Mou et al., 2016):
0: The response cannot be used as a reply to the message. It is either semantically irrelevant or not fluent (e.g., with grammatical errors or UNK).
1: The response can be used as a reply to the message, which includes the universal replies such as "Yes, I see" , "Me too" and "I dont know".
2: The response is not only relevant and natural, but also informative and interesting.

4.1.4 TRAINING PROCEDURES

For all of the models, LSTM was chosen as the recurrent cell, and there were 512 hidden units for both the encoder and decoder (Greff et al., 2017). Embedding size and batch size were set to 200 and 20 respectively. The Adam algorithm was employed for gradient optimization (Kingma & Ba, 2015), and the initial learning rate was 1e-4. All of the models were implemented in Theano (Theano Development Team, 2016), and each ran on a standalone K40m GPU device for 7 epochs, which took 7 days; twice longer time was required for training models with rank regularization.

Models' Cross-Entropy Losses

s2s+ Attention ranking loss
Epoches
Figure 2: Learning curve for the two models.

The last two models with the rank regularization share the related hyper-parameters. We set  to 0.1 and  to 0.18, according to the model's performance on the validation set.

Fig. 2 shows cross-entropy loss flows vs. training epoch numbers. The model with max-marginal ranking regularization converges faster than S2SA throughout the training. This shows that the additional regularization term helps to speed up the fitting by removing these sub-optimal paths.

4.2 RESULTS AND ANALYSIS
4.2.1 EXPERIMENTAL RESULTS.
The performance of four models on existing metrics is summarized in Table 3. The model with the max-marginal ranking regularization outperforms the model with primary loss function on the target loss PPL. As the MMI method is performing during inference, losses of models with MMI are identical to those without revision.
However, the results are opposite regarding the ROGUE scores. The generated responses by the S2SA model contain more words appearing in the ground truth answers. These experimental results can be attributed to mainly two factors. a) The very low ROUGE scores reflect few words shared by any predictions and the ground truth. Most n-gram overlaps belonging to the common words, such as "I", "are", "that". b) A certain proportion of replies in the test set are universal themselves. Therefore, S2SA has achieved higher ROUGE score as its' results are more consistent with those common ground truth responses.

8

Under review as a conference paper at ICLR 2019

Query: University are far away, and the city's most famous commercial street are near to me.

Replies from S2S+Attention:
1) Where is your home? 2) Where is your city? 3) Where is your location? 4) Where is your hometown? 5) Where is your city, hn? 6) Where is your location? 7) Where is your home, mine is near?

Replies from Ranking Loss :
1) Joy City Shopping mall? 2) Is shopping mall? 3) Joy City Shopping mall! 4) Where is your location? 5) Where? 6) Near that <unk> road. 7) That Joy City shopping mall is great.

Query: Most Banks are not reliable.
Replies from S2S+Attention:
1) Bank 2) I thought banks are ok. 3) We go to bank, well 4) We go to bank bank 5) We go to bank, have a look. 6) We go to bank, have a look? 7) I thought bank are more than bank 8) I thought bank are more reliable than bank.

Replies from Ranking Loss :
1) That's it. 2) I thought bank are more reliable. 3) Why bank are not reliable? 4) We go to bank. 5) What kind of unreliable? 6) What kind of unreliable? 7) We go to bank! 8) I thought bank are well.

Figure 3: Response re-rank capability. Responses generated by the basic model and model with rank loss are linked by arrows, and same topics are typeset using the same color.

The human evaluation is the most important metric, and it is clear from Table 3 that the models with rank regularization beat S2SA with a large margin. It increases the number of meaningful responses by around 10% and reduces the number of irrelevant cases by around 4%. Meanwhile, most the acceptable replies (labeled as "1" or "2") of S2SA is labeled as "1", which indicates the model prefer the safe responses. We attribute the gaps to the promotion of highly related words and reducing of the universal replies. Same trend can be also spotted on Distinct-1 and Distinct-2, it reveals the model's ability to generate diverse responses (Li et al., 2016b; Serban et al., 2015). The seq2seq model yields lower levels of unigram and bigram diversity than the rank loss model.
As another comparison, we note that the improvement introduced by MMI is much smaller than that introduced by the ranking regularization, whereas MMI is a widely used mechanism for promoting diverse responses during inference. Besides, performing it upon the regularization reduces the rate of informative and interesting responses. This observation indicates that the fundamental reason behind generating tasteless or inappropriate replies is that Seq2Seq model learned from conversational corpora prefers universal replies. Moreover, the revision during the greedy search is less effective on solving the underlying problems than the proposed ranking regularization.
4.2.2 RANKING LOSS FOR GENERIC RESPONSES.
From the generated results, it is found that the seq2seq model with the ranking regularization term prefers meaningful content when the query contains sufficient amount of information. We present top responses for two queries generated by different models in Fig. 3. As shown in the first case, user posts a query which initiates a complicated discussion about locations. It is observed that S2SA converges to a typical "where is your" pattern of replies when discussing locations, which is an example of universal replies. As the greedy beam search strategy is utilized during inference, many location-related constraints further promote these relevant universal replies instead of more varied results from different beams. In contrast, some of the responses in the right column captured the "commercial street" clues and inferred a possible location "Joy City shopping mall" demoting the generic beams results. We attributed this to the boosting ability associated with semantically relevant words, as mentioned in Section 3.
The second case is quite different. In this case, the seq2seq model did not perform satisfactorily. Even though the subject "bank" was extracted into the generated candidates, we cannot perceive the results aligned with the same "not reliable" topic, and most of them were just chosen from two beams. Inspecting the replies generated by the rank loss model, we found that more complicated and diverse sentences that discuss "unreliable" can be generated, and irrelevant answers about "bank" are
9

Under review as a conference paper at ICLR 2019
lower-ranked. To further investigate the difference brought by the max-marginal ranking regularization, we randomly sampled more cases shown in the Fig. 4 as appendix. Even though some of them were bad cases and contained some grammatical errors, overall the model with rank regularization tends to generate more informative and interesting sentences compared with baselines.
In conclusion, the seq2seq model with rank regularization can not only formulate the conditional language model but also boost related answers to higher ranks than the rest of universal or inappropriate replies.
5 RELATED WORK
Recent years have witnessed the rapid development of data-driven dialog models with the help of accumulated conversational data from online communities. Query-response pairs are modeled by Seq2Seq models with attention mechanism (Sutskever et al., 2014; Serban et al., 2016; Bahdanau et al., 2015), and NRG model are designed to maximize the likelihood of target response given the source query. As there exist various reasonable responses given a query, some researches conclude that the limited information in many queries constrains the model inference, which makes the NRG models prefer universal replies (Shao et al., 2017; Mou et al., 2016).
To address this issue, various works are conducted on bringing more information to Seq2Seq models. Some works focus on constraining the replies with topic information or keywords (Mou et al., 2016; Xing et al., 2017; Wang et al., 2017; Wu et al., 2018). Other researchers argue that diverse responses are buried by the greedy beam-search rules (Li & Jurafsky, 2016), so their works mainly focus on involving more punishment or randomness in the inference stages. For example, Li et al. (2016b) constrain the search space using mutual information with the query, while Shao et al. (2017) randomly chose candidate words from top beams to constrain short phrases. These existing works mainly focus on the generation strategies during inference, in contrast, the model's architecture and loss function have rarely been explored.
Serban et al. (2017b) introduce to model the underlying distribution over possible replies directly with supposing various latent variables to affect the response generation. Shen et al. (2017) further constructs a variational lower bound for response constraint. During inference, these models generate responses by first sampling an assignment of latent variables, so that models can generate more diverse responses. Such methods attempt to improve the diversity of responses by modifying the Seq2Seq architecture, and our analysis may be also helpful to design more effective latent variable based models to restrain current problems. Besides, the ranking penalty has also been used by Wiseman & Rush (2016), they employ a word-level margin to promote ground-truth sequences appearing in the beam search results. Different from our method, they directly optimize the beam search procedure to fine-tune the trained model.
6 CONCLUSION
Eliminating generic responses is the essence for the widely practical utilization of the Seq2Seq based neural response generation architectures, and thus, this paper has conducted a thorough investigation on the cause of such uninformative responses and proposed the solution from the statistical perspective. The main contributions of this work can be summarized as follows: a) The theoretical analysis is performed to capture the root reason of NRG models producing generic responses through the optimization goal of models and the statistical characteristics of human-to-human conversational corpora, which has been little studied currently. In detail, we have decomposed the goal of NRG into the optimizations of word selection and word ordering, and finally derived that NRG models tend to select common words as responses and order words from the language model perspective which ignores queries. b) According to the analysis, a max-marginal ranking regularization term is proposed to cooperate with the learning target of Seq2Seq, so as to help NRG models converge to the status of producing informative responses, rather than merely manipulating the decoding procedure to constrain the generation of universal replies. Furthermore, the empirical experiments on the conversation dataset indicate that the models utilizing this strategy notably outperform the current baseline models.
10

Under review as a conference paper at ICLR 2019
REFERENCES
Shivani Agarwal and Michael Collins. Maximum margin ranking algorithms for information retrieval. In Proc. of ECIR, pp. 332­343, 2010.
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. In Proc. of ICLR, 2015.
Stephen Boyd and Lieven Vandenberghe. Convex Optimization. Cambridge University Press, New York, NY, USA, 2004. ISBN 0521833787.
Klaus Greff, Rupesh K Srivastava, Jan Koutn´ik, Bas R Steunebrink, and Ju¨rgen Schmidhuber. Lstm: A search space odyssey. IEEE transactions on neural networks and learning systems, 28(10): 2222­2232, 2017.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. international conference on learning representations, 2015.
Chaozhuo Li, Yu Wu, Wei Wu, Chen Xing, Zhoujun Li, and Ming Zhou. Detecting context dependent messages in a conversational environment. In Proc. of COLING, pp. 1990­1999, 2016a.
Jiwei Li and Dan Jurafsky. Mutual information and diverse decoding improve neural machine translation. CoRR, abs/1601.00372, 2016.
Jiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao, and Bill Dolan. A diversity-promoting objective function for neural conversation models. In Proc. of NAACL-HLT, pp. 110­119, 2016b.
Jiwei Li, Michel Galley, Chris Brockett, Georgios P. Spithourakis, Jianfeng Gao, and William B. Dolan. A persona-based neural conversation model. In Proc. of ACL, pp. 994­1003, 2016c.
Jiwei Li, Will Monroe, and Dan Jurafsky. A simple, fast diverse decoding algorithm for neural generation. CoRR, abs/1611.08562, 2016d.
Chin-Yew Lin. Rouge: A package for automatic evaluation of summaries. In Proc. of ACL workshop, volume 8, 2004.
Christopher Maning, Prabhaker Raghavan, and Hinrich Schtze. An introduction to information retrieval. 2009.
Lili Mou, Yiping Song, Rui Yan, Ge Li, Lu Zhang, and Zhi Jin. Sequence to backward and forward sequences: A content-introducing approach to generative short-text conversation. In Proc. of COLING, pp. 3349­3358, 2016.
Ramesh Nallapati, Bowen Zhou, C´icero Nogueira dos Santos, C¸ aglar Gu¨lc¸ehre, and Bing Xiang. Abstractive text summarization using sequence-to-sequence rnns and beyond. In Proc. of CoNLL, pp. 280­290, 2016.
Alan Ritter, Colin Cherry, and William B. Dolan. Data-driven response generation in social media. In Proc. of EMNLP, pp. 583­593, 2011.
Alexander M Rush, Sumit Chopra, and Jason Weston. A neural attention model for abstractive sentence summarization. empirical methods in natural language processing, pp. 379­389, 2015.
Iulian Vlad Serban, Ryan Lowe, Peter Henderson, Laurent Charlin, and Joelle Pineau. A survey of available corpora for building data-driven dialogue systems. CoRR, abs/1512.05742, 2015.
Iulian Vlad Serban, Alessandro Sordoni, Yoshua Bengio, Aaron C. Courville, and Joelle Pineau. Building end-to-end dialogue systems using generative hierarchical neural network models. In Proc. of AAAI, pp. 3776­3784, 2016.
Iulian Vlad Serban, Alessandro Sordoni, Ryan Lowe, Laurent Charlin, Joelle Pineau, Aaron C. Courville, and Yoshua Bengio. A hierarchical latent variable encoder-decoder model for generating dialogues. In Proc. of AAAI, pp. 3295­3301, 2017a.
11

Under review as a conference paper at ICLR 2019
Iulian Vlad Serban, Alessandro Sordoni, Ryan Lowe, Laurent Charlin, Joelle Pineau, Aaron C Courville, and Yoshua Bengio. A hierarchical latent variable encoder-decoder model for generating dialogues. In AAAI, pp. 3295­3301, 2017b.
Lifeng Shang, Zhengdong Lu, and Hang Li. Neural responding machine for short-text conversation. In Proc. of ACL, pp. 1577­1586, 2015.
Yuanlong Shao, Stephan Gouws, Denny Britz, Anna Goldie, Brian Strope, and Ray Kurzweil. Generating high-quality and informative conversation responses with sequence-to-sequence models. In Proc. of EMNLP, pp. 2210­2219, 2017.
Xiaoyu Shen, Hui Su, Yanran Li, Wenjie Li, Shuzi Niu, Yang Zhao, Akiko Aizawa, and Guoping Long. A conditional variational framework for dialog generation. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), volume 2, pp. 504­509, 2017.
Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. Sequence to sequence learning with neural networks. In Proc. of NIPS, pp. 3104­3112, 2014.
Theano Development Team. Theano: A Python framework for fast computation of mathematical expressions. arXiv e-prints, abs/1605.02688, May 2016.
Oriol Vinyals and Quoc V. Le. A neural conversational model. arXiv preprint arXiv:1506.05869, 2015.
Oriol Vinyals, Samy Bengio, and Manjunath Kudlur. Order matters: Sequence to sequence for sets. In Proc. of ICLR, 2016.
Di Wang, Nebojsa Jojic, Chris Brockett, and Eric Nyberg. Steering output style and topic in neural response generation. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pp. 2140­2150, 2017.
Sam Wiseman and Alexander M. Rush. Sequence-to-sequence learning as beam-search optimization. In Proc. of EMNLP, pp. 1296­1306, 2016.
Yu Wu, Wei Wu, Zhoujun Li, Can Xu, and Dejian Yang. Neural response generation with dynamic vocabularies. national conference on artificial intelligence, 2018.
Chen Xing, Wei Wu, Yu Wu, Jie Liu, Yalou Huang, Ming Zhou, and Wei-Ying Ma. Topic aware neural response generation. In Proc. of AAAI, pp. 3351­3357, 2017.
George Kingsley Zipf. The psychobiology of language. 1935.
12

Under review as a conference paper at ICLR 2019

A CASES

Query: What should I do? Replies form S2S+Attention: 1) Do nothing. 2) Go on. 3) Do nothing, hm. 4) How? 5) Do nothing do nothing. 6) Then go ahead. 7) So how do you do?
Query: How about the salary? Replies from S2S+Attention: 1) negotiable 2) Salary negotiable. 3) Salary negotiable. 4) Salary negotiable 5) Salary and payment negotiable. 6) Payment is nice. 7) Payment is nice, and salary negotiable.
Query: I'm 23, why I'm still in growth? Replies from S2S+Attention: 1) 23 2) 22. 3) 21. 4) 20. 5) 25. 6) 23 years old. 7) 22 years old.
Query: Where are you graduate? Replies from S2S+Attention: 1) Xi'an. 2) Wuhan. 3) <unk>. 4) Nanjing. 5) Junior. 6) In Junior. 7) In junior junior Shanghai.
Query: My child is born. Replies from S2S+Attention: 1) <unk>. 2) born. 3) born baby. 4) children born. 5) born born children. 6) born born born children. 7) born children born children.

Replies from Ranking Loss : 1) Do nothing. 2) Do nothing. 3) Go to sleep. 4) Don't worry. 5) You should keep on. 6) Then go ahead. 7) Keep finding.
Replies from Ranking Loss : 1) Salary negotiable. 2) Salary negotiable. 3) High salary. 4) Basic salary and plus 5) Salary negotiable! 6) Salary and payment negotiable. 7) Payment is nice, send your CV.
Replies from Ranking Loss : 1) 23. 2) 23. 3) just 23. 4) just 22. 5) 23 years old are not 6) 23 years old are not old 7) 23 years old are not old!
Replies from Ranking Loss : 1) Peking. 2) Chengdu. 3) Xi'an. 4) In Chengdu. 5) I study in Chengdu. 6) I study in Shanghai. 7) I study in Beijing.
Replies from Ranking Loss : 1) ok 2) Cheers! 3) Em. 4) ok, born child. 5) cheers, congulations! 6) born born born children. 7) born children born children.

Figure 4: Cases for comparing the S2SA and the model with ranking regularization, and the topics or expressions of the generated replies marked with blue are excluded in the responses generated by SASA.

13


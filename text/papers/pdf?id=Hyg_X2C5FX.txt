Under review as a conference paper at ICLR 2019
VISUALIZING AND UNDERSTANDING GENERATIVE ADVERSARIAL NETWORKS
Anonymous authors Paper under double-blind review
ABSTRACT
Generative Adversarial Networks (GANs) have recently achieved impressive results for many real-world applications. As an active research topic, many GAN variants have emerged with immprovements in sample quality and training stability. However, visualization and understanding of GANs is largely missing. How does a GAN represent our visual world internally? What causes the artifacts in GAN results? How do architectural choices affect GAN learning? Answering such questions could enable us to develop new insights and better models. In this work, we present an analytic framework to visualize and understand GANs at the unit-, object-, and scene-level. We first identify a group of interpretable units that are closely related to object concepts with a segmentation-based network dissection method. Then, we quantify the causal effect of interpretable units by measuring the ability of interventions to control objects in the output. Finally, we examine the contextual relationship between these units and their surrounding by inserting the discovered object concepts into new images. We show several practical applications enabled by our framework, from comparing internal representations across different layers, models, and datasets, to improving GANs by locating and removing "artifacts" units, to interactively manipulating objects in the scene. We will open source our interactive online tools to help researchers and practitioners better understand their models.
1 INTRODUCTION
Generative Adversarial Networks (GANs) (Goodfellow et al., 2014) have been able to produce photorealistic images, often indistinguishable from real images (Figure 1a). This remarkable ability has powered many real-world applications ranging from visual recognition (Wang et al., 2017), to image manipulation (Isola et al., 2017; Zhu et al., 2017), to video prediction (Mathieu et al., 2016). Since its invention in 2014, researchers have proposed many GAN variants (Radford et al., 2016; Zhang et al., 2018), often producing more realistic and diverse samples with better training stability.
Despite this tremendous success, many questions remain to be answered. For example, to produce a church image (Figure 1a), what knowledge does a GAN need to learn? Alternatively, why does a GAN sometimes produce terribly unrealistic images (Figure 1f)? What causes the mistakes? Why does one GAN variant work better than another? What differences are encoded in their weights?
In this work, we investigate the internal representations of GANs. To a human observer, a well-trained GAN appears to have learned facts about the objects in the image: for example a door can appear on a building but not on a tree. We wish to understand how a GAN represents such structure. Do the objects emerge as pure pixel patterns without any explicit representation of objects such as doors and trees, or does the GAN contain internal variables that correspond to the objects that humans perceive? If the GAN does contain variables for doors and trees, do those variables cause the generation of those objects, or do they merely correlate? How are relationships between objects represented?
We present a general method for visualizing and understanding GANs at different levels of abstraction, from each neuron, to each object, to the contextual relationship between different objects. We first identify a group of interpretable units that are related to object concepts (Figure 1b). These units' featuremap closely matches the semantic segmentation of a particular object class (e.g., trees). Second, we directly intervene within the network to identify sets of units that cause a type of object
1

Under review as a conference paper at ICLR 2019

(a) Generate images of churches

(b) Identify GAN units that match trees

(e) Identify GAN units that cause artifacts

(c) Ablating units removes trees

(f) Bedroom images with artifacts

(d) Activating units adds trees

(g) Ablating "artifacts" units improves results

Figure 1: Overview: (a) A number of realistic outdoor church images generated by Progressive GANs (Karras et al., 2018). (b) Given a pre-trained GAN model (e.g., Progressive GANs), we first identify a set of interpretable units, whose featuremap is highly correlated to the region of an object class across different images. For example, unit #157 can localize tree regions with diverse visual appearance. (c) We ablate the units by forcing the activation to be zero and quantify the average casual effect of the ablation. Here we successfully remove these trees from churches. (d) We can insert these tree causal units to other locations in the generated images. The same set of units can synthesize different trees visually compatible with their surrounding context. In addition, our method can diagnose and improve GANs by identifying artifact-causing units (e). We can remove the artifacts that appear in (f) and significantly improve the results by ablating the "artifacts" units (g). Please see our video for more results.

to disappear (Figure 1c) or appear (Figure 1d). We quantify the causal effect of these units using a standard causality metric. Finally, we examine the contextual relationship between these causal object units and the background. We study where we can insert the object concepts in new images and how this intervention interacts with other objects in the image (Figure 1d). To our knowledge, our work provides the first systematic analysis for understanding the internal representations of GANs.
Finally, we show several practical applications enabled by this new analytic framework, from comparing internal representations across different layers, GAN variants, and datasets; to debugging and improving GANs by locating and ablating "artifact" units (Figure 1e); to understanding contextual relationships between objects in natural scenes; to manipulating images with interactive object-level control. Our interactive tools are demonstrated in a video. We will release our code and data as well as our web-based interactive interface upon publication to help researchers and practitioners better understand and develop their own models.

2 RELATED WORK
Generative Adversarial Networks. GANs (Goodfellow et al., 2014) have kept improving the quality and diversity of results, from generating simple digits and faces (Goodfellow et al., 2014), to synthesizing natural scene images (Radford et al., 2016; Denton et al., 2015), to generating 1k photorealistic portraits (Karras et al., 2018), to producing one thousand object classes (Miyato et al., 2018; Zhang et al., 2018). In addition to image generation, GANs have also enabled many applications such as visual recognition (Wang et al., 2017; Hoffman et al., 2018), image manipulation (Isola et al., 2017; Zhu et al., 2017), and video generation (Mathieu et al., 2016; Wang et al., 2018). Despite the huge success, little work has been done to visualize what GANs have learned. Prior

2

Under review as a conference paper at ICLR 2019

single unit u

featuremap

thresholded

upsample

ru,P

ru > t

agreement

(a) generated image

segmentation

IoUu,c

h r f generate

z
generator
G

segment
x sc(x)

force rU,P on

inserted image

segmentation

(b)

unforced units causal units U

h z

rU,P

rU,P

f force U off

segment
xi sc(xi)

ablated image

segmentation

f

causal effect
Uc

force rU,P off

segment
xa sc(xa)

Figure 2: Measuring the relationship between representation units and trees in the output using (a)

dissection and (b) intervention. Dissection measures agreement between a unit u and a concept c by

comparing a semantic segmentation of the generated image sc(x) with the thresholded upsampled

unit. Intervention measures the causal effect of a set of units U on a concept c by comparing the

effect of forcing the unit on (unit insertion) and off (unit ablation). In this case, the segmentation

sc reveals that trees in the generated image increase after insertion and decrease after ablation. The

average difference in the tree pixels measures the average causal effect.

work (Radford et al., 2016; Zhu et al., 2016) manipulates latent vectors and analyzes how the results change accordingly. However, none of the methods investigate the internal representations.
Visualizing Deep Neural Networks. To understand the the representation learned by the networks, various methods have been developed to visualize their internal weights, such as the visualization for CNNs (Zeiler & Fergus, 2014) and RNNs (Karpathy et al., 2016; Strobelt et al., 2018). We can visualize a CNN by locating and reconstructing salient image features (Simonyan et al., 2014; Mahendran & Vedaldi, 2015) or by mining patches that maximize hidden layers' activations (Zeiler & Fergus, 2014), or we can synthsize the input images to invert a feature layer (Dosovitskiy & Brox, 2016). Alternately, we can identify the semantics of each unit (Zhou et al., 2015; Bau et al., 2017) by measuring agreement between unit activation and object segmentation masks. Visualization of an RNN has also revealed interpretable units, which keep track of long-range dependencies (Karpathy et al., 2016). Most previous work on network visualization has focused on networks trained for classification; our work explores deep generative models trained for image generation.
Explaining the Decisions of Deep Neural Networks. We can explain individual network decisions using informative heatmaps (Zhou et al., 2016; Selvaraju et al., 2017), or through modified backpropagation (Simonyan et al., 2014; Bach et al., 2015; Sundararajan et al., 2017). The heatmaps highlight which regions contribute most to the categorical prediction given by the networks. Morcos et al. (2018) examined the effect of individual units by ablating them. Recent work has also studied the contribution of feature vectors (Kim et al., 2017; Zhou et al., 2018) or individual channels (Olah et al., 2018) to the final prediction. Those methods only work for explaining a classification network. Our method aims at explaining how a photo-realistic image can be generated by the network, which is much less explored.

3 METHOD
Our goal is to analyze how objects such as trees are encoded by the internal representations of a GAN generator G : z  x. Here z  R|z| denotes a latent vector sampled from a low-dimensional distribution and x  RH×W ×3 denotes an H ×W generated image. We use representation to describe the tensor r output from a particular layer of the generator G, where the generator creates an image x from random z through a composition of layers: r = h(z) and x = f (r) = f (h(z)) = G(z).

3

Under review as a conference paper at ICLR 2019

Thresholding unit #65 layer 3 of a dining room generator matches `table' segmentations with IoU=0.34.

Thresholding unit #37 layer 4 of a living room generator matches `sofa' segmentations with IoU=0.29.
Figure 3: Visualizing the activations of individual units in two GANs. 10 top activating images are shown, and IoU is measured over a sample of 1000 images. In each image, the unit feature is upsampled and thresholded as described in Eqn. 2.

Since r has all the data necessary to produce the image x = f (r), r certainly contains the information

to deduce the presence of any visible class c in the image. Therefore the question we ask is not

whether information about c is present in r -- it is -- but how such information is encoded in r. In

particular, we seek to understand whether r explicitly represents the concept c in some way where it

is possible to factor r at locations P into components

rU,P = (rU,P, rU,P),

(1)

where the generation of the object c at locations P depends mainly on the units rU,P, and is insensitive to the other units rU,P. Here we refer to each channel of the featuremap as a unit; U denotes the set of
unit indices of interest and U denotes its complement; we will write U and P to refer to the entire set of units and featuremap pixels in r. Our method for studying the structure of r consists of two phases.

· Dissection: starting with a large dictionary of object classes, we identify the classes that have an explicit representation in r by measuring the agreement between individual units of r and every class c (Figure 1b).
· Intervention: for the represented classes identified through dissection, we measure causal effects between units and classes by forcing sets of units on and off r (Figure 1c and Figure 1d).

3.1 CHARACTERIZING UNITS BY DISSECTION

We first focus on individual units of the representation. Recall that ru,P is the one-channel h × w featuremap of unit u in a convolutional generator, where h × w is typically smaller than the image
size. We want to know if a specific unit ru,P encodes a semantic class such as a "tree". For image classification networks, Bau et al. (2017) has observed that many units can approximately locate
emergent object classes when the units are upsampled and thresholded. In that spirit, we quantify the
spatial agreement between the unit U's thresholded featuremap and a concept c' segmentation with
the following intersection-over-union (IoU) measure:

IoUu,c 

Ez Ez

(ru,P > tu,c)  sc(x) (ru,P > tu,c)  sc(x)

,

where

tu,c

=

arg

max
t

I(ru ,P H(ru,P

> >

t; sc(x)) t, sc(x))

,

(2)

where  and  denotes the intersection and union operations, x = G(z) denotes the image generated
from z. The one-channel feature map ru,P slices the entire featuremap r = h(z) at unit u. As shown in Figure 2a, we upsample ru,P to the output image resolution as ru,P. (ru,P > tu,c) produces a binary mask by thresholding the ru,P at a fixed level tu,c. The binary mask sc(x) denotes a semantic segmentation, whose pixel indicates the class c's presence in the generated image x. We automatically
choose a threshold tu,c (using a separate validation set) that maximizes the information quality ratio,
that is, the portion of the joint entropy H which is mutual information I (Wijaya et al., 2017).

Given a dictionary of concepts c  C for which we have semantic segmentations sc, we can use IoUu,c to rank the concepts related to each unit and label each unit with a candidate concept that matches it best. Figure 3 shows examples of interpretable units with high IoUu,c. They are not the

4

Under review as a conference paper at ICLR 2019

0 5 10 20 Number of tree units ablated
Figure 4: Ablating successively larger sets of tree-causal units from a GAN trained on LSUN outdoor church images, showing that the more units are removed, the more trees are reduced, while buildings remain. The choice of units to ablate is specific to the tree class and does not depend on the image. At right, the causal effect of removing successively more tree units is plotted, comparing units chosen to optimize ACE and units chosen with highest IoU for tree.

only units in their layers to match tables and trees: layer3 of the dining room generator has 30 other units (of 512) that match tables and table parts; and layer4 of the church generator has 24 (of 512) tree units.
Once we have identified an object class that a set of units match closely, we next ask: which of those units are responsible for triggering the rendering of that object? A unit that correlates highly with an output object might not actually cause that output. Furthermore, any output will jointly depend on several parts of the representation. Therefore, we need way to identify combinations of units that cause an object.

3.2 MEASURING CAUSAL RELATIONSHIPS USING INTERVENTION

To answer the above question about causality, we probe the network using interventions: we test whether a set of units U in r cause the generation of c by forcing the units of U on and off.

Recall that rU,P denotes the featuremap r at units U and locations P. We ablate those units by forcing

rU,P = 0. Similarly, we insert those units by forcing rU,P = c, where c is a big constant. We

decompose the featuremap r into two parts (rU,P, rU,P), where rU,P are unforced components of r:

Original image :

x = G(z)  f (r)  f (rU,P, rU,P)

(3)

Image with U ablated at pixels P :

xa = f (0, rU,P)

Image with U inserted at pixels P :

xi = f (c, rU,P)

An object is caused by U if the object appears in xi and disappears from xa. Figure 1c demonstrates the ablation of units that remove trees, and Figure 1d demonstrates insertion of units at specific
locations to make trees appear. This causality can be quantified by comparing the presence of trees in xi and xa and averaging effects over all locations and images. Following (Holland, 1988; Pearl, 2009), we define the average causal effect of units U on the generation of on class c as:

Uc  Ez,P[sc(xi)] - Ez,P[sc(xa)],

(4)

where sc(x) denotes a segmentation indicating the presence of class c in the image x at P. To permit comparisons of Uc between classes c which are rare, we normalize our segmentation sc by Ez,P[sc(x)]. While these measures can be applied to a single unit, we have found that objects tend to depend on more than one unit. Thus we need to identify a set of units U that maximize the average
causal effect Uc for a class c.

Finding sets of units with high ACE. Given a representation r with d units, exhaustively searching

for a fixed-size set U with high Uc is prohibitive as it has

d |U|

subsets. Instead, we optimize a

continuous intervention   [0, 1]d, where each dimension u indicates the degree of intervention

for a unit u. We maximize the following average causal effect formulation c:

Image with partial ablation at pixels P : Image with partial insertion at pixels P : Objective :

xa = f ((1 - ) rU,P, rU,P) xi = f ( c + (1 - ) rU,P, rU,P) c = Ez,P [sc(xi)] - Ez,P [sc(xa)] ,

(5)

5

Under review as a conference paper at ICLR 2019

where rU,P denotes the all-channel featuremap at locations P, rU,P denotes the all-channel featuremap
at other locations P, and applies a per-channel scaling vector  to the featuremap rU,P. We optimize  over the following loss with an L2 regularization:

 = arg min(-c + ||||2),

(6)

where  = 100 controls the relative importance of each term. We add the L2 loss as we seek for a
minimal set of casual units. We optimize using stochastic gradient descent, sampling over both z and featuremap locations P and clamping  within the range [0, 1]d at each step. Finally, we can rank units by u and achieve a larger causal effect (i.e., removing trees) when ablating successively larger sets of tree-causing units as shown in Figure 4.

4 RESULTS
We study three variants of Progressive GANs (Karras et al., 2018) trained on LSUN scene datasets (Yu et al., 2015). To segment the generated images, we use a recent model (Xiao et al., 2018) trained on ADE20K scene dataset (Zhou et al., 2017). The model can segment the input image into 366 object classes, 29 parts of large objects, and 25 materials. To further identify units that specialize in object parts, we expand each object class c into additional object part classes c-t, c-b, c-l, and c-r, which denotes the top, bottom, left, or right half of the bounding box of a connected component.
Below, we use dissection for analyzing units ( Section 4.1), comparing units across datasets, layers, and models (Section 4.2), and locating artifact units (Section 4.3). Then, we start with a set of dominant object classes and use intervention to locate causal units that can remove and insert objects in different images (Section 4.4 and 4.5). In addition, our video demonstrates our interactive tool.
4.1 EMERGENCE OF INDIVIDUAL UNIT OBJECT DETECTORS
We are particularly interested in any units that are correlated to instances of an object class with diverse visual appearances; these would suggest that GANs generate those objects using similar abstractions as humans. Figure 3 illustrates two such units. For example in the dining room dataset, a unit emerges to match the dining table regions. Note that the matched tables have different colors, materials, geometry, viewpoints, and level of clutter: the only obvious commonality among these regions is the concept of table. Evaluating this unit against a recent segmentation network (Xiao et al., 2018) shows that the unit's featuremap correlates to the fully supervised segmentation model with a high IoU of 0.34.
4.2 COMPARING UNITS ACROSS DATASETS, LAYERS, AND MODELS
Interpretable units for different scene categories The set of all object classes matched by the units of a GAN provides a map of what a GAN has learned about the data. Figure 5 examines units from generators train on four LSUN (Yu et al., 2015) scene categories. The units that emerge are object classes appropriate to the scene type: for example, when we examine a GAN trained on kitchen scenes, we find units that match stoves, cabinets, and the legs of tall kitchen stools. Another striking phenomenon is that many units represent parts of objects: for example, the conference room GAN contains separate units for the body and head of a person.
Interpretable units for different network layers. In classifier networks, the type of information explicitly represented changes from layer to layer (Zeiler & Fergus, 2014). We find a similar phenomenon in a GAN. Figure 6 compares early, middle, and late layers of a progressive GAN with 14 internal convolutional layers. The output of the first convolutional layer, one step away from the input z, remains entangled: individual units do not correlate well with any semantic object classes except for two units that are biased towards the ceiling of the room. Mid-level layers 4 to 7 have a large number of units that match semantic objects and object parts. Units in layers 10 and beyond match local pixel patterns such as materials and shapes.
Interpretable units for different GAN models. Interpretable units can provide insight about how GAN architecture choices affect the structures learned inside a GAN. Figure 7 compares three models (Karras et al., 2018) that introduce two innovations on baseline Progressive GANs. By examining unit semantics, we confirm that providing minibatch stddev statistics to the discriminator

6

Under review as a conference paper at ICLR 2019

table #96

Units in scene generator

iou=0.30 person-b #91

iou=0.21 seat #83

iou=0.13 32

Unit class distribution

units 12 objects
27 parts 4 materials

conference rm

sislislsviwvlsieervvrwpieesplarcvspmicpcccetpecewrtfccoetrspnmitleiaelcrfeccsltarifciaifiliiscellshhanilteaotlawnieioehisearifccbhrnawiwotllennoeaaborsnbnoltabasdiiloenaliaoniitsellhhnarbawiaoolttrrrengogniaopelkblrorrengltrengil----------snoeenlrraeagiaii-----oo------eal-----llllllttttttttrrrrrrrrrcgnbbbbdnbebbbnbobgw

chandelier-l #184 iou=0.21 chair-l #456

iou=0.19 table #89

16
1 iou=0.31
20

units 10 objects
29 parts 2 materials

dining room

ccchhchapahapwniawcwwniiiacnpicactdinewtfcdtcnnitnnediaenlcfeccfcltaifceiindilliilllhaeluadbtltadwdendiieoglhiahiicblhwrpnowiliwoolnniabolrboulttabaeodoilneoalnaiilllhnnaafasaiaeoiolirrrggewileofpalablolrrrgewlrregwilt--------nnlrrregagwksi-------e------ee--------llllllllttttttttrrrrrrrrrysngbbbeebbbbgbw

units 6 objects 34 parts 2 materials

kitchen

stove-t #312 tree #157

iou=0.12 chair-b #166 iou=0.32 grass #14

iou=0.11 cabinet-b #70 iou=0.25 dome #43

10
1 iou=0.15
27
14
1 iou=0.07
24

kkiiwttwwwocrcoorworrhrehmkofikkkerericsckwnrnscscsicwcwgwuaiiiciiccouaurssaunaeewfcrsfttspbrsnisrneienlcrfeilftfcbllftifubifdbwilliiillhileoaadtrlwdndiegoainaaohifaaowfraaowilwotonlvcnnaobnolaaodoicvnolcvennaislcvnnaaataaoiooeltrreeogwleoleibloltreegwltrrreegdewll--------cnoltreegdws-------o-------------------lllllllltttttttttttrrrrrrrrsegnbbbebebbbbw

units 7 objects 13 parts materials

church/outdoor

2

12 1

mbbouuiuiwlglitnrtrssddctdsgnirltrwissarstedeskoadenktrooooinksksayeoaoyyseg---onueeamyngsk-o-------lllttttrrsyeddbpdeebbw

Figure 5: Comparing representations learned by progressive GANs trained on different scene types. The units that emerge match objects that commonly appear in the scene type: seats in conference rooms and stoves in kitchens. Units from layer 4 are shown. A unit is counted as a class predictor if it matches a supervised segmentation class with pixel accuracy > 0.75 and IoU > 0.05 when upsampled and thresholded. The distribution of units over classes is shown in the right column.

Units in layer

Unit class distribution

layer1 512 units total

ceiling layer1 #457

iou=0.12 ceiling layer1 #194

iou=0.08

2

units 1 object

2 object units 0 part units 0 material units

1

ceiling

units 11 objects
29 parts 2 materials

cccoofocfffffffofieipifbeerpfrfrpweiiawcwowceiieaeeiecrpiceacitinewoftutcnnpiepneiaepnelcfesclnltlarifilidniklsiillstaltsuadtltwdnpdiieogiaoaiifbawlrscnaowilwoollfnnoborsaobolttaaodolfconnilfcniilfcnnahaaasaaolrgogeawlopolbalolrgegaewlanegewlif-------scnnlrgegawkse--------ol-------e-------lllllllfttttttttrrrrrrrrrysgbegbbnbebbbaew

units 9 objects 25 parts 3 materials

layer4 512 units total
89 object units 159 part units 7 material units
layer7 256 units total
52 object units 69 part units 17 material units
layer10 128 units total
18 object units 9 part units 13 material units

sofa layer4 #37 painting layer7 #15 carpet layer10 #53

iou=0.28 fireplace layer4 #23

iou=0.16

22

11

iou=0.28 coffee table-t #247

1

iou=0.08

15

8

iou=0.15 glass layer10 #126

iou=0.22

1
7 4 1

ccoocfffffifoircpfbeepfrpfwciieauwcwoeiieaeieacrpciuispnewoftltrcnniepneiaeelcfscfnlttlhifniidklliiillstaltauadtwdnpiieogiioawaawlrcnposilwooifcnnoooblrsbolttaaodlnnolfcnniiolkaaaaaoolreggnanwloopabalolrggewryaegegwlif---------scnnolrs--------------en--llttttttttttrrrrrrsggbebbnebdabbbbebw

units 6 objects
5 parts 3 materials

wiwipcnwfcniaeclcfiidlludnegoiawrpnoilolrlttodoniiosaorwapaorgwi--onnsk---netttttsybnbdggw

Figure 6: Comparing layers of a progressive GAN trained to generate 256x256 LSUN living room images. The output of the first convolutional layer has almost no units that match semantic objects, but many objects emerge at layers 4-7. Later layers are dominated by low-level materials and shapes.

increases not only the visible GAN output, but also the diversity of concepts represented by units of a GAN: the number of types of objects, parts, and materials matching units increases by more than 70%. The second architecture applies pixelwise normalization to achieve better training stability. As applied to Progressive GANs, pixelwise normalization increases the number of units that match semantic classes by 19%.

4.3 DIAGNOSING AND IMPROVING GANS
While our framework can reveal how GANs succeed in producing realistic images, it can also analyze the causes of failures in their results. Figure 14a shows several annotated units that are responsible for typical artifacts consistently appearing across different images. Human annotation is efficient and it typically takes 10 minutes to locate 20 artifact-causing units out of 512 units in layer4.
More importantly, we can fix these errors by ablating the above 20 artifact-causing units. Figure 14b shows that artifacts are successfully removed and the artifact-free pixels stay the same, improving the

7

Under review as a conference paper at ICLR 2019

units 11 objects
24 parts 2 t il

  interpretable units   SWD   Best "bed" unit   Best "window" unit  

base prog GAN  512 units total

bed layer4 #253

iou=0.22 window layer4 #142 iou=0.21 29

61 object units  77 part units  5 material units 

143 units   

7.60

 

15
 1

+batch stddev  512 units total

bed layer4 #88

iou=0.15 window layer4 #422 iou=0.26 17

54 object units  133 part units  6 material units 

193 units   

6.48

 

9
 1

+pixelwise norm  512 units total

bed layer4 #129

iou=0.29 window layer4 #494 iou=0.28 24

71 object units  230 units

144 part units 

15 material units   

 

4.01

 

12
 1

wiwcwwiiciccnewcnineiencfleliidiibtlludldndeigboibribrosilflneotlodoeonneibenekarogdwaodgwygdw-i---endgews---o--------ttttrrlrlllrsdngbbbbw

units 5 objects 15 parts 1 material

Unit class distribution

ppppwciawcwwciiaiaccpciaciuiinewpurcnnineiauenicenttlrniiindliflfliibtlflttllupldstlwdndeiibgiibiwiibarnoilwolotiflnneoaaoottlhaoldonneinneiriioblennaaoolcggdonwoaloorggdwwrnggdw---i---nooeenmrldggws------ao------------ttttttrrrrrlrlrlllrlsgdpnnbbegdbbbbww

units 9 objects 24 parts 2 materials

ppbwciawcwwiiaicpciucbciuinewlncrnineiuaenucecnttliiidiiflfliibtlflludpdlslwdndeiigbiibaiibawrnowsilwoloiennflorottlhdaoldonennneiiibilenkaaaaooldgnogwloplaloolgdgrwglgdrywioennn------ldgrwso-------e-----------ttttttttnlllllnbbbdggbbbgrrrrrrrsww

Figure 7: Comparing layer4 representations learned by different training variations. Lower SWD indicates a higher-quality model: as the quality of the model improves, the number of interpretable units also rises. Progressive GANs apply several innovations including making the discriminator aware of minibatch statistics, and pixelwise normalization at each layer. We can see batch awareness increases the number of object classes matched by units, and pixel norm increases the number of units matching objects.

Unit #63

(b) Bedroom images with artifacts

Unit #231

(a) Example artifact-causing units

(c) Ablating "artifacts" units improves results

Figure 8: (a) We show two example "artifacts" units that are responsible for visual artifacts in GAN results. There are 20 units in total. By ablating these units, we can fix the artifacts in (b) and largely improve the visual quality as shown in (c).

generated results. To further quantify the improvement, we report two standard metrics. First, we compute the popular Fre´chet Inception Distance (Heusel et al., 2017) between the generated images and real images. We use 50 000 real images and generate 10 000 images with high activations on these units. Second, we ask human participants on Amazon MTurk to determine which image looks more realistic, given two images produced by different methods. In total, we collected 20 000 annotations for 1 000 images per method. In Table 1, we compare our improved images to both the original artifacts images and a simple baseline that ablates 20 randomly chosen units. As demonstrated, our framework significantly improves GAN results based on these two metrics.

4.4 LOCATING CAUSAL UNITS WITH ABLATION
Errors are not the only type of output that can be affected by directly intervening in a GAN. A variety of specific object types can also be removed from GAN output by ablating a set of units in a GAN. In Figure 9 we apply the method in Section 3.2 to identify sets of 20 units that have causal effects on common object classes in conference rooms scenes. We find that, by turning off these small sets of units, most of the output of people, curtains, and windows can be removed from the generated scenes. However, not every object has a simple causal encoding: tables and chairs cannot be removed. Ablating those units will reduce the size and density of these objects, but will rarely eliminate them.
The ease of object removal depends on the scene type. Figure 10 shows that, while windows can be removed well from conference rooms, they are more difficult to remove from other scenes. In particular, windows are just as difficult to remove from a bedroom as tables and chairs from a conference room. We hypothesize that the difficulty of removal reflects the level of choice that a

8

Under review as a conference paper at ICLR 2019

Table 1: We compare generated images before and after ablating 20 "artifacts" units. We also report a simple baseline that ablates 20 randomly chosen units.

Fre´chet Inception Distance (FID)

original images

52.87

"artifacts" units ablated (ours) 32.11

random units ablated

52.27

Human preference score
"artifacts" units ablated (ours) random units ablated

original images
79.0% 50.8%

ablate person units

ablate curtain units

ablate window units

ablate table units

ablate chair units

Figure 9: Measuring the effect of ablating units in a GAN trained on conference room images. Five different sets of units have been ablated related to a specific object class. In each case, 20 (out of 512) units are ablated from the same GAN model. The 20 units are specific to the object class and independent of the image. The average causal effect is reported as the portion of pixels that are removed in 1, 000 randomly generated images. We observe that some object classes are easier to remove cleanly than others: a small ablation can erase most pixels for people, curtains, and windows, whereas a similar ablation for tables and chairs only reduces object sizes without erasing them.

GAN has learned for a concept: a conference room is defined by the presence of chairs, so they cannot be removed. And modern building codes mandate that all bedrooms must have windows; the GAN seems to have caught on to that pattern.

4.5 CHARACTERIZING CONTEXTUAL RELATIONSHIPS VIA INSERTION
We can also learn about the operation of a GAN by forcing units on and inserting these features into specific locations in scenes. Figure 11 shows the effect of inserting 20 layer4 causal door units in church scenes. In this experiment, we insert units by setting their activation to a fixed 99% percentile level at a single feature pixel. Although this intervention is the same in each case, the effects vary widely depending on the context. For example, the doors added to the five buildings in Figure 11 appear with a diversity of visual attributes, each with an orientation, size, material, and style that matches the building.
We also observe that doors cannot be added in most locations. The locations where a door can be added are highighted by a yellow box. The bar chart in Figure 11 shows average causal effects of insertions of door units, conditioned on the object class at the location of the intervention. We find that, on average, the easiest way to increase doors in the output is to enlarge an existing door. This is shown in example (d). New doors can also be created, but only in appropriate locations. In general it is not possible to trigger a door in the sky or on trees. Interventions provide insight on how a GAN enforces relationships between objects. Even if we try to add a door in layer4, that choice can be vetoed later if the object is not appropriate for the context.

5 DISCUSSION
By carefully examining representation units, we have found that many parts of GAN representations can be interpreted, not only as signals that correlate with object concepts but as variables that have a causal effect on the synthesis of semantic objects in the output. These interpretable effects can be used to compare, debug, modify, and reason about a GAN model. Our method can be potentially applied to other encoder-based generative models such as VAEs (Kingma & Welling, 2014) and RealNVP (Dinh et al., 2017).

9

Under review as a conference paper at ICLR 2019

conference room

church

kitchen

living room

bedroom

Figure 10: Comparing the effect of ablating 20 window-causal units in GANs trained on five scene categories. In each case, the 20 ablated units are specific to the class and the generator and independent of the image. In some scenes, windows are reduced in size or number rather than eliminated completely, or replaced by visually similar objects such as paintings.

(a) (b)
(c) (d) (e)
Figure 11: Inserting door units by setting 20 causal units to a fixed high value at one pixel in the representation. Whether the door units can cause the generation of doors is dependent on local context: we highlight every location that is responsive to insertions of door units on top of the original image, including two separate locations in (b) (we intervene at left). The same units are inserted in every case, but the door that appears has a size, alignment, and color appropriate to the location. One way to add door pixels is to emphasize a door that is already present: the result is a larger door (d). The chart summarizes the causal effect of inserting door units at one pixel with different context.
We have focused on the generator rather than the discriminator (as did in Radford et al. (2016)) because the generator must represent all the information necessary to approximate the target distribution, while the discriminator only learns to capture the difference between real and fake images. Alternatively, we can train an encoder to invert the generator (Donahue et al., 2017; Dumoulin et al., 2017). However, this incurs additional complexity and errors. Many GANs also do not have an encoder. Our method is not designed to compare the quality of GANs with one other, and it is not intended as a replacement for well-studied GAN metrics such as SWD and FID. Our goal has been to identify interpretable structure and provide a window into the internal mechanisms of a GAN. Prior visualization methods (Zeiler & Fergus, 2014; Bau et al., 2017; Karpathy et al., 2016) have brought many new insights to CNN and RNNs research. Motivated by that, in this work we have taken a small step towards understanding the internal representations of a GAN, and we have uncovered many questions that we cannot yet answer with the current method. For example: why can't a door be inserted in the sky? How does the GAN suppress the signal in the later layers? Further work will be needed to understand the relationships between layers of a GAN. Nevertheless, we hope that our work can help researchers and practitioners better analyze and develop their own GANs.
10

Under review as a conference paper at ICLR 2019
REFERENCES
Sebastian Bach, Alexander Binder, Gre´goire Montavon, Frederick Klauschen, Klaus-Robert Mu¨ller, and Wojciech Samek. On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation. PloS one, 10(7):e0130140, 2015. 3
David Bau, Bolei Zhou, Aditya Khosla, Aude Oliva, and Antonio Torralba. Network dissection: Quantifying interpretability of deep visual representations. In CVPR, 2017. 3, 4, 10
Emily L Denton, Soumith Chintala, Rob Fergus, et al. Deep generative image models using a laplacian pyramid of adversarial networks. In NIPS, 2015. 2
Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. Density estimation using real nvp. In ICLR, 2017. 9
Jeff Donahue, Philipp Kra¨henbu¨hl, and Trevor Darrell. Adversarial feature learning. In ICLR, 2017. 10
Alexey Dosovitskiy and Thomas Brox. Generating images with perceptual similarity metrics based on deep networks. In NIPS, 2016. 3
Vincent Dumoulin, Ishmael Belghazi, Ben Poole, Alex Lamb, Martin Arjovsky, Olivier Mastropietro, and Aaron Courville. Adversarially learned inference. In ICLR, 2017. 10
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In NIPS, 2014. 1, 2
Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by a two time-scale update rule converge to a local nash equilibrium. In NIPS, 2017. 8
Judy Hoffman, Eric Tzeng, Taesung Park, Jun-Yan Zhu, Phillip Isola, Kate Saenko, Alexei A Efros, and Trevor Darrell. Cycada: Cycle-consistent adversarial domain adaptation. In ICML, 2018. 2
Paul W Holland. Causal inference, path analysis and recursive structural equations models. ETS Research Report Series, 1988(1):i­50, 1988. 5
Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A Efros. Image-to-image translation with conditional adversarial networks. In CVPR, 2017. 1, 2
Andrej Karpathy, Justin Johnson, and Li Fei-Fei. Visualizing and understanding recurrent networks. In ICLR, 2016. 3, 10
Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. Progressive growing of gans for improved quality, stability, and variation. In ICLR, 2018. 2, 6
Been Kim, Justin Gilmer, Fernanda Viegas, Ulfar Erlingsson, and Martin Wattenberg. Tcav: Relative concept importance testing with linear concept activation vectors. arXiv preprint arXiv:1711.11279, 2017. 3
Diederik P Kingma and Max Welling. Auto-encoding variational bayes. ICLR, 2014. 9
Aravindh Mahendran and Andrea Vedaldi. Understanding deep image representations by inverting them. In CVPR, 2015. 3
Michael Mathieu, Camille Couprie, and Yann LeCun. Deep multi-scale video prediction beyond mean square error. In ICLR, 2016. 1, 2
Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida. Spectral normalization for generative adversarial networks. In ICLR, 2018. 2
Ari S Morcos, David GT Barrett, Neil C Rabinowitz, and Matthew Botvinick. On the importance of single directions for generalization. arXiv preprint arXiv:1803.06959, 2018. 3
Chris Olah, Arvind Satyanarayan, Ian Johnson, Shan Carter, Ludwig Schubert, Katherine Ye, and Alexander Mordvintsev. The building blocks of interpretability. Distill, 3(3):e10, 2018. 3
11

Under review as a conference paper at ICLR 2019
Judea Pearl. Causality. Cambridge university press, 2009. 5
Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep convolutional generative adversarial networks. In ICLR, 2016. 1, 2, 3, 10
Ramprasaath R Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh, and Dhruv Batra. Grad-cam: Visual explanations from deep networks via gradient-based localization. In ICCV, 2017. 3
Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman. Deep inside convolutional networks: Visualising image classification models and saliency maps. In ICLR, 2014. 3
Hendrik Strobelt, Sebastian Gehrmann, Hanspeter Pfister, and Alexander M. Rush. LSTMVis: A tool for visual analysis of hidden state dynamics in recurrent neural networks. IEEE Transactions on Visualization and Computer Graphics, 24(1):667­676, Jan 2018. ISSN 1077-2626. doi: 10.1109/TVCG.2017.2744158. 3
Mukund Sundararajan, Ankur Taly, and Qiqi Yan. Axiomatic attribution for deep networks. In Doina Precup and Yee Whye Teh (eds.), Proceedings of the 34th International Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Research, pp. 3319­3328, International Convention Centre, Sydney, Australia, 06­11 Aug 2017. PMLR. URL http: //proceedings.mlr.press/v70/sundararajan17a.html. 3
Ting-Chun Wang, Ming-Yu Liu, Jun-Yan Zhu, Guilin Liu, Andrew Tao, Jan Kautz, and Bryan Catanzaro. Video-to-video synthesis. In NIPS, 2018. 2
Xiaolong Wang, Abhinav Shrivastava, and Abhinav Gupta. A-fast-rcnn: Hard positive generation via adversary for object detection. In CVPR, 2017. 1, 2
Dedy Rahman Wijaya, Riyanarto Sarno, and Enny Zulaika. Information quality ratio as a novel metric for mother wavelet selection. Chemometrics and Intelligent Laboratory Systems, 160:59­71, 2017. 4
Tete Xiao, Yingcheng Liu, Bolei Zhou, Yuning Jiang, and Jian Sun. Unified perceptual parsing for scene understanding. In ECCV, 2018. 6
Fisher Yu, Ari Seff, Yinda Zhang, Shuran Song, Thomas Funkhouser, and Jianxiong Xiao. Lsun: Construction of a large-scale image dataset using deep learning with humans in the loop. arXiv preprint arXiv:1506.03365, 2015. 6
Matthew D Zeiler and Rob Fergus. Visualizing and understanding convolutional networks. In ECCV, 2014. 3, 6, 10
Han Zhang, Ian Goodfellow, Dimitris Metaxas, and Augustus Odena. Self-attention generative adversarial networks. arXiv preprint arXiv:1805.08318, 2018. 1, 2
Bolei Zhou, Aditya Khosla, Agata Lapedriza, Aude Oliva, and Antonio Torralba. Object detectors emerge in deep scene cnns. In ICLR, 2015. 3
Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Barriuso, and Antonio Torralba. Scene parsing through ade20k dataset. In CVPR, 2017. 6
Bolei Zhou, Yiyou Sun, David Bau, and Antonio Torralba. Interpretable basis decomposition for visual explanation. In Proceedings of the European Conference on Computer Vision (ECCV), pp. 119­134, 2018. 3
Tinghui Zhou, Philipp Krahenbuhl, Mathieu Aubry, Qixing Huang, and Alexei A Efros. Learning dense correspondence via 3d-guided cycle consistency. In CVPR, 2016. 3
Jun-Yan Zhu, Philipp Kra¨henbu¨hl, Eli Shechtman, and Alexei A. Efros. Generative visual manipulation on the natural image manifold. In ECCV, 2016. 3
Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A Efros. Unpaired image-to-image translation using cycle-consistent adversarial networks. In ICCV, 2017. 1, 2
12

Under review as a conference paper at ICLR 2019

no intervention

fixed

no intervention

fixed

no intervention

fixed

no intervention

fixed

no intervention

fixed

no intervention

fixed

no intervention

fixed

no intervention

fixed

no intervention

fixed

no intervention

fixed

Figure 12: Further examples of images, before and after ablation of 20 artifact units.
6 SUPPLEMENTARY MATERIAL
6.1 ABLATION OF ARTIFACT UNITS Further examples of generated images are shown, before and after ablations of artifact units.
13

14

6.2 ALL LAYERS OF A GAN
A complete listing of the matching concepts that emerge at the 14 layers of a Progressive GAN trained on LSUN living room scenes.

Figure 13: Layers 1-8

6
 1
iou=0.22 9
5
 1
iou=0.22 7
4
 1

18 object units  9 part units 
13 material units   

layer10  128 units total

iou=0.16 wood layer9 #78 iou=0.15 glass layer10 #126

carpet layer10 #53

25 object units  18 part units 
10 material units   

layer9  128 units total

window layer9 #89

 

47 object units  51 part units  9 material units 

 1
iou=0.09 12

layer8  256 units total

iou=0.16 plantr layer8 #234

curtain layer8 #186

52 object units  69 part units 
17 material units   

10
 1
iou=0.08 15

layer7  256 units total

iou=0.28 coffee tablet #247

painting layer7 #15

72 object units  131 part units 
16 material units   

wwpicwnincaecflcdifludiwnieoganplooirodiolrnotltwlaosapiiaworgnnoskr--eniw---gbgdbnsyttttt

units
6 objects 5 parts
3 materials

pwwacpicwineinncaecflcsdiifltuflldinwieogoaiinploonnoirsodiolnrfotltwlaosaggopaoiiaworgnonksr-----renfiw---bgbadgbbbn-sytttttl

units
7 objects
9 parts 3 materials

cofiwppfpwwrcfcaceeapaiccuwnieiuinepinceaiecnflprncsndiiwrfltusdfllldiinwieogwoatlwiatatlntltloaonoioiaaiflrioasiaodinolnnrfnoincttawlaanosagnofopomiwibailwogrgnegagnongllskltr----o--refllilw------------bgpbbebadbnbg---ysrrrrrtttttttttlll

units 9 objects
21 parts 3 materials

ccoocfffioffpficbeweprpfwwrcfuaefioceeaeaepiccuwrsipnieioninecipneaeectflnrhnsctldiikwflaflutatasfldlldiiwpniweogoatwiiiatalnptlocosoonnobioiriicboasaoldinonlnnfroicattaalwklaaoogneganfloopalwiaiblwoegrggegaenonlylscslrr---------rlfneillw--------------gdnbeebbbagbbbbeb--srrrrrrttttttttttll

8

units 9 objects
25 parts 3 materials

cocffioffificbweprpfpwwrrcwfeufiocceaeceeapaicccucwrpsnieioiuuinpepinceeuaineecnflrhnsctlndsiikwflrmrsfltauasdlfllldipsindiweogiwoatataiitathlnttloclosoonooiioaaicirbohasiiaoldoinolninnrfcninceaatatawklaroofsgnenafoiopilwiibialwowgrragennegageonnggllylcslkrr-o-------flrfelliw-------------------enbgbbebbgbebabn----yrrrrrrrrrttttttttttttllll

units 11 objects

cocfioffipffifibwprpefpwwrrwfearfiocaeceeaepeaicccwrpinieioiulninpepinceeapineeecnflncslntdiikwflflsrautsdlfllaldipinldieogiwaaotawiiattlntatolclooonnoiioicraioasiibaoldoinnoltnfrncnncattaacwlaaofoshagegfopoiwibiallwowggraeegangnngegelcslkslrre-------refllilw--------------ebgbbeabbngbeb-------ysrrrrrrrtttttttttlllllll

31 parts

units 9 objects
28 parts 3 materials

cccooocffffofeffipfifibewepfpwwrerwfarfioceeaeceepaeicccwrinieioiutnipnepnceaeeipnecfltnasctndiikwflarstausflldlldsiipnildweogiwoatawbiitalntltaocsloonnoobiiaiflobroasaiioldoionnlnfrclnncaatahtacwlaaosfegogafolopiflwiibalwowregeganegaennlgeaglscelkslr-----o--rlfellilw---------------bbegebagbbbben-------ylsrrrrrrrrrttttttfttlllllll

units 11 objects

ccoocfoffpffiewpefpwwrwfaficecaceeepaicccwrinieiiuuninecpneaeeinectflncstndiiwflrarflsutsfldalldsiipnidweoiwaottwiitalntltolosoonnobioairaiooasbaiioldoinonnfrnncattaawklaofoggafloiopifwiaibllwowegrnganegannelggaylcllrr------rlfelillw----------------gnbbebgabbbe-------rrrrrrrrtttttttttlllllll

29 parts 2 materials

17

units 7 objects

cocfoffibwefwwrwfoeeeicccwniioinpenceneecflcstdikwflausdlfldindieoiaoaltloclosoiobflrioasiaodoionfrncntawklosaofopolwbailwowrgeeagnglyskr--o--refliw---------bbebgneba---yrrrrrttttttlll

29 parts 1 material

units 8 objects
16 parts 1 material

16
 1
iou=0.31 bookcase layer6 #308 iou=0.12 20

layer6  512 units total

window layer6 #393

81 object units  170 part units 
20 material units   

11
 1
iou=0.31 paintingb layer5 #133 iou=0.16 32

layer5  512 units total

sofat layer5 #190

 

89 object units  159 part units  7 material units 

 1
iou=0.16 22

layer4  512 units total

iou=0.28 fireplace layer4 #23

sofa layer4 #37

 

81 object units  135 part units  8 material units 

20
 1
iou=0.24 34

layer3  512 units total

iou=0.29 sofat layer3 #55

window layer3 #305

 

87 object units  82 part units  4 material units 

1
 
iou=0.23 40

layer2  512 units total

iou=0.24 floor layer2 #315

window layer2 #70

 

ceiling

2 object units  0 part units  0 material units 

units 1 object

 
2

layer1  512 units total

iou=0.08

Units in layer
iou=0.12 ceiling layer1 #194

ceiling layer1 #457

Unit class distribution

 

Under review as a conference paper at ICLR 2019

Under review as a conference paper at ICLR 2019

 

layer9  128 units total

window layer9 #89

25 object units  18 part units 
10 material units   

layer10  128 units total

carpet layer10 #53

18 object units  9 part units 
13 material units   

layer11  64 units total

sky layer11 #14

9 object units  1 part units  7 material units 

 

layer12  64 units total

wood layer12 #26

8 object units  1 part units  4 material units 

 

layer13  32 units total

carpet layer13 #13

6 object units  0 part units  3 material units 

 

layer14  32 units total

window layer14 #9

2 object units  0 part units  7 material units 

 

Units in layer
iou=0.16 wood layer9 #78 iou=0.15 glass layer10 #126 iou=0.07 ceiling layer11 #49 iou=0.23 sky layer12 #19 iou=0.11 wood layer13 #23 iou=0.15 wood layer14 #25

iou=0.22

 
9

5

 1
iou=0.22 7

4

 1
iou=0.12 5
3
1
 
iou=0.04 2

1
 
iou=0.16 3 2 1
 
iou=0.17 7
4
 1

Figure 14: Layers 9-14

winwdooowd

units 1 object
1 material

wciccuneawrirtldiopaoionetndgw

units 3 objects 2 materials

cwcieciulnegiwrpillltdnoiasaaognoiskn-gndttsyw

units 5 objects
1 part 2 materials

wcicuwnegwriltladiolsalao-inoksysgdnbw

units 4 objects
1 part 2 materials

wiwipcnwcnaiecflciidflludneogiawnproilolrttlodoniioasorwapaogrwnino--skne---tttttgbgdbnsyw

units 6 objects
5 parts 3 materials

pwiacwiipcnewnciniaecflsctliidfliiflludneogiaownproilofnnolrsttlodoniioasoaggrwoapoaogrwfnoni-----rksne----tttttlbgbadgbbbnsyw

units 7 objects
9 parts 3 materials

Unit class distribution

15


Under review as a conference paper at ICLR 2019
DOUBLY REPARAMETERIZED GRADIENT ESTIMATORS FOR MONTE CARLO OBJECTIVES
Anonymous authors Paper under double-blind review
ABSTRACT
Deep latent variable models have become a popular model choice due to the scalable learning algorithms introduced by (Kingma & Welling, 2013; Rezende et al., 2014). These approaches maximize a variational lower bound on the intractable log likelihood of the observed data. Burda et al. (2015) introduced a multi-sample variational bound, IWAE, that is at least as tight as the standard variational lower bound and becomes increasingly tight as the number of samples increases. Counterintuitively, the typical inference network gradient estimator for the IWAE bound performs poorly as the number of samples increases (Rainforth et al., 2018; Le et al., 2018). Roeder et al. (2017) propose an improved gradient estimator, however, are unable to show it is unbiased. We show that it is in fact biased and that the bias can be estimated efficiently with a second application of the reparameterization trick. The doubly reparameterized gradient (DReG) estimator does not suffer as the number of samples increases, resolving the previously raised issues. The same idea can be used to improve many recently introduced training techniques for latent variable models. In particular, we show that this estimator reduces the variance of the IWAE gradient, the reweighted wake-sleep update (RWS) (Bornschein & Bengio, 2014), and the jackknife variational inference (JVI) gradient (Nowozin, 2018). Finally, we show that this computationally efficient, drop-in estimator translates to improved performance for all three objectives on several modeling tasks.
1 INTRODUCTION
Following the influential work by (Kingma & Welling, 2013; Rezende et al., 2014), deep generative models with latent variables have been widely used to model data such as natural images (Rezende & Mohamed, 2015; Kingma et al., 2016; Chen et al., 2016; Gulrajani et al., 2016), speech and music time-series (Chung et al., 2015; Fraccaro et al., 2016; Krishnan et al., 2015), and video (Babaeizadeh et al., 2017; Ha & Schmidhuber, 2018; Denton & Fergus, 2018). The power of these models lies in combining learned nonlinear function approximators with a principled probabilistic approach, resulting in expressive models that can capture complex distributions. Unfortunately, the nonlinearities that empower these model also make marginalizing the latent variables intractable, rendering maximum likelihood training inapplicable. Instead of directly maximizing the marginal likelihood, a common approach is to maximize a tractable lower bound on the likelihood such as the variational evidence lower bound (ELBO) (Jordan et al., 1999; Blei et al., 2017). The tightness of the bound is determined by the expressiveness of the variational family. For tractability, a factorized variational family is commonly used, which can cause the learned model to be overly simplistic.
Burda et al. (2015) introduced a multi-sample bound, IWAE, that is at least as tight as the ELBO and becomes increasingly tight as the number of samples increases. Counterintuitively, although the bound is tighter, Rainforth et al. (2018) theoretically and empirically showed that the standard inference network gradient estimator for the IWAE bound performs poorly as the number of samples increases due to a diminishing signal-to-noise ratio (SNR). This motivates the search for novel gradient estimators.
Roeder et al. (2017) proposed a lower-variance estimator of the gradient of the IWAE bound. They speculated that their estimator was unbiased, however, were unable to prove the claim. We show that it is in fact biased, but that it is possible to construct an unbiased estimator with a second
1

Under review as a conference paper at ICLR 2019

application of the reparameterization trick which we call the IWAE doubly reparameterized gradient (DReG) estimator. Our estimator is an unbiased, computationally efficient drop-in replacement, and does not suffer as the number of samples increases, resolving the counterintuitive behavior from previous work (Rainforth et al., 2018). Furthermore, our insight is applicable to alternative multisample training techniques for latent variable models: reweighted wake-sleep (RWS) (Bornschein & Bengio, 2014) and jackknife variational inference (JVI) (Nowozin, 2018).
In this work, we derive DReG estimators for IWAE, RWS, and JVI and demonstrate improved scaling with the number of samples on a simple example. Then, we evaluate DReG estimators on MNIST generative modeling, Omniglot generative modeling, and MNIST structured prediction tasks. In all cases, we demonstrate substantial unbiased variance reduction, which translates to improved performance over the original estimators.

2 BACKGROUND

Our goal is to learn a latent variable generative model p(x, z) = p(z)p(x|z) where x are observed data and z are continuous latent variables. The marginal likelihood of the observed data, p(x) =
p(x, z) dz, is generally intractable. Instead, we maximize a variational lower bound on log p(x) such as the ELBO

log p(x) = log Ep(z)[p(x|z)]  Eq(z|x)

log

p(x, z) q(z|x)

,

(1)

where q(z|x) is a variational distribution. Following the influential work by (Kingma & Welling, 2013; Rezende et al., 2014), we consider the amortized inference setting where q(z|x), referred to as the inference network, is parameterized by a learnable function that maps from x to a distribution
over z. The tightness of the bound is coupled to the expressiveness of the variational family (i.e., {q}). As a result, limited expressivity of q, can negatively affect the learned model.

Burda et al. (2015) introduced the importance weighted autoencoder (IWAE) bound which alleviates

this coupling

Ez1:K log

1 K p(x, zi) K i=1 q(zi|x)

 log p(x),

(2)

with z1:K  i q(zi|x). The IWAE bound reduces to the ELBO when K = 1, is non-decreasing as K increases, and converges to log p(x) as K   under mild conditions (Burda et al., 2015). When q is reparameterizable1, the standard gradient estimator of the IWAE bound is

,Ez1:K log

1K K wi
i=1

= ,E 1:K log

1K K wi
i=1

= E 1:K

K i=1

wi j wj

,

log

wi

where wi = p(x, zi)/q(zi|x). A single sample estimator of this expectation is typically used as the gradient estimator.

As K increases, the bound becomes increasingly tight, however, Rainforth et al. (2018) show that as K  , the signal-to-noise ratio (SNR) of the gradient estimator for the inference network parameters () goes to 0. This does not happen for the model parameters (). Following up on this work, Le et al. (2018) demonstrate that this deteriorates the performance of learned models on practical problems.

Because the IWAE bound converges to log p(x) regardless of q as K  , q's affect on the bound must diminish as K increases and hence the gradient with respect to  goes to 0. It is tempting to conclude that the SNR of the inference network gradient estimator must also inevitably degrade as K  . However, low SNR is a limitation of the gradient estimator, not necessarily of the bound. Although the magnitude of the gradient converges to 0 as K increases, if the variance of the gradient decreases more quickly, then the SNR need not degrade. This motivates the search for lower variance gradient estimators (for ) of the IWAE bound.

1Meaning that we can express zi as z( i, ), where z is a deterministic, differentiable function and p( i) does not depend on  or . This allows gradients to be estimated using the reparameterization trick (Kingma & Welling, 2013; Rezende et al., 2014).

2

Under review as a conference paper at ICLR 2019

To derive improved gradient estimators for , it is informative to expand the total derivative2 of the IWAE bound with respect to 

E 1:K

K i=1

wi

K j=1

wj

- 

log

q(zi|x)

+



log wi zi

dzi d

.

(3)

Previously, Roeder et al. (2017) found that the first term within the parentheses of Eq. 3 can contribute significant variance to the gradient estimator. When K = 1, this term analytically vanishes in expectation, so when K > 1 they suggested dropping it. Below, we abbreviate this estimator as STL. As we show in Section 6.1, the STL estimator introduces bias when K > 1.

3 DOUBLY REPARAMETERIZED GRADIENT ESTIMATORS (DREGS)

Our insight is that we can estimate the first term within the parentheses of Eq. 3 efficiently with a second application of the reparameterization trick. To see this, first note that

E 1:K

K i=1

wi
K j=1

wj

 

log

q(zi|x)

K
= E 1:K
i=1

wi
K j=1

wj

 

log

q(zi|x)

,

so it suffices to focus

on one of the K

terms.

Because the

derivative is a partial derivative

 

,

it

treats zi = z( i, ) as a constant, so we can freely change the random variable that the expectation

is over to z1:K

Ez1:K

wi j wj

 

log

q(zi|x)

= Ez-i Ezi

wi j wj

 

log

q(zi|x)

,

(4)

where z-i = z1:i-1,i+1:K is the set of z1:K without zi. The inner expectation resembles a REIN-

FORCE gradient term, where we interpret

wi j wj

as

the

"reward".

Now,

we

can

use

the

following

well-known equivalence between the REINFORCE gradient and the reparameterization trick gradi-

ent (See Appendix 8.1 for a derivation)

 f (z) z( , ) Eq(z|x) f (z)  log q(z|x) = E z  .

(5)

This holds even when f depends on . Typically, the reparameterization gradient estimator has lower variance than the REINFORCE gradient estimator, which suggests using it will improve performance. Applying the identity from Eq. 5 to the right hand side of Eq. 4 gives

=Ei

Ezi 1- j wj (

wi j wj

 

log

q(zi|x)

=Ei

wi j wj )2

wi zi zi 

=Ei

 wi zi zi j wj 

wi - j wj (

wi2 j wj )2

 log wi zi zi 

. (6)

This last expression can be efficiently estimated with a single Monte Carlo sample. When zi is not

reparameterizable (e.g., the models in (Mnih & Rezende, 2016)), we can use a control variate (e.g.,

1 K

 

log

q(zi|x)).

In both cases, when K

=

1, this term vanishes exactly and we recover the

estimator proposed in (Roeder et al., 2017) for the ELBO.

Substituting Eq. 6 into Eq. 3, we obtain a simplification due to cancellation of terms

Ez1:K log

1K K wi
i=1


K
= E 1:K 
i=1

2 wi  log wi zi  . j wj zi 

(7)

We call the algorithm that uses the single sample Monte Carlo estimator of this expression for the inference network gradient the IWAE doubly reparameterized gradient estimator (IWAE-DReG). This

2log wi depends on  in two ways:  and zi = z( i, ). The total derivative accounts for both sources of

dependence

and

the

partial

derivative

 

considers

zi

as

a

constant.

3

Under review as a conference paper at ICLR 2019

estimator has the property that when q(z|x) is optimal (i.e., q(z|x) = p(z|x)), the estimator vanishes exactly and has zero variance, whereas this does not hold for the standard IWAE gradient estimator. We provide an informal asymptotic analysis of the IWAE-DReG estimator in Appendix 8.2. The conclusion of that analysis is that, in contrast to the standard IWAE gradient estimator, the SNR of the IWAE-DReG estimator exhibits the same scaling behaviour of O( K) for both the generation and inference network gradients (i.e., improving in K).

4 ALTERNATIVE TRAINING ALGORITHMS

Now, we review alternative training algorithms for deep generative models and derive their doubly reparameterized versions.

4.1 REWEIGHTED WAKE SLEEP (RWS)

Bornschein & Bengio (2014) introduced RWS, an alternative multi-sample update for latent variable models that uses importance sampling. Computing the gradient of the log marginal likelihood

 log p(x) = 

z p(x, z) dz = p (x)

z p(x, z) log p(x, z) dz p (x)

=

Ep(z|x) [ log p(x, z)] ,

requires samples from p(z|x), which is generally intractable. We can approximate the gradient with a self-normalized importance sampling estimator

Ep(z|x) [ log p(x, z)]  Ez1:K

i

wi j wj



log

p

(x,

zi)

,

where z1:K  i q(zi|x). Interestingly, this is precisely the same as the IWAE gradient of , so the RWS update for  can be interpreted as maximizing the IWAE lower bound in terms of . Instead of optimizing a joint objective for p and q, RWS optimizes a separate objective for the inference network. (Bornschein & Bengio, 2014) propose a "wake" update and a "sleep" update for the inference network. Le et al. (2018) provide empirical support for solely using the wake update for the inference network, so we focus on that update.

The wake update approximately minimizes the KL divergence from p(z|x) to q(z|x). The gradient of the KL term is
 Ep(z|x) [log p(z|x) - log q(z|x)] = -Ep(z|x)  log q(z|x) .

The wake update of the inference network approximates the intractable expectation by selfnormalized importance sampling

 - Ep(z|x)  log q(z|x)  -Ez1:K

i

wi j wj

 

log

q(zi|x)

,

(8)

with zi  q(zi|x). Le et al. (2018) note that this update does not suffer from diminishing SNR as K increases. However, a downside is that the updates for p and q are not gradients of a unified
objective, so could potentially lead to instability or divergence.

DOUBLY REPARAMETERIZED REWEIGHTED WAKE UPDATE

The wake update gradient for the inference network (Eq. 8) can be reparameterized

- Ez1:K

i

wi j wj

 

log

q(zi|x)

= E 1:K

i

(

wi2 j wj )2

-

wi j wj

 log wi zi . zi  (9)

We call the algorithm that uses the single sample Monte Carlo estimator of this expression as the

wake update for the inference network RWS-DReG.

Interestingly, the inference network gradient estimator from (Roeder et al., 2017) can be seen as the sum of the IWAE gradient estimator and the wake update of the inference network (as the wake

4

Under review as a conference paper at ICLR 2019

update minimizes, we add the negative of Eq. 9). Their positive results motivate further exploration of convex combinations of IWAE-DReG and RWS-DReG

E 1:K

i



wi + (1 - 2) j wj (

wi2 j wj )2

 log wi zi . zi 

(10)

We refer to the algorithm that uses the single sample Monte Carlo estimator of this expression as DReG(). When  = 1, this reduces to RWS-DReG, when  = 0, this reduces to IWAE-DReG and when  = 0.5, this reduces STL.

4.2 JACKKNIFE VARIATIONAL INFERENCE (JVI)

Alternatively, Nowozin (2018) reinterprets the IWAE lower bound as a biased estimator for the log marginal likelihood. He analyzes the bias and introduces a novel family of estimators, Jackknife Variational Inference (JVI), which trade off reduction in bias for increased variance. This additional flexibility comes at the cost of no longer being a stochastic lower bound on the log marginal likelihood. The first-order JVI has significantly reduced bias compared to IWAE, which empirically results in a better estimate of the log marginal likelihood with fewer samples (Nowozin, 2018). For simplicity, we focus on the first-order JVI estimator

1

K × Ez1:K log K

wi

i





K -1

1

- K

Ez-i log  K - 1 wj .

i j=i

It is straightforward to apply our approach to higher order JVI estimators.

DOUBLY REPARAMETERIZED JACKKNIFE VARIATIONAL INFERENCE (JVI)
The JVI estimator is a linear combination of K and K - 1 sample IWAE estimators, so we can use the doubly reparameterized gradient estimator (Eq. 7) for each term.

5 RELATED WORK
Mnih & Rezende (2016) introduced a generalized framework of Monte Carlo objectives (MCO). The log of an unbiased marginal likelihood estimator is a lower bound on the log marginal likelihood by Jensen's inequality. In this view, the ELBO can be seen as the MCO corresponding to a single importance sample estimator of the marginal likelihood with q as the proposal distribution. Similarly, IWAE corresponds to the K-sample estimator. Maddison et al. (2017) show that the tightness of an MCO is directly related to the variance of the underlying estimator of the marginal likelihood.
However, Rainforth et al. (2018) point out issues with gradient estimators of multi-sample lower bounds. In particular, they show that although the IWAE bound is tighter, the standard IWAE gradient estimator's SNR scales poorly with large numbers of samples, leading to degraded performance. Le et al. (2018) experimentally investigate this phenomenon and provide empirical evidence of this degradation across multiple tasks. They find that RWS (Bornschein & Bengio, 2014) does not suffer from this issue and find that it can outperform models trained with the IWAE bound. We conclude that it is not sufficient to just tighten the bound; it is important to understand the gradient estimators of the tighter bound as well.
Wake-sleep is an alternative approach to fitting deep generative models, first introduced in (Hinton et al., 1995) as a method for training Hemholtz machines. It was extended to the multi-sample setting by (Bornschein & Bengio, 2014) and the sequential setting in (Gu et al., 2015). It has been applied to generative modeling of images (Ba et al., 2015).

6 EXPERIMENTS
To evaluate DReG estimators, we first measure variance and signal-to-noise ratio (SNR) of gradient estimators on a toy example which we can carefully control. Then, we evaluate gradient variance and model learning on MNIST generative modeling, Omniglot generative modeling, and MNIST structured prediction tasks.

5

Under review as a conference paper at ICLR 2019

6.1 TOY GAUSSIAN

We reimplemented the Gaussian example from (Rainforth et al., 2018). Consider the generative

model

with

z



N (, I)

and

x|z



N (z, I)

and

inference

network

q(z|x)



N (Ax +

b,

2 3

I

),

where  = {A, b}. As in (Rainforth et al., 2018), we sample a set of parameters for the model

and inference network close to the optimal parameters (perturbed by zero-mean Gaussian noise

with standard deviation 0.01), then estimate the gradient of the inference network parameters for

increasing number of samples (K).

In addition to signal-to-noise ratio (SNR), we plot the squared bias and variance of the gradient estimators3 in Fig. 1. The bias is computed relative to the expected value of the IWAE gradient estimator. Importantly, SNR does not penalize estimators that are biased, so trivial constant estimators can have infinite SNR. Thus, it is important to consider additional measures as well. As K increases, the SNR of the IWAE-DReG estimator increases, whereas the SNR of the standard gradient estimator of IWAE goes to 0, as previously reported. Furthermore, we can see the bias present in the STL estimator. As a check of our implementation, we verified that the observed "bias" for IWAE-DReG was statistically indistinguishable from 0 with a paired t-test. For the biased estimators (e.g., STL), we could easily reject the null hypothesis with few samples.

SNR Bias Squared
Variance

102 101 100 10-1 10-2 10-3 10-4
100

101

102

K

103

10-1 10-2 10-3 10-4 10-5 10-6 10-7
100

101 102 K

100 10-1

10-2

10-3 10-4 10-5

10-6 10-7 10-8

K ELBOs IWAE STL (Roeder et al. 2017)

10-9

IWAE-DReG

103

10-10 100

101

102

K

103

Figure 1: Signal-to-noise ratios (SNR), bias squared, and variance of gradient estimators with increasing K over 10 random trials with 1000 measurement samples per trial (mean in bold). The observed "bias" for IWAE-DReG is not statistically significant under a paired t-test (as expected because IWAE-DReG is unbiased). IWAE-DReG is unbiased, its SNR increases with K, and it has the lowest variance of the estimators considered here.

6.2 GENERATIVE MODELING
Training generative models of the binarized MNIST digits dataset is a standard benchmark task for latent variable models. For this evaluation, we used the single latent layer architecture from (Burda et al., 2015). The generative model used 50 Gaussian latent variables, 2 deterministic layers of 200 tanh units, and Bernoulli likelihoods. The inference network modeled the latent variables as factorized Gaussians with 2 deterministic layers of 200 tanh units. Because our interest was in improved gradient estimators and optimization performance, we used the dynamically binarized MNIST dataset, which minimally suffers from overfitting. We used the standard split of MNIST into train, validation, and test sets.
We trained models with the IWAE gradient, the RWS wake update, and with the JVI estimator. In all three cases, the doubly reparameterized gradient estimator reduced variance and as a result substantially improved performance (Fig. 2).
We found similar behavior with different numbers of samples (Fig. 3). Interestingly, the biased gradient estimators STL and RWS-DReG perform best on this task with RWS-DReG slightly outperforming STL. As observed in (Le et al., 2018), RWS increasingly outperforms IWAE as K increases. Finally, we experimented with convex combinations of IWAE-DReG and RWS-DReG (right Fig. 3). On this dataset, convex combinations that heavily weighted RWS-DReG had the best performance. However, as we show below, this is dataset and task dependent.
3All dimensions of  behaved qualitatively similarly, so for clarity, we show curves for a single randomly chosen dimension of .
6

Under review as a conference paper at ICLR 2019

Gradient Variance

0.025
0.020
0.015
0.010
0.005
0.000 0
86.5 87.0 87.5 88.0 88.5 89.0 89.5 90.0 0

IWAE IWAE-DReG 1000 2000 3000 4000 5000 Steps (thousands) IWAE IWAE-DReG
1000 2000 3000 4000 5000 Steps (thousands)

Test objective (nats)

Gradient Variance

0.014 0.012 0.010 0.008 0.006 0.004 0.002 0.000 0
86.5 87.0 87.5 88.0 88.5 89.0 89.5 90.0 0

RWS RWS-DReG 1000 2000 3000 4000 5000 Steps (thousands)
RWS RWS-DReG 1000 2000 3000 4000 5000 Steps (thousands)

Test objective (nats)

Gradient Variance

0.09 0.08 0.07 0.06 0.05 0.04 0.03 0.02 0.01 0.00 0
86.5 87.0 87.5 88.0 88.5 89.0 89.5 90.0 0

JVI JVI-DReG 1000 2000 3000 4000 5000 Steps (thousands) JVI JVI-DReG
1000 2000 3000 4000 5000 Steps (thousands)

Test objective (nats)

Figure 2: MNIST generative modeling trained according to IWAE (left), RWS (middle), and JVI (right). The top row compares the variance of the original gradient estimator (dashed) with the variance of the doubly reparameterized gradient estimator (solid). The bottom row compares test performance. The left and middle plots show the IWAE (stochastic) lower bound on the test set. The right plot shows the JVI estimator (which is not a bound) on the test set. The bold lines are the average over three trials, and individual trials are displayed as semi-transparent). All methods used K = 64.

Test objective (nats) Test objective (nats) Relative test objective (nats)

86.5 K = 32
87.0 87.5 88.0 88.5 89.0 89.5 90.0 0 1000 2000 3000 4000 5000
Steps (thousands)

86.0 86.5 87.0 87.5 88.0 88.5 89.0 89.5 90.0 0

K = 256
IWAE IWAE-DReG RWS RWS-DReG STL (Roeder 2017) 1000 2000 3000 4000 5000 Steps (thousands)

0.7 K = 64

0.6

alpha=0.5, STL alpha=0.7

0.5

alpha=0.8 alpha=0.9

0.4 alpha=1, RWS-DReG

0.3

0.2

0.1 0 1000 2000 3000 4000 5000 Steps (thousands)

Figure 3: Log-likelihood lower bounds for generative modeling on MNIST. The left and middle plots compare performance with different number of samples K = 32, 256. The bold lines are the average over three trials, and individual trials are displayed as semi-transparent). The right plot compares performance as the convex combination between IWAE-DReG and RWS-DReG is varied (Eq. 10). To highlight differences, we plot the difference between the test IWAE bound and the test IWAE bound IWAE-DReG achieved at that time point.

Next, we performed the analogous experiment with the dynamically binarized Omniglot dataset using the same model architecture. Again, we found that the doubly reparameterized gradient estimator reduced variance and as a result improved test performance (Figs. 5 and 6 in the Appendix).
6.3 STRUCTURED PREDICTION ON MNIST
Structured prediction is another common benchmark task for latent variable models. In this task, our goal is to model a complex observation x given a context c (i.e., model the conditional distribution p(x|c)). We can use a conditional latent variable model p(x, z|c) = p(x|z, c)p(z|c), however, as before, computing the marginal likelihood is generally intractable. It is straightforward to adapt the bounds and techniques from the previous section to this problem.
To evaluate our method in this context, we use the standard task of predicting the bottom half of a binarized MNIST digit from the top half. We use a similar architecture, but now learn a conditional prior distribution p(z|c) where c is the top half of the MNIST digit. We use a two 200 node tanh layer network parameterizing a factorized Gaussian distribution. To model the conditional
7

Under review as a conference paper at ICLR 2019

distribution p(x|c, z), we concatenate z with c and feed it into a two 200 node tanh layer network parameterizing factorized Bernoulli outputs.
As in the previous tasks, the doubly reparameterized gradient estimator improves across all three updates (IWAE, RWS, and JVI; Fig. 7). However, on this task, the biased estimators (STL and RWS) underperform unbiased IWAE gradient estimators (Fig. 4). In particular, RWS becomes unstable later in training. We suspect that this is because RWS does not directly optimize a consistent objective.

Test objective (nats) Test objective (nats) Relative test objective (nats)

40.0 K = 64
40.5 41.0 41.5 42.0 42.5 0 1000 2000 3000 4000 5000
Steps (thousands)

40.0 40.5 41.0 41.5 42.0 42.5 0

K = 256
IWAE IWAE-DReG RWS RWS-DReG STL (Roeder 2017) 1000 2000 3000 4000 5000
Steps (thousands)

K = 64
0.0
0.5 alpha=0.1
1.0 alpha=0.2 alpha=0.5, STL alpha=1, RWS-DReG
1.5 0 1000 2000 3000 4000 5000 Steps (thousands)

Figure 4: Log-likelihood lower bounds for structured prediction on MNIST. The left plot uses K = 64 samples and the right plot uses K = 256 samples. The bold lines are the average over three trials, and individual trials are displayed as semi-transparent). The right plot compares performance as the convex combination between IWAE-DReG and RWS-DReG is varied (Eq. 10). To highlight differences, we plot the difference between the test IWAE bound and the test IWAE bound IWAEDReG achieved at that time point.

7 DISCUSSION
In this work, we introduce doubly reparameterized estimators for the updates in IWAE, RWS, and JVI.We demonstrate that across tasks they provide unbiased variance reduction, which leads to improved performance. As a result, we recommend that DReG estimators should be used as drop-in, computationally efficient replacements.
Variational Sequential Monte Carlo (Maddison et al., 2017; Naesseth et al., 2017; Le et al., 2018) and Neural Adapative Sequential Monte Carlo (Gu et al., 2015) extend IWAE and RWS to sequential latent variable models, respectively. It would be interesting to develop DReG estimators for these approaches as well.
We found that a convex combination of IWAE-DReG and RWS-DReG performed best, however, the weighting was task dependent. In future work, we intend to apply ideas from (Baydin et al., 2017) to automatically adapt the weighting based on the data.
Finally, the form of the IWAE-DReG estimator (Eq. 7) is surprisingly simple and suggests that there may be a more direct derivation that is applicable to general MCOs.
REFERENCES
Jimmy Ba, Ruslan R Salakhutdinov, Roger B Grosse, and Brendan J Frey. Learning Wake-Sleep Recurrent Attention Models. In C Cortes, N D Lawrence, D D Lee, M Sugiyama, and R Garnett (eds.), Advances in Neural Information Processing Systems 28, pp. 2593­2601. 2015.
Mohammad Babaeizadeh, Chelsea Finn, Dumitru Erhan, Roy H Campbell, and Sergey Levine. Stochastic variational video prediction. arXiv preprint arXiv:1710.11252, 2017.
Atilim Gunes Baydin, Robert Cornish, David Martinez Rubio, Mark Schmidt, and Frank Wood. Online learning rate adaptation with hypergradient descent. arXiv preprint arXiv:1703.04782, 2017.
David M Blei, Alp Kucukelbir, and Jon D McAuliffe. Variational inference: A review for statisticians. Journal of the American Statistical Association, (just-accepted), 2017.
8

Under review as a conference paper at ICLR 2019
Jo¨rg Bornschein and Yoshua Bengio. Reweighted wake-sleep. arXiv preprint arXiv:1406.2751, 2014.
Yuri Burda, Roger Grosse, and Ruslan Salakhutdinov. Importance weighted autoencoders. arXiv preprint arXiv:1509.00519, 2015.
George Casella and Roger L Berger. Statistical inference, volume 2. Duxbury Pacific Grove, CA.
Xi Chen, Diederik P Kingma, Tim Salimans, Yan Duan, Prafulla Dhariwal, John Schulman, Ilya Sutskever, and Pieter Abbeel. Variational lossy autoencoder. arXiv preprint arXiv:1611.02731, 2016.
Junyoung Chung, Kyle Kastner, Laurent Dinh, Kratarth Goel, Aaron C Courville, and Yoshua Bengio. A recurrent latent variable model for sequential data. In Advances in neural information processing systems, pp. 2980­2988, 2015.
Emily Denton and Rob Fergus. Stochastic video generation with a learned prior. arXiv preprint arXiv:1802.07687, 2018.
Marco Fraccaro, Søren Kaae Sønderby, Ulrich Paquet, and Ole Winther. Sequential neural models with stochastic layers. In Advances in neural information processing systems, pp. 2199­2207, 2016.
Shixiang Gu, Zoubin Ghahramani, and Richard E Turner. Neural adaptive sequential monte carlo. In Advances in Neural Information Processing Systems, pp. 2629­2637, 2015.
Ishaan Gulrajani, Kundan Kumar, Faruk Ahmed, Adrien Ali Taiga, Francesco Visin, David Vazquez, and Aaron Courville. Pixelvae: A latent variable model for natural images. arXiv preprint arXiv:1611.05013, 2016.
David Ha and Ju¨rgen Schmidhuber. World models. arXiv preprint arXiv:1803.10122, 2018.
Geoffrey E Hinton, Peter Dayan, Brendan J Frey, and Radford M Neal. The" wake-sleep" algorithm for unsupervised neural networks. Science, 268(5214):1158­1161, 1995.
Michael I Jordan, Zoubin Ghahramani, Tommi S Jaakkola, and Lawrence K Saul. An introduction to variational methods for graphical models. Machine learning, 37(2):183­233, 1999.
Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013.
Diederik P Kingma, Tim Salimans, Rafal Jozefowicz, Xi Chen, Ilya Sutskever, and Max Welling. Improved variational inference with inverse autoregressive flow. In Advances in Neural Information Processing Systems, pp. 4743­4751, 2016.
Rahul G Krishnan, Uri Shalit, and David Sontag. Deep kalman filters. arXiv preprint arXiv:1511.05121, 2015.
Tuan Anh Le, Adam R Kosiorek, N Siddharth, Yee Whye Teh, and Frank Wood. Revisiting reweighted wake-sleep. arXiv preprint arXiv:1805.10469, 2018.
Chris J Maddison, John Lawson, George Tucker, Nicolas Heess, Mohammad Norouzi, Andriy Mnih, Arnaud Doucet, and Yee Teh. Filtering variational objectives. In Advances in Neural Information Processing Systems, pp. 6573­6583, 2017.
Andriy Mnih and Danilo J Rezende. Variational inference for monte carlo objectives. arXiv preprint arXiv:1602.06725, 2016.
Christian A Naesseth, Scott W Linderman, Rajesh Ranganath, and David M Blei. Variational sequential monte carlo. arXiv preprint arXiv:1705.11140, 2017.
Sebastian Nowozin. Debiasing evidence approximations: On importance-weighted autoencoders and jackknife variational inference. 2018.
9

Under review as a conference paper at ICLR 2019 Tom Rainforth, Adam R Kosiorek, Tuan Anh Le, Chris J Maddison, Maximilian Igl, Frank Wood,
and Yee Whye Teh. Tighter variational bounds are not necessarily better. arXiv preprint arXiv:1802.04537, 2018. Danilo Jimenez Rezende and Shakir Mohamed. Variational inference with normalizing flows. arXiv preprint arXiv:1505.05770, 2015. Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and approximate inference in deep generative models. arXiv preprint arXiv:1401.4082, 2014. Geoffrey Roeder, Yuhuai Wu, and David Duvenaud. Sticking the landing: An asymptotically zerovariance gradient estimator for variational inference. Advances in Neural Information Processing Systems, 2017.
10

Under review as a conference paper at ICLR 2019

8 APPENDIX

Gradient Variance

0.014 0.012 0.010 0.008 0.006 0.004 0.002 0.000 0
105
106
107
108
109
110 0

IWAE IWAE-DReG 1000 2000 3000 4000 5000 Steps (thousands) IWAE IWAE-DReG
1000 2000 3000 4000 5000 Steps (thousands)

Test objective (nats)

Gradient Variance

0.008 0.007 0.006 0.005 0.004 0.003 0.002 0.001 0.000 0
105
106
107
108
109
110 0

RWS RWS-DReG 1000 2000 3000 4000 5000 Steps (thousands)
RWS RWS-DReG 1000 2000 3000 4000 5000 Steps (thousands)

Test objective (nats)

Gradient Variance

0.045 0.040 0.035 0.030 0.025 0.020 0.015 0.010 0.005 0.000 0
105 106 107 108 109 110 0

JVI JVI-DReG 1000 2000 3000 4000 5000 Steps (thousands) JVI JVI-DReG
1000 2000 3000 4000 5000 Steps (thousands)

Test objective (nats)

Figure 5: Omniglot generative modeling trained according to IWAE (left), RWS (middle), and JVI (right). The top row compares the variance of the original gradient estimator (dashed) with the variance of the doubly reparameterized gradient estimator (solid). The bottom row compares test performance. The left and middle plots show the IWAE (stochastic) lower bound on the test set. The right plot shows the JVI estimator (which is not a bound) on the test set. The bold lines are the average over three trials, and individual trials are displayed as semi-transparent). All methods used K = 64.

Test objective (nats) Test objective (nats) Relative test objective (nats)

104 K = 64
105 106 107 108 109 0 1000 2000 3000 4000 5000
Steps (thousands)

104 K = 256
105 106 IWAE 107 IWAE-DReG
RWS 108 RWS-DReG
STL (Roeder 2017) 109 0 1000 2000 3000 4000 5000
Steps (thousands)

1.4 K = 64
1.2

alpha=0.1 alpha=0.2

1.0 alpha=0.3

0.8 0.6 0.4

alpha=0.4 alpha=0.5, STL alpha=0.6 alpha=0.7

0.2 alpha=0.8

0.0 0.2 0

1000 2000 3000 4000 5000

alpha=0.9 alpha=1, RWS-DReG

Steps (thousands)

Figure 6: Log-likelihood lower bounds for structured prediction on Omniglot. The left plot uses K = 64 samples and the right plot uses K = 256 samples. The bold lines are the average over three trials, and individual trials are displayed as semi-transparent). The right plot compares performance as the convex combination between IWAE-DReG and RWS-DReG is varied. To highlight differences, we plot the difference between the test IWAE bound and the test IWAE bound IWAE-DReG achieved at that time point.

8.1 EQUIVALENCE BETWEEN REINFORCE GRADIENT AND REPARAMETERIZATION TRICK
GRADIENT

Given a function f (z, ), we have

Eq(z)

f (z, )  log q(z) 

=E

f (z, ) z( , ) ,
z 

for a reparameterizable distribution q(z). To see this, consider the following function of  and ~

g(, ~) = Eq(z) f (z, ~) .

11

Under review as a conference paper at ICLR 2019

Gradient Variance

0.0014 0.0012 0.0010 0.0008 0.0006 0.0004 0.0002 0.0000 0
40.5

IWAE IWAE-DReG 1000 2000 3000 4000 5000 Steps (thousands)

41.0

41.5

42.0 IWAE IWAE-DReG
42.5 0 1000 2000 3000 4000 5000 Steps (thousands)

Test objective (nats)

Gradient Variance

0.0045 0.0040 0.0035 0.0030 0.0025 0.0020 0.0015 0.0010 0.0005 0.0000 0
40.5 41.0

RWS RWS-DReG
1000 2000 3000 4000 5000 Steps (thousands) RWS RWS-DReG

41.5

42.0

42.5 0 1000 2000 3000 4000 5000 Steps (thousands)

Test objective (nats)

Gradient Variance

0.006 0.005 0.004 0.003 0.002 0.001 0.000 0
40.5
41.0

JVI JVI-DReG
1000 2000 3000 4000 5000 Steps (thousands) JVI JVI-DReG

41.5

42.0

42.5 0 1000 2000 3000 4000 5000 Steps (thousands)

Test objective (nats)

Figure 7: Structured prediction on MNIST according to IWAE (left), RWS (middle), and JVI (right). The top row compares the variance of the original gradient estimator (dashed) with the variance of the doubly reparameterized gradient estimator (solid). The bottom row compares test performance. The left and middle plots show the IWAE (stochastic) lower bound on the test set. The right plot shows the JVI estimator (which is not a bound) on the test set. The bold lines are the average over three trials, and individual trials are displayed as semi-transparent). All methods used K = 64.

On one hand,

 g(, ~) 

=

  Eq(z)

f (z, ~)

 =


q(z)f (z, ~) dz =
z

z

 

q(z

)f

(z

,

~)

dz

=

z

 q(z) 

log

q(z)f (z,

~)

dz

=

Eq(z)

f (z, ~)  log q(z) 

,

via the REINFORCE gradient. On the other hand,

 g(, ~) 

=

 E

f (z( , ), ~) = E

 f (z( , ), ~) 

=E

f (z, ~) z( , ) ,
z 

via the reparameterization trick. Thus, we conclude that

Eq(z)

f (z, ~)  log q(z) 

=E

f (z, ~) z( , ) z 

for any choice of ~, in particular for ~ = .

8.2 INFORMAL ASYMPTOTIC ANALYSIS
At a high level, Rainforth et al. (2018) show that the expected value of the IWAE gradient of the inference network collapses to zero with rate 1/K, while its standard deviation is only shrinking at a rate of 1/ K. This is the essence of the problem that results in the SNR (expectation divided by standard deviation) of the inference network gradients going to zero at a rate O((1/K)/(1/ K)) = O(1/ K), worsening with K. In contrast, Rainforth et al. (2018) show that the generation network gradients scales like O( K), improving with K.
Because the IWAE-DReG estimator is unbiased, we cannot hope to change the scaling of the expected value in K, but we can hope to change the scaling of the variance. In particular, in this subsection, we provide an informal argument, via the delta method, that thestandard deviation of IWAE-DReG scales like K-3/2, which results in an overall scaling of O( K) for the inference network gradient's SNR (i.e., increasing with K). Thus, the SNR of the IWAE-DReG estimator improves similarly in K for both inference and generation networks.

12

Under review as a conference paper at ICLR 2019

We will appeal to the delta method on a two-variable function g : R2  R. Define the following notation for the partials of g evaluated at the mean of random variables X, Y ,

g(x, y) gx(X, Y ) = x (x,y)=(E(X),E(Y ))

The delta method approximation of Var(g(X, Y )) is given by (Section 5.5 of Casella & Berger),

Var(g(X, Y ))  gx(X, Y )2Var(X) + 2gx(X, Y )gy(X, Y )Cov(X, Y ) + gy(X, Y )2Var(Y )

Now, assume without loss of generality that  is a single real-valued parameter. Let ui =

wi2



log wi zi

zi 

,

X

=

K i=1

ui

,

and

Y

=

K i=1

wi.

Let g(X, Y )

=

X/Y 2, then g(X, Y ) is the

IWAE-DReG estimator whose variance we seek to understand. Letting Z = E(wi) and U = E(ui)

we get in this case after cancellations,

1 Var(X) 4U Cov(X, Y ) 4U 2 Var(Y ) Var(g(X, Y ))  Z4 K4 - Z5 K4 + Z6 K4
Because wi are all mutually independent, we get Var(X) = KVar(wi). Similarly for ui. Because the wi and ui are identically distributed and independent for i = j, we have Cov(X, Y ) = KCov(wi, ui). All together we can see that Var(g(X, Y )) scales like K-3. Thus, the standard deviation scales like K-3/2.

8.3 UNIFIED SURROGATE OBJECTIVES FOR ESTIMATORS

In the main text, we assumed that  and  were disjoint, however, it can be helpful to share parameters between p and q (e.g., (Fraccaro et al., 2016)). With the IWAE bound, we differentiate a single objective with respect to both the p and q parameters. Thus it is straightforward to adapt IWAE and IWAE-DReG to the shared parameter setting. In this section, we discuss how to deal with shared parameters in RWS.

Suppose that both p and q are parameterized by . If we denote the unshared parameters of q by
, then we can restrict the RWS wake update to only . Alternatively, with a modified RWS wake
update, we can derive a single surrogate objective for each scenario such that taking the gradient with
respect to  results in the proper update. For clarity, we introduce the following modifier notation for p(x, zi), q(zi|x), and wi which are functions of  and zi = z(, i). We use X~ to mean X with stopped gradients with respect to zi, X^ to mean X with stopped gradients with respect to  (but not  is not stopped in z(, i)), and X¯ to mean X with stopped gradients for all variables. Then, we can use the following surrogate objectives:

IWAE:

LIW AE () = E 1:K

K i=1

w¯i j w¯j

log wi

(11)

DReG IWAE:

K
LDReG-IW AE () = E 1:K 
i=1

w¯i j w¯j

log p~(x, zi)

+

2

w¯i j w¯j

log w^i

(12)

RWS:

LRW S () = E 1:K

K i=1

w¯i j w¯j

(log

p~(x, zi)

+

log

q~ (zi |x))

(13)

DReG RWS:

K
LDReG-RW S () = E 1:K 
i=1



w¯i j w¯j

log p~(x, zi)

+



w¯i - j w¯j

2 

w¯i j w¯j

 log w^i (14)

13

Under review as a conference paper at ICLR 2019

STL: DReG():

LST L() = E 1:K

K i=1

w¯i j w¯j

(log p~(x, zi)

+

log w^i)

(15)


K
LDReG()() = E 1:K 
i=1



w¯i j w¯j

log p~(x, zi)

+



w¯i + (1 - 2) j w¯j

2 

w¯i j w¯j

 log w^i

(16)

The only subtle difference is that DReG( = 0.5) does not correspond exactly to STL due to the scaling between terms:

LDReG(=0.5)() = E 1:K

K i=1

w¯i j w¯j

(log

p~(x, zi)

+

0.5 log w^i)

(17)

14


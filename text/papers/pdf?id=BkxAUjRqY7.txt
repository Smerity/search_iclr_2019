Under review as a conference paper at ICLR 2019
AN INFORMATION-THEORETIC METRIC OF TRANSFERABILITY FOR TASK TRANSFER LEARNING
Anonymous authors Paper under double-blind review
ABSTRACT
An important question in task transfer learning is to determine task transferability, i.e. given a common input domain, estimating to what extent representations learned from a source task can help in learning a target task. Typically, transferability is either measured experimentally or inferred through task relatedness, which is often defined without a clear operational meaning. In this paper, we present a novel metric, H-score, an easily-computable evaluation function that estimates the performance of transferred representations from one task to another in classification problems. Inspired by a principled information theoretic approach, H-score has a direct connection to the asymptotic error probability of the decision function based on the transferred feature. This formulation of transferability can further be used to select a suitable set of source tasks in task transfer learning problems or to devise efficient transfer learning policies. Experiments using both synthetic and real image data show that not only our formulation of transferability is meaningful in practice, but also it can generalize to inference problems beyond classification, such as recognition tasks for 3D indoor-scene understanding.
1 INTRODUCTION
Transfer learning is a learning paradigm that exploits relatedness between different learning tasks in order to gain certain benefits, e.g. reducing the demand for supervision (Pratt (1993)). In task transfer learning, we assume that the input domain of the different tasks are the same. Then for a target task TT , instead of learning a model from scratch, we can initialize the parameters from a previously trained model for some related source task TS. For example, deep convolutional neural networks trained for the ImageNet classification task have been used as the source network in transfer learning for target tasks with fewer labeled data (Donahue et al. (2014)), such as medical image analysis (Shie et al. (2015)) and structural damage recognition in buildings (Gao & Mosalam).
An imperative question in task transfer learning is transferability, i.e. when a transfer may work and to what extent. Given a metric capable of efficiently and accurately measuring transferability across arbitrary tasks, the problem of task transfer learning, to a large extent, is simplified to search procedures over potential transfer sources and targets as quantified by the metric. Traditionally, transferability is measured purely empirically using model loss or accuracy on the validation set (Yosinski et al. (2014); Zamir et al. (2018); Conneau et al. (2017)). There have been theoretical studies that focus on task relatedness (Baxter (2000); Maurer (2009); Pentina & Lampert (2014); Ben-David et al. (2003)). However, they either cannot be computed explicitly from data or do not directly explain task transfer performance. In this study, we aim to estimate transferability analytically, directly from the training data.
We quantify the transferability of feature representations across tasks via an approach grounded in statistics and information theory. The key idea of our method is to show that the error probability of using a feature of the input data to solve a learning task can be characterized by a linear projection of this feature between the input and output domains. Hence we adopt the projection length as a metric of the feature's effectiveness for the given task, and refer to it as the H-score of the feature. More generally, H-score can be applied to evaluate the performance of features in different tasks, and is particularly useful to quantify feature transferability among tasks. Using this idea, we define task transferability as the normalized H-score of the optimal source feature with respect to the target task.
1

Under review as a conference paper at ICLR 2019

As we demonstrate in this paper, the advantage of our transferability metric is threefold. (i) it has a strong operational meaning rooted in statistics and information theory; (ii) it can be computed directly and efficiently from the input data, with fewer samples than those needed for empirical learning; (iii) it can be shown to be strongly consistent with empirical transferability measurements.
In this paper, we will first present the theoretical results of the proposed transferability metric in Section 2-4. Section 5 presents several experiments on real image data , including image classificaton tasks using the Cifar 100 dataset and 3D indoor scene understanding tasks using the Taskonomy dataset created by Zamir et al. (2018). A brief review of the related works is included in Section 6.

2 BACKGROUND
In this section, we will introduce the notations used throughout this paper, as well as some related concepts in Euclidean information theory and statistics.
X, x, X and PX represent a random variable, a value, the alphabet and the probability distribution respectively. PX denotes the vector with entries PX (x) and [ PX ] the diagonal matrix of
PX (X). For joint distribution PY X , PY X represents the |Y| × |X | probability matrix. Depending on the context, f(X) is either a |X |-dimensional vector whose entries are f (x), or a |X | × k feature matrix. Further, we define a task to be a tuple T = (X, Y, PXY ), where X is the training features and Y is the training label, and PXY the joint probability (possibly unknown). Subscripts S and T are used to distinguish the source task from the target task.

2.1 LOCAL INFORMATION GEOMETRY AND HYPOTHESIS TESTING

Our definiton of transferability uses concepts in local information geometry developed by Makur

et al. (2015), which characterizes probability distributions as vectors in the information space.

Consider the following binary hypothesis testing problem: test whether i.i.d. samples {x(i)}im=1 are

drawn from distribution P1 or distribution P2, where P1, P2 belong to an -neighborhood N (P0)

{P |

xX

(P (x)-P0(x))2 P0 (x)



2} centered at a reference distributon P0.

And denote i

=

Pi (x)-P0 (x)
P0 (x)

as the information vector corresponding to Pi for i = 1, 2.

Given k normalized feature functions f (x) = [f1(x), . . . , fk(x)], let (x) = [1(x), . . . , k(x)] be the matrix of feature vectors i(x) = P0(x)fi(x). It can be shown that the error exponent 1 corresponding to f (x) can be written as the squared length of a projection:

k k2

Efk =

Efi =

8 i, 1 - 2 2 + o( 2)

i=1 i=l

(See Appendix A for details.) In the rest of this paper, we assume is small.

(1)

2.2 DIVERGENCE TRANSITION MATRIX BD~e=finitioPnY1.-M1 PaYtrXix B~PisXthe-1D-ivergPeYncePTXraTn.sition Matrix (DTM) of a joint probability PY X if

The singular values of B~ satisfy that 1  1  · · ·  K = 0, K min{|X |, |Y|}. Let  = [1, . . . , K ] and  = [1, . . . , K ] be the left and right singular vectors of B~. Define functions

fi(x)

=

i(x)
PX (x)

and gi(y)

=

i(y)
PY (y)

for each i

=

1, . . . , K

- 1.

Makur et al. (2015) proved

that fi and gi are solutions to the maximal HGR correlation problem studied by Hirschfeld (1935);

Gebelein (1941); Re´nyi (1959), which finds the K strongest, independent modes in PXY from data.

Huang et al. (2017) further showed that they are the universal minimum error probability features in

the sense that they can achieve the smallest error probability over all possible inference tasks.

1which

characterizes

the

decay

of

error

probability

with

sample

size

m

(i.e.

limm

1 m

log(Pe)

=

Efk

)

2

Under review as a conference paper at ICLR 2019

3 MEASURING THE EFFECTIVENESS OF FEATURES IN CLASSIFICATION TASKS

In this section, we present a performance metric of a given feature representation for a learning task.

3.1 H-SCORE
For a classification task involving input variable X and label Y , most learning algorithms work by finding a k-dimensional functional representation f (x) of X that is most discriminative for the classification. To measure how effective f (x) is in predicting Y , rather than train the model via gradient descent and evaluate its accuracy, we present an analytical approach based on the definition below: Definition 2. Given data matrix X  Rm×d and label Y  {1, . . . , |Y|}. Let f (x) be a kdimensional, zero-mean feature function. The H-Score of f (x) with respect to the learning task represented by PY X is:
H(f ) = tr(cov(f (X))-1cov(EPX|Y [f (X)|Y ]))
This definition is intuitive from a nearest neighbor search perspective. i.e. a high H-score implies the inter-class variance cov(EPX|Y [f (X)|Y ]) of f is large, while feature redundancy tr(cov(f (X))) is small. Such a feature is discriminative and efficient for learning label Y. More importantly, H(f ) has a deeper operational meaning related to the asymptotic error probability for a decision rule based on f in the hypothesis testing context, discussed in the next section.

3.2 OPERATIONAL MEANING OF H-SCORE

Without loss of generality, we consider the binary classification task as a hypothesis testing problem defined in Section 2.1, with P1 = PX|Y =0, and P2 = PX|Y =1. For any k-dimensional feature representation f (x), we can quantify its performance with respect to the learning task using its error exponent Efk.
Theorem 1. Given PX|Y =0, PX|Y =1  N X (P0,X ) and features f such that E [f (X)] = 0 and E[f (X)f (X)T] = I, there exists some constant c independent of f such that Efk = cH(f ).

See Appendix B for the proof. The above theorem shows that H-score H(f ) is proportional to the error exponent of the decision function based on f (x) when f (x) is zero-mean with identity covariance. To compute the H-score of arbitrary f , we can center the features fS(x) - E[fS(x)], and incorporate normalization into the computation of the error exponent, which results in Definition 2.
The details are presented in Appendix C.

The proof for Theorem 1 uses the fact that H(f ) =

B~ 

2 F

,

where

B~

is

the

DTM

matrix,



[1 · · · k] is the matrix composed of information vectors i and c is a constant. This allows us to

infer an upper bound for the H-score of a given learning task:

Corollary 1. For all normalized feature f (x) = [f1(x), . . . , fk(x)], its H-score satisfies H(f ) 

k i=1

i2



k,

where

i

is

the

ith

singular

value

of

B~ .

The first inequality is achieved when  is composed of the right singular vectors of B~, i.e.
max ||B~||2F = ||B~T ||2F . The corresponding feature functions fopt(X)is in fact the same as the universal minimum error probability features from the maximum HGR correlation problem. The final
inequality in Corollary 1 is due to the fact all singular values of B~ are less than or equal to 1.

4 TRANSFERABILITY
Next, we apply H-score to efficiently measure the effectiveness of task transfer learning. We will also discuss how this approach can be used to solve the source task selection problem.
4.1 TASK TRANSFERABILITY
A typical way to transfer knowledge from the source task TS to target task TT is to train the target task using source feature representation fS(x). In a neural network setting, this idea can be implemented

3

Under review as a conference paper at ICLR 2019

input X

fS(x)

source task Ys
target task YT

frozen layers free layers

Figure 1: A simple neural network topology for transfer learning

by copying the parameters from the first N layers in the trained source model to the target model, assuming the model architecture on those layers are the same. The target classifier then can be trained while freezing parameters in the copied layers (Figure 1). Under this model, a natural way to quantify transferability is as follows:

Definition 3 (Task transferability). Given source task TS and target task TT , and trained source

feature representation fS(x), the transferability from TS to TT is T(S, T )

,HT (fS )
HT (fTopt )

where

fTopt (x) is the minimum error probability feature of the target task.

The statement T(S, T ) = r means the error exponent of transfering from TS via feature representation

fS

is

1 r

of

the

optimal

error

exponent

for

predicting

the

target

label

YT .

This

definition

also

implies

0  T(S, T )  1, which satisfies the data processing inequality if we consider the transferred feature

fS(X) as post-processing of input X for solving the target task. And it can not increase information about predicting the target task T .

A common technique in task transfer learning is fine-tuning, which adds before the target classifier additional free layers, whose parameters are optimized with respect to the target label. For the operational meaning of transferability to hold exactly, we require the fine tuning layers consist of only linear transformations, such as the model illustrated in Figure 2. It can be shown that under the local assumption, H-score is equivalent to the log-loss of the linear transfer model up to a constant offset (Appendix D). Nevertheless, later we will demonstrate empirically that this transferability metric can still be used for comparing the relative task transferability with fine-tuning.

4.2 EFFICIENT COMPUTATION OF TRANSFERABILITY

With a known f , computing H-score from m sample data only takes O(mk2) time, where k is the dimension of f (x) for k < m. The majority of the computation time is spent on computing the sample covariance matrix cov(f (X)).

The remaining question is how to obtain fTopt efficiently. We use the fact that HT (fopt) = ||B~T ||2F = E[f (X)Tg(Y )], where f and g are the solutions of the HGR-Maximum Correlation problem. This
problem can be solved efficiently using the Alternative Conditional Expectation (ACE) algorithm for a discrete alphabet X (Makur et al. (2015)), or equivalently by solving the following problem for a
continuous scenario:

max 2E[f (X)Tg(Y )] - tr(cov(f (X))cov(g(Y )))
f,g
s.t. E[f (X)] = 0, E[g(Y )] = 0

(2)

Makur et al. (2015) showd that the sample complexity of ACE is only 1/k of the complexity of estimating PY,X directly. Hence transferability can be computed with much less samples than actually training the transfer network. In many cases though, the computation of HT (fopt) can even be skipped entirely, such as the problem below:
Definition 4 (Source task selection). Given N source tasks TS1 , . . . , TSN with labels YS1 , . . . , YSN and a target task TT with label YT . Let fS1 , . . . , fSN be the minimum error probability feature functions of the source tasks. Find the source task TSi that maximizes the testing accuracy of predicting YT with feature fSi .

4

Under review as a conference paper at ICLR 2019

fS(x) gs(y)

source task Ys

0.3 0.2

input X

fS(x)W target task W gs(y) YT

0.1

0.0

H-score: 2I(X; Y)

HY(f(x))

123456

dim(f(x))

Figure 2: Network topology of linear feature transfer. Figure 3: H-score and mutual information.

We can solve this problem by selecting the source task with the largest transferability to TT . In fact, we only need to compute the numerator in the transferability definition since the denominator is the same for all source tasks, i.e. argmaxi T(Si, T ) = argmaxi(HT (fSi,opt )/HT (fTopt ) = argmaxi H(fSi ).

4.3 RELATIONSHIP WITH MUTUAL INFORMATION

Under the local assumption that PX|Y  N (PX ), we can show that mutual information I(X; Y ) =

1 2

||B||2F

+ o(

2).

(See

Appendix

E

for

details.)

Hence

H-score

is

related

to

mutual

information

by

H(f (x))  2I(X; Y ) for any zero-mean features f (x) satisfying the aforementioned conditions.

Figure 3 compares the optimal H-score of a synthesized task when |Y| = 6 with the mutual information between input and output variables, when the feature dimension k changes. The value of H-score increases as k increases, but reaches the upper bound when k  6, since the rank of the joint probability between X and YT , as well as the rank of its DTM is 6. Function 2I(X; Y ), on the other hand, is constant for all k. It is slightly larger than H-score, due to the constant o( 2). We consider H-score a better transferability metric than mutual information since it is adaptive to feature dimensions, and can capture asymmetric transfer performance. For example, in the visual recognition domain, we found it's more effective to transfer from 3D tasks to 2D tasks than the other way around.

5 EXPERIMENTS
In this section, we validate and analyze our transferability metric through experiments on real image data. 2 The tasks considered cover both object classification and non-classification tasks in computer vision, such as depth estimation and 3D (occlusion) edge detection.
5.1 VALIDATION OF TRANSFER PERFORMANCE
To demonstrate that our transferability metric is indeed a suitable measurement for task transfer performance, we compare it with the empirical performance of transfering features learned from ImageNet 1000-class classification (Krizhevsky et al. (2012)) to Cifar 100-class classification (Krizhevsky & Hinton (2009)), using a network similar to Figure 1. Comparing to ImageNet-1000, Cifar-100 has smaller sample size and its images have lower resolution. Therefore it is considered to be a more challenging task than ImageNet, making it a suitable case for transfer learning. In addition, we use a pretrained ResNet-50 as the source model due to its high performance and regular structure.
Validation of H-score. The training data for the target task in this experiemnt consists of 20, 000 images randomly sampled from Cifar-100. It is further split 9:1 into a training set and a testing set. The transfer network was trained using stochastic gradient descent with batch size 20, 000 for 100 epochs.
Figure 4.a compares the H-score of transferring from five different layers (4a-4f) in the source network with the target log-loss and test accuracy of the respective features. As H-score increases, log-loss of the target network decreases almost linearly while the training and testing accuracy increase. Such behavior is consistent with our expectation that H-score reflects the learning performance of the target task. We also demonstrated that target sample size does not affect the relationship between H-score and log-loss (Figure 4.b).
2Test data and code can be found at https://goo.gl/uoXj8m

5

Under review as a conference paper at ICLR 2019

This experiment also showed another potential application of H-score for selecting the most suitable layer for fine-tuning in transfer learning. In the example, transfer performance is better when an upper layer of the source networks is transferred. This could be because the target task and the source task are inherently similar such that the representation learned for one task can still be discriminative for the other.

Hscore

2.0 5K

10K

1.5

20K 30K

1.0 50K

20 25 L3o0g-loss 35 40 45
(a) (b) (c)

Figure 4: H-score and transferability vs. the empirical transfer performance measured by log-loss, training and testing accuracy. a.) Performance of ImageNet-1000 features from layers 4a-4f for Cifar100 classification. b.) Effect of sample size (5K-50K) on H-score for Cifar 100. c.): Transferability from ImageNet-1000 to 4 different target tasks based on Cifar-100.

Validation of Transferability. We further tested our transferability metric for selecting the best target task for a given source task. In particular, we constructed 4 target classification tasks with 3, 5, 10, and 20 object categories from the Cifar-100 dataset. We then computed the transferability from ImageNet-1000 (using the feature representation of layer 4f) to the target tasks. The results are compared to the empirical transfer performance trained with batch size 64 for 50 epochs in Figure4.c. We observe a similar behavior as the H-score in the case of a single target task in Figure 4.a, showing that transferability can directly predict the empirical transfer performance.

5.2 TASK TRANSFER FOR 3D SCENE UNDERSTANDING
In this experiment, we solve the source task selection problem for a collection of 3D sceneunderstanding tasks using the Taskonomy dataset from Zamir et al. (2018). In the following, we will introduce the experiment setting and explain how we adapt the transferability metric to pixel-to-pixel recognition tasks. Then we compare transferability with task affinity, an empirical transferability metric proposed by Zamir et al. (2018).
Data and Tasks. The Taskonomy dataset contains 4,000,000 images of indoor scenes of 600 buildings. Every image has annotations for 26 computer vision tasks. We randomly sampled 20, 000 images as training data. Eight tasks were chosen for this experiment, covering both classifications and lower-level pixel-to-pixel tasks. Table 6 summaries the specifications of these tasks and sample outputs are shown in Figure 5.

Figure 5: Ground truth of different tasks for a given query image.
Feature Extraction and Data Preprocessing. For each task, Zamir et al. (2018) trained a fully supervised network with an encoder-decoder structure. When testing the transfer performance from TS to TT , the encoder output of TS is used for training the decoder of TT . For a fair comparison, we use the same trained encoders to extract source features. The output dimension of all encoders are 16 × 16 × 8 and we flatten the output into a vector of length 2048. To reduce the computational complexity, we also resize the ground truth images into 64 × 64.
For classification tasks, H-score can be easily calculated given the source features. But for pixel-topixel tasks such as Edges and Depth, their ground truths are represented as images, which can not
6

Under review as a conference paper at ICLR 2019

be quantized easily. As a workaround, we cluster the pixel values in the ground truth images into a palette of 16 colors. Then compute the H-score of the source features with respect to each pixel, before aggregating them into a final score by averaging over the whole image.
We ran the experiment on a workstation with 3.40 GHz ×8 CPU and 16 GB memory. Each pairwise H-score computation finished in less than one hour including preprocessing. Then we rank the source tasks according to their H-scores of a given target task and compare the ranking with that in Zamir et al. (2018).

Tasks
2D Edges 3D Occlusion Edges 2D Keypoints 3D Keypoints Reshading Depth
Object Class. Scene Class.

k
2048 2048
2048 2048 2048 2048 2048 2048

Output
images images
images images images images labels labels

Quantize -level
16 16
16 16 16 16
none none

DCG

Transferability Task Hierarchy

Affinity

Ranking Correlation

Figure 6: Task descriptions Figure 7: Ranking comparison between transferability and affinity score.

Pairwise Transfer Results. Source task ranking results using transferability and affinity are visualized side by side in Figure 7, with columns representing source tasks and rows representing target tasks. For classification tasks (the bottom two rows in the transferability matrix), the top two transferable source tasks are identical for both methods. The best source task is the target task itself, as the encoder is trained on a task-specific network with much larger sample size. Scene Class. and Object Class. are ranked second for each other, as they are semantically related. Similar observations can be found in 2D pixel-to-pixel tasks (top two rows). The results on lower rankings are noisier.
A slightly larger difference between the two rankings can be found in 3D pixel-to-pixel tasks, especially 3D Occlusion Edges and 3D Keypoints. Though the top four ranked tasks of both methods are exactly the four 3D tasks. It could indicate that these low level vision tasks are closely related to each other so that the transferability among them are inherently ambiguous. We also computed the ranking correlations between transferability and affinity using Spearman's R and Discounted Cumulative Gain (DCG). Both criterion show positive correlations for all target tasks. The correlation is especially strong with DCG as higher ranking entities are given larger weights.
The above observations inspire us to define a notion of task relatedness, as some tasks are frequently ranked high for each other. Specifically, we represent each task with a vector consisting of H-scores of all the source tasks, then apply agglomerative clustering over the task vectors. As shown in the dendrogram in Figure 7, 2D tasks and most 3D tasks are grouped into different clusters, but on a higher level, all pixel-to-pixel tasks are considered one category compared to the classifications tasks.

Figure 8: Ranking of second order transferability for all recog- Figure 9: First and second order

nition tasks

pixel-wise transferability to Depth.

Higher Order Transfer. Sometimes we need to combine features from two or more source tasks for better transfer performance. Our transferability definition can be easily adapted to high order feature transfer, by concatenating features from multiple source tasks.

7

Under review as a conference paper at ICLR 2019
Figure 8 shows the ranking results of all combinations of source task pairs for each target task. For all tasks except for 3D Occlusion Edges and Depth, the best seond-order source feature is the combination of the top two tasks of the first-order ranking. We examine the exception in Figure 9, by visualizing the pixel-by-pixel H-scores of first and second order transfers to Depth using a heatmap (lighter color implies a higher H-score). Note that different source tasks can be good at predicting different parts of the image. The top row shows the results of combining tasks with two different "transferability patterns" while the bottom row shows those with similar patterns. Combining tasks with different transferability patterns has a more significant improvement to the overall performance of the target task.
6 RELATED WORKS
Transfer learning. Transfer learning can be devided into two categories: domain adaptation, where knowledge transfer is achieved by making representations learned from one input domain work on a different input domain, e.g. adapt models for RGB images to infrared images (Wang & Deng (2018)); and task transfer learning, where knowledge is transferred between different tasks on the same input domain (Torrey & Shavlik (2010)). Our paper focus on the latter prolem.
Empirical studies on transferability. Yosinski et al. (2014) compared the transfer accuracy of features from different layers in a neural network between image classification tasks. A similar study was performed for NLP tasks by Conneau et al. (2017). Zamir et al. (2018) determined the optimal transfer hierarchy over a collection of perceptual indoor scene understanidng tasks, while transferability was measured by a non-parameteric score called "task affinity" derived from neural network transfer losses coupled with an ordinal normalization scheme.
Task relatedness. One approach to define task relatedness is based on task generation. Generalization bounds have been derived for multi-task learning (Baxter (2000)), learning-to-learn (Maurer (2009)) and life-long learning (Pentina & Lampert (2014)). Although these studies show theoretical results on transferability, it is hard to infer from data whether the assumptions are satisfied. Another approach is estimating task relatedness from data, either explicitly (Bonilla et al. (2008); Zhang (2013)) or implicitly as a regularization term on the network weights (Xue et al. (2007); Jacob et al. (2009)). Most works in this category are limited to shallow ones in terms of the model parameters.
Representation learning and evaluation. Selecting optimal features for a given task is traditionally performed via feature subset selection or feature weight learning. Subset selection chooses features with maximal relevance and minimal redundancy according to information theoretic or statistical criteria (Peng et al. (2005); Hall (1999)). The feature weight approach learns the task while regularizing feature weights with sparsity constraints, which is common in multi-task learningLiao & Carin (2006); Argyriou et al. (2007). In a different perspective, Huang et al. (2017) consider the universal feature selection problem, which finds the most informative features from data when the exact inference problem is unknown. When the target task is given, the universal feature is equivalent to the minimum error probability feature used in this work.
7 CONCLUSION
In this paper, we presented H-score, an information theoretic approach to estimating the performance of features when transferred across classification tasks. Then we used it to define a notion of task transferability in multi-task transfer learning problems, that is both time and sample complexity efficient. The resulting transferability metric also has a strong operational meaning as the ratio between the best achievable error exponent of the transferred representation and the minium error exponent of the target task.
Our transferability score successfully predicted the performance for transfering features from ImageNet-1000 classification task to Cifar-100 task. Moreover, we showed how the transferability metric can be applied to a set of diverse computer vision tasks using the Taskonomy dataset.
In future works, we plan to extend our theoretical results to non-classification tasks, as well as relaxing the local assumptions on the conditional distributions of the tasks. On the application side, as transferability tells us how different tasks are related, we hope to use this information to design better task hierarchies for transfer learning.
8

Under review as a conference paper at ICLR 2019
REFERENCES
Andreas Argyriou, Theodoros Evgeniou, and Massimiliano Pontil. Multi-task feature learning. In Advances in neural information processing systems, pp. 41­48, 2007.
Jonathan Baxter. A model of inductive bias learning. Journal of artificial intelligence research, 12: 149­198, 2000.
Shai Ben-David, Reba Schuller, et al. Exploiting task relatedness for multiple task learning. Lecture notes in computer science, pp. 567­580, 2003.
Edwin V Bonilla, Kian M Chai, and Christopher Williams. Multi-task gaussian process prediction. In Advances in neural information processing systems, pp. 153­160, 2008.
Alexis Conneau, Douwe Kiela, Holger Schwenk, Loic Barrault, and Antoine Bordes. Supervised learning of universal sentence representations from natural language inference data. arXiv preprint arXiv:1705.02364, 2017.
Jeff Donahue, Yangqing Jia, Oriol Vinyals, Judy Hoffman, Ning Zhang, Eric Tzeng, and Trevor Darrell. Decaf: A deep convolutional activation feature for generic visual recognition. In International conference on machine learning, pp. 647­655, 2014.
Yuqing Gao and Khalid M Mosalam. Deep transfer learning for image-based structural damage recognition. Computer-Aided Civil and Infrastructure Engineering.
Hans Gebelein. Das statistische problem der korrelation als variations-und eigenwertproblem und sein zusammenhang mit der ausgleichsrechnung. ZAMM-Journal of Applied Mathematics and Mechanics/Zeitschrift fu¨r Angewandte Mathematik und Mechanik, 21(6):364­379, 1941.
Mark Andrew Hall. Correlation-based feature selection for machine learning. 1999.
Hermann O Hirschfeld. A connection between correlation and contingency. Mathematical Proceedings of the Cambridge Philosophical Society, 31(04):520­524, 1935.
Shao-Lun Huang, Anuran Makur, Lizhong Zheng, and Gregory W Wornell. An information-theoretic approach to universal feature selection in high-dimensional inference. In Information Theory (ISIT), 2017 IEEE International Symposium on, pp. 1336­1340. IEEE, 2017.
Laurent Jacob, Jean-philippe Vert, and Francis R Bach. Clustered multi-task learning: A convex formulation. In Advances in neural information processing systems, pp. 745­752, 2009.
Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. 2009.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. In Advances in neural information processing systems, pp. 1097­1105, 2012.
Xuejun Liao and Lawrence Carin. Radial basis function network for multi-task learning. In Advances in Neural Information Processing Systems, pp. 792­802, 2006.
Anuran Makur, Fabia´n Kozynski, Shao-Lun Huang, and Lizhong Zheng. An efficient algorithm for information decomposition and extraction. In Communication, Control, and Computing (Allerton), 2015 53rd Annual Allerton Conference on, pp. 972­979. IEEE, 2015.
Andreas Maurer. Transfer bounds for linear feature learning. Machine learning, 75(3):327­350, 2009.
Hanchuan Peng, Fuhui Long, and Chris Ding. Feature selection based on mutual information criteria of max-dependency, max-relevance, and min-redundancy. IEEE Transactions on pattern analysis and machine intelligence, 27(8):1226­1238, 2005.
Anastasia Pentina and Christoph H. Lampert. A pac-bayesian bound for lifelong learning. In International Conference on International Conference on Machine Learning, pp. II­991, 2014.
9

Under review as a conference paper at ICLR 2019

Lorien Y Pratt. Discriminability-based transfer between neural networks. In Advances in neural information processing systems, pp. 204­211, 1993.
Alfre´d Re´nyi. On measures of dependence. Acta mathematica hungarica, 10(3-4):441­451, 1959.
C. K. Shie, C. H. Chuang, C. N. Chou, M. H. Wu, and E. Y. Chang. Transfer representation learning for medical image analysis. In 2015 37th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC), pp. 711­714, Aug 2015. doi: 10.1109/EMBC.2015.7318461.
Lisa Torrey and Jude Shavlik. Transfer learning. In Handbook of research on machine learning applications and trends: algorithms, methods, and techniques, pp. 242­264. IGI Global, 2010.
Mei Wang and Weihong Deng. Deep visual domain adaptation: A survey. Neurocomputing, 2018.
Ya Xue, Xuejun Liao, Lawrence Carin, and Balaji Krishnapuram. Multi-task learning for classification with dirichlet process priors. Journal of Machine Learning Research, 8(Jan):35­63, 2007.
Jason Yosinski, Jeff Clune, Yoshua Bengio, and Hod Lipson. How transferable are features in deep neural networks? In Advances in neural information processing systems, pp. 3320­3328, 2014.
Amir Zamir, Alexander Sax, William Shen, Leonidas Guibas, Jitendra Malik, and Silvio Savarese. Taskonomy: Disentangling task transfer learning. Computer Vision and Pattern Recognition (CVPR), 2018.
Yu Zhang. Heterogeneous-neighborhood-based multi-task local learning algorithms. In Advances in neural information processing systems, pp. 1896­1904, 2013.

APPENDIX A INFERENCE USING LOCAL INFORMATION GEOMETRY

Consider the binary hypothesis testing problem over m samples {x(i)}im=1 drawn i.i.d from distri-

butions

P1

or

P2,

with

P1, P2



N

(P0X ).

Let

i

=

Pi(x)-P0X (x)
P0X (x)

denote

the

information

vectors

corresponding to Pi for i = 1, 2.

Makur et al. (2015) uses local information geometry to connect the error exponent in hypothesis testing to the length of certain information vectors, summarized in the following two lemmas.
Lemma 1. Given zero-mean, unit variance feature function f (x) : X  R, the optimal error exponent (a.k.a. Chernoff exponent) of this hypothesis testing problem is

2

E= 8

1 - 2 2 + o( 2)

Lemma 2. Given zero-mean, unit variance feature function f (x) : X  R, the error exponent of a

mismatched

decision

function

of

the

form

l

=

1 m

m i=1

(f

(x(i)))

is

2
Ef = 8 , 1 - 2 2 + o( 2)

where (x) = P0(x)f (x) is the feature vector associated with f (x).

As our discussion of transferability mostly concerns with multi-dimensional features, we present the k-dimensional generalization of Lemma 2 below: (Equation 1 in the main paper.)

Lemma 3. Given k normalized feature functions f (x) = [f1(x), . . . , fk(x)], such that E[fi(X)] = 0

for all i, and cov(f (X)) = I , we define a k-d statistics of the form lk = (l1, ..., lk) where

li

=

1 m

m l=1

fi(x(l)).

Let

(x)

=

[1(x), . . . , k(x)]

be

the

corresponding

feature

vectors

with

i(x) = PX (x)fi(x).

k k2

Efk =

Efi =

8 i, 1 - 2 2 + o( 2)

i=1 i=l

(3)

10

Under review as a conference paper at ICLR 2019

Proof. According to Crame´r's theorem, the error exponent under Pi is

Ei() = min D(P ||Pi)
P ()
where  () P : EP [f (X)k] = EP1 [f (X)k] + (1 - )EP2 [f (X)k] . With the techniques developed in local information geometry, the above above problem is equivalent to the following problem:

1

Ei()

=

min
~k ()

2

~k - i

2

where () ~k : ~k , l = 1 + (1 - )2, l , l = 1, ..., k and i = 1, 2. Further, using

the equation ~k (x) =

k l=1

ll

(x)

+i

(x)

-(k

)

P0(x)+o( ) and the equation (k) = o( 2),

it

is easy to show that E1()

=

E1()

when 

=

1 2

.

Then the overall error probability has the

exponent as shown in Equation (1).

APPENDIX B PROOF OF THEOREM 1
To simplify the proof, we first consider the case when the feature function is 1-dimensional.i.e. f : X  R. We have the following lemma: Lemma 4. Let f : X  R be an arbitrary feature function of X such that E[f (X)] = 0 and E[f (X)2] = 1 , then ||B~||2F = EPY [(E[f (X)|Y ])2] where  is the feature vector corresponding to f.

Proof. Since (x) = PX (x)f (x), we have

||B~||2F = TB~TB~

=
yY
=
yY

PY X (y, x) - PX (x) PY (y) · PX (x)f (x) xX PX (x) PY (y)

2

xX

PY X (y, x) PY (y)

· f (x) -

xX

f (x)PX (x)

· PY (y)

2

= (E[f (X)|Y = y] - E[f (x)])2 · PY (y)
yY
= EPY [(E[f (X)|Y ])2]

The last equality uses the assumption that E[f (x)] = 0.

Theorem 2 (1D version of Theorem 1). Given PX|Y =0, PX|Y =1  N X (P0,X ) and features f : X  R such that E [f (X)] = 0 and E[f (X)2] = 1, then there exists some constant c independent

of f such that

Ef

=c

B~ 

2 F

(4)

Proof. From E[f (X)] = PY (0)E[f (X)|Y = 0] + PY (1)E[f (X)|Y = 1] = 0, we first derive the following properties of the conditional expectations of f (x):

E[f (X)|Y = 1] = - E[f (X)|Y = 0] = -

PY (0) PY (1) PY (1) PY (0)

E[f (X)|Y = 0] E[f (X)|Y = 1]

11

Under review as a conference paper at ICLR 2019

On the R.H.S. of Equation 4, we apply Lemma 4 to write

B~ 

2 F

= EPY

(E [f (X)|Y ])2

=

PY

(0)PY (1) + PY (0)

PY

(1)2

(E

[f

(X

)|Y

= 1])2

Next consider the L.H.S. of the equation, by Lemma 2, we have E(h) =

2
8

, 1 - 2

2 + o(

2) =

c0 , 1 - 2 2 for some constant c0.

c0 , P1 - P2 2

=c0 PX|Y =0(x) - PX|Y =1(x) · xX PX (x)

2
PX (x)f (x)

=c0 (E [f (X)|Y = 0] - E [f (X)|Y = 1])2

=c0

-

(PY (0) + PY (1))2 PY (0)PY (1)

· (E [f (X)|Y

= 1])(E [f (X)|Y

= 0])

=c0

PY (0) + PY (1) PY (0)

2
· (E [f (X)|Y = 1])2

=c0

PY (0) + PY (1) PY (0)PY (1)

PY (0)PY (1) + PY (1)2 PY (0)

· (E [f (X)|Y = 1])2

=c ||B~||F2

For k  2, Lemma 4 can be restated as follows:
Lemma 5. Let f : X  Rk be a k-dimensional feature function of X, where f (x) = [f1(x), . . . , fk(x)]. Further we assume that E[f (X)] = 0 and cov(f (X)) = E[f (X)Tf (X)] = I . Then ||B^||F2 = tr(cov(E[f (X)|Y ])) where columns of  are the information vectors corresponding to f1(x), . . . , fk(x).

Proof. First, note that B~ =

-1
PY PY X

-1 T
PX - PY PX

PX f(X)

= PY [PY ]-1 PY X f(X) - 1 · E[f (X)]T

where 1 is a column vector with all entries 1 and length |Y|. Since E[f (X)] = 0, we have B~ = PY [PY ]-1 PY X f(X)

It follows that

||B~||2F =tr(TB~TB~)

=tr

[PY ]-1 PY X f(X)

T
[PY ]

[PY ]-1 PY X f(X)

=tr EPY (E[f (X)|Y ]]T)T(E[f (X)|Y ]]T) =tr (cov(E[f (X)|Y ]))

Finally, we derive the multi-dimensional case for Theorem 1. 12

Under review as a conference paper at ICLR 2019

Proof of Theorem 1. Using Lemma 5 and a similar argument as in the simplified proof, the R.H.S of the equation becomes

||B~||2F =tr (cov(E[f (X)|Y ]))

=tr EPY (E[f (X)|Y ]]T)T(E[f (X)|Y ]]T)

=PY (0)E[f (X)|Y = 0]]TE[f (X)|Y = 0]] + PY (1)E[f (X)|Y = 1]]TE[f (X)|Y = 1]]

=

PY

(0)PY (1) + PY (0)

PY

(1)2

(E

[f

(X

)|Y

= 1])T(E [f (X)|Y

= 1])

By Lemma 3, the L.H.S. of the equation can be written as Efk = c0

k i=l

i, 1 - 2 2 for some

constant c0. It follows that

k
c0 i, 1 - 2 2
i=l
=c0 PX|Y =0 - PX|Y =1

T f(X)

PX|Y =0 - PX|Y =1 T f(X) T

=c0 (E[f (X)|Y = 0] - E[f (X)|Y = 1])T (E[f (X)|Y = 0] - E[f (X)|Y = 1])

=c0

PY (0) + PY (1) PY (0)PY (1)

PY (0)PY (1) + PY (1)2 PY (0)

· E [f (X)|Y = 1]T E [f (X)|Y = 1]

=c

B~ 

2 F

APPENDIX C GENERALIZATION OF H-SCORE FOR ARBITRARY FEATURE SET
 Since  = PX f(X) and E[f (X)] = 0, we have

T =

T
PX f(X)

PX f(X) = E[f (X)Tf (X)] = cov(f (X))

(5)

Equation (5) gives a more understandable expression of the normalization term. We can also write B~ as follows:

B~ =

-1
PY PY X

-1 T
PX - PY PX

PX f(X)

= PY [PY ]-1 PY X f(X) - 1 · E[f (X)]T

where 1 is a column vector with all entries 1 and length |Y|, we have

TB~TB~ =

[PY ]-1 PY X f(X) - 1 · E[f (X)]T

T
[PY ]

[PY ]-1 PY X f(X) - 1 · E[f (X)]T

=EPY (E[f (X)|Y ] - 1 · E[f (X)]T)T(E[f (X)|Y ] - 1 · E[f (X)]T) =cov (E[f (X)|Y ])

(6)

On the other hand,

||B~

(T

)-

1 2

||F2

=tr

(T

)-

1 2

T

B~

T

B~

(T

)-

1 2

=tr (T)-1TB~TB~

=tr cov(f (X))-1cov(E[f (X)|Y ])

(7)

The last equality is derived by substituting (5) and (6) into (7).

13

Under review as a conference paper at ICLR 2019

Figure 10: Comparing task transferability (H-score) and transfer log-loss on synthesized tasks.

APPENDIX D H-SCORE AND SOFTMAX REGRESSION

In softmax regression, given m training examples {(x(i), y(i))}im=1, the cross-entropy loss of the model is

m |Y|

(f, ) = -

1{y(i) = k} log

i=1 k=1 m
- log QY |X (y(i)|x(i))
i=1

e-kT f (x(i)) eC -jT f (x(i))
j=1

(8) (9)

= - EPY X [log QY |X (Y |X)]

(10)

Minimizing is equivalent to minimizing D PY X ||PX QY |X where PY X is the joint empirical distribution of (X, Y ).

Using information geometry, it can be shown that under a local assumption

argmin D
f,

PY X ||PX QY |X

1 = argmin
, 2

B~ - T

2 F

+ o(

2)

(11)

which reveals a close connection between log loss and the modal decomposition of B~. In consequence,

it is reasonable to measure the classification performance with

B~ - T

2 F

given a pair of (f, )

associated with (, ).

In the context of estimating transferability, we are interested in a one-sided problem, where S is given by the source feature and  becomes the only variable.

min


B~T - TS

(12)

Training the network is equivalent to finding the optimal weight  that minimizes the log-loss. By

taking the derivative of the Objective function with respect to , we get

 = B~T S (TS S )-1

(13)

Substituting (13) in the Objective of (12), we can derive the following close-form solution for the log

loss.

B~TTB~T

2 F

-

B~T

S

(TS

S

)-

1 2

2 F

(14)

The first term in (14) is fixed given TT while the second term has exactly the form of H-score. This implies that log loss is negatively linearly related to H-score.

We demonstrates this relationship experimentally, using a collection of synthesized tasks (Figure 10).
In particular, the target task is generated based on a random stochastic matrix PY0|X , and 20 source tasks are generated with the conditional probability matrix PYi|X = PY0|X + iI for some positive constant .

The universal minimum error probability features for each source task are used as the source features
fSi (x), while the respective log-loss are obtained through training a simple neural network in Figure 2 with cross-entropy loss. The relationship is clearly linear with a constant offset.

14

Under review as a conference paper at ICLR 2019

APPENDIX E DTM AND MUTUAL INFORMATION

Proposition 1. Under the local assumption that PX|Y  N (PX ), mutual information I(X; Y ) =

1 2

||B~ ||F2

+ o(

2)

,

where

B~

of

the

DTM

matrix

of

X

and

Y.

Proof.

First, we define yX|Y (x) =

PX|Y (x|y)-PX (x)
PX (x)

and let X|Y

 R|X |×|Y| denote its matrix

version. Then we have

X|Y [ PY ] = [ PX ]-1(PXY - PX )[ PY ] = B~T

Next, we express the mutual information in terms of information vector X|Y ,

I(X; Y ) = I(Y ; X) = PY (y)DKL(PX|Y ||PX )

yY

2

= 2

PY (y)||yX|Y ||2 + o( 2)

yY

= 1 || X|Y [ 2

PY ]||2F + o( 2)

=

1 2

||B~ ||F2

+ o(

2)

APPENDIX F SUPPLEMENTARY RESULTS ON EXPERIMENT 5.2

F.1 COMPARISON OF H-SCORES AND AFFINITIES
Table 1, with columns representing source tasks and rows representing target tasks. For each target task, the upper row shows our results while the lower one shows the results in Zamir et al. (2018). Score values are included in parenteses.
Table 1: Transferability ranking comparison, between H-score's estimation and task affinity

Tasks

2D Edges

2D Edges

1 (1.8216) 1 (0.0389)

2D Keypoints 2 (1.6698) 2 (0.0002)

3D Edges

5 (1.4828) 6 (0.0117)

3D Keypoints 6 (1.5375) 5 (0.0141)

Reshading

5 (1.5504) 6 (0.0147)

Depth

6 (1.6542) 7 (0.0175)

Object Class. 5 (22.866) 7 (0.0205)

Scene Class. 5 (14.575) 8 (0.0149)

2D Keypoints 2 (1.7334) 2 (0.0117) 1 (1.7859) 1 (0.0542) 4 (1.4910) 7 (0.0108) 5 (1.5466) 6 (0.0136) 6 (1.5426) 8 (0.0143) 5 (1.6870) 8 (0.0154) 4 (23.627) 6 (0.0217) 4 (15.074) 7 (0.0165)

3D Edges 5 (1.5704) 4 (5.8920e-5) 7 (1.5248) 7 (7.7797e-5) 3 (1.5167) 1 (0.1179) 4 (1.5910) 2 (0.0531) 3 (1.8174) 2 (0.0781) 3 (1.8504) 3 (0.0595) 7 (22.371) 3 (0.0350) 7 (14.206) 3 (0.0335)

3D Keypoints 6 (1.5696) 3 (8.8011e-5) 5 (1.5287) 5 (8.1029e-5) 7 (1.4701) 2 (0.0734) 3 (1.6456) 1(0.1275) 4 (1.7990) 4(0.0545) 4 (1.8176) 4 (0.0617) 8 (21.950) 4 (0.0318) 8 (13.801) 4 (0.0305)

Reshading 4 (1.6146) 7 (2.9001e-5) 4 (1.5481) 6 (7.8464e-5) 2 (1.5405) 4 (0.0622) 1 (1.7198) 3 (0.0400) 1 (2.2339) 1 (0.1121) 2 (2.1700) 2 (0.0867) 6 (22.452) 5 (0.0286) 6 (14.332) 5 (0.0263)

Depth 3 (1.6201) 8 (2.2110e-5) 3 (1.5632) 8 (7.2724e-5) 1 (1.6739) 3 (0.0636) 2 (1.7122) 4 (0.0247) 2 (2.1200) 3 (0.0765) 1 (2.2441) 1 (0.0989) 3 (23.697) 8 (0.0147) 3 (15.474) 6 (0.0198)

Object Class. 7 (1.5097) 5 (4.9141e-5) 6 (1.5253) 3 (0.0002) 8 (1.4644) 8 (0.0094) 7 (1.4709) 7 (0.0132) 7 (1.4774) 7 (0.0144) 7 (1.6008) 6 (0.0217) 1 (33.468) 1 (0.0959) 2 (25.750) 2 (0.0504)

Scene Class. 8 (1.4402) 6 (4.8720e-5) 8 (1.4725) 4 (0.0001) 6 (1.4730) 5 (0.0151) 8 (1.4121) 8 (0.0121) 8 (1.3804) 5 (0.0174) 8 (1.5099) 5 (0.0237) 2 (28.013) 2 (0.0774) 1 (25.962) 1 (0.1474)

Here we present some detailed results on the comparison between H-score and the affinity score in Zamir et al. (2018) for pairwise transfer.
The results of the classification tasks are shown in Figure 11 and the results of Depth is shown in 12. We can see in general, although affinity and transferability have totally different value ranges, they tend to agree on the top few ranked tasks.

15

Under review as a conference paper at ICLR 2019
Figure 11: Source task transferability ranking for classification tasks. For each target task, the left figure shows H-score results, and the right figure shows task affinity results.
Figure 12: Comparison between source task rankings for Depth. with H-score results on the left and affinity scores Zamir et al. (2018) on the right. The Top 3 transferable source tasks in both methods are the same: Depth, Image Reshading and 3D Occlusion Edges. F.2 QUANTIZATION During the quantization process of the pixel-to-pixel task labels (ground truth images), we are primarily concerned with two factors: computational complexity and information loss. Too much information loss will lead to bad approximation of the original problems. On the other hand, having little information loss requires larger cluster size and computation cost. Figure (13) shows that even after quantization, much of the information in the images are retained. To test the sensitivity of the cluster size, we used cluster centroids to recover the ground truth image pixel-by-pixel. The 3D occlusion Edge detection results on a sample image is shown in Figure 14.
Figure 13: Quantization. Recover is done with the centroid of corresponding cluster of each pixel. 16

Under review as a conference paper at ICLR 2019 When the cluster number is set to N = 5 (right), most detected Edges in the ground truth image (left) are lost. We found that N = 16 strikes a good balance between recoverability and computation cost.
Figure 14: Effect of quantization cluster size for 3D occlusion Edge detection.
17


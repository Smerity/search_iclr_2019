Under review as a conference paper at ICLR 2019
TOWARDS RESISTING LARGE DATA VARIATIONS VIA INTROSPECTIVE LEARNING
Anonymous authors Paper under double-blind review
ABSTRACT
Learning deep networks which can resist large variations between training and testing data is essential to build accurate and robust image classifiers. Towards this end, a typical strategy is to apply data augmentation to enlarge the training set. However, standard data augmentation is essentially a brute-force strategy which is inefficient, as it performs all the pre-defined transformations to every training sample. In this paper, we propose a principled approach to train networks with significantly improved resistance to large variations between training and testing data. This is achieved by embedding a learnable transformation module into the introspective networks (Jin et al., 2017; Lazarow et al., 2017; Lee et al., 2018), which is a convolutional neural network (CNN) classifier empowered with generative capabilities. Our approach alternatively synthesizes pseudo-negative samples with learned transformations and enhances the classifier by retraining it with synthesized samples. Experimental results verify that our approach significantly improves the ability of deep networks to resist large variations between training and testing data and achieves classification accuracy improvements on several benchmark datasets, including MNIST, affNIST, SVHN and CIFAR-10.
1 INTRODUCTION
Classification problems have rapidly progressed with advancements in convolutional neural networks (CNNs) (LeCun et al., 1989; Krizhevsky et al., 2012; Simonyan & Zisserman, 2014; Szegedy et al., 2015; He et al., 2016; Huang et al., 2017). CNNs are able to produce promising performance, given sufficient training data. However, when the training data is limited and unable to cover all the data variations in the testing data (e.g., the training set is MNIST, while the testing set is affNIST), the trained networks generalize poorly on the testing data. Consequently, how to learn deep networks which can resist large variations between training and testing data is a significant challenge for building accurate and robust image classifiers.
To address this issue, a typical strategy is to apply data augmentation to enlarging the training set, i.e., applying various transformations, including random translations, rotations and flips as well as Gaussian noise injection, to the existing training data. This strategy is very effective in improving the performance, but it is essentially a brute-force strategy which is inefficient, as it exhaustively performs all these transformations to every training samples. Neither is it theoretically formulated.
Alternatively, we realize that we can synthesize extra training samples with generative models. But, the problem is how to generate synthetic samples which are able to improve the robustness of CNNs to large variations between training and testing data. In this paper, we achieve this by embedding a learnable transformation module into introspective networks (Jin et al., 2017; Lazarow et al., 2017), a CNN classifier empowered with generative capabilities. We name our approach introspective transformation network (ITN), which performs training by a reclassification-by-synthesis algorithm. It alternatively synthesizes samples with learned transformations and enhances the classifier by retraining it with synthesized samples. We use a min-max formulation to learn our ITN, where the transformation module transforms the synthesized pseudo-negative samples to maximize their variations to the original training samples and the CNN classifier is updated by minimizing the classification loss of the transformed synthesized pseudo-negative samples. The transformation modules are learned jointly with the CNN classifier, which augments training data in an intelligent manner by narrowing down the search space for the variations.
1

Under review as a conference paper at ICLR 2019
Our approach can work with any models that have generative and discriminative abilities, such as generative adversarial networks (GANs) and introspective networks. In this paper, we choose the introspective networks to generate extra training samples rather than GANs, because introspective networks have several advantages over GANs. Introspective learning framework maintains one single CNN discriminator that itself is also a generator while GANs have separate discriminators and generators. The generative and discriminative models are simultaneously refined over iterations. Additionally, Introspective networks are easier to train than GANs with gradient descent algorithms by avoiding adversarial learning.
The main contribution of the paper is that we propose a principled approach that endows classifiers with the ability to resist larger variations between training and testing data in an intelligent and efficient manner. Experimental results show that our approach achieves better performance than standard data augmentation on both classification and cross-dataset generalization. Furthermore, we also show that our approach has great abilities in resisting different types of variations between training and testing data.
2 RELATED WORK
In recent years, a significant number of works have emerged focus on resisting large variations between training and testing data. The most widely adopted approach is data augmentation that applies pre-defined transformations to the training data. Nevertheless, this method lacks efficiency and stability since the users have to predict the types of transformations and manually applies them to the training set. Better methods have been proposed by building connections between generative models and discriminative classifiers (Friedman et al., 2001; Liang & Jordan, 2008; Tu et al., 2008; Jebara, 2012; Welling et al., 2003). This type of methods capture the underlying generation process of the entire dataset. The discrepancy between training and test data is reduced by generating more samples from the data distribution.
GANs (Goodfellow et al., 2014) have led a huge wave in exploring the generative adversarial structures. Combining this structure with deep CNNs can produce models that have stronger generative abilities. In GANs, generators and discriminators are trained simultaneously. Generators try to generate fake images that fool the discriminators, while discriminators try to distinguish the real and fake images. Many variations of GANs have emerged in the past three years, like DCGAN (Radford et al., 2015), WGAN (Arjovsky et al., 2017) and WGAN-GP (Gulrajani et al., 2017). These GANs variations show stronger learning ability that enables generating complex images. Techniques have been proposed to improve adversarial learning for image generation (Salimans et al., 2016; Gulrajani et al., 2017; Denton et al., 2015) as well as for training better image generative models (Radford et al., 2015; Isola et al., 2017).
Introspective networks (Tu, 2007; Lazarow et al., 2017; Jin et al., 2017; Lee et al., 2018) provide an alternative approach to generate samples. Introspective networks are closely related to GANs since they both have generative and discriminative abilities but different in various ways. Introspective networks maintain one single model that is both discriminative and generative at the same time while GANs have distinct generators and discriminators. Introspective networks focus on introspective learning that synthesizes samples from its own classifier. On the other hand, GANs emphasize adversarial learning that guides generators with separate discriminators. The generators in GANs are mappings from the features to the images. However, Introspective networks directly models the underlying statistics of an image with an efficient sampling/inference process.
3 METHOD
We now describe the details of our approach in this section. We first briefly review the introspective learning framework proposed by (Tu, 2007). This is followed by our the detailed mathematical explanation of our approach. In particular, we focus on explaining how our model generates unseen examples that complement the training dataset.
3.1 INTROSPECTIVE LEARNING
We only briefly review introspective learning for binary-class problems, since the same idea can be easily extended to multi-class problems. Let us denote x  Rd as a data sample and y  +1, -1
2

Under review as a conference paper at ICLR 2019

as the corresponding label of x. The goal of introspective learning is to model positive samples by learning the generative model p(x|y = +1). Under Bayes rule, we have

p(x|y

=

+1)

=

p(y p(y

= =

+1|x)p(y -1|x)p(y

= =

-1) p(x|y +1)

=

-1),

(1)

where p(y|x) is a discriminative model. For pedagogical simplicity, we assume p(y = 1) = p(y = -1) and this equation can be further simplified as:

p(x|y

=

+1)

=

p(y p(y

= =

+1|x) -1|x)

p(x|y

=

-1).

(2)

The above equation suggests that a generative model for the positives p(x|y = +1) can be obtained from the discriminative model p(y|x) and a generative model p(x|y = -1) for the negatives. However, to faithfully learn p(x|y = +1), we need to have a representative p(x|y = -1), which is very difficult to obtain. A solution was provided in (Tu, 2007) which learns p(x|y = -1) by using an iterative process starting from an initial reference distribution of the negatives p0(x|y = -1), e.g., p0(x|y = -1) = U (x), a Gaussian distribution on the entire space Rd. This is updated by

pt+1(x|y

=

-1)

=

1 Zt

qt(y qt(y

= =

+1|x) -1|x)

pt

(x|y

=

-1),

(3)

where qt(y|x) is a discriminative model learned on a given set of positives and a limited number

of pseudo-negatives sampled from pt(x|y = -1) and Zt =

qt qt

(y=+1|x) (y=-1|x)

pt

(x|y

=

-1)dx

is

the

normalizing factor. It has been proven that KL(p(x|y = +1)||pt+1(x|y = -1))  KL(p(x|y =

+1)||pt(x|y = -1))) (as long as each qt(y|x) makes a better-than-random prediction, the inequal-

ity holds) in (Tu, 2007), where KL(·||·) denotes the Kullback-Leibler divergences, which implies

pt(x|y = -1) t= p(x|y = +1). Therefore, gradually learning pt(x|y = -1) by following this

iterative process of Eqn.(3), the samples drawn from x  pt(x|y = -1) become indistinguishable

from the given training samples.

3.2 LARGE VARIATIONS RESISTANCE VIA INTROSPECTIVE LEARNING

Introspective Convolutional Networks (ICN) (Jin et al., 2017) and Wasserstein Introspective Neural Networks (WINN) (Lee et al., 2018) adopt the introspective learning framework and strengthen the classifiers by a reclassification-by-synthesis algorithm. However, both of them fail to capture large data variations between the training and testing data, since most of the generated pseudo-negatives are very similar to the original samples. But in practice, it is very common that the test data contain unseen variations that are not in training data, such as the same objects viewed from different angles and suffered from shape deformation.

To address this issue, we present our approach building upon the introspective learning framework to resist large data variations between training and test data. Arguably, even large training sets cannot fully contains all the possible variations. Our goal is to quickly generate extra training samples with beneficial unseen variations that is not covered by the training data to help classifiers become robust. We assume that we can generates such training samples by applying a transformation function T (· ; ) parametrized by learnable parameters  to the original training samples. Let us denote g(· ; ) as the function that maps the samples x to the transformation parameters , where  is the model parameter of the function g. The generated samples still belong to the same category of the original samples, since the transformation function T only changes the high-level geometric properties of the samples. The outline of training procedures of ITN is presented in Algorithm 1. We denote S+ = {(x+i , +1), i = 1...|S+|} as the positive sample set, Tt(S+) = {(xiT , +1), i = 1...|S+|, xTi = T (x+i ; t)} as the transformed positive sample set at tth iteration with transformation parameter t and St- = {(x-i , -1), i = 1...|S-|} as the set of pseudonegatives drawn from pt(x|y = -1). We then will describe the detail of the training procedure.

Discriminative model We first demonstrate the approach of building robust classifiers with given t. For a binary classification problem, at tth iteration, the discriminative model is represented as

qt(y|x; t)

=

1 1 + exp(-y

ft(x; t))

(4)

3

Under review as a conference paper at ICLR 2019

Algorithm 1: Outline of ITN Training Algorithm
1: Input: Positive sample set S+, initial reference distribution p0(x|y = -1) and transformation function T 2: Output: Parameters ,  and  3: Build S0- by sampling |S+| pseudo-negatives samples from p0(x|y = -1) 4: initialize parameters ,  and , set t = 1
5: while not converge do 6: for each x+i  S+ and xi-  St- do 7: Compute transformation parameters i = g(x+i ; ) 8: Choose i  U (0, 1) and compute x^i = iT (xi+; i) + (1 - i)xi- 9: end for
10: Compute ,  by Eqn.(6)
11: Compute  by Eqn.(8) 12: Sample pseudo-negatives samples Zt = {zit, i = 1, ..., |S+|} from p0(x|y = -1) 13: Update all samples in Zt by Eqn.(12) 14: Augment pseudo-negatives sample set St- = St--1  {(zit, -1), i = 1, ..., |S+|} and t = t + 1 15: end while

where t represents the model parameters at iteration t, and ft(x; t) represents the model output at tth iteration. Note that, qt(y|x; t) is trained on S+, T (S+; t) and pseudo-negatives drawn from pt(x|y = -1). In order to achieve stronger ability in resisting unseen variations, we want the distribution of T (S+; t) to be approximated by the distribution of pseudo negatives pt(x|y = -1),
which can be achieved by minimizing the following Wasserstein distance (Gulrajani et al., 2017):

D(t, t) = ExT T (S+;t)[ft(xT ; t, t)] - Ex-St- [ft(x-; t, t)] + Ex^X^t [

x^ft(x^; t, t) - 1

2 2

],

(5)

where t is the extra parameter together with ft(·; t) to compute the Wasserstein distance. Each x^

in the set X^t is computed with the formula x^ = xT + (1 - )x-, where samples from uniform distribution U (0, 1), xT  T (S+; t) and x-  St-. The term (||x^ft(x^; t)||2 - 1)2 is the gradient penalty that stabilizes the training procedure of the Wasserstein loss function.

The goal of the discriminative model is to correctly classify any given x+, xT and x-. Thus, the objective function of learning the discriminative model at iteration t is

min J (t) + D(t, t),
t ,t

where

J (t) = E(x,y)S+St-T (S+;t)[-log q(y|x; t)]

(6)

The classifiers obtain the strong ability in resisting unseen variations by training on the extra samples while preserving the ability to correctly classify the original samples. We discussed the binary classification case above. When dealing with multi-class classification problems, it is needed to adapt the above reclassification-by-synthesis scheme to the multi-class case. We can directly follow the strategies proposed in (Jin et al., 2017) to extend ITN to deal with multi-class problems by learning a series of one-vs-all classifiers or a single CNN classifier.

Exploring variations. The previous section describes how to learn the robust classifiers when the
t is given. However, t is unknown and there are huge number of possibilities to selecting t. Now, the problem becomes how do we learn the t in a principled manner and apply it towards building robust classifiers? We solve this issue by forming a min-max problem upon the Eqn.(6):

min max J(, ) + D(, , ),
, 

(7)

Here, we rewrite J() and D(, ) in Eqn.(5) and Eqn.(6) as J(, ) and D(, , ), since  is
now an unknown variable. We also subsequently drop the subscript t for notational simplicity. This
formulation gives us a unified perspective that encompasses some prior work on building robust
classifiers. The inner maximization part aims to find the transformation parameter  that achieves
the high loss values. On the other hand, the goal of the outer minimization is expected to find the the model parameters  that enables discriminators to correctly classify xT and  allows the negative distribution to well approximate the distribution of T (S+; t) . However, direclty solving Eqn. 7 is difficult. Thus, we break this learning process and first find a  that satisfies

max 

E(xT ,y)T (S+;)[- log(q(y|xT ))] + ExT T (S+;)[f (xT ; , )] + Ex^X^ [

x^f (x^; , ) - 1

2 2

]

(8)

4

Under review as a conference paper at ICLR 2019

where  and  are fixed. Then,  and  are learned with Eqn.(6) by keep  = . Empirically, the first term in the Eqn. 8 dominates over other terms, therefore we can drop the second and third terms to focus on learning more robust classifiers. The purpose of empirical approximation is to find the  that make xT hard to classify correctly. Instead of enumerating all possible examples in the data augmentation, Eqn.(8) efficiently and precisely finds a proper  that increase the robustness of the current classifiers.

We use g(· ; ) to learn , thus  = g(x; )+, where  is random noise follows the standard normal distribution. The function parameter  is learned by Eqn.(8). Notably, following the standard backpropagation procedure, we need to compute the derivative of the transformation function T in each step. In other words, the transformation function T (·; ) need to be differentiable with respect to the parameter  to allow the gradients to flow through the transformation function T
when learning by backpropagation.

Generative model In the discriminative models, the updated discriminative model p(y|x) is learned by Eqn.(6). The updated discriminative model is then used to compute the generative model by the Eqn.(3) in section 3.1. The generative is learned by maximizing the likelihood function p(x). However, directly learning the generative model is cumbersome since we only need samples from the latest generative model.

Let's denote initial reference distribution as p-0 (x) and pn(x|y = -1) as p-n (x) for simplicity. Following standard introspective learning, we can approximate samples drawn from latest negative distribution by first sampling from p0-(x) and iteratively update them to approach desired samples. With p0- and Eqn.(3), we have

p-n (x)

=

n-1
(
t=1

1 Zt

qt(y qt(y

= =

+1|x) -1|x)

)p0-

(x),

(9)

where Zt indicates the normalizing factor at tth iteration. The random samples x are updated by

increasing maximize the log likelihood of pn-(x). Note that maximizing log p-n (x) can be simplified

as maximizing

n-1 t=1

qt (y =+1|x) qt (y =-1|x)

since

Zt

and

p-0

are

fixed

in

Eqn.(9).

From

this

observation,

we

can directly learn a model ht(x) such that

ht(x)

=

qt(y qt(y

= =

+1|x) -1|x)

=

exp(ft(x; t))

(10)

Taking natural logarithm on both side of the equation above, we can get ln ht(x) = ft(x; t). Therefore, log p-n (x) can be rewritten as

log

p-n (x)

=

n-1
log(
t=1

1 Zt

qt(y qt(y

= =

+1|x) -1|x)

)p0-(x)

=

C

n-1 t=1

ft(x;

t)

p0-(x),

(11)

where C is the constant computed with normalizing factors Zt. This conversion allows us to max-

imize log p-n (x) by maximizing

n-1 t=1

ft(x; t).

By

taking

the

derivative

of

log pn-(x),

the

update

step x is:

n-1

x = ( ft(x; t)) + ,

(12)

t=1
where   N (0, 1) is the random Gaussian noise and  is the step size that is annealed in the

sampling process. In practice, we update from the samples generated from previous iterations to

reduce time and memory complexity. An update threshold is introduced to guarantee the generated

negative images are above certain criteria, which ensures the quality of negative samples. We modify

the update threshold proposed in (Lee et al., 2018) and keep track of the ft(x; t) in every iteration. In particular, we build a set D by recording E[ft(x; t)], where x  S+ in every iteration. We form a normal distribution N (a, b), where a and b represents mean and standard deviation computed from

set D. The stop threshold is set to be a random number sampled from this normal distribution. The

reason behind this threshold is to make sure the generated negative images are close to the majority

of transformed positive images in the feature space.

4 EXPERIMENTS

In this section, we demonstrate the ability of our algorithm in resisting the large variations between training and testing data through a series of experiments. First, we show the outstanding classification performance of ITN on several benchmark datasets. We also analyze the properties of the

5

Under review as a conference paper at ICLR 2019

generated examples from different perspectives. We then further explore the ability of our algorithm in resisting large variations with two challenging classification tasks and show the consistently better performance. Finally, we illustrate the flexibility of our architecture in addressing different types of unseen variations.
Baselines We compare our method against CNNs, DCGAN (Radford et al., 2015), WGAN-GP (Gulrajani et al., 2017), ICN (Jin et al., 2017) and WINN (Lee et al., 2018). For generative models DCGAN and WGAN-GP, we adopt the evaluation metric proposed in (Jin et al., 2017). The training phase becomes a two-step implementation. We first generate negative samples with the original implementation. Then, the generated negative images are used to augment the original training set. We train a simple CNN that has the identical structure with our method on the augmented training set. All results reported in this section are the average of multiple repetitions.
Experiment Setup All experiments are conducted with a simple CNN architecture (Lee et al., 2018) that contains 4 convolutional layers, each having a 5 × 5 filter size with 64 channels and stride 2 in all layers. Each convolutional layer is followed by a batch normalization layer (Ioffe & Szegedy, 2015) and a swish activation function (Ramachandran et al., 2018). The last convolutional layer is followed by two consecutive fully connected layers to compute logits and Wasserstein distances. The training epochs are 200 for both our method and all other baselines. The optimizer used is the Adam optimizer (Kingma & Ba, 2014) with parameters 1 = 0 and 2 = 0.9. Our method relies on the transformation function T (·) to convert the original samples to the unseen variations. In the following experiments, we demonstrate the ability of ITN in resisting large variations with spatial transformers (STs) (Jaderberg et al., 2015) as our transformation function unless specified. Theoretically, STs can represent all affine transformations, which endows more flexible ability in resisting unseen variations. More importantly, STs are fully differentiable, which allows the learning procedure through standard backpropagation.
4.1 CLASSIFICATION
To demonstrate the effectiveness of ITN, we first evaluate our algorithm on 4 benchmark datasets, MNIST (LeCun et al., 1998), affNIST (Tieleman, 2013), SVHN (Netzer et al., 2011) and CIFAR-10 (Krizhevsky & Hinton, 2009). The MNIST dataset includes 55000, 5000 and 10000 handwritten digits in the training, validation and testing set, respectively. The affNIST dataset is a variant from the MNIST dataset and it is built by applying various affine transformations to the samples in MNIST dataset. To accord with the MNIST dataset and for the purpose of the following experiments, we reduce the size of training, validation and testing set to 55000, 5000 and 10000, respectively. SVHN is a real-world dataset that contains house numbers images from Google Street View and CIFAR-10 contains 60000 images of ten different objects from the natural scenes. The purpose of introducing these two datasets is to further verify the performance of ITN on real-world datasets. The data augmentation we applied in the following experiments is the standard data augmentation that includes affine transformations, such as rotation, translation, scaling and shear.

Method
CNN DCGAN WGAN-GP ICN WINN ITN

MNIST
0.89% 0.79% 0.74% 0.72% 0.67% 0.49%

w/o DA affNIST SVHN

2.82% 2.78% 2.76% 2.97% 2.56% 1.52%

9.86% 9.78% 9.73% 9.72% 9.84% 6.73%

CIFAR-10
31.31% 31.22% 31.08% 32.34% 30.72% 21.93%

MNIST
0.57% 0.57% 0.56% 0.56% 0.52% 0.47%

w/ DA affNIST SVHN

1.65% 1.63% 1.56% 1.54% 1.48% 1.09%

7.01% 6.98% 6.73% 6.81% 6.44% 5.92%

CIFAR-10
24.35% 24.18% 24.07% 24.98% 23.74% 20.65%

Table 1: testing errors of the classification experiments discussed in Section 4.1, where w/DA and w/o DA indicates whether data augmentation is applied.

As shown in Table 1, our method achieves the best performance on all four datasets. The overall improvements can be explained by the fact that our method generates novel and reliable negative images (shown in Figure 1) that effectively strengthen the classifiers. The images we generate are different from the previous ones, but can still be recognized as the same class. The boosted performance in value on MNIST dataset is marginal perhaps because the performance on the MNIST dataset is close to saturation. The difference between training and testing split in MNIST dataset is

6

Under review as a conference paper at ICLR 2019

w/o DA w/DA

CNN
72.06% 40.74%

WGAN-GP
70.60% 36.29%

WINN
70.36% 33.53%

ITN
34.29% 21.31%

Table 2: testing errors of the classification experiments described in Section 4.2.1, where w/DA and w/o DA indicates whether data augmentation is applied.

also very small compared to other datasets. Moreover, the amount of improvements increases as the dataset becomes complicated. Based on the observation of the results, we conclude that our method has stronger ability in resisting unseen variations especially when the dataset is complicated. On the other hand, we can clearly observe that our method outperforms the standard data augmentation on all datasets. This result confirms the effectiveness and the advantages of our approach. Additionally, ITN does not contradict with data augmentation since ITN shows even greater performance when integrating with data augmentation techniques. The possible reason for this observation is that the explored space between ITN and data augmentation is not overlapped. Therefore, the algorithm achieves greater performance when combining two methods together since more unseen variations are discovered in this case.

Figure 1: Images generated by our method on MNIST, affNIST, SVHN and CIFAR-10 dataset. In each sector, the top row is the original images and the bottom row is our generated images.
4.2 QUANTITATIVE ANALYSIS
4.2.1 CROSS DATASET GENERALIZATION
We have shown the substantial performance improvements of ITN against other baselines on several benchmark datasets. In this section, we want to further explore the ability of our method in resisting large variations. We design a challenging cross dataset classification task between two significantly different datasets (cross dataset generalization). The training set in this experiment is the MNIST dataset while the testing set is the affNIST dataset. The difficulty of this classification tasks is clearly how to overcome such huge data discrepancy between training and testing set since the testing set includes much more variations. Another reason why we pick these two datasets as training and testing set is that they share the same categories, which ensures the challenge is only about resisting large data variations.
As shown in Table 2, ITN has clear improvements over CNN, WGAN-GP and WINN. The amount of improvement is much larger than on the regular training and testing splits shown in Section 4.1. More importantly, our performance in this challenging task is still better than CNN with data augmentation. This encouraging result further verifies the efficiency and effectiveness of ITN compared with data augmentation. It's not surprising that data augmentation improves the performance by a significant margin since the space of unseen variations is huge. Data augmentation increases the classification performance by enumerating a large number of unseen samples, however, this bruteforce searching inevitably lacks efficiency and precision. 4.2.2 RESISTING ABILITY UNDER DATA PAUCITY
Another way to evaluate the ability of resisting variations is to reduce the amount of training samples. Intuitively, the discrepancy between the training and testing sets increases when the number of samples in the training set shrinks. The purpose of this experiment is to demonstrate the potential of ITN in resisting unseen variations from a different perspective. We design the experiments where the training set is the MNIST dataset with only 0.1%, 1%, 10% and 25% of the whole training set while the testing set is the whole MNIST testing set. Each sample is randomly selected from the pool while keeps the number of samples per class same. Similarly, we repeat the same experiments on the CIFAR-10 dataset to further verify the results on a more complicated dataset. As shown in Table 3, our method has better results on all tasks. This result is consistent with Section 4.2.1 and Section 4.1, which undoubtedly illustrate the strong ability of ITN in resisting unseen variations in the testing set. The constant superior performance over data augmentation also proves the efficiency of ITN.
7

Under review as a conference paper at ICLR 2019

Method
0.1%(M) 1%(M) 10%(M) 25%(M)
0.1%(C) 1%(C) 10%(C) 25%(C)

CNN
36.50% 7.66% 2.02% 1.29%
81.99% 72.31% 59.02% 51.35%

w/o DA WGAN-GP WINN

29.43% 6.86% 1.63% 1.13%

27.18% 5.10% 1.49% 1.00%

80.92% 71.34% 57.37% 49.01%

78.24% 69.79% 56.02% 48.43%

ITN
16.47 % 3.48 % 0.98 % 0.78 %
72.50 % 61.48 % 45.06 % 34.56 %

CNN
18.07% 4.48% 1.24% 0.83%
79.04% 65.23% 47.75% 36.50%

w/ DA WGAN-GP WINN

15.35% 3.98% 1.18% 0.79%

14.46% 3.66% 1.00% 0.77%

78.75% 64.84% 46.86% 35.46%

76.97% 63.26% 46.04% 34.29%

ITN
12.67% 2.82% 0.92% 0.66%
70.43% 58.07% 42.62% 30.60%

Table 3: testing errors of the classification tasks described in Section 4.2.2, where M and C represents the experiments conducted on the MNIST dataset and CIFAR-10 dataset, respectively.
4.3 BEYOND SPATIAL TRANSFORMER
Even though we utilize STs to demonstrate our ability in resisting data variations, our method actually has the ability to generalize to other types of transformations. Our algorithm can take other types of differentiable transformation functions and strengthen the discriminators in a similar manner. Moreover, our algorithm can utilize multiple types of transformations at the same time and provide even stronger ability in resisting variations. To verify this, we introduce another recently proposed work, Deep Diffeomorphic Transformer (DDT) Networks (Detlefsen et al., 2018). DDTs are similar to STs in a way that both of them can be optimized through standard backpropagation.
We replace the ST modules with the DDT modules and check whether our algorithm can resist such type of transformation. Then, we include both STs and DDTs in our model and verify the performance again. Let MNIST dataset be the training set of the experiments while the testing sets are the MNIST dataset with different types of transformation applied. We introduce two types of testing sets in this section. The first one is the normal testing set with random DDT transformation only. The second one is similar to the first one but includes both random DDT and affine transformations. The DDT transformation parameters are drawn from N (0, 0.7 × Id) as suggest in (Detlefsen et al., 2018), where Id represents the d dimensional identity matrix. Then the transformed images are randomly placed in a 42 × 42 images. We replicate the same experiment on the CIFAR-10 dataset.

CNN WGAN-GP WINN ITN(DDT) ITN(DDT + ST)

MNIST DDT DDT + ST 17.75% 55.11% 17.53% 53.24% 17.20% 52.43% 12.85 % 40.60% 9.41% 34.37%

CIFAR-10 DDT DDT + ST 76.14 % 78.01 % 75.93% 77.02 % 75.43 % 76.92 % 53.62% 63.56 % 45.26% 56.95 %

Table 4: testing errors of cross dataset classification experiments, where CNN (w/ DA) represents the CNNs with data augmentation.

We can make some interesting observations from the Table 4. First, ITN can integrate with flexibly with DDT or DDT + ST to resist the corresponding variations. Second, ITN can resist partial unseen variations out of a mixture of transformations in the testing data. More importantly, the performance of ITN won't degrade when the model has transformation functions that doesn't match the type of variations in the testing data, e.g. ITN(DDT + ST) on testing data with DDT only. This observation allows us to apply multiple transformation functions in ITN without knowing the types of variations in the testing data and still maintain good performance.
5 CONCLUSION

We proposed a principled and smart approach that endows the classifiers with the ability to resist larger variations between training and testing data. Our method, ITN strengthens the classifiers by generating unseen variations with various learned transformations. Experimental results show consistent performance improvements not only on the classification tasks but also on the other challenging classification tasks, such as cross dataset generalization. Moreover, ITN demonstrates its advantages in both effectiveness and efficiency over data augmentation. Our future work includes applying our approach to large scale datasets and extending it to generate samples with more types of variations.

8

Under review as a conference paper at ICLR 2019
REFERENCES
Martin Arjovsky, Soumith Chintala, and Le´on Bottou. Wasserstein gan. arXiv preprint arXiv:1701.07875, 2017.
Emily L Denton, Soumith Chintala, Rob Fergus, et al. Deep generative image models using a laplacian pyramid of adversarial networks. In Advances in neural information processing systems, pp. 1486­1494, 2015.
Nicki Skafte Detlefsen, Oren Freifeld, and Søren Hauberg. Deep diffeomorphic transformer networks. Conference on Computer Vision and Pattern Recognition (CVPR), 2018.
Jerome Friedman, Trevor Hastie, and Robert Tibshirani. The elements of statistical learning, volume 1. Springer series in statistics New York, NY, USA:, 2001.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural information processing systems, pp. 2672­2680, 2014.
Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron C Courville. Improved training of wasserstein gans. In Advances in Neural Information Processing Systems, pp. 5769­5779, 2017.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770­778, 2016.
Gao Huang, Zhuang Liu, Kilian Q Weinberger, and Laurens van der Maaten. Densely connected convolutional networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, volume 1, pp. 3, 2017.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. arXiv preprint arXiv:1502.03167, 2015.
Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A Efros. Image-to-image translation with conditional adversarial networks. arXiv preprint, 2017.
Max Jaderberg, Karen Simonyan, Andrew Zisserman, et al. Spatial transformer networks. In Advances in neural information processing systems, pp. 2017­2025, 2015.
Tony Jebara. Machine learning: discriminative and generative, volume 755. Springer Science & Business Media, 2012.
Long Jin, Justin Lazarow, and Zhuowen Tu. Introspective classification with convolutional nets. In Advances in Neural Information Processing Systems, pp. 823­833, 2017.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.
Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. 2009.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. In Advances in neural information processing systems, pp. 1097­1105, 2012.
Justin Lazarow, Long Jin, and Zhuowen Tu. Introspective neural networks for generative modeling. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 2774­ 2783, 2017.
Yann LeCun, Bernhard Boser, John S Denker, Donnie Henderson, Richard E Howard, Wayne Hubbard, and Lawrence D Jackel. Backpropagation applied to handwritten zip code recognition. Neural computation, 1(4):541­551, 1989.
Yann LeCun, Le´on Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278­2324, 1998.
9

Under review as a conference paper at ICLR 2019
Kwonjoon Lee, Weijian Xu, Fan Fan, and Zhuowen Tu. Wasserstein introspective neural networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2018.
Percy Liang and Michael I Jordan. An asymptotic analysis of generative, discriminative, and pseudolikelihood estimators. In Proceedings of the 25th international conference on Machine learning, pp. 584­591. ACM, 2008.
Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y Ng. Reading digits in natural images with unsupervised feature learning. In NIPS workshop on deep learning and unsupervised feature learning, volume 2011, pp. 5, 2011.
Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep convolutional generative adversarial networks. arXiv preprint arXiv:1511.06434, 2015.
Prajit Ramachandran, Barret Zoph, and Quoc V Le. Searching for activation functions. 2018. Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen.
Improved techniques for training gans. In Advances in Neural Information Processing Systems, pp. 2234­2242, 2016. Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014. Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, Andrew Rabinovich, et al. Going deeper with convolutions. CVPR, 2015. Tijmen Tieleman. affnist, 2013. URL https://www.cs.toronto.edu/~tijmen/ affNIST/. Zhuowen Tu. Learning generative models via discriminative approaches. In Computer Vision and Pattern Recognition, 2007. CVPR'07. IEEE Conference on, pp. 1­8. IEEE, 2007. Zhuowen Tu, Katherine L Narr, Piotr Dolla´r, Ivo Dinov, Paul M Thompson, and Arthur W Toga. Brain anatomical structure segmentation by hybrid discriminative/generative models. IEEE transactions on medical imaging, 27(4):495­508, 2008. Max Welling, Richard S Zemel, and Geoffrey E Hinton. Self supervised boosting. In Advances in neural information processing systems, pp. 681­688, 2003.
10


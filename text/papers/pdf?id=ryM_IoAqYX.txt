Under review as a conference paper at ICLR 2019
ANALYSIS OF QUANTIZED DEEP NETWORKS
Anonymous authors Paper under double-blind review
ABSTRACT
Weight-quantized networks have small storage and fast inference, but training can still be time-consuming. This can be improved with distributed learning. To reduce the high communication cost due to worker-server synchronization, recently gradient quantization has also been proposed to train networks with full-precision weights. In this paper, we theoretically study how the combination of both weight and gradient quantization affects convergence. We show that (i) weight-quantized networks converge to an error related to the weight quantization resolution and weight dimension; (ii) quantizing gradients slows convergence by a factor related to the gradient quantization resolution and dimension; and (iii) clipping the gradient before quantization renders this factor dimension-free, thus allowing the use of fewer bits for gradient quantization. Empirical experiments confirm the theoretical convergence results, and demonstrate that quantized networks can speed up training and have comparable performance as full-precision networks.
1 INTRODUCTION
Deep neural networks are usually huge. Such a high demand in time and space can significantly limit the deployment on low-end devices. To alleviate this problem, many approaches have been recently proposed to compress deep networks. One direction is network quantization, which represents each network weight with a small number of bits. Besides significantly reducing the model size, it also accelerates network training and inference. Many weight quantization methods aim at approximating the full-precision weights in each iteration (Courbariaux et al., 2015; Lin et al., 2016; Rastegari et al., 2016; Li & Liu, 2016; Lin et al., 2017; Guo et al., 2017). Recently, loss-aware quantization minimizes the loss directly w.r.t. the quantized weights (Hou et al., 2017; Hou & Kwok, 2018; Leng et al., 2018), and often achieves better performance than approximation-based methods.
Distributed learning can further speed up training of weight-quantized networks (Dean et al., 2012). A key challenge is on reducing the expensive communication cost incurred during synchronization of the gradients and model parameters (Li et al., 2014a;b). Recently, algorithms that sparsify (Aji & Heafield, 2017; Wangni et al., 2017) or quantize the gradients (Seide et al., 2014; Wen et al., 2017; Alistarh et al., 2017; Bernstein et al., 2018) have been proposed.
In this paper, we consider quantization of both the weights and gradients in a distributed environment. Quantizing both weights and gradients has been explored in the DoReFa-Net (Zhou et al., 2016), QNN (Hubara et al.), WAGE (Wu et al., 2018) and ZipML (Zhang et al., 2017). We differ from them in two aspects. First, existing methods mainly consider learning with a single machine, and gradient quantization is used to reduce the computations in backpropagation. On the other hand, we consider a distributed environment, and use gradient quantization to reduce communication cost and accelerate distributed learning of weight-quantized networks. Weight update and quantization are performed at the server, and only quantized weights are transmitted over the network, leading to a reduction in communication cost from server to workers. Gradient quantization is performed by the workers. As only quantized gradients are transmitted, worker-to-server communication is also reduced. Moreover, instead of keeping both the full-precision and quantized weights as in a nondistributed environment, here the server only keeps the full-precision weights, while each worker only keeps the quantized weights. Thus, workers can be run on low-end devices with small storage.
Second, while DoReFa-Net, QNN and WAGE show impressive empirical results on the quantized network, theoretical guarantees are not provided. ZipML provides convergence analysis, but is limited to stochastic weight quantization, square loss with the linear model, and requires the stochastic
1

Under review as a conference paper at ICLR 2019

gradients to be unbiased. This can be restrictive as most state-of-the-art weight quantization methods (Rastegari et al., 2016; Lin et al., 2016; Li & Liu, 2016; Guo et al., 2017; Hou et al., 2017; Hou & Kwok, 2018) are deterministic, and the resultant stochastic gradients are biased.
In this paper, we relax the restrictions on the loss function, and study in an online learning setting how biased stochastic gradients affect convergence. The main findings are:

1. With either full-precision or quantized gradients, the average regret of loss-aware weight quantization does not converge to zero, but to an error related to the weight quantization resolution w and dimension d. The smaller the w or d, the smaller is the error (Theorems 1 and 2). 
2. With either full-precision or quantized gradients, the average regret converges with a O(1/ T ) rate to the error, where T is the number of iterations. However, gradient quantization slows convergence (relative to using full-precision gradients) by a factor related to gradient quantization resolution g and d. The smaller the g or d, the faster is the convergence (Theorems 1 and 2).

3. Empirically, gradient clipping has been found effective in use with quantized gradient (Wen et al., 2017). We show theoretically that for gradients following the normal distribution, gradient clipping renders the speed degradation mentioned above dimension-free. However, an additional error is incurred. The convergence speedup and error are related to how aggressive clipping is performed. More aggressive clipping results in faster convergence, but a larger error (Theorem 3).

4. Empirical results verify the convergence analysis and show that quantizing gradients can significantly reduce communication cost, speed up training of weight-quantized networks, and obtain accuracy comparable with the use of full-precision gradients.

Notations.

For

a

vector

x,

 x

is

the

element-wise

square

root,

x2

is

the

element-wise

square,

Diag(x) returns a diagonal matrix with x on the diagonal, and x y is the element-wise multiplica-

tion of vectors x and y. For a matrix Q,

x

2 Q

=

x

Qx. For a matrix X,

X is the element-wise

square root, and diag(X) returns a vector extracted from the diagonal elements of X.

2 PRELIMINARIES

2.1 ONLINE LEARNING

Online learning continually adapts the model with a sequence of observations. It has been com-

monly used in the analysis of deep learning optimizers (Duchi et al., 2011; Kingma & Ba, 2015;

Reddi et al., 2018). At time t, the algorithm picks a model with parameter wt  S, where S

is a convex compact set. The algorithm then incurs a loss ft(wt). After T rounds, performance

is evaluated by the regret R(T ) =

T t=1

ft(wt)

-

ft(w)

and

average

regret

R(T )/T ,

where

w = arg minwS

T t=1

ft(w)

is

the

best

model

parameter

in

hindsight.

2.2 WEIGHT QUANTIZATION

In BinaryConnect (Courbariaux et al., 2015), each weight is binarized using the sign function either deterministically or stochastically. In ternary-connect (Lin et al., 2016), each weight is stochastically quantized to {-1, 0, 1}. Stochastic weight quantization often suffers severe accuracy degradation, while deterministic weight quantization (as in the binary-weight-network (BWN) (Rastegari et al., 2016) and ternary weight network (TWN) (Li & Liu, 2016)) achieves much better performance.

In this paper, we will focus on loss-aware weight quantization, which further improves performance
by considering the effect of weight quantization on the loss. Examples include loss-aware binariza-
tion (LAB) (Hou et al., 2017) and loss-aware quantization (LAQ) (Hou & Kwok, 2018). Let the full-precision weights from all L layers in the deep network be w = [w1 , w2 , . . . , wL ]  Rd, where wl = vec(Wl), and Wl is the weight matrix at layer l. The corresponding quantized weight is denoted Qw(w) = w^ = [w^ 1 , w^ 2 , . . . , w^ L ] , where Qw(и) is the weight quantization function.

At the tth iteration, the second-order Taylor expansion of ft(w^ ), i.e., ft(w^ t) + ft(w^ t) (w^ -

w^ t)

+

1 2

(w^

-

w^ t)

Ht(w^ - w^ t) is minimized w.r.t.

w^ , where Ht is the Hessian at w^ t.

Ht is

expensive to compute. In practice, this is approximated by Diag( v^t) where v^t is the moving

average: v^t = v^t-1 + (1 - )g^t2 =

t j

=1

(1

-

)t-j g^j2

with



1, and is readily available

2

Under review as a conference paper at ICLR 2019



in popular deep network optimizers such as RMSProp and Adam. Moreover, Diag( v^t) is also an

estimate of be obtained

Diag( by first

diag(Ht2)) performing

(Dauphin et al., 2015). Computationally, a preconditioned gradient descent wt+1 =

the wt

q-uant Dtiziaegd(wev^itg)h-t 1cg^atn,

followed by quantization via solving of the following optimization problem:1

w^ t+1 = Qw(wt+1) = arg min
w^

wt+1 - w^

2 Diag( v^t)

s.t.

w^ = b,  > 0, b  Swd .

(1)

For binarization, Sw = {-1, +1}, and a simple closed-form solution is obtained in (Hou et al., 2017). For m-bit linear quantization, Sw = {-Mk, . . . , -M1, M0, M1, . . . , Mk}, where k = 2m-1 - 1, 0 = M0 < и и и < Mk are uniformly spaced, with weight quantization resolution w = Mr+1 - Mr. An efficient approximate solution of (1) is obtained in (Hou & Kwok, 2018).

2.3 STOCHASTIC GRADIENT QUANTIZATION
In a distributed learning environment, the main bottleneck is often on worker-to-server communication. This cost can be significantly reduced by quantizing the gradients (computed by the workers) before pushing to the server (Seide et al., 2014; Wen et al., 2017; Alistarh et al., 2017). For example, assuming that the full-precision gradient is 32-bit, the communication cost is reduced 32/m times when gradients are quantized to m bits. In TernGrad (Wen et al., 2017), the gradients are ternarized. Here, we consider the more general m-bit linear quantization (Alistarh et al., 2017): Qg(gt) = st и sign(gt) qt, where gt is the stochastic gradient, st = gt , qt  Sgd, and Sg = {-Bk, . . . , -B1, B0, B1, . . . , Bk}, with k = 2m-1 - 1, 0 = B0 < B1 < и и и < Bk are uniformly spaced, and the gradient quantization resolution g = Br+1 - Br. The ith element qt,i in qt is equal to Br+1 with probability (|gt,i|/st - Br) /(Br+1 - Br), and Br otherwise. Here, r is an index satisfying Br  |gt,i|/st < Br+1. Note that Qg(gt) is an unbiased estimator of gt.

3 PROPERTIES OF QUANTIZED NETWORK

In this section, we consider quantization of both weights and gradients in a distributed environment

with N workers in data parallelism (Figure 1). At the tth iteration, worker n  {1, 2, . . . , N }

computes the full-precision gradient g^t(n) w.r.t. the quantized weight and quantizes g^t(n) to g~t(n) =

Qg(g^t(n)). The quantized gradients are then synchronized and averaged at the parameter server as:

g~t

=

1 N

precision

N n=1

g~t(n)

.

The

weight as wt+1

server = wt

-updtaDteiasgt(he v~set)c-o1ng~dt

moment v~t based on g~t, and also the full. The weight is quantized using loss-aware

weight quantization to produce w^ t+1 = Qw(wt+1), which is then sent back to all the workers.

Figure 1: Distributed weight and gradient quantization with data parallelism.
3.1 ASSUMPTIONS
Analysis on quantized networks has only been performed on models with (i) full-precision gradients and weights quantized by stochastic weight quantization (Li et al., 2017; De Sa et al., 2018), or simple deterministic weight quantization using the sign (Li et al., 2017); (ii) full-precision weights and quantized gradients (Alistarh et al., 2017; Wen et al., 2017; Bernstein et al., 2018); (iii) quantized weights and quantized gradients (Zhang et al., 2017), but limited to stochastic weight quantization, square loss on linear model (i.e., ft(wt) = (xt wt - yt)2) in Section 2.1), and unbiased gradient. In this paper, we study the more advanced loss-aware weight quantization. As it is deterministic and has biased gradients, the above analysis do not apply. Moreover, we relax the assumptions on
1For simplicity of notations, we assume that the same scaling parameter  is used for all layers. Extension to layer-wise scaling is straightforward.
3

Under review as a conference paper at ICLR 2019

ft as: (A1) ft is convex; (A2) ft is twice differentiable with Lipschitz-continuous gradient; and (A3) ft has bounded gradient, i.e., ft(w)  G and ft(w)   G for all w  S. These assumptions have been commonly used in convex online learning (Hazan, 2016; Duchi et al., 2011; Kingma & Ba, 2015) and quantized networks (Alistarh et al., 2017; Li et al., 2017). Obviously, the convexity assumption A1 does not hold for deep networks. However, this facilitates analysis of deep learning models, and has been used in (Kingma & Ba, 2015; Reddi et al., 2018; Li et al., 2017; De Sa et al., 2018). Moreover, as will be seen, it helps to explain the empirical behavior in Section 4.
As in (Duchi et al., 2011; Kingma & Ba, 2015; Li et al., 2017), we assume that wm - wn D and wm - wn   D for all wm, wn  S. Moreover, the learning rate t decays as / t, where  is a constant (Hazan, 2016; Duchi et al., 2011; Kingma & Ba, 2015; Li et al., 2017).
For simplicity of notations, we denote the full-precision gradient ft(wt) w.r.t. the full-precision weight by gt, and the full-precision gradient ft(Qw(wt)) w.r.t. the quantized weight by g^t. As ft is twice differentiable (Assumption A2), using the mean value theorem, there exists p  (0, 1) such that gt - g^t = ft(wt) - ft(w^ t) = 2ft (w^ t + p(wt - w^ t)) (wt - w^ t). Let Ht = 2ft(w^ t + p(wt - w^ t)) be the Hessian at w^ t + p(wt - w^ t). Moreover, let  = max{1, . . . , T }, where t is the scaling parameter in (1) at the tth iteration.

3.2 WEIGHT QUANTIZATION WITH FULL-PRECISION GRADIENT

When only weights are quantized, the update for loss-aware weight quantization is

wt+1 = wt - tDiag( v^t)-1g^t,

where v^t is the moving average of the (squared) gradients g^t2 as discussed in Section 2.2. 

Theorem 1.

For

loss-aware

weight 

quantization

with

full-precision

gradients 

and

t

= /

t,

R(T )  D2 dT 2

T
(1 - )T -t
t=1

g^t

2 + G d 1-

T
g^t 2
t=1

T

+ LD

t=1

wt - w^ t

2 Ht

,

R(T )  O d + LD D2 + d22w .

TT

4

(2) (3)

For standard online gradient descent with the same learning rate scheme, R(T )/T converges to zero

at a O(1/ T ) rate (Hazan, 2016). From Theorem 1, the average regret converges at the same rate,

 but only to a nonzero error LD

D2 +

d2 w2 4

related to

the weight

quantization resolution

w

and dimension d.

3.3 WEIGHT QUANTIZATION WITH QUANTIZED GRADIENT

When both weights and gradients are quantized, the update for loss-aware weight quantization is

wt+1 = wt - tDiag( v~t)-1g~t,

where g~t is the stochastically quantized gradient Qg(ft(Qw(wt))). The second moment v~t is
the moving average of the (squared) quantized gradients g~t2. The following Proposition shows that gradient quantization significantly blows up the norm of the quantized gradient relative to its full-

precision counterparts. Moreover, the difference increases with the gradient quantization resolution

g and dimension d.



Proposition 1.

E(

g~t

2)  ( 1+

2d-1 2

g

+

1)

g^t

2.



Theorem 2.

For

loss-aware weight 

quantization

with

quantized

gradients 

and

t

=

/

t,

E(R(T ))  D2 dT 2

T
(1 - )T -tE(
t=1

g~t

2) + G d 1-

T
E( g~t 2)
t=1

 + LD

T
E(
t=1

wt - w^ t

2 Ht

),

(4)

R(T ) E
T







O

1+

2d 2

-

1 g

+

1

и

d T



+

LD

D2 + d22w . 4

(5)

4

Under review as a conference paper at ICLR 2019

The regrets in (2) and (4) are of the same form and differ only in the gradient used. Simi-

larly, for the average regrets in (3) and (5), quantizing gradients slows convergence by a factor



of

1+

2d-1 2

g

+

1,

which

is

a

direct

consequence

of

the

blowup

in

Proposition

1.

These obser-

vations can be problematic as (i) deep networks typically have a large d; and (ii) distributed learning

prefers using a small number of bits for the gradients, and thus a large g.

3.4 WEIGHT QUANTIZATION WITH QUANTIZED CLIPPED GRADIENTS

To reduce convergence speed degradation caused by gradient quantization, gradient clipping has

been proposed as an empirical solution (Wen et al., 2017). The gradient g^t is clipped to Clip(g^t),

where Clip(g^t,i) = g^t,i if |g^t,i|  c, and sign(g^t,i) и c otherwise. Here, c is a constant clipping

factor,

and



is

the

standard

deviation wt+1

of =

ewletm-enttsDiniag^gt(.Tvhte)-u1pgdat,te

then

becomes

where gt  Qg(Clip(g^t))  Qg(Clip(ft(Qw(wt)))) is the quantized clipped gradient. The second moment vt is computed using the (squared) quantized clipped gradient gt2.

As shown in Figure 2(a) of (Wen et al., 2017), the distribution of gradients before quantization is close to the normal distribution. Recall from Section 3.3 that the difference between E( g~t 2) of the quantized gradient g~t and the full-precision gradient g^t 2 is related to the dimension d. The following Proposition shows that E( gt 2)/E( g^t 2) becomes independent of d if g^t follows the
normal distribution and clipping is used.

Proposition 2.

Assume that g^t follows N (0, 2I), we have E(

gt

2)



((2/)

1 2

cg +1)E(

g^t

2).

However, the quantized clipped gradient may now be biased (i.e., E(gt) = Clip(g^t) = g^t). The following Proposition shows that the bias is related to the clipping factor c. A larger c (i.e., less

severe gradient clipping) leads to smaller bias.

Proposition 3. Assume that g^t follows N (0, 2I), we have E( Clip(g^t) - g^t 2) 

d2

(2/)

1 2

R(c),

where

R(c)

=

-ce-

c2 2

+

 2

(1

+

c2)(1

-

erf(

c 2

)),

and

erf(z)

=

2 

z 0

e-t2

dt

is the error function.

Theorem 3. Assume that g^t follows N (0, 2I). For loss-aware weight quantization with quantized

clipped

gradients

and

t

= 

/

t,



E(R(T ))  D2 dT 2

T
(1 - )T -tE(
t=1

gt

2) + G d 1-

T
E( gt 2)
t=1

 + LD

T
E(
t=1

wt - w^ t

2 Ht

)

+

D

T
E(
t=1

Clip(g^t) - g^t 2),

(6)

R(T ) E

O

T

(2/)

1 2

cg

+

1

d

T

+ LD

D2

+

d2w2

1 + dD(2/) 4

4

R(c).

(7)

Note that terms involving g~t in Theorem 2 are replaced by gt. Moreover, the regret has an addi-

tional term D

T t=1

E(

Clip(g^t) - g^t 2) over that in Theorem 2. Comparing the average re-

grets in Theorems 1 and 3, gradient clipping before quantization slows convergence by a factor

of

(2/

)

1 2

cg

+

1,

as

compared

to

using

full-precision

gradients.

This is independent of d as

the increase in E( gt 2) is independent of d (Proposition 2). Hence, a g larger than the one in

Theorem 2 can be used, and this reduces the communication cost in distributed learning.

A in

lTahrgeeorrecm(i2.e,.,thleesesxsteraveerrerogrradideDntc(l2ip/pi)n41g)

makes R(c) R(c) in (7)

smaller (Figure is thus smaller,

2). but

Compared with (4) convergence is also

slower. Hence, there is a trade-off between the two.

4 EXPERIMENTS
4.1 SYNTHETIC DATA
In this section, we first study the effect of dimension d on the convergence speed and final error. As popular deep networks usually have hand-crafted architectures, we consider here a linear model with

5

Under review as a conference paper at ICLR 2019

(a) 1Gbps Ethernet. (b) 10Gbps Ethernet.

Figure 2: R(c) vs the Figure 3: Convergence Figure 4: Speedup of ImageNet training on a

clipping factor c.

of LAB on the linear 16-node GPU cluster. Each node has 4 1080ti

model with different d's. GPUs with one PCI switch.

square loss as in (Zhang et al., 2017). Each entry of the model parameter is generated by uniform
sampling from [-0.5, 0.5]. Samples xi's are generated such that each entry of xi is drawn uniformly from [-0.5, 0.5], and the corresponding output yi from N (xi w, (0.2)2). At the tth iteration, a
mini-batch of B = 64 samples are drawn to form Xt = [x1, . . . , xB] and yt = [y1, . . . , yB] . The corresponding loss is ft(wt) = Xt wt - yt 2/2B. The weights are quantized to 1 bit using LAB. The gradients are either full-precision (denoted FP) or stochastically quantized to 2 bits (denoted SQ2). The optimizer is RMSProp, and the learning rate is t = / t, where  = 0.03.

Figure 3 shows convergence of the average training loss

T t=1

ft(wt)/T ,

which

differs

from

the

average regret only by a constant.2 As can be seen, for both full-precision and quantized gradients,

a larger d leads to a larger loss upon convergence. Moreover, convergence is slower for larger d,

particularly when the gradients are quantized. These agree with the results in Theorems 1 and 2.

4.2 CIFAR-10
We follow (Wen et al., 2017) and use the same train/test split, Cifarnet model, data preprocessing, augmentation and distributed Tensorflow setup. Adam is used as the optimizer. The learning rate is decayed by a factor of 0.1 every 200 epochs.

4.2.1 WEIGHT QUANTIZATION RESOLUTION w
Weights are quantized to 1 bit (LAB), 2 bits (LAQ2), or m bits (LAQm). The gradients are fullprecision (FP) or stochastically quantized to m = {2, 3, 4} bits (SQm) without gradient clipping. Two workers are used in this experiment. Figure 5 shows convergence of the average training loss with different numbers of bits for the quantized weight. With full-precision or quantized gradients, weight-quantized networks have larger training losses than full-precision networks upon convergence. The more bits are used, the smaller is the final loss. This agrees with the results in Theorems 1 and 2. Table 1 shows the test set accuracies. Weight-quantized networks are less accurate than their full-precision counterparts, but the degradation is small when 3 or 4 bits are used.

(a) G(FP).

(b) G(SQ2).

(c) G(SQ3).

(d) G(SQ4).

Figure 5: Convergence with different numbers of bits for the weights on CIFAR-10. The gradient is full-precision (denoted G(FP)) or m-bit quantized (denoted G(SQm)) without gradient clipping.

4.2.2 GRADIENT QUANTIZATION RESOLUTION g
Figure 6 shows convergence of the average training loss with different numbers of bits for the quantized gradients, again without gradient clipping. Using fewer bits yields a larger final error, and using 2- or 3-bit gradients yields larger training loss and worse accuracy than full-precision gradients (Figure 6 and Table 1). The fewer bits for the gradients, the larger the gap. The degradation is negligible
2Legend "W(LAB)-G(FP)" means that weights are quantized using LAB and gradients are full-precision.

6

Under review as a conference paper at ICLR 2019

Table 1: Testing accuracy (%) on CIFAR-10 with

two workers.

weight gradient

FP

LAB LAQ2 LAQ3 LAQ4

FP 83.74 80.37 82.11 83.14 83.35

SQ2 (no clipping) 81.40 78.67 80.27 81.27 81.38

SQ2 (clip,c = 3) 82.99 80.25 81.59 83.14 83.40

SQ3 (no clipping) 83.24 80.18 81.63 82.75 83.17

SQ3 (clip, c = 3) 83.89 80.13 81.77 82.97 83.43

SQ4 (no clipping) 83.64 80.44 81.88 83.13 83.47

SQ4 (clip, c = 3) 83.80 79.27 81.42 82.77 83.43

Table 2: Testing accuracy (%) on CIFAR-10 with

varying number of workers (N ).

weight

gradient

N = 4 N = 8 N = 16

FP FP 83.28 83.38 83.76

FP 82.92 82.93 83.12

SQ2 (no clipping) 81.53 81.08 81.30

SQ2 (clip, c = 3) 83.01 82.94 82.73

LAQ3 SQ3 (no clipping) 82.64 82.42 82.27

SQ3 (clip, c = 3) 82.93 83.16 82.54

SQ4 (no clipping) 83.03 82.53 82.61

SQ4 (clip, c = 3) 82.51 83.07 82.52

when 4 bits are used. Indeed, 4-bit gradient sometimes has even better accuracy than full-precision gradient, as its inherent randomness encourages escape from poor sharp minima (Wen et al., 2017). Moreover, using a larger m results in faster convergence, which agrees with Theorem 2.

(a) W(LAB)

(b) W(LAQ2).

(c) W(LAQ3).

(d) W(LAQ4).

Figure 6: Convergence with different numbers of bits for the gradients on CIFAR-10. The weight is binarized (denoted W(LAB)) or m-bit quantized (denoted W(LAQm)). Gradients are not clipped.

4.2.3 GRADIENT CLIPPING
In this section, we perform experiments on gradient clipping, with clipping factor c in {1, 2, 3}. LAQ2 is used for weight quantization and SQ2 for gradient quantization. Figure 7(a) shows histograms of the full-precision gradients before clipping. As can be seen, the gradients at each layer before clipping roughly follow the normal distribution, which verifies the assumption in Section 3.4. Figure 7(b) shows the average g~t 2/ g^t 2 (for non-clipped gradients) and gt 2/ g^t 2 (for clipped gradients) over all iterations. The dimensionalities (d) of the various Cifarnet layers are "conv1": 1600, "conv2": 1600, "fc3": 884736, "fc4": 73728, "softmax": 1920. Layers with large d have large g~t 2/ g^t 2 values, which agrees with Proposition 1. With clipped gradients,
gt 2/ g^t 2 is much smaller and does not depend on d, agreeing with Proposition 3. Figure 7(c) shows convergence of the average training loss. Using a smaller c (more aggressive clipping) leads to faster training (at the early stage of training) but larger final training loss, agreeing with Theorem 3.
Figure 8 shows convergence of the average training loss with different numbers of bits for the quantized clipped gradient, with c = 3. By comparing3 with Figure 6, gradient clipping achieves faster convergence, especially when the number of gradient bits is small. For example, 2-bit clipped gradient has comparable speed (Figure 8) and accuracy (Table 1) as full-precision gradient.
4.2.4 VARYING THE NUMBER OF WORKERS
We use 3-bit quantized weight (LAQ3), and gradients are full-precision or stochastically quantized to m = {2, 3, 4} bits (SQm). The setup follows (Wen et al., 2017), and is in Appendix C. Table 2 shows the testing accuracies with varying number of workers N . Observations are similar to those in Section 4.2.3. 2-bit quantized clipped gradient has comparable performance as full-precision gradient, while the non-clipped counterpart requires 3 to 4 bits for comparable performance.
4.3 IMAGENET
In this section, we train the AlexNet on ImageNet. We follow (Wen et al., 2017) and use the same data preprocessing, augmentation, learning rate, and mini-batch size. Quantization is not performed
3Full-precision gradient curves are the same in Figures 6 and 8, and can be used as a common baseline.
7

Under review as a conference paper at ICLR 2019

(a) Histograms of gradients before clipping.

(b)

g~t g^t

2 2

or

.gt 2
g^t 2

(c) Trianing curve.

Figure 7: Results for LAQ2 with SQ2 on CIFAR-10 with two workers. (a) Histograms of gradients at different Cifarnet layers before clipping (visualized by Tensorboard); (b) Average g~t 2/ g^t 2 (for non-clipped gradients) and gt 2/ g^t 2 (for clipped gradients); and (c) Training curves.

(a) W(LAB).

(b) W(LAQ2).

(c) W(LAQ3).

(d) W(LAQ4).

Figure 8: Convergence with different numbers of bits for gradients (with c = 3) on CIFAR-10.

in the first and last layers, as is common in the literature (Zhou et al., 2016; Zhu et al., 2017; Polino et al., 2018; Wen et al., 2017). We use Adam as the optimizer. We experiment with 4-bit loss-aware weight quantization (LAQ4), and the gradients are either full-precision or quantized to 3 bits (SQ3).
Table 3 shows the accuracies with different numbers of workers. Weight-quantized networks have slightly worse accuracies than full-precision networks. Quantized clipped gradient outperforms the non-clipped counterpart, and achieves comparable accuracy as full-precision gradient.

weight FP
LAQ4

Table 3: Top-1 and top-5 accuracies (%) on ImageNet.

gradient

N =2

N =4

N =8

top-1 top-5 top-1 top-5 top-1 top-5

FP 55.08 78.33 55.45 78.57 55.40 78.69

FP 53.79 77.21 54.22 77.53 54.73 78.12

SQ3 (no clipping) 52.48 75.97 52.87 76.40 53.18 76.62

SQ3 (clip, c = 3) 54.13 77.27 54.23 77.55 54.34 78.07

Figure 4 shows the speedup in distributed training of a weight-quantized network with quantized/full-precision gradient compared to training with one worker using full-precision gradient. We use the performance model in (Wen et al., 2017), which combines lightweight profiling on a single node with analytical communication modeling. We also use the all-reduce communication model (Rabenseifner, 2004), in which each GPU communicates with its neighbor until all gradients are accumulated to a single GPU. We do not include the server's computation effort on weight quantization and the worker's effort on gradient clipping, which are negligible compared to the forward and backward propagations in the worker. As can be seen, when the bandwidth is small (Figure 4(a)), communication is the bottleneck, and using quantizing gradients is significantly faster than the use of full-precision gradients. With a larger bandwidth (Figure 4(b)), the difference in speedups is smaller. Moreover, note that on the 1Gbps Ethernet with quantized gradients, its speedup is similar to those on the 10Gbps Ethernet with full-precision gradients.

5 CONCLUSION
In this paper, we studied loss-aware weight-quantized networks with quantized gradient for efficient communication in a distributed environment. Convergence analysis is provided for weight-quantized networks with full-precision, quantized and quantized clipped gradients. Empirical experiments confirm the theoretical results, and demonstrate that quantized networks can speed up training and have comparable performance as full-precision networks.

8

Under review as a conference paper at ICLR 2019
REFERENCES
A. F. Aji and K. Heafield. Sparse communication for distributed gradient descent. In International Conference on Empirical Methods in Natural Language Processing, pp. 440Г445, 2017.
D. Alistarh, D. Grubic, J. Li, R. Tomioka, and M. Vojnovic. QSGD: Communication-efficient SGD via gradient quantization and encoding. In Advances in Neural Information Processing Systems, pp. 1707Г1718, 2017.
J. Bernstein, Y. Wang, K. Azizzadenesheli, and A. Anandkumar. signSGD: Compressed optimisation for non-convex problems. In International Conference on Machine Learning, 2018.
M. Courbariaux, Y. Bengio, and J. P. David. BinaryConnect: Training deep neural networks with binary weights during propagations. In Advances in Neural Information Processing Systems, pp. 3105Г3113, 2015.
Y. Dauphin, H. de Vries, and Y. Bengio. Equilibrated adaptive learning rates for non-convex optimization. In Advances in Neural Information Processing Systems, pp. 1504Г1512, 2015.
C. De Sa, M. Leszczynski, J. Zhang, A. Marzoev, C. R. Aberger, K. Olukotun, and C. Re┤. Highaccuracy low-precision training. Preprint arXiv:1803.03383, 2018.
J. Dean, G. Corrado, R. Monga, K. Chen, M. Devin, Q. V. Le, M. Z. Mao, M. Ranzato, A. Senior, P. Tucker, K. Yang, and A. Y. Ng. Large scale distributed deep networks. In Advances in Neural Information Processing Systems, pp. 1223Г1231, 2012.
J. Duchi, E. Hazan, and Y. Singer. Adaptive subgradient methods for online learning and stochastic optimization. Journal of Machine Learning Research, 12:2121Г2159, 2011.
Y. Guo, A. Yao, H. Zhao, and Y. Chen. Network sketching: Exploiting binary structure in deep CNNs. In International Conference on Computer Vision and Pattern Recognition, pp. 5955Г5963, 2017.
E. Hazan. Introduction to online convex optimization. Foundations and Trends in Optimization, 2 (3-4):157Г325, 2016.
L. Hou and J. T. Kwok. Loss-aware weight quantization of deep networks. In International Conference on Learning Representations, 2018.
L. Hou, Q. Yao, and J. T. Kwok. Loss-aware binarization of deep networks. In International Conference on Learning Representations, 2017.
I. Hubara, M. Courbariaux, D. Soudry, R. El-Yaniv, and Y. Bengio. Quantized neural networks: Training neural networks with low precision weights and activations. Journal of Machine Learning Research, 18:187Г1.
D. Kingma and J. Ba. Adam: A method for stochastic optimization. In International Conference on Learning Representations, 2015.
C. Leng, H. Li, S. Zhu, and R. Jin. Extremely low bit neural network: Squeeze the last bit out with admm. In AAAI Conference on Artificial Intelligence, 2018.
F. Li and B. Liu. Ternary weight networks. Preprint arXiv:1605.04711, 2016.
H. Li, S. De, Z. Xu, C. Studer, H. Samet, and Goldstein T. Training quantized nets: A deeper understanding. In Advances in Neural Information Processing Systems, 2017.
M. Li, D. G. Andersen, J. W. Park, A. J. Smola, and A. Ahmed. Scaling distributed machine learning with the parameter server. In USENIX Symposium on Operating Systems Design and Implementation, volume 583, pp. 598, 2014a.
M. Li, D. G. Andersen, A. J. Smola, and K. Yu. Communication efficient distributed machine learning with the parameter server. In Advances in Neural Information Processing Systems, pp. 19Г27, 2014b.
9

Under review as a conference paper at ICLR 2019
X. Lin, C. Zhao, and W. Pan. Towards accurate binary convolutional neural network. In Advances in Neural Information Processing Systems, pp. 344Г352, 2017.
Z. Lin, M. Courbariaux, R. Memisevic, and Y. Bengio. Neural networks with few multiplications. In International Conference on Learning Representations, 2016.
A. Polino, R. Pascanu, and D. Alistarh. Model compression via distillation and quantization. In International Conference on Learning Representations, 2018.
R. Rabenseifner. Optimization of collective reduction operations. In International Conference on Computational Science, pp. 1Г9, 2004.
M. Rastegari, V. Ordonez, J. Redmon, and A. Farhadi. XNOR-Net: ImageNet classification using binary convolutional neural networks. In European Conference on Computer Vision, 2016.
S. J. Reddi, S. Kale, and S. Kumar. On the convergence of adam and beyond. In International Conference on Learning Representations, 2018.
F. Seide, H. Fu, J. Droppo, G. Li, and D. Yu. 1-bit stochastic gradient descent and application to data-parallel distributed training of speech DNNs. In Interspeech, 2014.
J. Wangni, J. Wang, J. Liu, and T. Zhang. Gradient sparsification for communication-efficient distributed optimization. Preprint arXiv:1710.09854, 2017.
W. Wen, C. Xu, F. Yan, C. Wu, Y. Wang, Y. Chen, and H. Li. TernGrad: Ternary gradients to reduce communication in distributed deep learning. In Advances in Neural Information Processing Systems, 2017.
S. Wu, G. Li, F. Chen, and L. Shi. Training and inference with integers in deep neural networks. In International Conference on Learning Representations, 2018.
H. Zhang, J. Li, K. Kara, D. Alistarh, J. Liu, and C. Zhang. The ZipML framework for training models with end-to-end low precision: The cans, the cannots, and a little bit of deep learning. In International Conference on Machine Learning, 2017.
S. Zhou, Z. Ni, X. Zhou, H. Wen, Y. Wu, and Y. Zou. DoReFa-Net: Training low bitwidth convolutional neural networks with low bitwidth gradients. Preprint arXiv:1606.06160, 2016.
C. Zhu, S. Han, H. Mao, and W. J. Dally. Trained ternary quantization. In International Conference on Learning Representations, 2017.
10

Under review as a conference paper at ICLR 2019

A LEMMAS

Lemma 1. E(g~t) = g^t, and E( g~t - g^t 2)  g g^t  g^t 1.

A.1 PROOF FOR LEMMA 1

Proof. For the ith element g~t,i of the quantized gradient g~t, denote its two adjacent quantized values as Br,i and Br+1,i with Br,i  |gt,i|/st < Br+1,i, its expectation satisfies:

E(g~t,i) = E(st и sign(g^t,i) и qt,i) = st и sign(g^t,i) и E(qt,i)

= st и sign(g^t,i) и (pBr+1,i + (1 - p)Br,i)

= st и sign(g^t,i) и (p(Br+1,i - Br,i) + Br,i)

=

st

и sign(g^t,i) и

|g^t,i| st

=

g^t,i.

Thus E(g~t) = g^t, and the variance of the quantized gradients satisfy

d

E( g~t - g^t 2) =

(stBr+1,i - |g^t,i|)(|g^t,i| - stBr,i)

i=1

=

s2t

d
(Br+1,i
i=1

-

|g^t,i| )( |g^t,i| st st

-

Br,i)



st2

d
(Br+1,i
i=1

-

Br,i)

|g^t,i| st

= g g^t  g^t 1.

Lemma 2. For vt = element vt,i satisfies
and

t j

=1(1

-

)t-j gj2

with

j

 {1, 2 и и и , t},

gj



< G, then its ith

vt,i  (1 - )gt2,i,

 vt,i



G,

 vt 2 =

t
(1 - )t-j gj 2.
j=1

Proof.

 vt,i =

t
vt,i = (1 - )t-j gj2,i  (1 - )gt2,i.
j=1

t
(1 - )t-j gj2,i 
j=1

t
(1 - )t-j G2  G.
j=1

 vt 2 =

d
vt,i =
i=1

dt
(1 - )t-j gj2,i =
i=1 j=1

t
(1 - )t-j gj 2.
j=1

Lemma 3. [Lemma 10.3 in (Kingma & Ba, 2015)] Let g1:T,i = [g1,i, g2,i, . . . , gT,i]  RT be the vector containing the ith element of the gradients for all iterations up to T , and gt be bounded as in Assumption A3,

T

gt2,i t

 2G

g1:T ,i

2.

t=1

11

Under review as a conference paper at ICLR 2019

B PROOFS

B.1 PROOF FOR THEOREM 1

Proof. When only weights are quantized, the update for loss-aware weight quantization is

wt+1 = wt - tDiag( v^t)-1g^t,

Consider

the

ith

parameter

of

wt.

the

update

for

it

is

wt+1,i

=

wt,i

-

t

g^t,i
v^t,i

,

which

implies

(wt+1,i - wi)2

=

(wt,i - wi)2 - 2t(wt,i - wi)

gt,i v^t,i

+2t(wt,i

-

wi) (gt,i

- g^t,i) v^t,i

+

t2(

g^t,i )2. v^t,i

After rearranging,

(8)

gt,i(wt,i - wi) =

v^t,i 2t

((wt,i

-

wi)2

-

(wt+1,i

-

wi)2)

+(wt,i - wi)(gt,i - g^t,i) + t

v^t,i ( 2

g^t,i )2. v^t,i

(9)

Since ft is convex, we have

d
ft(wt) - ft(w)  gt (wt - w) = gt,i(wt,i - wi).
i=1

(10)

As ft is convex (Assumption A1) and twice differentiable (Assumption A2),

ft(u) - ft(v) 2  L u - v 2 for any u, v is equivalent to 2ft

LI where I is

the identity matrix. Thus from the domain bound assumption wm - wn 2  D, we have

wt - w

2 Ht



L

wt - w

2  LD2,

wt - w^ t

2 Ht



L

wt - w^ t

2.

(11)

Combining (9) and (10) sum over all the dimensions for i  {1, 2 и и и , d} and over all the iterations for t  {1, 2, и и и , T }, we have

R(T ) =

T ft(wt) - ft(w)
t=1



d i=1

v^1,i 21

(w1,i

-

wi)2

+

d i=1

T
(
t=2

v^t,i - 2t

v^t-1,i 2t-1

)(wt,i

-

wi)2

T
+

d
wt - w, gt - g^t +

T t ( 2

t=1 i=1 t=1

g^t2,i ) (1 - )g^t2,i

 D2 d 2

T
T v^T,i +

1
Ht 2

(wt

-

w),

Ht-

1 2

(gt

-

g^t)

+ G 1-

d

g^1:T,i 2

i=1 t=1

i=1

 D2 d 2

T
T v^T,i +

i=1 t=1

wt - w

2 Ht

wt - w^ t

2 Ht

+

G 1-

d

g^1:T,i 2

i=1



D2

 dT

2

v^T

2

+

G 1-

d

T g^1:T,i 2 + LD

i=1 t=1

wt - w^ t

2 Ht



D2

 dT

2

T

(1 - )T -t

g^t

2 + G

 d

1-

T
g^t 2

t=1 t=1

T + LD
t=1

wt - w^ t

2 Ht

.

(12)

12

Under review as a conference paper at ICLR 2019

The first inequality comes from Lemma 2. The second inequality comes from the definition of Ht. The third inequality comes from the definition of Ht and Cauchy's inequality. The fourth inequality
comes from (11). The last inequality comes from Lemma 2.

For m-bit (m > 1) loss-aware weight quantization in (8), as w^ t = arg minw

w-wt

2

=

Diag( v^t-1)

d i=1

v^t-1,i(w - wt,i)2. If wt,i is within the representable range, i.e., tMk-1  wt,i  tMk,

as v^t-1,i > 0, the optimal w^t,i satisfies w^t,i  {tMr,i, tMr+1,i}, where r is the index that

satisfies tMr,i  wt,i  tMr+1,i. Since  = max{1, и и и , T }, we have

(w^t,i - wt,i)2 = min{(tMr+1,i - |wt,i|)2, (tMr,i - |wt,i|)2}  ( tMr+1,i - tMr,i )2  22w . 24

(13)

In the other case, if wt,i is exterior of the representable range, the optimal w^t,i is just the nearest representable value of wt,i, thus

(w^t,i - wt,i)2 = (|wt,i| - tMk)2  wt2,i.

(14)

From (13) and (14), and sum over all the dimensions we have

w^ t - wt

2  d22w + 4

wt

2  d22w + D2. 4

(15)

From (11) and (15),

w^ t - wt

2 Ht

 L(D2 + d2w2 ). 4

From (16) and Lemma 2, we have from (12)



R(T )  D2 dG

T + dG2

T + LD

2 1 - 

D2 + d2w2 T. 4

Thus the average regret

R(T )/T  D2 G + G2 d + LD D2 + d2w2 .

2 1 -  T

4

(16)

B.2 PROOF FOR PROPOSITION 1

Proof. From Lemma 1,

E( g~t 2) = E( g~t - g^t 2) + g^t 2  g g^t  g^t 1 + g^t 2.

Denote {x1, x2, . . . , xd} as the absolute values of the elements in g^t sorted in ascending order. From Cauchy inequality, we have

g^t  g^ 1

=

xd

d i=1

xi

=

d i=1

xdxi



d-1 1 (
i=1

+

 2d 2

-

1 x2i

+

1

+

1 2d

-

1 x2d)

+

xd2

=

 1 + 2d - 1
2

d



xi2 = 1 +

2d - 1 2

g^t 2.

i=1



The

equality

holds

iff

x1

=

x2

=

иии

=

2 1+ 2d-1

xd.

Thus

we

have



E( g~t 2)  ( 1 +

2d - 1 2 g + 1)

g^t 2.

13

Under review as a conference paper at ICLR 2019

B.3 PROOF FOR THEOREM 2

Proof. When both weights and gradients are quantized, the update is

wt+1 = wt - tDiag( v~t)-1g~t.

(17)

Similar to the proof for Theorem 1, and using that E(g~t) = g^t, we can get

E(R(T ))



d
E(
i=1

v~1,i 21

(w1,i

-

wi)2)

+

d i=1

T t=2

E

( v~t,i - 2t

v~t-1,i 2t-1

)(wt,i

-

wi)2



Td
+ E( wt - w, gt - g~t ) +

T t E( 2

t=1 i=1 t=1


g~t2,i ) (1 - )g~t2,i



D2

d
E(

2

T

T v~T,i) +

E

1
Ht 2 (wt

-

w

),

Ht-

1 2

(gt

- g^t)

i=1 t=1

+ G 1-

d
E( g~1:T,i 2)

i=1



D2

d
E(

2

T
T v~T,i) + E(

i=1 t=1

wt - w

2 Ht

wt - w^ t

2 Ht

)

+ G 1-

d
E( g~1:T,i 2)

i=1



D2

 dT

E(

2

 v~T

2)

+

G 1-

d

E( g~1:T,i 2)

i=1

T + LD E(
t=1

wt - w^ t

2 Ht

)



D2

 dT

2

T

(1 - )T -tE(

g~t

2) + G

 d

1-

T
E( g~t 2)

t=1 t=1

T + LD E(
t=1

wt - w^ t

2 Ht

).

As (16) in the proof for Theorem 1 still hold, using Proposition 1 and Assumption A3, we have



E(R(T ))





D2

 dT

2

T

(1 - )T -t

g^t

2 + G

 d

1-

t=1


T

g^t

2


t=1



О

1 + 2d - 1 2 g + 1 + LD

D2 + d22w 4



D2 G + G2 2 1 - 

 dT



1+

2d - 1 2 g + 1 + LD

D2 + d2w2 . 4

E(R(T )/T ) 

D2 G + G2 2 1 - 



1+

2d - 1 2 g

+ 1 d T

+ LD

D2 + d22w . 4

14

Under review as a conference paper at ICLR 2019

B.4 PROOF FOR PROPOSITION 2

Proof. From Lemma 1,

E(Qg(Clip(g^t))) = Clip(g^t),

and E( Qg(Clip(g^t)) 2)  E(g Clip(g^t)  Clip(g^t) 1 + Clip(g^t) 2).

As

[g^t]i



N (0, 2),

its

pdf

is

f (x)

=

e 1

-

x2 22

22

,

thus

d +

+

E( Clip(g^t) 1)  E( g^t 1) =

E(|[g^t]i|) = d

|x|f (x)dx = 2d

xf (x)dx

i=1 -

0

= d +  1

e-

x2 22

dx2

=

d -22

e-

u 22

+

1

= (2/) 2 d.

0 22

22

0

As E( Clip(g^t) 2)  E( g^t 2), and that E( g^t 2) = d2, we have

E( Qg(Clip(g^t)) 2)  E(g Clip(g^t)  Clip(g^t) 1 + Clip(g^t) 2)



1
gc(2/) 2 d + E(

Clip(g^t)

2)



1
((2/) 2 cg + 1)E(

g^t

2).

B.5 PROOF FOR PROPOSITION 3

Proof.

+

-c

E( Clip(g^t) - g^t 2) = d(

(x - c)2f (x)dx +

(-x - c)2f (x)dx)

c -

+

 2d

(x - c)2f (x)dx

c

=

 2d

3

(-ce-

c2 2

+

 (1 + c2)(1 - erf( c )))

22

2

2

=

(2/)

1 2

d2

R(c).

B.6 PROOF FOR THEOREM 3

Proof. When both weights and gradients are quantized, and gradient clipping is applied before gra-

dient quantization, the update then becomes

wt+1

=

wt

-

t

 Diag( vt

)-1

gt.

(18)

Similar to the proof for Theorem 1, and using that E(Qg(Clip(g^t))) = Clip(g^t), we can get

E(R(T ))



d
E(
i=1

v1,i 21

(w1,i

-

wi)2)

+

d i=1

T t=2

E

( vt,i - 2t

vt-1,i 2t-1

)(wt,i

-

wi)2



Td
+ E( wt - w, gt - gt ) +

T

t 2

E



t=1 i=1 t=1

 gt2,i

(1 - )gt2,i



D2 d E( 2

TT
T vT,i) + E wt - w, gt - g^t + E wt - w, g^t - Clip(g^t)

i=1 t=1

t=1

+ G 1-

d

E( g1:T,i 2)

i=1

15

Under review as a conference paper at ICLR 2019



D2 d E( 2

T

T vT,i) +

E

1
Ht 2 (wt

-

w

),

Ht-

1 2

(gt

-

g^t)

i=1 t=1

T
+

E wt - w, g^t - Clip(g^t)

+ G 1-

d

E( g1:T,i 2)

t=1 i=1





D2

dT E(

2

 vT

2)

+

G 1-

d

E( g1:T,i 2)

i=1

T + LD E(
t=1

T

wt - w^ t

2 Ht

)

+

E(

t=1

wt - w 2

(g^t - Clip(g^t)) 2)

  D2 dT
2

T

(1 - )T -tE(

gt

 2) + G d
1-

t=1

T
E( gt 2)
t=1

T + LD E(
t=1

T

wt - w^ t

2 Ht

)

+

D

E(

t=1

g^t - Clip(g^t) 2).

(16) in the proof for Theorem 1 still hold. Similar to the proof for Theorem 2, using the domain

tbioounn3daansdsutmhapttionTt=(1w1t m-2wTn

,

2  D and we have

wm - wn   D for any wm, wn  S), Proposi-

E(R(T )/T ) 

D2 G + G2 2 1 - 

(2/)

1 2

cg

+

1

d

T

+LD

D2

+

d22w

+

1 dD(2/) 4

R(c).

4

C EXPERIMENTAL SETUP
The detailed setup for the CIFAR-10 and ImageNet data sets are shown in Table 4.

Table 4: Experimental setup. Here, B is the total batch size summed over N workers, and T is the

number of iterations. dataset optimizer network weight decay N initial learning rate B

T

2 0.0002 128 300k

CIFAR-10 Adam Cifarnet

-

4

0.0004

256 150k

8 0.0004 512 75k

16 0.0008 1024 37.5k

ImageNet Adam AlexNet

0.0005

2 4 8

0.0001 0.0002 0.0004

256 370k 512 185k 1024 92.5k

16


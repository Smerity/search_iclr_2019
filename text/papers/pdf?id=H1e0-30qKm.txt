Under review as a conference paper at ICLR 2019
UNLABELED DISENTANGLING OF GANS WITH GUIDED SIAMESE NETWORKS
Anonymous authors Paper under double-blind review
ABSTRACT
Adversarial learning has greatly improved the abilities of generative models to learn from data (e.g. images) and generate convincing data samples by sampling from simple latent noise distributions. However, applications which could benefit from this generative power, such as image manipulation, are hindered by the lack of controlling specific interpretable factors in the latent space of the learned models. Guiding the sampling process to produce sample variation which is semantically disentangled and meaningful for humans is not possible with current generative models unless large amounts of attribute labelled data are available. In this paper, we propose Unlabeled Disentangling GAN (UD-GAN), which decomposes the latent noise space into semantically meaningful dimensions, using only weak supervision, and without using labels or explicit attributes. When we sample a new data point we can independently control each latent noise slice of UD-GAN and manipulate in a targeted fashion the properties of generated data. Our contributions encompass the introduction of our novel architecture combining an adversarial with contrastive loss, an analysis and probabilistic interpretation of the loss function, and multiple experiments to illustrate the capabilities of our method.
1 INTRODUCTION
Generative Adversarial Networks (GANs) (Goodfellow et al., 2014) are generative model estimators, where two neural networks (generator and discriminator) are trained in an adversarial setting, so that likelihood-based probabilistic modeling is not necessary. This works particularly well for sampling from a complex probability distribution, such as images. Although GANs yield realistic looking images (Radford et al., 2015), the original formulation in (Goodfellow et al., 2014) only allows for randomly sampling from the data distribution without structural or semantic control over the generated data points. For example, it might be desirable for an application to control the color of an object in an image independently from its other attributes.
One way to control the sampling process is to use conditional GANs (Mirza & Osindero, 2014; Odena et al., 2017). These models modify the generator by conditioning it with supervised labels. Then, they either take the same labels as input in the discriminator (Mirza & Osindero, 2014) and measure the image-label compatibility, or classify the correct label at the output, given the generated image (Odena et al., 2017). Conditional GANs rely on a dataset with labels, which might not always be available or might be time-consuming to collect.
In this paper, we propose Unlabeled Disentangling GAN (UD-GAN), which decomposes various semantic attributes of a dataset without using labels, and therefore enables to independently control each attribute when sampling a data point. This control is possible by passing the generated images through multiple Siamese Networks (Chopra et al., 2005), each of which embeds the images into a different representation space.
2 RELATED WORK
There have been many studies on learning disentangled representations in generative models, which can be grouped into the level of supervision/labeled data they require.
1

Under review as a conference paper at ICLR 2019

Disentangled representations (supervised). In (Zhu et al., 2014; Yang et al., 2015), the identity and the viewpoint of an object are disentangled via reconstructing the same object from a different viewpoint and minimizing a reconstruction loss. Whereas in (Kingma et al., 2014; Makhzani et al., 2016), the style and category of an object is separated via autoencoders, where an encoder embeds the style of an input image to a latent representation, and a decoder takes the category and style input to reconstruct the input image. In (Tran et al., 2017; Yin et al., 2017), autoencoders and GANs are combined to decompose identity and attribute of an object, where the disentangled representation is obtained at the encoder outputs, and image labels are used at the output of the discriminator.
Disentangled representations (semi-supervised). In (Reed et al., 2014), they clamp the hidden units for a pair of images with the same identity but with different pose or expression to have the same identity representation. Whereas in (Kulkarni et al., 2015), synthesized images are used to disentangle pose, light, and shape of an object by passing a batch of images where only one attribute varies and the rest of the representation is clamped to be the same. These techniques only require a batch of samples with one attribute different at a time.
Disentangled representations (unsupervised). InfoGAN (Chen et al., 2016) is an unsupervised technique that discovers categorical and continuous factors by maximizing the mutual information between a GAN's noise variables and the generated image. The method uncovers disentangled latent spaces that correspond to data properties. After a model is trained, a human needs to investigate which factors map to which data property. As the method is unsupervised not all desirable factors might be represented. In contrast, our method can benefit from weak supervision and allows learning of factors which correspond to the intuition of the system designer.

3 UNLABELED DISENTANGLING GAN

3.1 BACKGROUND: GENERATIVE ADVERSARIAL NETWORKS

In GANs, the generator, G(.), maps a latent variable z, which has an easy-to-sample distribution, into a more complex and unknown distribution, such as images. On the other hand, the discriminator D(.) tries to distinguish real images from the ones that are generated by G. In (Goodfellow et al., 2014), the training is performed as a minimax game as follows:

min max V (G, D) = E [log D(x)] + E [log(1 - D(G(z)))],

GD

xPR

zPZ

(1)

where PR and PZ are the probability distributions of real images and images generated from the latent variable z, respectively. Recently, there have been improvements on training speed and stability of GANs (Berthelot et al., 2017; Seward et al., 2018; Arjovsky et al., 2017). One such example is to use Wasserstein-1 distance along with a gradient penalty (Gulrajani et al., 2017) as follows:

min max V (G, D) = E [D(x)] - E [D(G(z))] + GP E [(||x^D(x^)||2 - 1)2]. (2)

G DD

xPR

zPZ

x^Px^

Here, D is the set of 1-Lipschitz functions, GP is the coefficient for the gradient penalty, and x^ is a pixel-wise linear combination of a real and a generated image, and is distributed with Px^. We train the GAN part of our model using the loss in equation 2.

3.2 A NOVEL GAN ARCHITECTURE: UD-GAN
In a standard GAN setting, all of the variation in the distribution of real images is captured by the latent variable z. However, a single dimension or a slice of z does not necessarily have a semantic meaning. In this paper, our target is to slice the latent variable into multiple vectors, where each vector controls a different semantic variation.
Our network architecture is visualized in Figure 1. In our method, the latent vector z = [q1, q2, ..., qNA , r] is the concatenation of NA knobs, {qi}iN=A1, which represent different attributes we aim to disentangle, and a final variable r, which captures the remaining variation (and the noise) that is not picked up by the knobs. In our notation, qi refers to all of the knobs, except qi. Each latent variable is sampled from a multivariate, zero-mean, unit-variance normal distribution.

2

Under review as a conference paper at ICLR 2019

Sampled Latents

Generated Images

Contrastive Loss Pull
Push

Discriminator Generator

Real Images

Real or Fake?

Figure 1: The flowchart of our architecture. Sampled latents from different attributes are combined into latent vectors. Generated images are grouped with respect to different attributes (here, represented by shape) by Siamese Networks (denoted as i).

In order to train our model, first, for each qi, we sample two different vectors, qi(1) and q(i2), from the noise prior. If we would attempt to form a batch by combinatorially concatenating all knob
samples, we get a batch size of 2NA , which grows exponentially with the number of attributes. To

avoid this computational burden, we train our model through stochastic sampling of one attribute at a time. For example, if the ith attribute is chosen, we generate three images using the following

combinations:

x11

=

G(qi(1),

q(1),
i

r),

x12

=

G(qi(1)

,

q(2),
i

r),

x21

=

G(qi(2),

q(1),
i

r).

(3)

Here, {x11, x12, x21} are the generated images, {q(i1), qi(2)} are the samples for the ith knob, and

{q(1),
i

q(2)}
i

represent

the

samples

for

all

knobs

except

qi.

The

main

premise

is

that

the

two

images

that are generated with the same qi vectors, namely {x11, x12}, should have the same ith attribute,

regardless of the values of qi. Similarly, {x11, x21} should have a different ith attribute. We can

ensure this via employing a Contrastive Loss (Hadsell et al., 2006) on generated image pairs. To

this end, the generated images are processed by Siamese Networks (Chopra et al., 2005), which we

denote as i(.). These networks compute embedding vectors for each generated image, which are used to calculate distances in these spaces. We then use them to disentangle the generation process,

with the goal such that if we change the value of a knob qi, only the corresponding embedding vector i(x) changes, and the rest of the embedding vectors stay the same. We use a Contrastive
Loss function to pull similar image pairs together, and push dissimilar pairs apart. We can write the

loss as:

L = Ei

{q(i1)

,qi(2)

,q(1)
i

,q(2)
i

,r}N

i(x11) - i(x12)

2 2

+

max(0,



-

i(x11) - i(x21)

2)2,

(4)

where, Li is the Contrastive Loss for the ith Siamese Network i, and  is the margin for the Euclidean distance between two image embeddings.

The discriminator network D is not modified and is trained to separate real and generated image distributions. Donahue et al. (2018) use a similar latent variable slicing for capturing illumination and pose variations of a face with a fixed identity. Their discriminator needs image pairs, which must be labeled for real images, to judge the quality and identity of the faces. Our method does not require any labels for the real images. Instead, we create pseudo-similar and dissimilar image pairs via concatenating latent variables and generating image batches. Our final loss function is:

L = Li , i  Cat(NA)

min
G

max
DD

V

(G,

D)

=

LW

GAN

+

L,

(5)

3

Under review as a conference paper at ICLR 2019

where, LW GAN is the Wasserstein loss described in equation 2,  is the weight of the embedding loss, and the sampling of the latent variables depends on i and is performed as described above.
3.3 PROBABILISTIC INTERPRETATION
Siamese Networks, or similar energy-based models, have been used to learn an invariant mapping for a particular task (Chopra et al., 2005). In our case, if each Siamese Network i implements a mapping to different semantic attributes, we can achieve disentanglement, and consequently, control over the generation process.
We can gain a probabilistic interpretation of this process on a toy example. Let us assume a problem, where we want to generate images of colored polygons (see Figure 1). The two knobs qi and qj represent the shape and the color of a polygon, respectively. r controls the remaining variation in the data distribution, i.e., there can be variation other than shape and color, such as location. When we set qj to a certain value and vary qi, we want to generate polygons with the same color, but different shapes, and vice versa. In order to ensure that, we can get two samples for qi and qj (the rest of the knobs have the same value), and generate four images as shown in Figure 2:

x11 x12 x21 x22

(a) G(q(i1), qj(1), ..)

(b) G(q(i1), q(j2), ..)

(c) G(qi(2), qj(1), ..)

(d) G(q(i2), qj(2), ..)

Figure 2: A combination of knobs are used to generate four images with two varying attributes (in this case, shape and color).

Let P be the probability distribution of colored polygons. For each attribute, P can be decomposed into a mixture distribution as follows:

Ni

P=

i(k)

(k)
Qi

k=1

 for attribute i,

Nj

P=

j(k)

(k)
Qj

k=1

 for attribute j

(6)

where,

(k)
Qi

is

a

mixture

component

and

i(k)

is

its

corresponding

probability

of

choosing

it,

and

Ni

is the number of different values an attribute (in the example case, i corresponds to shape) can take.

A similar explanation can be made for attribute j, i.e. color. For the sake of this analysis, we accept

that for each attribute, P can be decomposed into different discrete mixture distributions as shown

in

Figure

3.

For

this

specific

case,

(1)
Qi

and

(2)
Qi

are

the

distributions

of

colored

squares

and

colored

diamonds,

respectively.

For

the

color

attribute,

which

is

indexed

by

j,

each

(k)
Qj

corresponds

to

a

distribution of polygons with a single color (i.e., green polygons). Our Siamese Networks have two

tasks.

First,

they

need

to

separate

all

(k)
Qi

from

each

other

within

each

attribute

i.

This

is

achieved

via the second term of the contrastive loss in equation 4. Second, the networks need to minimize

the

variation

within

each

mixture

component

(k)
Qi

.

If

all

samples

from

(k)
Qi

are

mapped

to

the

same

embedding vector, the effect of j (and any other attribute) on determining the shape disappears and

disentangling is achieved. The first term in equation 4 is minimizing the variance at each mixture

component. In the following, we focus on the shape attribute that is represented by i, however,

derivations carry over to the color attribute j.

In order to push mixture components apart and minimize their variances, we can maximize a diver-

gence

between

all

pairs

from

(k)
Qi

.

One

way

to

measure

the

distance

between

these

distributions

is

4

Under review as a conference paper at ICLR 2019

(1)
Qi

(2)
Qi

(2)
Qj

(4)
Qj

(1)
Qj

(3)
Qj

(a) i-space (shape)

(b) j-space (color)

Figure 3: Illustration of the embedding spaces and separated probability distributions after training our model.

to use the energy distance (Sze´kely & Rizzo, 2004):

DE (Qi(1),

Qi(2);

i,

j)

=

x1nj

E
Qi(1),x2nj Q(i2)

i(x1nj , x2nj )

- 21 Ex1nj ,x1nj Q(i1) i(x1nj , x1nj )

1 - 2 Ex2nj ,x2nj Qi(2) i(x2nj , x2nj ) ,

(7)

where the index nj represents an image that is generated with a sample qj(nj) for the jth knob. The function i has the form:

i(x, x ) =

i(x) - i(x )

2 2

.

(8)

The energy distance in equation 7 can be interpreted as an instance of Maximum Mean Discrepancy (Bin´kowski et al., 2018) and resembles the Contrastive Loss (Hadsell et al., 2006). If we rewrite equation 7 with an unbiased estimator, we have:

DE

(Qi(1),

(2)
Qi

;

i

,

j

)

=

1 4

2

2

nj =1 nj =1

i(x1nj ) - i(x2nj )

2 2

-1 2

i(x11) - i(x12)

2 2

-1 2

i(x21) - i(x22) 22.

We can rewrite equation 9 using the Contrastive Loss of equation 4 as follows:

(9)

DE (Q(i1), Qi(2); i, j)

=

1 4

2

2

nj =1 nj =1

i(x1nj ) - i(x2nj )

2 2

12 2 + max(0,  -
4
nj =1 nj =1

i(x1nj ) - i(x2nj ) 2)2

- Li .

(10)

Note that it was used that {x11, x12} and {x21, x22} are equivalent positive pairs, {x11, x21} and {x11, x22} are equivalent negative pairs. The sum of the first two terms is a piecewise quadratic function and has its minimum at i(x) - i(x ) 2 =  /2 and the value of the minimum is 2 /2. So, we can rewrite equation 10 as follows:

DE (Qi(1), Q(i2); i, j)



2 2

- Li

.

(11)

5

Under review as a conference paper at ICLR 2019

Therefore, as the margin  is a constant, minimizing our embedding loss Li maximizes the lower

bound for the energy distance DE. This corresponds to learning a Siamese Network i that separates

the

probability

distributions

(1)
Qi

and

(2)
Qi

,

i.e.,

colored

squares

and

colored

diamonds,

from

each

other and minimizes the variance within each distribution, thus resulting in disentangling the effect

of j from i. The same derivation can be made for the color attribute. After jointly training the

Siamese Networks, we hope to achieve the embedding spaces represented in Figure 3.

3.4 GUIDED SIAMESE NETWORKS
The Siamese Networks i are desired to map images into embedding spaces, where they can be grouped within a distinct semantic context. For the example shown in Figure 4, where we disentangle the shape and the color, this might not be achievable in a completely unsupervised setting, because the separation in equation 6 is not unique. However, we can still benefit from the disentangling capability of our method via small assumptions and domain knowledge, without collecting labeled data.
Consider the toy example, where we extend the MNIST dataset (LeCun & Cortes, 2010) to have a random color, sampled from a uniform RGB color distribution. We define our problem to independently capture the shape of a digit with q1 and its color with q2.
In Figure 4(a), we show images created by a generator, which is trained along with two networks, 1 and 2, without any guidance in an unsupervised setting. We can see that the knobs, q1 and q2, capture the variations in the data, however, these variations are not semantically meaningful. Each knob modifies a complicated combination of shape and color.
However, if we design a network architecture in a slightly smarter way, we should be able to separate the shape and the color attributes. This is exemplified in Figure 4(b), where instead of feeding the whole image to 2, we feed the average color of some randomly sampled pixels from a generated image. This choice prevents 2 to capture the spatial structure of the generated digit and to focus only on color. After the training our method with a modified 2, the first network captures shape of a digit, and the second one captures the color variations. This can also be observed in Figure 4(c) and 4(d), where we use t-SNE (van der Maaten & Hinton, 2008) to visualize embedding spaces for shape and color, respectively.

q(21)

q(22)

q(11)

q(21)

q(22)

(a) Unsupervised

q1(2)
(b) Guided

(c) Shape t-SNE

(d) Color t-SNE

Figure 4: (a) Samples from the colored version of the MNIST dataset. (b) Images generated after an unsupervised training with two knobs and (c) after a guided training (the knob values are interpolated between two values and then concatenated to generate the final image). (d) The t-SNE representation of the embedding vectors for shape and (e) color.

4 EXPERIMENTS
4.1 SETUP We perform our experiments on a server with Intel Xeon Gold 6134 CPU, 256GB system memory, and an NVIDIA V100 GPU with 16GB of graphics memory. Our generator and discriminator layers are the same as in (Karras et al., 2018), except our architecture is modified to generate 64 × 64 pixel
6

Under review as a conference paper at ICLR 2019

images. Each knob is a 4-dimensional slice of the latent variable, and the remaining variation vector r has 128 dimensions.
In order to guide the training, we modify our Siamese Networks to obtain different effects as illustrated in Figure 5. We adjust our guided networks to map images into 8-dimensional embeddings, which are on patch-level for Figure 5(a), 5(b), and 5(c), and image-level for Figure 5(d). The embedding vectors are normalized with L2 norms along the feature dimensions.

Dense Dense
8 x 8 x 16 Conv2D Conv2D
8 x 16 x 16 Conv2D Conv2D
8 x 16 x 16 Conv2D Conv2D

3 x 64 x 64

3 x 64 x 64 Sobel Edges

3 x 64 x 64 Crop

Average 3 x 64 x 64 Color

8

(a) Unguided

(b) Edges

(c) Cropped

(d) Average color

Figure 5: Siamese networks can be guided to measure (or ignore) certain information via feeding (a) a generated image, (b) detected edges of an image, (c) a cropped image, or (d) its average color.

We use ADAM (Kingma & Ba, 2014) as an optimizer for our training with the following parameters: learning rate=0.001, 1 = 0, 1 = 0, 2 = 0.99, = 10-8. The weight values for the Wasserstein gradient penalty is GP = 10 and for the contrastive loss is  = 100.
Our training images are from the aligned MS-Celeb-1M dataset (Guo et al., 2016). We randomly sample one picture for each celebrity, obtain a dataset of around 100K images, and resize the images to 64 × 64 pixels. This random sampling prevents our model to rely on near-identical images in the dataset and makes the learning process generalize better.
4.2 UNGUIDED DISENTANGLING
We first train a GAN with two attributes with the unguided network in Figure 5(a). Results from this experiment are illustrated in Figure 6(a). Here, we can see that, two knobs capture some variations in the data that are coupled, i.e. they both modify similar attributes, and are not semantically meaningful. This result is similar to what we obtain for colored MNIST digits in Figure 4(a).
4.3 GUIDED DISENTANGLING
In order to show the effect of the guided siamese networks, we perform three experiments on the MS-Celeb dataset by using different guiding networks from Figure 5. In the first experiment, only one of the two networks is guided with an edge detector at the input, as shown in Figure 5(b). Results of this experiment are shown in Figure 6(b). We can see that the first knob, which is connected to edges, captures the overall outline and roughly controls the identity of the generated face. On the other hand, the unguided second knob modifies the image with minimal changes to image edges. This change, in this case, corresponds to the lighting of the face.
We perform a second experiment with the edge detector, where in this case, the second knob is guided with the average color of the generated image (see Figure 5(d)). In Figure 6(c), we can observe the results of our disentangled image manipulation. The first knob with the edge detector again captures the outline of the face, and the second average color knob modifies a combination of the light and the skin color, similar to the results in Figure 6(b).
In our final experiment, we employ the cropped guidance networks in Figure 5(c). The two knobs receive the cropped top and bottom part of the image for training. Although these image crops are not independent, we still get acceptable results that are shown in Figure 6(d). Adjusting the first knob only modifies the upper part of the face; the hair and the eyes. Similarly, the second knob is responsible for determining the chin and mouth shape.
7

Under review as a conference paper at ICLR 2019

Unguided

Unguided

Edges

Unguided

(a) Unguided Average Color

(b) Edges & Pixels Cropped (bottom)

Cropped (top)

Edges

(c) Edges & Average Color

(d) Cropped

Figure 6: The results of UD-GAN using differently guided siamese networks.

4.4 DISCUSSION
In (Dumoulin et al., 2017; Donahue et al., 2016; Zhu et al., 2017), in order to perform semantic manipulation, an encoder infers the latent representation of an input image. Here, we instead employ multiple Siamese Networks, each of which is related to independent slices of the latent variable, thus enabling disentangled image manipulation with targeted control over potentially predefined features.
In our experiments, we use the aligned MS-Celeb dataset, where each face is roughly centered around the nose. This reduces the variation and simplifies the problem of embedder design. We can employ our method in more complex scenarios with better guided networks and pre-trained models. From a general perspective, we can use any differentiable function, whether it is fixed, trainable, or pre-trained, as our siamese networks i. In addition, with pre-trained models, information learned on one dataset can be used to disentangle other unlabeled datasets.
5 CONCLUSION
In this paper we introduced UD-GAN, a novel GAN formulation which employs Siamese networks with contrastive losses in order to make slices of the latent noise space disentangled and more semantically meaningful. Our experiments encompassed guided and unguided approaches for the embedding networks, and illustrate how UD-GAN can be used for semantically meaningful image manipulation. Our visualisations of the embedding spaces, and the performed image manipulations confirm that our method can adjust well to the intrinsic factors of variation of the data. In future work we plan to investigate more powerful forms of embedders, e.g. extracting information from pre-trained networks for semantic segmentation and landmark detection can allow for even more powerful novel image manipulation techniques.
8

Under review as a conference paper at ICLR 2019
REFERENCES
Martin Arjovsky, Soumith Chintala, and Le´on Bottou. Wasserstein generative adversarial networks. In ICML, 2017.
David Berthelot, Tom Schumm, and Luke Metz. Began: Boundary equilibrium generative adversarial networks. CoRR, 2017.
M. Bin´kowski, D. J. Sutherland, M. Arbel, and A. Gretton. Demystifying MMD GANs. ICLR, 2018.
Xi Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya Sutskever, and Pieter Abbeel. Infogan: Interpretable representation learning by information maximizing generative adversarial nets. NIPS, 2016.
Sumit Chopra, Raia Hadsell, and Yann LeCun. Learning a similarity metric discriminatively, with application to face verification. In CVPR, 2005.
C. Donahue, Z. C. Lipton, A. Balsubramani, and J. McAuley. Semantically Decomposing the Latent Spaces of Generative Adversarial Networks. ICLR, 2018.
Jeff Donahue, Philipp Kra¨henbu¨hl, and Trevor Darrell. Adversarial feature learning. CoRR, 2016.
Vincent Dumoulin, Mohamed Ishmael Diwan Belghazi, Ben Poole, Alex Lamb, Martin Arjovsky, Olivier Mastropietro, and Aaron Courville. Adversarially learned inference. 2017.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in Neural Information Processing Systems. 2014.
Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron C Courville. Improved training of wasserstein gans. In Advances in Neural Information Processing Systems. 2017.
Yandong Guo, Lei Zhang, Yuxiao Hu, Xiaodong He, and Jianfeng Gao. MS-Celeb-1M: A dataset and benchmark for large scale face recognition. In European Conference on Computer Vision, 2016.
R. Hadsell, S. Chopra, and Y. LeCun. Dimensionality reduction by learning an invariant mapping. CVPR, 2006.
Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. Progressive growing of GANs for improved quality, stability, and variation. In ICLR, 2018.
Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. CoRR, 2014.
Diederik P Kingma, Shakir Mohamed, Danilo Jimenez Rezende, and Max Welling. Semi-supervised learning with deep generative models. In Advances in Neural Information Processing Systems. 2014.
Tejas D Kulkarni, William F. Whitney, Pushmeet Kohli, and Josh Tenenbaum. Deep convolutional inverse graphics network. In Advances in Neural Information Processing Systems. 2015.
Yann LeCun and Corinna Cortes. MNIST handwritten digit database. 2010.
Alireza Makhzani, Jonathon Shlens, Navdeep Jaitly, and Ian Goodfellow. Adversarial autoencoders. In ICLR, 2016.
Mehdi Mirza and Simon Osindero. Conditional generative adversarial nets. CoRR, 2014.
A. Odena, C. Olah, and J. Shlens. Conditional image synthesis with auxiliary classifier gans. In ICML, 2017.
Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep convolutional generative adversarial networks. CoRR, 2015.
9

Under review as a conference paper at ICLR 2019
Scott Reed, Kihyuk Sohn, Yuting Zhang, and Honglak Lee. Learning to disentangle factors of variation with manifold interaction. In ICML, 2014.
Calvin Seward, Thomas Unterthiner, Urs Bergmann, Nikolay Jetchev, and Sepp Hochreiter. First order generative adversarial networks. In ICML, 2018.
Ga´bor J. Sze´kely and Maria L. Rizzo. Testing for equal distributions in high dimensions. InterStat, 2004.
L. Tran, X. Yin, and X. Liu. Disentangled representation learning gan for pose-invariant face recognition. In CVPR, 2017.
Laurens van der Maaten and Geoffrey Hinton. Journal of Machine Learning Research, 2008. Jimei Yang, Scott Reed, Ming-Hsuan Yang, and Honglak Lee. Weakly-supervised disentangling
with recurrent transformations for 3d view synthesis. In NIPS, 2015. Weidong Yin, Yanwei Fu, Leonid Sigal, and Xiangyang Xue. Semi-latent gan: Learning to generate
and modify facial images from attributes. CoRR, 2017. Jun-Yan Zhu, Richard Zhang, Deepak Pathak, Trevor Darrell, Alexei A Efros, Oliver Wang, and Eli
Shechtman. Toward multimodal image-to-image translation. In Advances in Neural Information Processing Systems. 2017. Zhenyao Zhu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Multi-view perceptron: a deep model for learning face identity and view representations. In NIPS, 2014.
10


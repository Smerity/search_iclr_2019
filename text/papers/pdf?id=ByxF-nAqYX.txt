Locally Linear Unsupervised Feature Selection

Guillaume Doquet

Mich`ele Sebag

LRI, CNRS - INRIA - Universit´e Paris-Sud, France

Abstract
The paper, interested in unsupervised feature selection, aims to retain the features best accounting for the local patterns in the data. The proposed approach, called Locally Linear Unsupervised Feature Selection (LluFS), relies on a dimensionality reduction method to characterize such patterns; each feature is thereafter assessed according to its compliance w.r.t. the local patterns, taking inspiration from Locally Linear Embedding (Roweis and Saul, 2000). The experimental validation of the approach on the scikit-feature benchmark suite demonstrates its effectiveness compared to the state of the art.
Keywords : Unsupervised learning, Feature Selection, Dimension Reduction
1 Introduction
Machine Learning faces statistical and computational challenges due to the increasing dimension of modern datasets. Dimensionality reduction aims at addressing such challenges through embedding the data in a lower dimensionality space, in an unsupervised [Karlen et al., 2008, Roweis and Saul, 2000, Tenenbaum et al., 2000, Wang et al., 2015] or supervised [Gaudel and Sebag, 2010, Fukumizu et al., 2004, Nazarpour and Adibi, 2015] way.
The requirement for understandable Machine Learning [Vellido et al., 2012, Doshi-Velez and Kim, 2017] however makes it desirable to achieve interpretable dimensionality reduction. In order to do so, the simplest way is to select a subset of the initial features, i.e. to achieve feature selection (FS), as opposed to generating compound new features from the initial ones, a.k.a. feature construction. For instance, determining the genes most important w.r.t. a given disease or the underlying generative model of the data can be viewed as the mother goal in bioinformatics [Guyon et al., 2002a, Libbrecht and Noble, 2015].
In the supervised ML setting, features are assessed and selected based on their relevance to the prediction goal [Guyon and Elisseeff, 2003, Sheikhpour et al., 2017, Chen et al., 2017]. Unsupervised learning, aimed at making sense of the data, however constitutes a primary and most important task of ML, as emphasized by [LeCun, 2017], while supervised ML intervenes at a later stage of the data exploitation process.
Unsupervised FS approaches [He et al., 2005, Zhao and Liu, 2007, Cai et al., 2010, Li et al., 2012, Zhu et al., 2017] (more in section 2) essentially rely on the assumption that the data samples are structured in clusters, and use the cluster partition in lieu of labels, making it possible to fall down on supervised FS, and select the features most amenable to characterize and separate the clusters. A main limitation of this methodology is that clusters are bound to rely on some metric defined from the initial features (with the notable exception of [Li et al., 2012]), although this metric can be arbitrarily corrupted based on irrelevant or random features. On the other hand, as far as one considers the unsupervised setting, a feature can hardly be considered irrelevant per se.
The main contribution of the paper is to address both limitations: the proposed approach, called Locally Linear Unsupervised Feature Selection (LluFS) jointly determines patterns in the data, and features relevant to characterize these patterns. LluFS is a 2-step process (Sec. 3): In a first step, a compressed representation of the data is built using Auto-Encoders [Vincent et al., 2008, Feng et al., 2014]. In a second step, viewing the initial dataset as a high-dimensional embedding of the compressed dataset, each feature is scored according to its contribution to the reconstruction error of the embedding, taking inspiration from Locally Linear Embedding [Roweis and Saul, 2000, Saul and Roweis, 2003, Wang, 2012].
1

After describing the goals of experiments and the experimental setting used to validate the approach, extensively relying on the scikit-feature project [Li et al., 2017, SKf, 2018] (Sec. 4), the empirical validation is presented and discussed (Sec. 5), establishing the merits and discussing the weaknesses of the approach. The paper concludes with a discussion and some perspectives for further research.

Notations

X denotes the m × D data matrix. Row X[i, :], also noted xi when no confusion is to fear, is the i-th sample (xi in RD). Column X[:, j] is the j-th feature. For j in [[1, D]], µj and j respectively denote the mean and the standard deviation of the j-th feature on the dataset. 1 denote the m-dimensional constant vector [1, ..., 1]t.

Let S denote an m × m similarity matrix (Si,i > 0), with  the associated diagonal degree matrix (i =

m i=1

Si,j

).

The un-normalized (resp.

normalized) Laplacian associated to S

is defined as L = M - S

(resp.

L

=



-1 2

L

-1 2

).

Two

particular

similarities

will

be

considered

in

the

following;

the

supervised

similarity

SU P ,

with SU Pi,j = 0 iff xi and xj do not belong to the same class, and 1/|Ci| if they both belong to class Ci , and

the

unsupervised

similarity

RBF ,

with

RBFi,j

=

exp{-

1 

xi - xj

2 2

},

and



a

hyper-parameter

of

the

method.

2 Formal Background

Supervised FS aims to select a subset of features such that it maximizes the eventual classifier accuracy. Supervised FS algorithms divide into filters, wrappers and embedded methods. Filter methods [Yu and Liu, 2003, Senawi et al., 2017] operate at the data pre-processing stage, and are agnostic to the classifier algorithm. Wrappers methods [Kabir et al., 2010, Rodrigues et al., 2014] aim to determine the feature subset yielding a best accuracy when used within a specific classifier, through solving a black-box optimization problem. Embedded methods [Guyon et al., 2002b, Zhang et al., 2015] alternatively learn and use the learned hypothesis to prune/select the unpromising/promising features. Admittedly, wrapper and embedded approaches might produce a candidate feature set with moderate generality (being linked to a particular classifier) and moderate interpretability (with the retained features being good as a gang). Since this paper focuses on unsupervised and interpretable FS, only filter methods will thus be considered in the following.
An early supervised filter method based on the so-called Fisher score was introduced by [Duda et al., 2000], independently ranking features according to their correlation with the labels. 1 A general limitation of such scores is that they achieve a myopic feature selection, with XOR problems - where all relevant features need to be taken into account simultaneously - as typical failure cases.
A prominent unsupervised filter approach is based on spectral clustering: data clusters are first built using some metric or similarity; thereafter supervised FS approaches are used with these clusters in lieu of classes [Von Luxburg, 2007, Binkiewicz et al., 2017]. [He et al., 2005] introduce the Laplacian score1, where each feature score measures how well this feature accounts for the sample similarity. Interestingly, while the Fisher score is a particular case of Laplacian score using the SU P similarity, the Laplacian score overcomes the myopic limitations of the Fisher score when using the RBF similarity. The Laplacian score is also remotely related to the MaxVariance FS method [Kantardzic, 2003], selecting features with large variance for the sake of their higher representative power.
Also relying on spectral clustering is the Spec approach [Zhao and Liu, 2007], proposing three scores respectively noted 1, 2 and 3. Spec relies on the core idea that relevant features be smooth w.r.t. the graph, i.e. slowly varying among samples close to each other. After the spectral clustering theory [Shi and Malik, 1997, Ng, 2001], considering eigenvectors 0, ..., m-1 of the normalized Laplacian L (respectively associated with eigenvalues 0 < 1 < ... < m-1), smooth features are aligned with the first eigenvectors, hence the score 1:

j



[[1, D]],

1j

=

X [:,

t
j] L

X [:,

j]

where

X [:,

j]

=

1 2

X [:,

j]/



1 2

X

[:,

j

]

(1)

1. For the sake of space limitations, formal definitions are reminded in Appendix 1.

2

Eigenvectors 0, ..., m-1 of L define soft cluster indicators, and eigenvalues 0 < 1 < ... < m-1 measure the

separability of the clusters. The smaller 1j, the more efficient the j-th feature is to separate the clusters.

As

the

first

eigenvector

0

=

M

11
2

does

not

carry

any

information,

with

0

=

0,

one

might

rather

consider

the projection of the feature vector X[:, j] on the orthogonal space of 0:

j  [[1, D]], 2j =

1

t
X[:, j] L X[:, j]

1 - X[:, j], 0

(2)

Finally, in the case where the target number of clusters  is known, only the top- eigenvectors are considered,

and score 3 is defined as:

-1
j  [[1, D]], 3j = (2 - k) X[:, j], k 2

(3)

k=1

Features are ranked in ascending order for 1 and 2, and in descending order for 3. The above three scores measure the overall capacity of a feature to separate clusters, which might prove

inefficient in multi-classes/multi clusters settings: a feature most efficient to separate a pair of clusters might

have a mediocre general score. The Multi-Cluster Feature Selection (Mcfs) [Cai et al., 2010] addresses this limitation by defining a score per cluster. Formally, the capacity of X[:, j] to separate clusters is estimated

through fitting the eigenvectors (reminding that k is a soft indicator of the k-th cluster) up to a regularization term. Letting A[k,:] denote a vector in RD, with  a regularization weight:

k  [[1, ]], A[k, :] = min
BRD

k - XB

2 2

+



B

1

(4)

The L1 regularization term enforces the sparsity of A[k, :], retaining only the features most relevant to this cluster. The overall MCFS score simply takes the maximum over all clusters of the absolute value of Ak,j:

j  [[1, D]], M CF Sj = max |Ak,j|
k[1,]

(5)

A general limitation of the above scores is to rely on a similarity metric that can be arbitrarily cor-

rupted by noisy features, potentially leading to irrelevant clusters and scores. This limitation is addressed

by [Li et al., 2012], introducing the Nonnegative Discriminative Feature Selection (Ndfs) approach. Ndfs

jointly optimizes the D × D feature importance matrix A together with a cluster indicator matrix , with an

L2 regularization:

, A = arg min T r(tL ) + (  - XA
,A

2 F

+

A

2 1

)

(6)

subject to  orthogonal and semi-positive definite (t = ID,   0), with ,  regularization weights. Following [Yu and Shi, 2003], the first term is rewritten as :

T r(tL) = 1

m

S[i, j]

2
[i, :] - [j, :]

2
i,j=1

i j 2

(7)

The minimization of Eq. (6) tends to enforce the intra-cluster similarity and the inter-cluster dissimilarity. The orthogonality and nonnegativity constraints on  further enforce that each sample belong in exactly one cluster.

3 Locally Linear Unsupervised Feature Selection (LLUFS)
This section first presents LLE for the sake of self-containedness, before presenting and discussing LluFS.
3.1 Locally Linear Embedding
LluFS takes inspiration from the Locally Linear Embedding (LLE) defined by [Roweis and Saul, 2000, Saul and Roweis, 2003]. LLE relies on the so-called Johnson Lindenstrauss lemma [Larsen and Nelson, 2017]:

3

Johnson-Lindenstrauss Lemma .



]0, 1[, d

>

8ln(m)
2

,

f

:

RD



Rd

s.t.

:

(i, j)  [[1, m]]2, (1 - ) xi - xj 2 < f (xi) - f (xj) 2 < (1 + ) xi - xj 2 with f being a linear mapping

(composed only of translations, rotations and rescalings).

As this lemma guarantees the existence of a low-dimensional embedding approximately preserving the pair-

wise distances among the points, LLE [Roweis and Saul, 2000]: i) defines the local structure of the m data

points xi  RD, through approximating each point as the barycenter of its nW nearest neighbors; ii) finds points z1, . . . zm in Rd, with d D, such that the zi satisfy the same local relationships as the xis. Formally,
let N (i) denote the set of indices of the nW nearest neighbors of xi; weights Wi,j such that they minimize the
Euclidean distance

xi -

Wi,j

jN (i)

subject to jN(i) Wi,j = 1, Wi,j  0 and Wi,j = 0 for j  N (i). Note that W is invariant under rotation,

translation or homothety on the dataset X: it captures the local structure of the xis. The LLE dimensionality

reduction thus proceeds by finding another set of points zis in Rd, such that they satisfy the local relationships

expressed by W :

Z = arg min

Z -WZ

2 F

Z Rm×d

(8)

3.2 Overview of LluFS
While LLE is performed as a dimensionality reduction technique, its principle is general: after the local structure of the data has been captured through matrix W , this matrix can be used to transport the data from any source to target representation.
As our goal is to achieve feature selection, we implicitly assume that the X data live in a low-dimension space. Accordingly, the proposed LluFS approach proceeds by: i) finding a low-dimension representation of the data in Rd; ii) characterizing the matrix W capturing the data structure in this low-dimension representation; iii) using W to assess the initial features, as detailed below.

Dimensionality reduction of X. This step can be achieved using linear or non-linear approaches, ranging from PCA [Wold et al., 1987] and SVD [Deerwester et al., 1990] to Isomap [Tenenbaum et al., 2000] or t-SNE [Maaten and Hinton, 2008]. For the sake of generality and robustness, LluFS uses the non-linear Stacked Denoising AutoEncoder neural networks (SDAE, [Vincent et al., 2010, Feng et al., 2014]), meant to achieve a non-linear compression robust w.r.t. input noise.
Let Z denote the (m, d) data obtained from X through this dimensionality reduction. For each i = [[1, m]], let N (i) be the set of nW nearest neighbors of zi, with W the (m, m) matrix minimizing Z - W Z under the positivity and sum-to-1 constraints defined in Sec. 3.1.

Distorsion score. Let us consider the Z as the "true" data, with the X as an inflated and corrupted image

of the Z. The overall loss of information from Z to X is measured as

X - WX

2 F

.

Most interestingly, this

overall loss of information can be decomposed with respect to examples, with:

 2

Err(xi) = xi -

Wi,j xj 

jN (i)

and with respect to the initial features, with:


m

2

Distorsion (X[:, j]) = X[i, j] -

Wi,kX[k, j]

i=1 kN (i)

4

The distorsion associated to the j-th feature is thus interpreted in terms of how much this feature is corrupted with respect to the "true" local structure of the data, defined from the Zs. The features with lowest distorsion thus are deemed the most representative of the data. Note that, although the distorsion score is defined for each initial feature, it might implicitly take into account the global structure of the data, captured by the W .

3.3 Discussion

One weakness of the method is that the distorsion scores depend on the latent representation produced by

the auto-encoder, which might be biased due to the redundancy of the initial features; typically, duplicating

an initial feature will entail that the latent representation is more able to express this feature, mechanically

reducing its distorsion score. For this reason, a preliminary step is to detect and reduce the redundancy of the

initial features.

In order to do so, LluFS i) normalizes the initial features (with zero mean and unit variance); ii) uses

Agglomerative Hierarchical feature clustering [Krier et al., 2007, VanDijck and VanHulle, 2006], using a high

number

of

clusters

nc

(nc

=

3 4

D

in

the

experiments);

iii)

selects

one

feature

per

cluster

(the

nearest

one

to

the

cluster mean); iv) apply the auto-encoder on the pruned data.

Further work is concerned with taking into account the feature redundancy within the AE loss.

A second limitation is due to the sensitivity of the distorsion score to the feature distribution. Typically,

while a constant feature carries no information, its distorsion is null. Likewise, the distorsion of discrete features

depends on their being balanced. In order to alleviate this issue, the reliability of the distorsion associated to

each feature is measured through an empirical p-value [Stoppiglia et al., 2003]. Given a p-value threshold  ,

1/ copies of each feature are generated and independently shuffled. The feature distorsion is deemed relevant

iff it is lower than the distorsion of all shuffled copies.

Algorithm 1: Locally Linear Unsupervised Feature Selection (LluFS)

1 LluFS (empirical p-value threshold  , number of clusters nc, embedding dimension d, number of

neighbors nW );

Input : X, nc,  , d, nW

2 Normalize features to zero mean and unit variance. 3 Perform Agglomerative Hierarchical Feature Clustering, producing Xfiltered  Rm×nc .

4 Train a SDAE on Xfiltered to produce compressed representation Z.

5

Solve W = arg min

Z -WZ

2 F

subject to the positivity and sum-to-1 constraints.

6 for j  [[1, nc]] do

mm

7 Compute Distorsion(Xfiltered[:, j]) = |Xfiltered[i, j] - W [i, l]Xfiltered[l, j]|

i=1 l=1

8 end

9 Initialize set of candidates Cand to all features, ranked in ascending order w.r.t. distorsion. Initialize

selection subset Sel = Ø.

10 while |Cand| > 0 do

11

Create

1 

random permutations 0, ...,  1 

of the first feature f0 in Cand.

12

for k  [[1,

1 

]] do

m

m

13 Compute Distorsion(k) = |f0[k(i)] - W [i, l]f0[k(l)]|

i=1 l=1

14 end

15

if

Distorsion(f0) < mink[[1,

1 

]] Distorsion(k) then

16 Sel  Sel  f0.

17 end

18 Cand  Cand - f0.

19 end

20 Return Sel

5

4 Experimental setting
Goals of experiments. The main goal of the experimental validation is to assess LluFS compared to state of the art unsupervised feature selection approaches. The performance assessment commonly falls back on the supervised setting, where the indicator is the predictive accuracy of a classifier trained from the selected features, where the number of selected features ranges from 1 to D [Chen et al., 2017]. 2 For the sake of clarity and to sidestep issues related to classifier hyper-parameter tuning, the classifier considered in the following is the 1-nearest neighbor classifier. Secondly (Q2), the respective impacts of both LluFS ingredients, the feature clustering pre-processing, and the proper FS mechanism, are assessed. Thirdly (Q3), experiments will investigate the robustness of the proposed approach specifically w.r.t. XOR concepts (Sec. 2).

Baselines and benchmarks. The experimental setting extensively relies on the scikit-feature project [Li et al., 2017, SKf, 2018], defining a de facto standard for FS approaches through algorithm implementations and datasets [Chen et al., 2017, Zadeh et al., 2017]. Five baseline algorithms are considered: Laplacian Score (Lap) [He et al., 2005], Spectral Feature Selection (Spec, considering the 1 score, Sec. 2) [Zhao and Liu, 2007], Multi-Cluster Feature Selection (Mcfs) [Cai et al., 2010], and Non-Negative Discriminative Feature Selection (Ndfs) [Li et al., 2012]; the fifth and last baseline (Random), aimed to assess the impact of the only feature pre-processing in LluFS (Q2), is defined by uniformly selecting the features after the feature clustering process (Alg. 1). 7 benchmark datasets from [Li et al., 2017, SKf, 2018] are considered (Appendix 2): 6 datasets in the domains of image and bioinformatics, and the Madelon XOR problem [Guyon and Elisseeff, 2003] to empirically investigate (Q3). The number of features range from 500 to 10,000; the number of classes range from 2 to 11, and the number of examples is less than 200 (except for Madelon, with 2,000 examples). As discussed in Sec. 3.3, only datasets with continuous features are considered. All features are normalized (zero mean, unit variance).

Hyper-parameters. LluFS involves three hyper-parameters: the number nc of clusters used in the feature-

preprocessing; the methodology used to build the latent representation; and the number of neighbors nW

considered in Sec. 3 to build the W matrix.

nc

is set

to

3 4

D

for

all datasets.

The methodology used to build the

latent representation is

a

5-layer

stacked

denoising

auto-encoder

[Vincent

et

al.,

2010]

with

architecture

D-

D 2

-

D 4

-

D 8

-

D 4

-

D 2

-D

with

tanh

activation

function, trained to minimize the MSE loss for 102 epochs with a 10-3 learning rate. The denoising process

uniformly selects 20% of the features and sets them to 0 for each example. nW is set to 6 for all datasets,

considering the small number of samples.

5 Empirical validation
This section reports and discusses the comparative performance of all unsupervised FS methods, in view of the experiment goals.
2. Another option is to cluster the samples w.r.t. the selected features, and consider the normalized mutual information between class labels and cluster labels as a performance indicator. Preliminary experiments however show that this indicator is hardly sensitive to the considered FS method.

6

Figure 1 ­ Assessment of unsupervised FS algorithms LluFS, Lap, Spec, Mcfs, Ndfs and Random: predictive accuracy of 1-nn classifier vs number d of selected features, on datasets Madelon, Allaml, Carcinom, Lung, Orlraws10p, Pixraw10p, Tox171 .
5.1 Comparison on biological and image datasets
Fig. 1 displays the performance curve (1-nearest neighbor accuracy) vs the number d of selected features. 3 On datasets Allaml and Tox171 (Fig.1 (a) and (b)), LluFS dominates all other methods over the whole learning curve. On Allaml, both Lap and Ndfs do much better than Mcfs, suggesting that the feature clusters are not much relevant to the classification task. Spec shows a robust performance after sufficiently many features have been selected (d > 50). On Tox171, both Lap and Mcfs do much better than Ndfs, suggesting that the feature set presents a complex cluster structure (captured by Ndfs) misleading to the classification task. Likewise, Spec yields good results after the beginning of the curve (d > 20). In both cases, Random is significantly outperformed, suggesting that quite a few feature clusters are irrelevant to the prediction task.
On datasets Pixraw10p and Orlraws10p (Fig.1 (c) and (d)), LluFS is dominated by Ndfs at the beginning of the curve (d < 5 for Pixraw10p and d < 10 for Orlraws10p); it thereafter dominates all other algorithms on Orlraws10p (resp. dominates the others then reaches the same nearly maximal performance as all other algorithms on Pixraw10p). The relative comparative weakness of LluFS at the very beginning of the curve is interpreted as LluFS capturing patterns related to subsets of features (as the latent representation is bound to globally account for the initial features). The importance of a feature standalone thus can hardly be accounted for, contrasting with Ndfs. On both datasets, Random yields a decent performance (ranking 2nd or 3rd, especially for high values of d), which suggests that a main issue with those image datasets is the redundance of the features.
On Carcinom (Fig.1 (e)), LluFS is dominated by all algorithms but Mcfs at the beginning of the curve (d < 20); it then catches up and dominates the other algorithms for d > 60. The best algorithm on this dataset is Lap, suggesting that the cluster structure defined from all features is relevant to the prediction task, which might explain why LluFS and Mcfs, more sensitive to local patterns, are outperformed.
On Lung (Fig.1 (f)), LluFS is consistently dominated by Ndfs and Spec over the whole learning curve and performs similarly as Random, suggesting that the compressed representation learned by the neural network does not accurately represent data structure.
3. The variance of the predictive accuracy over 25 independent runs of the Random baseline is circa 0.02 for d < 5, 5  10-3 for d = 20); the confidence bars are omitted in the figures for the sake of readability.
7

5.2 The XOR problem
As said, the Madelon problem (Appendix 2) is chosen to investigate the comparative performances of unsupervised FS methods in the case where, by construction, FS based on independent feature scores are bound to fail. Two clusters of methods are clearly seen on this problem (Fig. 1 (g)). Most methods fail to do better than random selection, due to the low signal to noise ratio (96% of the features being pure noise), adversely affecting spectral clustering methods and clusters based on Laplacian eigenvectors. Ndfs does much better, as it simultaneously learns cluster indicators and feature relevance. LluFS does even better as the latent representation tends to highlight the data patterns, and the distorsion score measures whether a feature is relevant to these patterns. The drop of performance of NDFS and LluFS, after the number of selected features goes beyond the number of relevant features (20), is blamed on the addition of noise features perturbing the metric and misleading the 1-nn classifier.
6 Conclusion and future work
A novel approach to unsupervised feature selection has been proposed in this paper, with a proof of concept of its empirical merits. The core idea is to find an "oracle" representation of the data, and to consider the actual data as an inflated and corrupted image of the oracle data. The quality of each feature is thereafter assessed depending on how it contributes to the loss of information between the "oracle" and the actual data.
A first perspective for further research, taking inspiration from Ndfs, is to allow a feature to be partially relevant, e.g. through considering the quantiles of its distorsion. A second perspective is to integrate the feature redundancy in the auto-encoder loss, to decrease the bias in favor of redundant features. The approach will also be extended to supervised feature selection.
References
[SKf, 2018] (2018). Scikit-feature project. http://featureselection.asu.edu/index.php. [Binkiewicz et al., 2017] Binkiewicz, N., Vogelstein, J. T., and Rohe, K. (2017). Covariate-assisted spectral
clustering. Biometrika, 104(2):361­377. [Cai et al., 2010] Cai, D., Zhang, C., and He, X. (2010). Unsupervised feature selection for multi-cluster data.
KDD. [Chen et al., 2017] Chen, J. et al. (2017). Kernel feature selection via conditional covariance minimization.
Advances in Neural Information Processing Systems, pages 6946­6955. [Deerwester et al., 1990] Deerwester, S. Dumais, S. T., Furnas, G. W., and Landauer, T. K. (1990). Indexing
by latent semantic analysis. Journal of the American Society for Information Science. [Doshi-Velez and Kim, 2017] Doshi-Velez, F. and Kim, B. (2017). Towards a rigorous science of interpretable
machine learning. arXiv preprint arXiv:1702.08608. [Duda et al., 2000] Duda, R. O., Hart, P. E., and Stork, D. G. (2000). Pattern classification. Wiley-Interscience,
2. [Feng et al., 2014] Feng, X., Zhang, Y., and Glass, J. (2014). Speech feature denoising and dereverberation
via deep autoencoders for noisy reverberant speech recognition. Acoustics, Speech and Signal Processing (ICASSP). [Fukumizu et al., 2004] Fukumizu, K., Bach, F. R., and Jordan, M. I. (2004). Dimensionality reduction for supervised learning with reproducing kernel hilbert spaces. Journal of Machine Learning Research, 5(Jan):73­ 99. [Gaudel and Sebag, 2010] Gaudel, R. and Sebag, M. (2010). Feature selection as a one-player game. In Proceedings of the International Conference on Machine Learning, pages 359­366. [Guyon, 2003] Guyon, I. (2003). Design of experiments for the nips 2003 variable selection benchmark. http: //clopinet.com/isabelle/Projects/NIPS2003/Slides/NIPS2003-Datasets.pdf.
8

[Guyon and Elisseeff, 2003] Guyon, I. and Elisseeff, A. (2003). An introduction to variable and feature selection. Journal of Machine Learning Research, 3:1157­1182.
[Guyon et al., 2002a] Guyon, I. et al. (2002a). Gene selection for cancer classification using support vector machines. Machine learning, 46(1-3):389­422.
[Guyon et al., 2002b] Guyon, I., Weston, J., Barnhill, S., and Vapnik, V. (2002b). Gene selection for cancer classification using support vector machines. Machine learning, 46(1-3):389­422.
[He et al., 2005] He, X., Cai, D., and Niyogi, P. (2005). Laplacian score for feature selection. Advances in Neural Information Processing Systems.
[Kabir et al., 2010] Kabir, M. M. et al. (2010). A new wrapper feature selection approach using neural network. Neurocomputing, 73(16-18):3273­3283.
[Kantardzic, 2003] Kantardzic, M. (2003). Data Reduction. Wiley Online Library.
[Karlen et al., 2008] Karlen, M., Weston, J., Erkan, A., and Collobert, R. (2008). Large scale manifold transduction. In Proceedings of the 25th international conference on Machine learning, pages 448­455.
[Krier et al., 2007] Krier, C. et al. (2007). Feature clustering and mutual information for the selection of variables in spectral data. ESANN, pages 157­162.
[Larsen and Nelson, 2017] Larsen, K. and Nelson, J. (2017). Optimality of the johson-lindenstrauss lemma. 2017 IEEE 58th Annual Symposium on Foundations of Computer Science (FOCS).
[LeCun, 2017] LeCun, Y. (2017). A path to ai. https://futureoflife.org/wp-content/uploads/2017/01/YannLeCun.pdf.
[Li et al., 2017] Li, J. et al. (2017). Feature selection: A data perspective. ACM Computing Surveys (CSUR), 50(6).
[Li et al., 2012] Li, Z., Yang, Y., Liu, Y., Zhou, X., and Lu, H. (2012). Unsupervised feature selection using non-negative spectral analysis. AAAI.
[Libbrecht and Noble, 2015] Libbrecht, M. W. and Noble, W. S. (2015). Machine learning applications in genetics and genomics. Nature Reviews Genetics, 16(6).
[Maaten and Hinton, 2008] Maaten, L. V. D. and Hinton, G. (2008). Visualizing data using t-sne. Journal of machine learning research, 9:2579­2605.
[Nazarpour and Adibi, 2015] Nazarpour, A. and Adibi, P. (2015). Two-stage multiple kernel learning for supervised dimensionality reduction. Pattern Recognition, 48(5):1854­1862.
[Ng, 2001] Ng, A. (2001). On spectral clustering : Analysis and an algorithm. Advances in Neural Information Processing Systems.
[Rodrigues et al., 2014] Rodrigues, D. et al. (2014). A wrapper approach for feature selection based on bat algorithm and optimum-path forest. Expert Systems with Applications, 41(5):2250­2258.
[Roweis and Saul, 2000] Roweis, S. T. and Saul, L. K. (2000). Nonlinear dimensionality reduction by locally linear embedding. Science, New Series, 290:2323­2326.
[Saul and Roweis, 2003] Saul, L. K. and Roweis, S. T. (2003). Think globally, fit locally: unsupervised learning of low dimensional manifolds. Journal of machine learning research, 4(Jun):119­155.
[Senawi et al., 2017] Senawi, A., Wei, H. L., and Billings, S. A. (2017). A new maximum relevance-minimum multicollinearity (mrmmc) method for feature selection and ranking. Pattern Recognition, 67:47­61.
[Sheikhpour et al., 2017] Sheikhpour, R. et al. (2017). A survey on semi-supervised feature selection methods. Pattern Recognition, 64:141­158.
[Shi and Malik, 1997] Shi, J. and Malik, J. (1997). Normalized cuts and image segmentation. CVPR.
[Stoppiglia et al., 2003] Stoppiglia, H. et al. (2003). 'ranking a random feature for variable and feature selection. Journal of machine learning research, 3:1399­1414.
[Tenenbaum et al., 2000] Tenenbaum, J., De Silva, V., and Langford, J. (2000). A global geometric framework for nonlinear dimensionality reduction. Science, 290:2319­2323.
9

[VanDijck and VanHulle, 2006] VanDijck, G. and VanHulle, M. M. (2006). Speeding up the wrapper feature subset selection in regression by mutual information relevance and redundancy analysis. In International Conference on Artificial Neural Networks, pages 31­40.
[Vellido et al., 2012] Vellido, A., Mart´in-Guerrero, J. D., and Lisboa, P. J. (2012). Making machine learning models interpretable. In ESANN, 12:163­172.
[Vincent et al., 2008] Vincent, P., Larochelle, H., Bengio, Y., and Manzagol, P. A. (2008). Extracting and composing robust features with denoising autoencoders. Proceedings of the 25th international conference on Machine learning, pages 1096­1103.
[Vincent et al., 2010] Vincent, P., Larochelle, H., Lajoie, I., Bengio, Y., and Manzagol, P. A. (2010). Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion. Journal of Machine Learning Research, 11:3371­3408.
[Von Luxburg, 2007] Von Luxburg, U. (2007). A tutorial on spectral clustering. Statistics and computing, 17(4):395­416.
[Wang, 2012] Wang, J. (2012). Locally linear embedding. In Geometric Structure of High-Dimensional Data and Dimensionality Reduction, pages 203­220.
[Wang et al., 2015] Wang, X., Liu, Y., Nie, F., and Huang, H. (2015). Discriminative unsupervised dimensionality reduction. In IJCAI, pages 3925­3931.
[Wold et al., 1987] Wold, S., Esbensen, K., and Geladi, P. (1987). Principal component analysis. Chemometrics and intelligent laboratory systems, 2:37­52.
[Yu and Liu, 2003] Yu, L. and Liu, H. (2003). Feature selection for high-dimensional data: A fast correlationbased filter solution. In Proceedings of the 20th international conference on machine learning (ICML), pages 856­863.
[Yu and Shi, 2003] Yu, S. and Shi, J. (2003). Multiclass spectral clustering. IEEE. [Zadeh et al., 2017] Zadeh, S. A. et al. (2017). Scalable feature selection via distributed diversity maximization.
AAAI, pages 2876­2883. [Zhang et al., 2015] Zhang, X. et al. (2015). Embedded feature-selection support vector machine for driving
pattern recognition. Journal of the Franklin Institute, 352(2):669­685. [Zhao and Liu, 2007] Zhao, Z. and Liu, H. (2007). Spectral feature selection for supervised and unsupervised
learning. ICML. [Zhu et al., 2017] Zhu, P. et al. (2017). Subspace clustering guided unsupervised feature selection. Pattern
Recognition, 66:364­374.
10

Appendix 1

Fisher score

Denoting c the number of classes, ni the number of samples in the i-th class, µij and ij respectively the mean and the standard deviation of the j-th feature on the i-th class, the Fisher score is defined as:

c
ni(µij - µj )2 1  j  D, Fj = i=1 c
nii2j
i=1

(9)

Laplacian score

With same notations,

1



j



D, Lj

=

1 j

i, j = 1m(Xi,j - Xk,j )Si,k

with j the standard deviation of the j-th feature and Si,k the similarity of the i-th and k-th examples. Note that Lj can be rewritten using the Laplacian matrix:

t

X[:, j] L X[:, j]

Lj =

t

X[:, j]  X[:, j]

(10)

with

X[:, j]

=

X[:, j]

-

X

[:,j ]t 1 1t 1

1.

Furthermore, the Fisher score is a particular case of the Laplacian score:

for similarity S set to the SUP similarity.

1 Lj = 1 + Fj

Appendix 2
Summary of datasets considered

Name Madelon orlraws10P pixraw10P ALLAML Carcinom lung TOX 171

# Instances 2000 100 100 72 174 203 171

# Features 500 10304 10000 7129 9182 3312 5748

# Classes 2 10 10 2 11 5 4

Data type Artificial XOR Face Image Face Image Biological Biological Biological Biological

Madelon, an artificial XOR-like dataset, was created for the Feature Selection Challenge of NIPS2003 [Guyon, 2003]. It involves 5 relevant features, the values of which are combined along two XOR concepts to define the positive and negative classes. Specifically, each class includes 25 Gaussian clusters, placed on the vertices of a hypercube. The relevant 5 features are duplicated, combined and perturbed to obtain 15 "distractor" features. 480 standard normal noise features are then added. Most classifiers (SVMs with linear, polynomial or Gaussian kernels; NNs with up to 10 layers) using all features yields the same 50% accuracy as random prediction, indicating that efficient FS is compulsory.

11

Appendix 3
Summary of experimental results

ALLAML Carcinom Lung Madelon orlraws10P pixraw10P TOX171
ALLAML Carcinom Lung Madelon orlraws10P pixraw10P TOX171

LLUFS 0.65/0.75/0.86/0.85/0.85/0.94 0.25/0.30/0.45/0.57/0.74/0.78 0.75/0.77/0.75/0.82/0.91/0.92 0.56/0.81/0.82/0.63/0.54/0.54 0.62/0.74/0.91/0.95/0.98/0.98 0.87/0.88/0.92/0.98/0.97/0.98 0.41/0.46/0.60/0.77/0.82/0.82 MCFS 0.55/0.55/0.49/0.58/0.64/0.67 0.22/0.18/0.31/0.37/0.53/0.60 0.58/0.67/0.79/0.83/0.87/0.90 0.49/0.49/0.51/0.51/0.49/0.51 0.25/0.56/0.75/0.77/0.81/0.83 0.89/0.91/0.86/0.93/0.93/0.98 0.30/0.35/0.36/0.53/0.74/0.78

LAP 0.54/0.60/0.70/0.74/0.75/0.72 0.22/0.31/0.50/0.58/0.75/0.75 0.56/0.63/0.71/0.79/0.88/0.89 0.48/0.46/0.48/0.54/0.51/0.52 0.41/0.39/0.44/0.54/0.66/0.77 0.58/0.88/0.87/0.95/0.98/0.97 0.21/0.29/0.36/0.58/0.71/0.84 NDFS 0.61/0.64/0.68/0.85/0.78/0.81 0.28/0.37/0.47/0.56/0.71/0.75 0.56/0.84/0.90/0.89/0.94/0.93 0.56/0.83/0.72/0.61/0.57/0.54 0.50/0.79/0.89/0.88/0.93/0.94 0.92/0.90/0.93/0.97/0.98/0.98 0.30/0.33/0.46/0.49/0.62/0.71

SPEC 0.61/0.56/0.58/0.69/0.76/0.87 0.15/0.28/0.46/0.58/0.71/0.71 0.60/0.69/0.78/0.90/0.92/0.91 0.52/0.49/0.53/0.51/0.49/0.53 0.45/0.40/0.45/0.53/0.77/0.78 0.56/0.47/0.57/0.81/0.80/0.85 0.26/0.35/0.42/0.65/0.77/0.78 RANDOM 0.54/0.60/0.65/0.70/0.78/0.78 0.23/0.36/0.36/0.48/0.65/0.75 0.58/0.70/0.75/0.82/0.89/0.91 0.49/0.51/0.49/0.52/0.52/0.51 0.41/0.68/0.81/0.89/0.92/0.93 0.68/0.77/0.88/0.96/0.98/0.99 0.30/0.38/0.49/0.58/0.70/0.80

Table 1 ­ Summary of predictive accuracy for each dataset and FS algorithm along the learning curve, resp. for 2/5/10/20/50/100 features selected

12


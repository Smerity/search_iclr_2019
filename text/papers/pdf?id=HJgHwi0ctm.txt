Under review as a conference paper at ICLR 2019
A COLLABORATIVE LOW-RANK RECONSTRUCTIVE LAYER FOR DOMAIN SHIFT PROBLEMS
Anonymous authors Paper under double-blind review
ABSTRACT
In many situations, the data one has access to at test time follows a different distribution from the training data. Over the years, this problem has been tackled by domain adaptation and domain generalization techniques. In this context, the most popular approach consists of learning features whose distributions are the same in all observed domains. However, with high-dimensional features, such as those of deep networks, we argue that matching distributions is an under-constrained problem; a network can learn to add noise in the representation to artificially achieve this goal. To address this, we propose to learn a collaborative low-rank representation of the data encoding information from all observed domains and well-suited for classification. We formulate this as a general layer that can either be used on its own within any standard base network, or incorporated into existing domain adaptation and generalization frameworks. We empirically show that it consistently boosts the performance of the baseline models that do not exploit our new layer, thus allowing us to achieve state-of-the-art results on several visual domain adaptation and generalization benchmark datasets.
1 INTRODUCTION
Domain shift, that is, the problem of the training and test data following different distributions, is pervasive in many application domains. In the context of computer vision, for example, it materializes from having images acquired by different sensors and in different illumination conditions, or even from the use of synthetic and real data. This phenomenon typically significantly impairs the ability of trained models to generalize well to the new test data.
Over the years, a large number of techniques have therefore been proposed to overcome the domain shift. These techniques can be roughly grouped into two categories: Those that perform domain adaptation and those that tackle domain generalization. The former work under the assumption of observing two domains, source and target, and having access to data from both these domains during training, albeit unannotated data in the target case. By contrast, the latter make use of labeled data from multiple source domains during training and aim to generalize to unseen target data from yet another domain at test time. In both scenarios, the recent trend consists of incorporating new loss functions in the training of deep networks (Ganin & Lempitsky, 2014; Long et al., 2015; Bousmalis et al., 2016; Tzeng et al., 2017; Long et al., 2016; Yan et al., 2017; Ghifary et al., 2015; Ding & Fu, 2018; Ghifary et al., 2017), so as to reduce the distribution mismatch between the different domains.
While effective, matching feature distributions suffers from an important drawback: As noted by Saito et al. (2018), features having globally the same distributions does not imply that the distribution of classes are well aligned. In (Saito et al., 2018), this was addressed by explicitly leveraging the notion of classes for distribution matching. Here, we argue that this problem goes beyond classbased alignment; the use of high-dimensional features gives existing techniques the flexibility to artificially match the distributions by introducing noise in the representations.
In this paper, we therefore introduce a simple yet effective strategy to better constrain the problem. Specifically, we propose to extract a low-rank representation of the data. For this representation to be less sensitive to the domain shift, we extract it jointly for all domains, which further makes our approach seamlessly applicable to both domain adaptation and domain generalization. Intuitively, our approach jointly denoises the features learned from all observed domains, including the target one when available, so as to extract their common information necessary for classification.
1

Under review as a conference paper at ICLR 2019

training images from various source domains with their associated categories
... `giraffe' `dog' `house' ...

photos
...

...

cartoons
...

...

... ... ...

sketches
...
target images (if available)

...

representation learning

labeled samples

classifier

all samples

CLRR

reconstructed labeled

layer samples

cross entropy loss

Figure 1: Overview of our CLRR network learning strategy. We propose to introduce an additional layer between the representation-learning and classification parts of a deep network. Intuitively, this layer jointly denoises the features learned from all observed domains so as to extract a domain-invariant representation that is well-suited for classification.

For our representation to preserve the important information contained in the data, we make use of low-rank singular value decomposition, which, for a given dimensionality, yields the lowest reconstruction error in the least-square sense. Such an operation can easily be incorporated as an additional layer in a deep network. We dub this layer Collaborative Low-Rank Reconstructive (CLRR) layer. While our CLRR layer can be exploited on its own within a standard base network, it can also be used in conjunction with most existing domain adaptation or generalization strategies.
Our experiments demonstrate the effectiveness of our approach in both these scenarios. For domain generalization, using our CLRR layer on its own within a base network, such as AlexNet (Krizhevsky et al., 2012), outperforms the state-of-the-art on the PACS dataset of Li et al. (2017), the VLCS dataset of Fang et al. (2013) and the Office-Caltech (Saenko et al., 2010; Gong et al., 2012) benchmark for object recognition task. For domain adaptation, we show that combining our approach with existing distribution-matching techniques consistently yields higher accuracies than these baselines on their own, thus evidencing the benefits of our low-rank representation. By combining it with the state-of-the-art method of French et al. (2018), this allows us to outperform this state-of-the-art on the challenging VisDA-2017 object recognition dataset of Peng et al. (2017).
Related Work: To the best of our knowledge, the methods most closely related to ours are those of (Bousmalis et al., 2016; Yang & Hospedales, 2016; Li et al., 2017). In particular, Bousmalis et al. (2016) propose to factorize the information contained in all domains into shared and domainspecific representations. While the shared representation also aims for domain invariance, as many approaches that rely on distribution matching techniques (Saito et al., 2018; Yan et al., 2017), it is not encouraged to have low rank, and thus may also suffer from the presence of noisy dimensions. In the context of multi-task learning, Yang & Hospedales (2016) proposed to incorporate a low-rank decomposition within a deep network. This was later extended to the problem of domain generalization by Li et al. (2017). These techniques, however, aim to obtain a low-rank decomposition of the network's weights, which does not translate into having a low-rank representation of the data itself. As such, the generalization method of Li et al. (2017) is subject to the same weakness as the one discussed above. By contrast, we explicitly encourage the data representation to have low rank and be shared by all observed domains. As evidenced by our results, this yields a compact, yet highly effective domain-invariant representation.
2 OUR APPROACH
In this section, we introduce our approach to tackle the domain shift problem by extracting compact, domain-agnostic features. The key idea behind our formulation is to find a joint low-rank representation of the observed domains that preserves the important information of the original data. This idea matches the observation of the contemporary work of Sanyal et al. (2018), showing that, for single domain problems, max-margin classifiers trained on low-rank representations have more dis-
2

Under review as a conference paper at ICLR 2019

Algorithm 1 : Collaborative Low-Rank Reconstructive Learning

Input:

Input

data

from

all

observed

domains:

Ii

=

{Iji

}Ni
j=1

,

i



1

·

·

·

M;

Labels

for

the

source

domains:

Yi

=

{yji

}Ni
j=1

,

i



1··

·

M

or

M

-

1

for

domain

adaptation;

Batch size ms;

Learning rate ;

Desired rank d D

Output: Learned parameters: ^ = {^r, ^c}
1: Initialize parameters  = {r, c} 2: repeat
3: for all batches of size ms from observed domains do 4: Forward pass through fr(r) 5: Flatten and concatenate the D-dimensional output features Xi into a single matrix, X =
[X1, X2, · · · , XM ]T
6: Compute Z by solving equation 1 via SVD 7: Split Z into Z1, Z2, · · · , ZM 8: Forward pass the labeled Xis and Zis through fc(c) 9: Update    - Lms () 10: end for
11: until convergence

criminatory power than ones trained on the original representations. In our context, as we obtain the low-rank representation jointly from all observed domains, it can be thought of as a denoised version of the data, encoding the information common to all domains, thus being more robust to the variations and perturbations across domains.
To obtain denoised, domain-agnostic representations, we introduce a Collaborative Low-Rank Reconstructive (CLRR) layer that can be either used on its own within a deep network, or incorporated into other domain adaptation and generalization frameworks. Below, we first derive our CLRR layer and its use within standard base networks, and then give examples, used in our experiments, of how it can be combined with other domain adaptation algorithms.
2.1 COLLABORATIVE LOW-RANK RECONSTRUCTIVE LEARNING
Let us first consider a general image recognition architecture. Such an architecture can typically be split into a representation-learning network fr(r), parametrized by r, and a classifier network fc(c), parametrized by c. Specifically, given an image Ij, fr(·) outputs a vector xj  RD that, in the standard setting, acts as input to the classifier fc(·).
Here, we tackle the scenario where we observe M domains during training. In the case of domain adaptation, M = 2 and one of the domains is the target one. Then, let Ii = {Iji}jN=i1 represent the Ni samples from the ith domain, with i  1 · · · M , and with corresponding labels Yi = {yji }Nj=i1, where yji  1 · · · C. Note that, in the case of domain adaptation, we assume to have access to unlabeled target data only and thus only have Y1 for the source domain. We propose to incorporate an additional layer between fr(·) and fc(·) that computes a joint low-rank representation of the data in all domains.
To this end, let Xi  RD×Ni be the matrix obtained by concatenating the feature vectors output by fr(·) for all samples in domain i, and let X = X1, · · · , XM T  Rn×D , n = N1 + · · · + NM be the matrix obtained by concatenating the Xis for all observed domains. Our goal is to obtain a low-rank approximation of X, so as to denoise these high-dimensional features and extract the information common to all domains. For our low-rank representation to preserve the important information in the data, we aim for it to remain close to X in the least-square sense. Formally, this
3

Under review as a conference paper at ICLR 2019

can be expressed as finding

Z = arg min
Z~
where d is the desired rank.

X - Z~

2 F

s.t. rank(Z~) = d ,

(1)

In principle, this optimization problem could be combined with a standard classification loss, such as the cross-entropy, on the low-rank representation of the labeled data. This, however, would require introducing a hyper-parameter to set the relative influence of the classification loss and the leastsquare reconstruction loss above. Furthermore, as discussed in (Sanyal et al., 2018) in the context of single-domain problems, this strategy tends to yield lower test accuracy while making training unstable. Instead, we therefore propose to leverage the fact that, according to the Eckart-YoungMirsky theorem (Eckart & Young, 1936), the problem in equation 1 has a closed form solution corresponding to the rank d Singular Value Decomposition (SVD) of X. Specifically, with X = U V T being the SVD of X, we have

Z = U~(n×d)~ (d×d)V~(Td×D) ,

(2)

where ~ (d×d) is the diagonal matrix containing the d largest singular values of X, and U~(n×d), resp. V~(Td×D), the corresponding left, resp. right, singular vectors.
As shown in (Papadopoulo & Lourakis, 2000), the SVD of a matrix can be differentiated, and thus incorporated as a layer in a deep network trained by backpropagation. This allows us to make use of a classification loss only to train our network. This loss can be written as

M Ni

M Ni

Ln(r, c) =

c(fc(zji ; c), yji ) +

c(fc(xji ; c), yji ) ,

i=1 j=1

i=1 j=1

(3)

where zji is the low-rank representation of sample j in domain i, and c is the cross-entropy loss. Note that, for notation convenience, we do not explicitly express the dependency of xj , and thus
of zj , on r in the equation above. Note also that Eq. 3 combines a classification loss on the low-rank representation with one on the original features. We found this to be more effective than
classification of the low-rank representation only and, more importantly, much more effective than
not using our CLRR layer at all. Since both terms are of commensurate magnitude, this does not
require any hyper-parameter tuning. Note also that, for domain adaptation, the first summation in both terms would only be on M - 1 = 1 domain, since we do not have access to target labels.

Figure 1 depicts the general architecture corresponding to our approach; the representation-learning network is followed by our collaborative low-rank reconstructive layer, whose corresponding loss term is implicitly minimized by the use of SVD, thus affecting the network parameters through gradient updates. In practice, to train our network, we do not make use of the whole data at once, but rely on mini-batches. While this does not guarantee preserving a low rank for the whole data, we observed that this strategy converges smoothly to a globally-consistent low-rank representation. Our CLRR network learning algorithm is summarized in Algorithm 1.

As mentioned earlier, our approach can be used either for domain adaptation or domain generalization. For adaptation, we follow the standard transductive setting, where we assume that the unlabeled target data is observed during training. The classification loss is thus defined on the labeled source data only, but the target data is leveraged to learn the low-rank representation. For generalization, we leverage multiple labeled source domains during training, but the target data is not seen. At test time, in both cases, we then simply forward pass the target data through the network, compute its low-rank representation and pass it to the classifier.

2.2 INCORPORATING A CLRR LAYER INTO OTHER FRAMEWORKS
In the context of domain generalization, as evidenced by our experiments, we found that our CLRR layer used on its own within a base network is highly effective, outperforming the state-of-the-art on standard benchmarks, thus showing its strength to learn useful domain-invariant representations by uncovering the common information of the categories available in the source domains. While this benefit translates to domain adaptation, in this case, one might also seek to better exploit the target

4

Under review as a conference paper at ICLR 2019
data to compensate for its lack of label information. An important advantage of our CLRR layer is the fact that it can seamlessly be incorporated within most frameworks that explicitly focus on better leveraging the target data, so as to further improve their accuracy. In particular, we consider the following two recent domain adaptation methods and integrate our CLRR layer into them.
· The Self Ensembling approach of French et al. (2018), which builds upon the mean teacher semi-supervised learning model proposed in (Tarvainen & Valpola, 2017). The underlying idea is that the network should make consistent predictions for the unlabeled samples under various stochastic augmentations. We integrate our CLRR layer into the student network before its classifier.
· The adversarial training method of Saito et al. (2018), which attempts to align the source and target distributions by considering task-specific decision boundaries. In other words, to detect target samples that are far from the support of the source data, and maximize the discrepancy between the source and target classifiers' outputs, a feature generator tries to fool the classifiers and generate target features near the support. Our CLRR layer can then simply be incorporated between the representation learning sub-network and the classifier.
When incorporating our CLRR layer to the above-mentioned methods, we remove the second classification loss in equation 3, as it is already encoded in the original frameworks. Furthermore, when dropout is used in the base network, we incorporate our layer before it, as we have found that it adversely affects our low-rank representation learning.
As evidenced by our experiments, incorporating our CLRR layer in these frameworks consistently outperforms the original algorithms, i.e., without our CLRR layer. This therefore allows us to achieve state-of-the-art results in several benchmark domain adaptation datasets.
3 EXPERIMENTS
In this section, we first evaluate our approach on the task of visual domain generalization and then for visual domain adaptation. Note that our method requires defining the desired rank of the learned representation. In practice, unless stated otherwise, we set this hyper-parameter to the number of categories in the dataset of interest. We have found this strategy to be effective with the standard datasets that typically contain around 10 classes. Thus, we have not tried to fine-tune this hyperparameter. Our implementation was done in PyTorch (Paszke et al., 2017) and will be made publicly available.
3.1 DOMAIN GENERALIZATION
For domain generalization, we make use of three standard datasets: the PACS dataset (Li et al., 2017), the VLCS dataset (Fang et al., 2013) and the Office+Caltech dataset (Saenko et al., 2010; Griffin et al., 2007). For this task, we make use of our CLRR layer on its own within a base network, as discussed in Section 2.1.
Results on the PACS dataset. The PACS dataset is a recent benchmark that includes four domains with very different stylistic object depictions (Photo (P), Art-painting (A), Cartoon (C) and Sketch (S)), hence being subject to large domain shifts. It contains 9,991 images spanning 7 categories. For our comparison to the state-of-the-art to be fair, we follow the standard protocol of previous works, i.e., Domain Separation Networks (DSN) (Bousmalis et al., 2016), AlexNet+TF (Li et al., 2017) and Meta-Learning for Domain Generalization (MLDG) (Li et al., 2018). More specifically, we use the AlexNet (Krizhevsky et al., 2012) as our base network, with pre-trained weights on ImageNet (Deng et al., 2009). We train our network on three domains and evaluate it on the remaining one.
In Table 1, we compare our results with those of the state-of-the-art techniques for all four leaveone-domain-out scenarios. Note that our method comfortably outperforms the state-of-the-art. On average, the improvement exceeds 6 percentage points, with our method being the top-performing one in all four scenarios. This, we believe, clearly evidences the benefits of our approach, which allows us to collaboratively learn discriminative domain-invariant representations from labeled source images, thus yielding a better representation for the unseen domain.
5

Under review as a conference paper at ICLR 2019

Unseen Domain Art painting Cartoon Photo Sketch Average

DSN (Bousmalis et al., 2016)
61.1 66.5 83.3 58.6 67.4

AlexNet+TF (Li et al., 2017)
62.9 67.0 89.5 57.5 69.2

MLDG (Li et al., 2018)
66.2 66.9 88.0 59.0 70.0

AlexNet + CLRR
68.1 78.6 92.1 66.1 76.2

Table 1: Recognition accuracy on the four leave-one-domain-out scenarios of the PACS dataset (Li et al., 2017). All methods utilize an AlexNet (Krizhevsky et al., 2012) pre-trained on ImageNet.

Unseen Domain(s)
V L C S V,L V,S L,S Average

CCSA (Motiian et al., 2017)
67.1 62.1 92.3 59.1 59.3 56.5 60.2 65.0

SCA (Ghifary et al., 2017)
64.3 59.6 88.9 59.2 59.5 55.9 60.7 64.0

SLRC (Ding & Fu, 2018)
65.3 62.3 92.8 63.5 59.7 56.8 63.1 66.2

CLRR + FC Classifier
69.3 64.9 94.4 66.9 64.6 60.3 65.3 69.4

Table 2: Recognition accuracy on the VLCS dataset (Fang et al., 2013). All methods utilize pre-computed DeCAF-fc6 features.

Results on the VLCS dataset. This dataset consists of images of 5 object categories shared by the PASCAL VOC2007 (V), LabelMe (L), Caltech-101 (C) and SUN09 (S) datasets. We follow the standard experimental protocol described in (Motiian et al., 2017), which randomly splits each domain into 70% training data and 30% test data. The random splitting is performed 20 times. Furthermore, publicly available 4096-dimensional DeCAF-fc6 features are utilized as input to a network with two Fully Connected (FC) layers with 1024 and 128 units, respectively, with ReLU activation functions. There is then a final FC layer to map the features to their correct category.
For this dataset, when setting the desired rank to the number of classes, i.e., 5, we empirically found that while our method performed extremely well for some scenarios (for example when V or C are unseen), it led to slightly lower accuracies than not using our low-rank representations in some others. We conjecture that this is caused by the number of classes being too low for the corresponding low-rank representation to preserve all the necessary information for classification, and by the use of input features that were not learned in an end-to-end fashion. For this dataset, we therefore report results obtained with a higher rank of 20.
In Table 2, we compare our results against those of several recent methods in both the leave-onedomain-out and leave-two-domains-out scenarios. As in the previous experiment, our method is the top-performing one in all cases. We outperform the closest competitor by 3.2% on average.
Results on the Office+Caltech dataset. The Office+Caltech dataset is used as a standard benchmark for visual domain generalization and was built by gathering images from 10 categories shared by the Office-31 (Saenko et al., 2010) and Caltech-256 (Griffin et al., 2007) datasets. This makes it a cross-domain dataset with four different domains: Amazon (A), Webcam (W), DSLR (D) and Caltech-256 (C). It contains 2,533 samples in total.
For evaluation, we followed the standard protocol of prior state-of-the-art methods, i.e., Denoising Multi-Task Auto Encoder (D-MTAE) (Ghifary et al., 2015), Scatter Component Analysis (SCA) (Ghifary et al., 2017) and Structured Low-Rank Constraint (SLRC) (Ding & Fu, 2018), in which the AlexNet (Krizhevsky et al., 2012) with pre-trained weights on ImageNet (Deng et al., 2009) was utilized as base network. As shown in Table 3, where we report results on the standard scenarios used in previous work, our method again achieves the best recognition accuracy on average among the reported methods.
6

Under review as a conference paper at ICLR 2019

Unseen Domain(s)
A C D,W A,C Average

D-MATE (Ghifary et al., 2015)
93.1 86.2 85.4 80.5 86.3

SCA (Ghifary et al., 2017)
92.4 86.7 85.8 75.5 85.1

SLRC (Ding & Fu, 2018)
94.2 87.6 86.3 82.2 87.6

AlexNet + CLRR
94.1 89.5 88.1 83.5 88.8

Table 3: Recognition accuracy on the Office+Caltech dataset (Saenko et al., 2010; Griffin et al., 2007). All methods utilize an AlexNet (Krizhevsky et al., 2012) pre-trained on ImageNet. For the domains D and W, following standard practice, the two domains were merged, instead of following a strict leave-one-domain-out rule, because each of them only contains a limited number of samples.

3.2 DOMAIN ADAPTATION
In this section, we demonstrate the effectiveness of our approach for domain adaptation. To this end, we make use of the VisDA-2017 benchmark and of digit recognition datasets that are commonly used to evaluate domain adaptation techniques. Below, we show that incorporating our CLRR layer in a base network substantially outperforms the baseline network that does not use it, corresponding to the source only method, since such a baseline cannot exploit the target data. Furthermore and more importantly, we show that existing domain adaptation pipelines can benefit from our CLRR layer. To this end, we employ two very recent methods, Self Ensembling (SE) (French et al., 2018) and Maximum Classifier Discrepancy (MCD) (Saito et al., 2018), and seamlessly integrate our CLRR layer into them, yielding again a substantial accuracy boost.
Results on the VisDA-2017 dataset. For our first domain adaptation experiment, we make use of the challenging VisDA-2017 dataset (Peng et al., 2017). This dataset constitutes a 12-class domain adaptation challenge in which the training (152,397 images) and validation (55,388 images) sets are used as source and target domains, respectively. Specifically, the source domain consists of synthetic images obtained by rendering various 3D models of the 12 object categories from different angles and under different lighting conditions. The target samples are real images drawn from the training and validation splits of the Microsoft COCO dataset (Lin et al., 2014).
Following the setup commonly employed in the literature, we make use of a ResNet-152 as base network, and incorporate our CLRR layer into it. For this dataset, we also evaluate the results of inserting our CLRR layer into SE, which was the winner of this challenge and also relies on a ResNet-152 as base architecture1. When doing so, we did not aim to tune the hyper-parameters of this approach and used the ones available in their code (i.e., we used the available script files for evaluation). Following (French et al., 2018), we consider two configurations of the SE approach: Reduced Augmentation (RA) and Competition Configuration (CC). The details and hyper-parameter settings of these configurations can be found in the supplementary material of (French et al., 2018).
In Table 4, we compare the results of our methods with state-of-the-art baselines on all 12 object categories of this dataset. Our CLRR layer used in conjunction with the SE (RA), achieves the highest correct recognition rate of 87.8% on average and improves the accuracy of the original method, SE (RA), on all but one categories. Note also that adding our CLRR layer to the base network yields a huge improvement of the source only counterpart, with an average accuracy that outperforms the popular DANN (Ganin & Lempitsky, 2014) technique by more than 7 percentage points. This, we believe, clearly evidences the benefits of our collaborative low-rank representations for domain adaptation.
Digit recognition datasets. As our last experiment, we evaluate our approach on two different domain shifts of handwritten digit images: Street View House Numbers (SVHN) (Netzer et al., 2011) to MNIST (LeCun et al., 1998) and USPS (Hull, 1994) to MNIST. While MNIST and USPS depict single white digits on a black background, SVHN contains images with colored background, multiple digits and extremely blurry digits. To match the image sizes, the MNIST images are resized to 32 × 32 and extended to three channels.
1Available at https://github.com/Britefury/self-ensemble-visual-domain-adapt-photo
7

Under review as a conference paper at ICLR 2019

aero. bcycl bus car horse knife mcycl person plant sktbrd train truck average

Method

Source Only

67.7 36.6 48.4 68.2 76.9 5.3 65.8 38.0 72.5 29.1 82.1 3.73 49.5

DANN

76.4 71.0 64.7 17.6 71.9 38.5 58.1 76.8 74.2 43.6 69.7 43.5 58.8

(Ganin & Lempitsky, 2014)

SimNet

94.3 82.3 73.5 47.2 87.9 49.2 75.1 79.7 85.3 68.5 81.1 50.3 72.9

(Pinheiro, 2018)

SE (CC)

96.9 89.1 85.5 59.7 96.6 97.6 91.0 81.6 95.6 94.3 89.3 49.2 85.5

(French et al., 2018)

SE (RA)

97.1 89.3 84.9 67.7 96.5 97.5 91.0 83.3 96.1 94.7 88.5 52.5 86.6

(French et al., 2018)

ResNet-152 + CLRR

78.3 58.2 73.4 50.3 83.2 55.6 82.1 62.9 80.3 55.8 79.0 35.1 66.2

SE (CC) + CLRR

97.4 88.5 87.0 77.4 97.6 98.1 92.7 88.3 96.7 96.6 89.4 41.2 87.6

SE (RA) + CLRR

97.9 91.7 85.2 72.1 97.7 98.4 91.3 86.1 96.5 95.9 89.9 51.2 87.8

Table 4: Recognition accuracy on the validation set of the VisDA-2017 dataset (Peng et al., 2017). All the methods use a ResNet-152 (He et al., 2016) as base network. RA and CC stand for Reduced Augmentation and Competition Configuration, respectively.

Method Source Only DSN (Bousmalis et al., 2016) ADDA (Tzeng et al., 2017) SimNet (Pinheiro, 2018) RAAN (Chen et al., 2018) MCD (Saito et al., 2018) Base network + CLRR MCD + CLRR

SVHN to MNIST 67.1 82.7 76.0 89.2 96.2 93.5 98.1

USPS to MNIST 63.4 90.1 95.6 92.1 94.1 93.1 96.2

Table 5: Recognition accuracy on the digits adaptation experiments.

To assess the benefits of our CLRR layer on its own, we incorporated it into a base network consisting of six convolutional units followed by two FC layers. Furthermore, to evaluate its use within a different approach, and thus its capability to generalize, we used the available implementation of MCD (Saito et al., 2018) 2 and, without any refinement, incorporated our layer between their representation-learning network and their classifier. The network consists of three convolutional and one FC layers for representation learning, followed by two FC layers as classifier. Table 5 compares the results of our approach and several baselines. Here, the improvement made by integrating our CLRR layer to the source only method is even larger than in the previous experiment, that is, over 26% and 29% for the SVHN to MNIST and USPS to MNIST shifts, respectively. This translates to this simple technique outperforming several popular baselines, such as DSN, ADDA and RAAN. Furthermore, incorporating our CLRR layer into MCD also allows us to boost the results of this baseline, thus showing the generality of our approach.
4 CONCLUSIONS
We have introduced a collaborative low-rank reconstructive layer that leverages the information jointly contained in multiple observed domains to produce a domain-invariant representation. Our method applies to both domain adaptation and generalization, and can be used either on its own within a base network, or in conjunction with existing domain adaptation techniques. Our experiments have consistently demonstrated the effectiveness of our collaborative low-rank representation at overcoming the domain shift. While defining our low-rank representation as that which minimizes reconstruction error is intuitive, and has the advantage of having a closed-form solution, there is no guarantee that it will yield the best classification performance. In the future, we will therefore investigate whether other low-rank decomposition techniques can be exploited in our framework.
2Available at https://github.com/mil-tokyo/MCD_DA
8

Under review as a conference paper at ICLR 2019
REFERENCES
Konstantinos Bousmalis, George Trigeorgis, Nathan Silberman, Dilip Krishnan, and Dumitru Erhan. Domain separation networks. In Proc. Advances in Neural Information Processing Systems (NIPS), pp. 343­351, 2016.
Qingchao Chen, Yang Liu, Zhaowen Wang, Ian Wassell, and Kevin Chetty. Re-weighted adversarial adaptation network for unsupervised domain adaptation. In Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 7976­7985, 2018.
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 248­255. Ieee, 2009.
Zhengming Ding and Yun Fu. Deep domain generalization with structured low-rank constraint. IEEE Transactions on Image Processing (TIP), 27(1):304­313, 2018.
Carl Eckart and Gale Young. The approximation of one matrix by another of lower rank. Psychometrika, 1(3):211­218, 1936.
Chen Fang, Ye Xu, and Daniel N Rockmore. Unbiased metric learning: On the utilization of multiple datasets and web images for softening bias. In Proc. Int. Conference on Computer Vision (ICCV), pp. 1657­1664, 2013.
Geoffrey French, Michal Mackiewicz, and Mark Fisher. Self-ensembling for visual domain adaptation. In Proc. Int. Conference on Learning Representations (ICLR), 2018.
Y. Ganin and V. Lempitsky. Unsupervised domain adaptation by backpropagation. In Proc. Int. Conference on Machine Learning (ICML), 2014.
Muhammad Ghifary, W Bastiaan Kleijn, Mengjie Zhang, and David Balduzzi. Domain generalization for object recognition with multi-task autoencoders. In Proc. Int. Conference on Computer Vision (ICCV), pp. 2551­2559, 2015.
Muhammad Ghifary, David Balduzzi, W Bastiaan Kleijn, and Mengjie Zhang. Scatter component analysis: A unified framework for domain adaptation and domain generalization. IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), (1):1­1, 2017.
B. Gong, Y. Shi, F. Sha, and K. Grauman. Geodesic flow kernel for unsupervised domain adaptation. In Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2012.
Gregory Griffin, Alex Holub, and Pietro Perona. Caltech-256 object category dataset. 2007.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 770­778, 2016.
Jonathan J. Hull. A database for handwritten text recognition research. IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 16(5):550­554, 1994.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. In Proc. Advances in Neural Information Processing Systems (NIPS), pp. 1097­1105, 2012.
Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278­2324, 1998.
Da Li, Yongxin Yang, Yi-Zhe Song, and Timothy M Hospedales. Deeper, broader and artier domain generalization. In Proc. Int. Conference on Computer Vision (ICCV), pp. 5543­5551. IEEE, 2017.
Da Li, Yongxin Yang, Yi-Zhe Song, and Timothy M. Hospedales. Learning to generalize: Metalearning for domain generalization. In AAAI Conference on Artificial Intelligence, 2018.
9

Under review as a conference paper at ICLR 2019
Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In Proc. European Conference on Computer Vision (ECCV), pp. 740­755. Springer, 2014.
M. Long, Y. Cao, J. Wang, and M. Jordan. Learning transferable features with deep adaptation networks. arXiv preprint arXiv:1502.02791, 2015.
M. Long, J. Wang, and M. Jordan. Deep transfer learning with joint adaptation networks. corr, vol. arXiv preprint arXiv:1605.06636, 2016.
Saeid Motiian, Marco Piccirilli, Donald A. Adjeroh, and Gianfranco Doretto. Unified deep supervised domain adaptation and generalization. In Proc. Int. Conference on Computer Vision (ICCV), 2017.
Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y Ng. Reading digits in natural images with unsupervised feature learning. In Proc. Advances in Neural Information Processing Systems (NIPS), volume 2011, pp. 5, 2011.
Théodore Papadopoulo and Manolis IA Lourakis. Estimating the jacobian of the singular value decomposition: Theory and applications. In Proc. European Conference on Computer Vision (ECCV), pp. 554­570. Springer, 2000.
Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in pytorch. 2017.
Xingchao Peng, Ben Usman, Neela Kaushik, Judy Hoffman, Dequan Wang, and Kate Saenko. Visda: The visual domain adaptation challenge, 2017.
Pedro O Pinheiro. Unsupervised domain adaptation with similarity learning. In Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018.
Kate Saenko, Brian Kulis, Mario Fritz, and Trevor Darrell. Adapting visual category models to new domains. In Proc. European Conference on Computer Vision (ECCV), pp. 213­226. Springer, 2010.
Kuniaki Saito, Kohei Watanabe, Yoshitaka Ushiku, and Tatsuya Harada. Maximum classifier discrepancy for unsupervised domain adaptation. In Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018.
Amartya Sanyal, Varun Kanade, and Philip H.S. Torr. Intriguing properties of learned representations, 2018.
Antti Tarvainen and Harri Valpola. Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results. In Proc. Advances in Neural Information Processing Systems (NIPS), pp. 1195­1204, 2017.
E. Tzeng, J. Hoffman, K. Saenko, and T. Darrell. Adversarial discriminative domain adaptation. In Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017.
H. Yan, Y. Ding, P. Li, Q. Wang, Y. Xu, and W. Zuo. Mind the class weight bias: Weighted maximum mean discrepancy for unsupervised domain adaptation. arXiv preprint arXiv:1705.00609, 2017.
Yongxin Yang and Timothy Hospedales. Deep multi-task representation learning: A tensor factorisation approach. arXiv preprint arXiv:1605.06391, 2016.
10

Under review as a conference paper at ICLR 2019

APPENDIX: HYPER-PARAMETER VALUES

In Table 6, for the sake of reproducibility, we provide the hyper-parameter values that were used in our experiments. As mentioned earlier, with the exception of the VLCS dataset that only has 5 classes, we set the desired rank to the number of classes in each dataset. The other hyper-parameters were set without fine-tuning. In particular, for domain generalization we used small batch sizes for individual domains, which combine into fairly large batches and used the same learning rate for all datasets. For domain adaptation, we relied on the batch sizes and learning rates coming from the source codes we used to define the base networks. When incorporating our CLRR layer into domain adaptation frameworks, we used the hyper-parameter values for these frameworks defined in their publicly available implementations.

Parameter Desired rank Batch size Learning rate

PACS 7 32
0.0001

VLCS 20 64
0.0001

Office+Caltech 10 32
0.0001

VisDA-2017 12 56
0.00001

SVHN to MNIST 10 128
0.0001

USPS to MNIST 10 128
0.0001

Table 6: Hyper-parameters used in our experiments.

11


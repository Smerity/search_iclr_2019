Under review as a conference paper at ICLR 2019
MANIFOLD MIXUP: LEARNING BETTER REPRESENTATIONS BY INTERPOLATING HIDDEN STATES
Anonymous authors Paper under double-blind review
ABSTRACT
Deep networks often perform well on the data distribution on which they are trained, yet give incorrect (and often very confident) answers when evaluated on points from off of the training distribution. This is exemplified by the adversarial examples phenomenon but can also be seen in terms of model generalization and domain shift. Ideally, a model would assign lower confidence to points unlike those from the training distribution. We propose a regularizer which addresses this issue by training with interpolated hidden states and encouraging the classifier to be less confident at these points. Because the hidden states are learned, this has an important effect of encouraging the hidden states for a class to be concentrated in such a way so that interpolations within the same class or between two different classes do not intersect with the real data points from other classes. This has a major advantage in that it avoids the underfitting which can result from interpolating in the input space. We prove that the exact condition for this problem of underfitting to be avoided by Manifold Mixup is that the dimensionality of the hidden states exceeds the number of classes, which is often the case in practice. Additionally, this concentration can be seen as making the features in earlier layers more discriminative. We show that despite requiring no significant additional computation, Manifold Mixup achieves large improvements over strong baselines in supervised learning, robustness to single-step adversarial attacks, semi-supervised learning, and Negative Log-Likelihood on held out samples.
1 INTRODUCTION
Machine learning systems have been enormously successful in domains such as vision, speech, and language and are now widely used both in research and industry. Modern machine learning systems typically only perform well when evaluated on the same distribution that they were trained on. However machine learning systems are increasingly being deployed in settings where the environment is noisy, subject to domain shifts, or even adversarial attacks. In many cases, deep neural networks which perform extremely well when evaluated on points on the data manifold give incorrect answers when evaluated on points off the training distribution, and with strikingly high confidence.
This manifests itself in several failure cases for deep learning. One is the problem of adversarial examples (Szegedy et al., 2013), in which deep neural networks with nearly perfect test accuracy can produce incorrect classifications with very high confidence when evaluated on data points with small (imperceptible to human vision) adversarial perturbations. These adversarial examples could present serious security risks for machine learning systems. Another failure case involves the training and testing distributions differing significantly. With deep neural networks, this can often result in dramatically reduced performance.
To address these problems, our Manifold Mixup approach builds on following assumptions and motivations: (1) we adopt the manifold hypothesis, that is, data is concentrated near a lower-dimensional non-linear manifold (this is the only required assumption on the data generating distribution for Manifold Mixup to work); (2) a neural net can learn to transform the data non-linearly so that the transformed data distribution now lies on a nearly flat manifold; (3) as a consequence, linear interpolations between examples in the hidden space also correspond to valid data points, thus providing novel training examples.
1

Under review as a conference paper at ICLR 2019
Figure 1: The top row (a,b,c) shows the decision boundary on the 2d spirals dataset trained with a baseline model (a fully connected neural network with nine layers where middle layer is a 2D bottleneck layer), Input Mixup with  = 1.0, and Manifold Mixup applied only to the 2D bottleneck layer. As seen in (b), Input Mixup can suffer from underfitting since the interpolations between two samples may intersect with a real sample. Whereas Manifold Mixup (c), fits the training data perfectly. The bottom row (d,e,f) shows the hidden states for the baseline, Input Mixup, and manifold mixup respectively. Manifold Mixup concentrates the labeled points from each class to a very tight region, as predicted by our theory (Section 3) and assigns lower confidence classifications to broad regions in the hidden space. The black points in the bottom row are the hidden states of the points sampled uniformly in x-space and it can be seen that manifold mixup does a better job of giving low confidence to these points. Additional results in Figure 6 of Appendix B show that the way Manifold Mixup changes the representations is not accomplished by other well-studied regularizers (weight decay, dropout, batch normalization, and adding noise to the hidden states).
Manifold Mixup performs training on the convex combinations of the hidden state representations of data samples. Previous work, including the study of analogies through word embeddings (e.g. king - man + woman  queen), has shown that such linear interpolation between hidden states is an effective way of combining factors (Mikolov et al., 2013). Combining such factors in the higher level representations has the advantage that it is typically lower dimensional, so a simple procedure like linear interpolation between pairs of data points explores more of the space and with more of the points having meaningful semantics. When we combine the hidden representations of training examples, we also perform the same linear interpolation in the labels (seen as one-hot vectors or categorical distributions), producing new soft targets for the mixed examples. In practice, deep networks often learn representations such that there are few strong constraints on how the states can be distributed in the hidden space, because of which the states can be widely distributed through the space, (as seen in Figure 1d). As well as, nearly all points in hidden space correspond to high confidence classifications even if they correspond to off-the-training distribution samples (seen as black points in Figure 1d). In contrast, the consequence of our Manifold Mixup approach is that the hidden states from real examples of a particular class are concentrated in local regions and the majority of the hidden space corresponds to lower confidence classifications. This concentration of the hidden states of the examples of a particular class into a local regions enables learning more discriminative features. A low-dimensional example of this can be seen in Figure 1 and a more detailed analytical discussion for what "concentrating into local regions" means is in Section 3. Our method provides the following contributions:
2

Under review as a conference paper at ICLR 2019
· The introduction of a novel regularizer which outperforms competitive alternatives such as Cutout (Devries & Taylor, 2017), Mixup (Zhang et al., 2017), AdaMix (Guo et al., 2016), and Dropout (Hinton et al., 2012). On CIFAR-10, this includes a 50% reduction in test Negative Log-Likelihood (NLL) from 0.1945 to 0.0957.
· Manifold Mixup achieves significant robustness to single step adversarial attacks. · A new method for semi-supervised learning which uses a Manifold Mixup based consis-
tency loss. This method reduces error relative to Virtual Adversarial Training (VAT) (Miyato et al., 2017) by 21.86% on CIFAR-10, and unlike VAT does not involve any additional significant computation. · An analysis of Manifold Mixup and exact sufficient conditions for Manifold Mixup to achieve consistent interpolations. Unlike Input Mixup, this doesn't require strong assumptions about the data distribution (see the failure case of Input Mixup in Figure 1): only that the number of hidden units exceeds the number of classes, which is easily satisfied in many applications.
2 MANIFOLD MIXUP
The Manifold Mixup algorithm consists of selecting a random layer (from a set of eligible layers including the input layer) k. We then process the batch without any mixup until reaching that layer, and we perform mixup at that hidden layer, and then continue processing the network starting from the mixed hidden state, changing the target vector according to the mixup interpolation. More formally, we can redefine our neural network function y = f (x) in terms of k: f (x) = gk(hk(x)). Here gk is a function which runs a neural network from the input hidden state k to the output y, and hk is a function which computes the k-th hidden layer activation from the input x.
For the linear interpolation between factors, we define a variable  and we sample from p(). Following (Zhang et al., 2017), we always use a beta distribution p() = Beta(, ). With  = 1.0, this is equivalent to sampling from U (0, 1).
We consider interpolation in the set of layers Sk and minimize the expected Manifold Mixup loss.
L = E(xi,yi),(xj,yj)p(x,y),p(),kSk (fk(gk(xi) + (1 - )gk(xj ))), yi + (1 - )yj ) (1)
We backpropagate gradients through the entire computational graph, including to layers before the mixup process is applied (Section 5.1 and appendix Section B explore this issue directly). In the case where k = 0 is the input layer and Sk = 0, Manifold Mixup reduces to the mixup algorithm of Zhang et al. (2017). With  = 2.0, about 5% of the time  is within 5% of 0 or 1, which essentially means that an ordinary example is presented. In the more general case, we can optimize the expectation in the Manifold Mixup objective by sampling a different layer to perform mixup in on each update. We could also select a new random layer as well as a new lambda for each example in the minibatch. In theory this should reduce the variance in the updates introduced by these random variables. However in practice we found that this didn't have a significant effect on the results, so we decided to sample a single lambda and a randomly chosen layer per minibatch.
In comparison to Input Mixup, the results in the Figure 2 demonstrate that Manifold Mixup reduces the loss calculated along hidden interpolations significantly better than Input Mixup, without significantly changing the loss calculated along visible space interpolations.
3 HOW MANIFOLD MIXUP CHANGES REPRESENTATIONS
Our goal is to show that if one does mixup in a sufficiently deep hidden layer in a deep network, then a mixup loss of zero can be achieved so long the dimensionality of that hidden layer dim (H) is greater than the number of classes d. More specifically the resulting representations for that class must fall onto a subspace of dimension dim (H) - d.
Assume X and H to denote the input and representation spaces, respectively. We denote the labelset by Y and let Z X × Y. Also, let us denote the set of all probability measures on Z by M (Z). Assume G  HX to be the set of all possible functions that can be generated by the neural network
3

Under review as a conference paper at ICLR 2019

mapping input to the representation space. In this regard, each g  G represents a mapping from input to the representation units. A similar definition can be made for F  YH, as the space of all possible functions from the representation space to the output.

We are interested in the solution of the following problem, at least in some specific asymptotic regimes:

J (L, P )

inf E

gG, f F

2
L (f  Mix (g (X1) , g (X2)) , Mix (y1, y2)) dP (Xi, yi) ,
Z2 i=1
(2)

where Mix (a, b) a + (1 - ) b,   [0, 1] ,
for any a and b defined on the same domain.

(3)

We analyze the above-mentioned minimization when the probability measure as the empirical distribution over a finite dataset of size n, denoted by D = f   F and g  G be the minimizers in (2) with P = PD.

{P(X=i,PyDi)}isin=c1h.oLseent

In particular, we are interested in the case where G = HX , F = YH, and H is a vector space; These
conditions simply state that the two respective neural networks which map input into representation space, and representation space to the output are being extended asymptotically1. In this regard, we show that the minimizer f  is a linear function from H to Y. This way, it is easy to show that the
following equality holds:

1n

1

J (L, PD) = inf
h1 ,...,hn H

n (n - 1)

i,j=1

inf
f F

L f  Mix (hi, hj) , Mix yi, yj
0

d ,

i=j

(4)

where hi g (Xi) is the representation of Xi.

Theorem 1. Assume H to be a vector space with dimension dim (H), and let d  N to represent the number of distinct classes in dataset D. Then, if dim (H)  d - 1, J (L, PD) = 0 and the minimizer function f  is a linear map from H to Rd.

Proof. With basic linear algebra, one can confirm that the following argument is true as long as dim (H)  d - 1:

A, H  Rdim(H)×d, b  Rd such that AT H + b1dT = Id×d,

(5)

where Id×d and 1d are the d-dimensional identity matrix and all-one vector, respectively. In fact, b1dT is a rank-one matrix, while the rank of identity matrix is d. Therefore, AT H only needs to be rank d - 1.

Let f  (h) Ah + b, for all h  H. Also, let g (Xi) = hi , where hi here means the ith column of matrix H, and i  {1, . . . , d} is the class-index of the ith sample. We show that such selections will make the objective in (2) equal to zero (which is the minimum possible value). More precisely,
the following relations hold:

1n
n (n - 1) i,j=1 i=j

1
L f   Mix (g (Xi) , g (Xj)) , Mix yi, yj d ,
0

1n =
n (n - 1) i,j=1 i=j

1
L AT hi + (1 - ) hj + b, yi + (1 - ) yj d ,
0

1n =
n (n - 1) i,j=1 i=j

1
L (u () , u ()) d
0

= 0.

(6)

1Due to the consistency theorem that proves neural networks with nonlinear activation functions are dense in the function space

4

Under review as a conference paper at ICLR 2019

The final equality is a direct result of AT hi + b = yi for i = 1, . . . , n.

Also, it can be shown that as long as dim (H) > d - 1, then data points in the representation space H have some degrees of freedom to move independently.
Corollary 1. Consider the setting in Theorem 1, and assume dim (H) > d - 1. Let g  G to be the true minimizer of (2) for a given dataset D. Then, data-points in the representation space, i.e. g (Xi), fall on a (dim (H) - d + 1)-dimensional subspace.

Proof. In the proof of Theorem 1, we have

AT H = Id×d - b1Td .

(7)

The r.h.s. of (7) can become a rank-(d - 1) matrix as long as vector b is chosen properly. Thus, A is free to have a null-space of dimension dim (H)-d+1. This way, one can assign g (Xi) = hi +ei, where hj and i (for j = 1, . . . , d and i = 1, . . . , n) are defined in the same way as in Theorem 1, and eis can are arbitrary vectors in the null-space of A, i.e. ei  ker (A) for all i.

This result implies that if the Manifold Mixup loss is minimized, then the representation for each class will lie on a subspace of dimension dim (H)-d+1. In the most extreme case where dim (H) = d - 1, each hidden state from the same class will be driven to a single point, so the change in the hidden states following any direction on the class-conditional manifold will be zero. In the more general case with a larger dim (H), a majority of directions in H-space will not change as we move along the class-conditional manifold.
Why are these properties desirable? First, it can be seen as a flattening of the class-conditional manifold which encourages learning effective representations earlier in the network. Second, it means that the region in hidden space occupied by data points from the true manifold has nearly zero measure. So a randomly sampled hidden state within the convex hull spanned by the data is more likely to have a classification score that is not fully confident (non-zero entropy). Thus it encourages the network to learn discriminative features in all layers of the network and to also assign low-confidence classification decisions to broad regions in the hidden space (this can be seen in Figure 1 and Figure 6).

4 RELATED WORK
Regularization is a major area of research in machine learning. Manifold Mixup closely builds on two threads of research. The first is the idea of linearly interpolating between different randomly drawn examples and similarly interpolating the labels (Zhang et al., 2017; Tokozume et al., 2017). These methods encourage the output of the entire network to change linearly between two randomly drawn training samples, which can result in underfitting. In contrast, for a particular layer at which mixing is done, Manifold Mixup allows lower layers to learn more concentrated features in such a way that it makes it easier for the output of the upper layers to change linearly between hidden states of two random samples, achieving better results (section 5.1 and Appendix B).
Another line of research closely related to Manifold Mixup involves regularizing deep networks by perturbing the hidden states of the network. These methods include dropout (Hinton et al., 2012), batch normalization (Ioffe & Szegedy, 2015), and the information bottleneck (Alemi et al., 2016). Notably Hinton et al. (2012) and Ioffe & Szegedy (2015) both demonstrated that regularizers already demonstrated to work well in the input space (salt and pepper noise and input normalization respectively) could also be adapted to improve results when applied to the hidden layers of a deep network. We believe that the regularization effect of Manifold Mixup would be complementary to that of these algorithms.
Zhao & Cho (2018) explored improving adversarial robustness by classifying points using a function of the nearest neighbors in a fixed feature space. This involved applying mixup between each set of nearest neighbor examples in that feature space. The similarity between Zhao & Cho (2018) and Manifold Mixup is that both consider linear interpolations in hidden states with the same interpolation applied to the labels. However an important difference is that Manifold Mixup backpropagates gradients through the earlier parts of the network (the layers before where mixup is applied) unlike

5

Under review as a conference paper at ICLR 2019

Table 1: Supervised Classification Results on CIFAR-10 (left) and CIFAR-100 (right). We note significant improvement with Manifold Mixup especially in terms of Negative log-likelihood (NLL). Please refer to Appendix C for details on the implementation of Manifold Mixup and Manifold Mixup All layers.  and  refer to the results reported in (Zhang et al., 2017) and (Guo et al., 2016) respectively.

Model

Test Error

PreActResNet18

No Mixup Input Mixup ( = 1.0)  AdaMix  Input Mixup ( = 1.0) Manifold Mixup ( = 2.0)

5.12 3.90 3.52 3.50 2.89

PreActResNet152

No Mixup Input Mixup ( = 1.0) Manifold Mixup ( = 2.0) Manifold Mixup
all layers ( = 6.0)

4.20 3.15 2.76
2.38

(a) CIFAR-10

Test NLL
0.2646 n/a n/a 0.1945 0.1407
0.1994 0.2312 0.1419
0.0957

Model

Test Error

PreActResNet18

No Mixup  No Mixup Input Mixup ( = 1.0)  Manifold Mixup ( = 2.0)

25.60 24.68 21.10 21.05

PreActResNet34

Input Mixup ( = 1.0)

22.79

Manifold Mixup ( = 2.0) 20.39

(b) CIFAR-100

Test NLL
n/a 1.284 n/a 0.913
1.085 0.930

Zhao & Cho (2018). As discussed in Section 5.1 and Appendix B this was found to significantly change the learning process.
5 EXPERIMENTS
5.1 REGULARIZATION ON SUPERVISED LEARNING
We present results on Manifold Mixup based regularization of networks using the PreActResNet architecture (He et al., 2016). We closely followed the procedure of (Zhang et al., 2017) as a way of providing direct comparisons with the Input Mixup algorithm. We used weight decay of 0.0001 and trained with SGD with momentum and multiplied the learning rate by 0.1 at regularly scheduled epochs. These results for CIFAR-10 and CIFAR-100 are in Table 1a and 1b. We also ran experiments where we took PreActResNet34 models trained on the normal CIFAR-100 data and evaluated them on test sets with artificial deformations (shearing, rotation, and zooming) and showed that Manifold Mixup demonstrated significant improvements (Appendix C Table 5), which suggests that Manifold Mixup performs better on the variations in the input space not seen during the training. We also show that the number of epochs needed to reach good results is not significantly affected by using Manifold Mixup in Figure 8.
To better understand why the method works, we performed an experiment where we trained with Manifold Mixup but blocked gradients immediately after the layer where we perform mixup. On CIFAR-10 PreActResNet18, this caused us to achieve 4.86% test error when trained on 400 epochs and 4.33% test error when trained on 1200 epochs. This is better than the baseline, but worse than Manifold Mixup or Input Mixup in both cases. Because we randomly select the layer to mix, each layer of the network is still being trained, although not on every update. This demonstrates that the Manifold Mixup method improves results by changing the layers both before and after the mixup operation is applied.
We also compared Manifold Mixup against other strong regularizers. We selected the best performing hyperparameters for each of the following models using a validation set. Using each model's best performing hyperparameters, test error averages and standard deviations for five trials (in %) for CIFAR-10 using PreResNet50 trained for 600 epochs are: vanilla PreResNet50 (4.96 ± 0.19), Dropout (5.09 ± 0.09), Cutout (Devries & Taylor, 2017) (4.77 ± 0.38), Mixup (4.25 ± 0.11) and Manifold Mixup (3.77 ± 0.18). This clearly shows that Manifold Mixup has strong regularizing
6

Under review as a conference paper at ICLR 2019

effects. (Note that the results in Table 1 were run for 1200 epochs and thus these results are not directly comparable.)
We also evaluate the quality of the representations learned by Manifold Mixup by applying K-Nearest Neighbour classifier on the feature extracted from the top layer of PreResNet18 for CIFAR-10. We achieved test errors of 6.09% (Vanilla PreResNet18), 5.54% (Mixup) and 5.16% (Manifold Mixup). It suggests that Manifold Mixup helps learning better representations. Further analysis of how Manifold Mixup changes the representations is given in Appendix B

5.2 SEMI-SUPERVISED LEARNING

Semi-supervised learning is concerned with building models which can take advantage of both labeled and unlabeled data. It is particularly useful in domains where obtaining labels is challenging, but unlabeled data is plentiful.

The Manifold Mixup

approach to semisupervised learning is closely related to the consistency regularization approach reviewed by

Table 2: Results on semi-supervised learning (SSL) on CIFAR-10 (4k labels) and SVHN (1k labels) (in test error %). All results use the same standardized architecture (WideResNet-28-2). Each experiment was run for 5 trials.  refers to the results reported in Oliver et al. (2018)

Oliver et al. (2018). It involves minimizing loss

SSL Approach

CIFAR-10

SVHN

on labelled samples as Supervised 

20.26 ± 0.38 12.83 ± 0.47

well as unlabeled samples Mean-Teacher 

15.87 ± 0.28 5.65 ± 0.47

by controlling the trade- VAT 

13.86 ± 0.27 5.63 ± 0.20

off between these two VAT-EM 

13.13 ± 0.39 5.35 ± 0.19

losses via a consistency Semi-supervised Input Mixup

10.71 ± 0.44 6.54 ± 0.62

coefficient. In the Man- Semi-supervised Manifold Mixup 10.26 ± 0.32 5.70 ± 0.48

ifold Mixup approach for

semi-supervised learning,

the loss from labeled examples is computed as normal. For computing loss from unlabelled samples,

the model's predictions are evaluated on a random batch of unlabeled data points. Then the normal

manifold mixup procedure is used, but the targets to be mixed are the soft target outputs from the

classifier. The detailed algorithm for both Manifold Mixup and Input Mixup with semi-supervised

learning are given in appendix D.

Oliver et al. (2018) performed a systematic study of semi-supervised algorithms using a fixed wide resnet architecture "WRN-28-2" (Zagoruyko & Komodakis, 2016). We evaluate Manifold Mixup using this same setup and achieve improvements for CIFAR-10 over the previously best performing algorithm, Virtual Adversarial Training (VAT) (Miyato et al., 2017) and Mean-Teachers (Tarvainen & Valpola, 2017). For SVHN, Manifold Mixup is competitive with VAT and Mean-Teachers. See Table 2. While VAT requires an additional calculation of the gradient and Mean-Teachers requires repeated model parameters averaging, Manifold Mixup requires no additional (non-trivial) computation.

In addition, we also explore the regularization ability of Manifold Mixup in a fully-supervised lowdata regime by training a PreResnet-152 model on 4000 labeled images from CIFAR-10. We obtained 13.64 % test error which is comparable with the fully-supervised regularized baseline according to results reported in Oliver et al. (2018). Interestingly, we do not use a combination of two powerful regularizers ("Shake-Shake" and "Cut-out") and the more complex ResNext architecture as in Oliver et al. (2018) and still achieve the same level of test accuracy, while doing much better than the fully supervised baseline not regularized with state-of-the-art regularizers (20.26% error).

5.3 ADVERSARIAL EXAMPLES
Adversarial examples in some sense are the "worst case" scenario for models failing to perform well when evaluated with data off the manifold2. Because Manifold Mixup only considers a sub-
2See the adversarial spheres (Gilmer et al., 2018) paper for a discussion of what it means to be off of the manifold.

7

Under review as a conference paper at ICLR 2019

Figure 2: Study of test Negative Log-likelihood (NLL) using the interpolated target values (lower is better) on interpolated points under models trained with the baseline, mixup, and Manifold Mixup. Manifold Mixup dramatically improves performance when interpolating in the hidden states, and very slightly reduces performance when interpolating in the visible space. Y-axis denotes NLL and X-axis denotes the interpolation coefficient

set of directions around data points (namely, those corresponding to interpolations), we would not expect the model to be robust to adversarial attacks which can consider any direction within an epsilon-ball of each example. At the same time, Manifold Mixup expands the set of points seen during training, so an intriguing hypothesis is that these overlap somewhat with the set of possible adversarial examples, which would force adversarial attacks to consider a wider set of directions, and potentially be more computationally expensive. To explore this we considered the Fast Gradient Sign Method (FGSM, Goodfellow et al., 2014) which only requires a single gradient update and considers a relatively small subset of adversarial directions. The resulting performance of Manifold Mixup against FGSM are given in Table 3. A challenge in evaluating adversarial examples comes from the gradient masking problem in which a defense succeeds solely due to reducing the quality of the gradient signal. Athalye et al. (2018) explored this issue in depth and proposed running an unbounded search for a large number of iterations to confirm the quality of the gradient signal. Our Manifold Mixup passed this sanity check (see Appendix F). While we found that Manifold Mixup greatly improved robustness to the FGSM attack, especially over Input Mixup (Zhang et al., 2017), we found that Manifold Mixup did not significantly improve robustness against the stronger iterative projected gradient descent (PGD) attack (Madry et al., 2017).

Table 3: CIFAR-10 Test Accuracy Results on white-box FGSM (Goodfellow et al., 2014) adversarial attack (higher is better) using PreActResNet18 (left). SVHN Test Accuracy Results on white-box FGSM using WideResNet20-10 (Zagoruyko & Komodakis, 2016). Note that our method achieves some degree of adversarial robustness, against the FGSM attack, despite not requiring any additional (significant) computation.  refers to results reported in (Madry et al., 2017)

CIFAR-10 Models

FGSM

Adv. Training (PGD 7-step)  Adversarial Training + Fortified Networks
Baseline (Vanilla Training) Input Mixup ( = 1.0) Manifold Mixup ( = 2.0)

56.10
81.80 36.32 71.51 77.50

CIFAR-100 Models

FGSM

Input Mixup ( = 1.0) Manifold Mixup ( = 2.0)

40.7 44.96

SVHN Models FGSM

Baseline Input Mixup ( = 1.0) Manifold Mixup
( = 2.0) Adv. Training
(PGD 7-step)

21.49 56.98 65.91 72.80

8

Under review as a conference paper at ICLR 2019
6 VISUALIZATION OF INTERPOLATED STATES
An important question is what kinds of feature combinations are being explored when we perform mixup in the hidden layers as opposed to linear interpolation in visible space. To provide a qualitative study of this, we trained a small decoder convnet (with upsampling layers) to predict an image from the Manifold Mixup classifier's hidden representation (using a simple squared error loss in the visible space). We then performed mixup on the hidden states between two random examples, and ran this interpolated hidden state through the convnet to get an estimate of what the point would look like in input space. Similarly to earlier results on auto-encoders (Bengio et al., 2013), we found that these interpolated h points corresponded to images with a blend of the features from the two images, as opposed to the less-semantic pixel-wise blending resulting from Input Mixup as shown in Figure 3 and Figure 4. Furthermore, this justifies the training objective for examples mixed-up in the hidden layers: (1) most of the interpolated points correspond to combinations of semantically meaningful factors, thus leading to the more training samples; and (2) none of the interpolated points between objects of two different categories A and B correspond to a third category C, thus justifying a training target which gives 0 probability on all the classes except A and B.
Figure 3: Interpolations in the input space with a mixing rate varied from 0.0 to 1.0.
Figure 4: Interpolations in the hidden states (using a small convolutional network trained to predict the input from the output of the second resblock). The interpolations in the hidden states show a better blending of semantically relevant features, and more of the images are visually consistent.
7 CONCLUSION
Deep neural networks often give incorrect yet extremely confident predictions on data points which are unlike those seen during training. This problem is one of the most central challenges in deep learning both in theory and in practice. We have investigated this from the perspective of the representations learned by deep networks. In general, deep neural networks can learn representations such that real data points are widely distributed through the space and most of the area corresponds to high confidence classifications. This has major downsides in that it may be too easy for the network to provide high confidence classification on points which are off of the data manifold and also that it may not provide enough incentive for the network to learn highly discriminative representations. We have presented Manifold Mixup, a new algorithm which aims to improve the representations learned by deep networks by encouraging most of the hidden space to correspond to low confidence
9

Under review as a conference paper at ICLR 2019 classifications while concentrating the hidden states for real examples onto a lower dimensional subspace. We applied Manifold Mixup to several tasks and demonstrated improved test accuracy and dramatically improved test likelihood on classification, better robustness to adversarial examples from FGSM attack, and improved semi-supervised learning. Manifold Mixup incurs virtually no additional computational cost, making it appealing for practitioners.
10

Under review as a conference paper at ICLR 2019
REFERENCES
Alexander A. Alemi, Ian Fischer, Joshua V. Dillon, and Kevin Murphy. Deep variational information bottleneck. CoRR, abs/1612.00410, 2016. URL http://arxiv.org/abs/1612.00410.
Martin Arjovsky, Soumith Chintala, and Léon Bottou. Wasserstein generative adversarial networks. In International Conference on Machine Learning, pp. 214­223, 2017.
A. Athalye, N. Carlini, and D. Wagner. Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples. ArXiv e-prints, February 2018.
Yoshua Bengio, Grégoire Mesnil, Yann Dauphin, and Salah Rifai. Better mixing via deep representations. In ICML'2013, 2013.
Terrance Devries and Graham W. Taylor. Improved regularization of convolutional neural networks with cutout. CoRR, abs/1708.04552, 2017. URL http://arxiv.org/abs/1708.04552.
J. Gilmer, L. Metz, F. Faghri, S. S. Schoenholz, M. Raghu, M. Wattenberg, and I. Goodfellow. Adversarial Spheres. ArXiv e-prints, January 2018.
I. J. Goodfellow, J. Shlens, and C. Szegedy. Explaining and Harnessing Adversarial Examples. ArXiv e-prints, December 2014.
Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron C Courville. Improved training of wasserstein gans. In Advances in Neural Information Processing Systems, pp. 5769­5779, 2017.
Hongyu Guo, Yongyi Mao, and Richong Zhang. MixUp as Locally Linear Out-Of-Manifold Regularization. ArXiv e-prints, 2016. URL https://arxiv.org/abs/1809.02499.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual networks. CoRR, abs/1603.05027, 2016. URL http://arxiv.org/abs/1603.05027.
Geoffrey E. Hinton, Nitish Srivastava, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Improving neural networks by preventing co-adaptation of feature detectors. CoRR, abs/1207.0580, 2012. URL http: //arxiv.org/abs/1207.0580.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. CoRR, abs/1502.03167, 2015. URL http://arxiv.org/abs/1502.03167.
A. Madry, A. Makelov, L. Schmidt, D. Tsipras, and A. Vladu. Towards Deep Learning Models Resistant to Adversarial Attacks. ArXiv e-prints, June 2017.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient estimation of word representations in vector space. CoRR, abs/1301.3781, 2013. URL http://arxiv.org/abs/1301.3781.
T. Miyato, S.-i. Maeda, M. Koyama, and S. Ishii. Virtual Adversarial Training: a Regularization Method for Supervised and Semi-supervised Learning. ArXiv e-prints, April 2017.
Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida. Spectral normalization for generative adversarial networks. CoRR, abs/1802.05957, 2018. URL http://arxiv.org/abs/1802.05957.
A. Oliver, A. Odena, C. Raffel, E. D. Cubuk, and I. J. Goodfellow. Realistic Evaluation of Deep SemiSupervised Learning Algorithms. ArXiv e-prints, April 2018.
Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training gans. In Advances in Neural Information Processing Systems, pp. 2234­2242, 2016.
C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. Goodfellow, and R. Fergus. Intriguing properties of neural networks. ArXiv e-prints, December 2013.
Antti Tarvainen and Harri Valpola. Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Advances in Neural Information Processing Systems 30, pp. 1195­1204. Curran Associates, Inc., 2017.
Yuji Tokozume, Yoshitaka Ushiku, and Tatsuya Harada. Between-class learning for image classification. CoRR, abs/1711.10284, 2017. URL http://arxiv.org/abs/1711.10284.
Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. CoRR, abs/1605.07146, 2016. URL http://arxiv.org/abs/1605.07146.
11

Under review as a conference paper at ICLR 2019 Hongyi Zhang, Moustapha Cissé, Yann N. Dauphin, and David Lopez-Paz. mixup: Beyond empirical risk
minimization. CoRR, abs/1710.09412, 2017. URL http://arxiv.org/abs/1710.09412. Jake Zhao and Kyunghyun Cho. Retrieval-augmented convolutional neural networks for improved robustness
against adversarial examples. CoRR, abs/1802.09502, 2018. URL http://arxiv.org/abs/1802. 09502.
12

Under review as a conference paper at ICLR 2019
A SYNTHETIC EXPERIMENTS ANALYSIS
We conducted experiments using a generated synthetic dataset where each image is deterministically rendered from a set of independent factors. The goal of this experiment is to study the impact of input mixup and an idealized version of Manifold Mixup where we know the true factors of variation in the data and we can do mixup in exactly the space of those factors. This is not meant to be a fair evaluation or representation of how Manifold Mixup actually performs - rather it's meant to illustrate how generating relevant and semantically meaningful augmented data points can be much better than generating points which are far off the data manifold. We considered three tasks. In Task A, we train on images with angles uniformly sampled between (-70, -50) (label 0) with 50% probability and uniformly between (50, 80) (label 1) with 50% probability. At test time we sampled uniformly between (-30, -10) (label 0) with 50% probability and uniformly between (10, 30) (label 1) with 50% probability. Task B used the same setup as Task A for training, but the test instead used (-30, -20) as label 0 and (-10, 30) as label 1. In Task C we made the label whether the digit was a "1" or a "7", and our training images were uniformly sampled between (-70, -50) with 50% probability and uniformly between (50, 80) with 50% probability. The test data for Task C were uniformly sampled with angles from (-30, 30). The examples of the data are in figure 5 and results are in table 4. In all cases we found that Input Mixup gave some improvements in likelihood but limited improvements in accuracy - suggesting that the even generating nonsensical points can help a classifier trained with Input Mixup to be better calibrated. Nonetheless the improvements were much smaller than those achieved with mixing in the ground truth attribute space.

Figure 5: Synthetic task where the underlying factors are known exactly. Training images (left), images from input mixup (center), and images from mixing in the ground truth factor space (right).

Table 4: Results on synthetic data generalization task with an idealized Manifold Mixup (mixing in the true latent generative factors space). Note that in all cases visible mixup significantly improved likelihood, but not to the same degree as factor mixup.

Task Model

Test Accuracy Test NLL

No Mixup

1.6

Task A Input Mixup (1.0)

0.0

Ground Truth Factor Mixup (1.0) 94.77

8.8310 6.0601 0.4940

No Mixup

21.25

Task B Input Mixup (1.0)

18.40

Ground Truth Factor Mixup (1.0) 84.02

7.0026 4.3149 0.4572

No Mixup Task C Input Mixup
Ground Truth Factor Mixup

63.05 66.09 99.06

4.2871 1.4181 0.1279

13

Under review as a conference paper at ICLR 2019
B ANALYSIS OF HOW Manifold Mixup CHANGES LEARNED
REPRESENTATIONS
Figure 6: An experiment on a network trained on the 2D spiral dataset with a 2D bottleneck hidden state in the middle of the network (the same setup as 1). Noise refers to gaussian noise in the bottleneck layer, dropout refers to dropout of 50% in all layers except the bottleneck itself (due to its low dimensionality), and batch normalization refers to batch normalization in all layers. This shows that the effect of concentrating the hidden states for each class and providing a broad region of low confidence between the regions is not accomplished by the other regularizers.
We have found significant improvements from using Manifold Mixup, but a key question is whether the improvements come from changing the behavior of the layers before the mixup operation is applied or the layers after the mixup operation is applied. This is a place where Manifold Mixup and Input Mixup are clearly differentiated, as Input Mixup has no "layers before the mixup operation" to change. We conducted analytical experimented where the representations are low-dimensional enough to visualize. More concretely, we trained a fully connected network on MNIST with two fully-connected leaky relu layers of 1024 units, followed by a 2-dimensional bottleneck layer, followed by two more fully-connected leaky-relu layers with 1024 units. We then considered training with no mixup, training with mixup in the input space, and training only with mixup directly following the 2D bottleneck. We consistently found that Manifold Mixup has the effect of making the representations much tighter, with the real data occupying more specific points, and with a more well separated margin between the classes, as shown in Figure 7
C SUPERVISED REGULARIZATION
For supervised regularization we considered architectures within the PreActResNet family: PreActResNet18, PreActResNet34, and PreActResNet152. When using Manifold Mixup, we selected the layer to perform mixing uniformly at random from a set of eligible layers. In our experiments on PreActResNets in Table 1a, Table 1b, Table 3 and Table 5, for Manifold Mixup, our eligible layers for mixing were : the input layer, the output from the first resblock, and the output from the second resblock. For PreActResNet18, the first resblock has four layers and the second resblock has four layers. For PreActResNet34, the first resblock has six layers and the second resblock has eight layers. For PreActResNet152, the first resblock has 9 layers and the second resblock has 24 layers. Thus the mixing is often done fairly deep in the network, for example in PreActResNet152 the output of the second resblock is preceded by a total of 34 layers (including the initial convolution which is not in a resblock). For Manifold Mixup All layers in Table 1a, our eligible layers for mixing were : the input layer, the output from the first resblock, and the output from the second resblock, and the output from the third resblock. We trained all models for 1200 epochs and dropped the learning rates by a factor of 0.1 at 400 epochs and 800 epochs.
14

Under review as a conference paper at ICLR 2019
Figure 7: Representations from a classifier on MNIST (top is trained on digits 0-4, bottom is trained on all digits) with a 2D bottleneck representation in the middle layer. No Mixup Baseline (left), Input Mixup (center), Manifold Mixup (right).
Figure 8: CIFAR-10 test set Negative Log-Likelihood (Y-axis) on PreActResNet152, wrt training epochs (X-axis).
D SEMI-SUPERVISED MANIFOLD MIXUP AND INPUT MIXUP ALGORITHM
We present the procedure for Semi-supervised Manifold Mixup and Semi-supervised Input Mixup in Algorithms 1 and 3 respectively.
15

Under review as a conference paper at ICLR 2019

Algorithm 1 Semi-supervised Manifold Mixup. f: Neural Network; M anif oldM ixup: Manifold Mixup Algorithm 2; DL: set of labelled samples; DUL: set of unlabelled samples;  : consistency coefficient (weight of unlabeled loss, which is ramped up to increase from zero to its max value over
the course of training); N : number of updates; y~i: Mixedup labels of labelled samples; y^i: predicted label of the labelled samples mixed at a hidden layer; yj: Psuedolabels for unlabelled samples; y~j : Mixedup Psuedolabels of unlabelled samples; y^j predicted label of the unlabelled samples mixed at a hidden layer

1: k  0
2: while k  N do
3: Sample (xi, yi)  DL 4: y^i, y~i = M anif oldM ixup(xi, yi, ) 5: LS = Loss(y^i, y~i) 6: Sample xj  DUL 7: yj = f(xj ) 8: y^j, y~j = M anif oldM ixup(xj, yj, ) 9: LUS = Loss(y^j, y~j) 10: L = LS + (k)LUS 11: g  L (Gradients of the minibatch Loss ) 12:   Update parameters using gradients g (e.g. SGD )
13: end while

Sample labeled batch
Cross Entropy loss Sample unlabeled batch Compute Pseudolabels
MSE Loss Total Loss

Algorithm 2 Manifold Mixup. f: Neural Network; D : dataset
1: Sample (xi, yi)  D 2: hi hidden state representation of Neural Network f at a layer k
randomly 3: (himixed, yimixed)  M ixup(hi, yi) 4: y^i  Forward Pass the hmi ixed from layer k to the output layer of f 5: return y^i, y~i

Sample a batch the layer k is chosen

Algorithm 3 Semi-supervised Input Mixup. f: Neural Network. InputM ixup: Mixup process of (Zhang et al., 2017); DL: set of labelled samples; DUL: set of unlabelled samples;  : consistency coefficient (weight of unlabeled loss, which is ramped up to increase from zero to its max value over the course of training); N : number of updates; ximixedup: mixed up sample; yimixedup: mixed up label; y^imixedup: mixed up predicted label

1: k  0
2: while k  N do
3: Sample (xi, yi)  DL
4: 5: (xmi ixedup, yimixedup) = InputM ixup(xi, yi) 6: LS = Loss(f(ximixedup), yimixedup) 7: Sample xj  DUL 8: y^j = f(xj) 9: (xj mixedup, y^j mixedup) = InputM ixup(xj , y^j ) 10: LUS = Loss(f(xj mixedup), y^imixedup) 11: L = LS + (k)  LUS 12: g  L 13:   Update parameters using gradients g (e.g. SGD )

Sample labeled batch
CrossEntropy Loss Sample unlabeled batch Compute Pseudolabels
MSE Loss Total Loss Gradients of the minibatch Loss

14: end while

16

Under review as a conference paper at ICLR 2019

Table 5: Models trained on the normal CIFAR-100 and evaluated on a test set with novel deformations. Manifold Mixup (ours) consistently allows the model to be more robust to random shearing, rescaling, and rotation even though these deformations were not observed during training. For the rotation experiment, each image is rotated with an angle uniformly sampled from the given range. Likewise the shearing is performed with uniformly sampled angles. Zooming-in refers to take a bounding box at the center of the image with k% of the length and k% of the width of the original image, and then expanding this image to fit the original size. Likewise zooming-out refers to drawing a bounding box with k% of the height and k% of the width, and then taking this larger area and scaling it down to the original size of the image (the padding outside of the image is black).

Test Set Deformation
Rotation U(-20,20) Rotation U(-40,40) Rotation U(-60,60) Rotation U(-80,80) Shearing U(-28.6, 28.6) Shearing U(-57.3, 57.3) Shearing U(-114.6, 114.6) Shearing U(-143.2, 143.2) Shearing U(-171.9, 171.9)
Zoom In (20% rescale)
Zoom In (40% rescale)
Zoom In (60% rescale)
Zoom In (80% rescale)
Zoom Out (120% rescale)
Zoom Out (140% rescale)
Zoom Out (160% rescale)
Zoom Out (180% rescale)

No Mixup Baseline
52.96 33.82 26.77 24.19 55.92 35.66 19.57 17.55 22.38 2.43 4.97 12.68 47.95 43.18 19.34 11.12 7.98

Input Mixup =1.0
55.55 37.73 28.47 26.72 58.16 39.34 22.94 21.66 25.53 1.9 4.47 13.75 52.18 60.02 41.81 25.48 18.11

Input Mixup =2.0
56.48 36.78 27.53 25.34 60.01 39.7 22.8 21.22 25.27 2.45 5.23 13.12 50.47 61.62 42.02 25.85 18.02

Manifold Mixup =2.0
60.08 42.13 33.78 29.95 62.85 44.27 24.69 23.56 28.02 2.03 4.17 11.49 52.7 63.59 45.29 27.02 15.68

17

Under review as a conference paper at ICLR 2019

E SEMI-SUPERVISED EXPERIMENTAL DETAILS
We use the WideResNet28-2 architecture used in (Oliver et al., 2018) and closely follow their experimental setup for fair comparison with other Semi-supervised learning algorithms. We used SGD with momentum optimizer in our experiments. For Cifar10, we run the experiments for 1000 epochs with initial learning rate is 0.1 and it is annealed by a factor of 0.1 at epoch 500, 750 and 875. For SVHN, we run the experiments for 200 epochs with initial learning rate is 0.1 and it is annealed by a factor of 0.1 at epoch 100, 150 and 175. The momentum parameter was set to 0.9. We used L2 regularization coefficient 0.0005 and L1 regularization coefficient 0.001 in our experiments. We use the batch-size of 100.
The data pre-processing and augmentation in exactly the same as in (Oliver et al., 2018). For CIFAR-10, we use the standard train/validation split of 45,000 and 5000 images for training and validation respectively. We use 4000 images out of 45,000 train images as labelled images for semi-supervised learning. For SVHN, we use the standard train/validation split with 65932 and 7325 images for training and validation respectively. We use 1000 images out of 65932 images as labelled images for semi-supervised learning. We report the test accuracy of the model selected based on best validation accuracy.
For supervised loss, we used  (of   Beta(, )) from the set { 0.1, 0.2, 0.3... 1.0} and found 0.1 to be the best. For unsupervised loss, we used  from the set {0.1, 0.5, 1.0, 1.5, 2.0. 3.0, 4.0} and found 2.0 to be the best.
The consistency coefficient is ramped up from its initial value 0.0 to its maximum value at 0.4 factor of total number of iterations using the same sigmoid schedule of (Tarvainen & Valpola, 2017). For CIFAR-10, we found max consistency coefficient = 1.0 to be the best. For SVHN, we found max consistency coefficient = 2.0 to be the best.
When using Manifold Mixup, we selected the layer to perform mixing uniformly at random from a set of eligible layers. In our experiments on WideResNet28-2 in Table 2, our eligible layers for mixing were : the input layer, the output from the first resblock, and the output from the second resblock.

F ADVERSARIAL EXAMPLES
We ran the unbounded projected gradient descent (PGD) (Madry et al., 2017) sanity check suggested in (Athalye et al., 2018). We took our trained models for the input mixup baseline and manifold mixup and we ran PGD for 200 iterations with a step size of 0.01 which reduced the mixup model's accuracy to 1% and reduced the Manifold Mixup model's accuracy to 0%. This is direct evidence that our defense did not improve results primarily as a result of gradient masking.
The Fast Gradient Sign Method (FGSM) Goodfellow et al. (2014) is a simple one-step attack that produces x = x +  sgn(xL(, x, y)).

G GENERATIVE ADVERSARIAL NETWORKS

The recent literature has suggested that regularizing the discriminator is beneficial for training GANs (Salimans

et al., 2016; Arjovsky et al., 2017; Gulrajani et al., 2017; Miyato et al., 2018). In a similar vein, one could

add mixup to the original GAN training objective such that the extra data augmentation acts as a beneficial

regularization to the discriminator, which is what was proposed in Zhang et al. (2017). Mixup proposes the following objective3:

max min Ex,z, (d(x1 + (1 - )x2), y(; x1, x2)), gd

(8)

where x1, x2 can be either real or fake samples, and  is sampled from a U nif orm(0, ). Note that we have used a function y(; x1, x2) to denote the label since there are four possibilities depending on x1 and x2:


,  1 - , y(; x1, x2) = 0,   1,

if x1 is real and x2 is fake if x1 is fake and x2 is real if both are fake if both are real

(9)

In practice however, we find that it did not make sense to create mixes between real and real where the label is set to 1, (as shown in equation 9), since the mixup of two real examples in input space is not a real example.
3The formulation written is based on the official code provided with the paper, rather than the description in the paper. The discrepancy between the two is that the formulation in the paper only considers mixes between real and fake.

18

Under review as a conference paper at ICLR 2019

So we only create mixes that are either real-fake, fake-real, or fake-fake. Secondly, instead of using just the equation in 8, we optimize it in addition to the regular minimax GAN equations:

max min Ex (d(x), 1) + Eg(z) (d(g(z)), 0) + GAN mixup term (Equation 8) gd

(10)

Using similar notation to earlier in the paper, we present the manifold mixup version of our GAN objective in which we mix in the hidden space of the discriminator:

min Ex,z,k (d(x), 1) + (d(g(z), 0) + (dk(hk(x1) + (1 - )hk(x2), y(; x1, x2)), d

(11)

where hk(·) is a function denoting the intermediate output of the discriminator at layer k, and dk(·) the output of the discriminator given input from layer k.

The layer k we choose the sample can be arbitrary combinations of the input layer (i.e., input mixup), or the first or second resblocks of the discriminator, all with equal probability of selection.

We run some experiments evaluating the quality of generated images on CIFAR10, using as a baseline JSGAN
with spectral normalization (Miyato et al., 2018) (our configuration is almost identical to theirs). Results are averaged over at least three runs4. From these results, the best-performing mixup experiments (both input and
Manifold Mixup) is with  = 0.5, with mixing in all layers (both resblocks and input) achieving an average Inception / FID of 8.04 ± 0.08 / 21.2 ± 0.47, input mixup achieving 8.03 ± 0.08 / 21.4 ± 0.56, for the baseline experiment 7.97 ± 0.07 / 21.9 ± 0.62. This suggests that mixup acts as a useful regularization on the
discriminator, which is even further improved by Manifold Mixup. See Figure 9 for the full set of experimental
results

Inception scores on CIFAR10
pixel h1 h2 h1,h2,pixel

FID scores on CIFAR10
pixel h1 h2 h1,h2,pixel

7.6 7.8 8.0 8.2 8.4 20 21 22 23 24 25 26

0.1 0.2 0.5 

1

0.1 0.2 0.5 

1

Figure 9: We test out various values of  in conjunction with either: input mixup ( pixel) (Zhang et al., 2017), mixing in the output of the first resblock (h1), mixing in either the output of the first resblock or the output of the second resblock (h1,2), and mixing in the input or the output of the first resblock or the output of the second resblock (1,2,pixel). The dotted line indicates the baseline Inception / FID score. Higher scores are better for Inception, while lower is better for FID.

4Inception scores are typically reported with a mean and variance, though this is across multiple splits of samples across a single model. Since we run multiple experiments, we average their respective means and variances.
19

Under review as a conference paper at ICLR 2019
REFERENCES
Alexander A. Alemi, Ian Fischer, Joshua V. Dillon, and Kevin Murphy. Deep variational information bottleneck. CoRR, abs/1612.00410, 2016. URL http://arxiv.org/abs/1612.00410.
Martin Arjovsky, Soumith Chintala, and Léon Bottou. Wasserstein generative adversarial networks. In International Conference on Machine Learning, pp. 214­223, 2017.
A. Athalye, N. Carlini, and D. Wagner. Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples. ArXiv e-prints, February 2018.
Yoshua Bengio, Grégoire Mesnil, Yann Dauphin, and Salah Rifai. Better mixing via deep representations. In ICML'2013, 2013.
Terrance Devries and Graham W. Taylor. Improved regularization of convolutional neural networks with cutout. CoRR, abs/1708.04552, 2017. URL http://arxiv.org/abs/1708.04552.
J. Gilmer, L. Metz, F. Faghri, S. S. Schoenholz, M. Raghu, M. Wattenberg, and I. Goodfellow. Adversarial Spheres. ArXiv e-prints, January 2018.
I. J. Goodfellow, J. Shlens, and C. Szegedy. Explaining and Harnessing Adversarial Examples. ArXiv e-prints, December 2014.
Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron C Courville. Improved training of wasserstein gans. In Advances in Neural Information Processing Systems, pp. 5769­5779, 2017.
Hongyu Guo, Yongyi Mao, and Richong Zhang. MixUp as Locally Linear Out-Of-Manifold Regularization. ArXiv e-prints, 2016. URL https://arxiv.org/abs/1809.02499.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual networks. CoRR, abs/1603.05027, 2016. URL http://arxiv.org/abs/1603.05027.
Geoffrey E. Hinton, Nitish Srivastava, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Improving neural networks by preventing co-adaptation of feature detectors. CoRR, abs/1207.0580, 2012. URL http: //arxiv.org/abs/1207.0580.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. CoRR, abs/1502.03167, 2015. URL http://arxiv.org/abs/1502.03167.
A. Madry, A. Makelov, L. Schmidt, D. Tsipras, and A. Vladu. Towards Deep Learning Models Resistant to Adversarial Attacks. ArXiv e-prints, June 2017.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient estimation of word representations in vector space. CoRR, abs/1301.3781, 2013. URL http://arxiv.org/abs/1301.3781.
T. Miyato, S.-i. Maeda, M. Koyama, and S. Ishii. Virtual Adversarial Training: a Regularization Method for Supervised and Semi-supervised Learning. ArXiv e-prints, April 2017.
Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida. Spectral normalization for generative adversarial networks. CoRR, abs/1802.05957, 2018. URL http://arxiv.org/abs/1802.05957.
A. Oliver, A. Odena, C. Raffel, E. D. Cubuk, and I. J. Goodfellow. Realistic Evaluation of Deep SemiSupervised Learning Algorithms. ArXiv e-prints, April 2018.
Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training gans. In Advances in Neural Information Processing Systems, pp. 2234­2242, 2016.
C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. Goodfellow, and R. Fergus. Intriguing properties of neural networks. ArXiv e-prints, December 2013.
Antti Tarvainen and Harri Valpola. Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Advances in Neural Information Processing Systems 30, pp. 1195­1204. Curran Associates, Inc., 2017.
Yuji Tokozume, Yoshitaka Ushiku, and Tatsuya Harada. Between-class learning for image classification. CoRR, abs/1711.10284, 2017. URL http://arxiv.org/abs/1711.10284.
Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. CoRR, abs/1605.07146, 2016. URL http://arxiv.org/abs/1605.07146.
20

Under review as a conference paper at ICLR 2019 Hongyi Zhang, Moustapha Cissé, Yann N. Dauphin, and David Lopez-Paz. mixup: Beyond empirical risk
minimization. CoRR, abs/1710.09412, 2017. URL http://arxiv.org/abs/1710.09412. Jake Zhao and Kyunghyun Cho. Retrieval-augmented convolutional neural networks for improved robustness
against adversarial examples. CoRR, abs/1802.09502, 2018. URL http://arxiv.org/abs/1802. 09502.
21


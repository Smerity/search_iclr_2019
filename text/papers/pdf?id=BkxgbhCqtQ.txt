Under review as a conference paper at ICLR 2019

PREDICTIVE UNCERTAINTY THROUGH QUANTIZATION
Anonymous authors Paper under double-blind review

ABSTRACT
High-risk domains require reliable confidence estimates from predictive models. Deep latent variable models provide these, but suffer from the rigid variational distributions used for tractable inference, which err on the side of overconfidence. We propose Stochastic Quantized Activation Distributions (SQUAD), which imposes a flexible yet tractable distribution over discretized latent variables. The proposed method is scalable, self-normalizing and sample efficient. We demonstrate that the model fully utilizes the flexible distribution, learns interesting non-linearities, and provides predictive uncertainty of competitive quality.

1 INTRODUCTION
In high-risk domains, prediction errors come at high costs. Luckily such domains often provide a failsafe in the form of rejecting a prediction: self-driving cars perform an emergency stop, doctors can run another diagnostic test, and industrial processes can be temporarily halted. For deep learning models, this can be achieved via a predetermined threshold on predicted confidence scores. As such, a low error rate can be guaranteed at the cost of rejecting some predictions. However, estimating confidence scores from neural networks that create well-ordered rankings of correct and incorrect predictions remains an active area of research.

x
<latexit sha1_base64="556K3KfvISjTw9nnZ+SMnXd6wz0=">AAAEbHicfZPritNAFMdnt1XXettV/LQIwWVFpJRUBPWDsKgLghVX3NpCU5bJ5KQddi4hM2mbDnkEv+qz+RS+gpM2QnZaPTDhn/M7k3OZTJgwqrTv/9rZbTSvXb+xd7N16/adu/f2D+5/UzJLCfSJZDIdhlgBowL6mmoGwyQFzEMGg/DyXckHM0gVleJc5wmMOZ4IGlOCtXV9DRazi/0jv+OvzNsU3UococrOLg4a50EkScZBaMKwUqOun+ixwammhEHRCjIFCSaXeAIjKwXmoMZmVWvhHVtP5MUytUtob+Wt7zCYK5Xz0EZyrKfKZaVzGxtlOn41NlQkmQZB1onijHlaemXjXkRTIJrlVmCSUlurR6Y4xUTb8WzL0q4qaYf8ak+rmqkoT0fGWkZSte3MExARwUk51raGhVZ0CW80FfnYlDFCalBF69irZyrTaCnZBrCFs8QKm1jAnEjOsYhMcFqYoNwThua0cBiZ/YWxIS5c1ODChUNSQYKZGbq0V6c9ly5rH166MK/B3IWzGpy5cFCDAxfOa3DuwrAGwxK+B/uTpvDJuj4nkGItUxN87BXGrq30mQkmGQ+BKXu6hbH6n3HriPLJ8cJt/j+9J1NaK7N8LQp7D7vurdsU/eed1x3/y4ujk7fVhdxDh+gxeoq66CU6QR/QGeojgiboO/qBfjZ+Nx82D5uP1qG7O9WeB+iKNZ/8Aa7zj1A=</latexit>

z1 <latexit sha1_base64="mep7itZ0m4hEPojuKMEVkDGOogU=">AAAEbnicfZPdihMxFMez26pr/dhdvfBCxMGysEgpUxHUC2FRFwQrrrC1hU4pmcxpGzaTDEmm7TTMM3irj+Zj+AZm2hFm0+qBDP85v5M5H5mECaNK+/6vvf1a/cbNWwe3G3fu3rt/eHT84JsSqSTQI4IJOQixAkY59DTVDAaJBByHDPrh1fuC9+cgFRX8UmcJjGI85XRCCdbW1QtW83FnfNT02/7avG3RKUUTlXYxPq5dBpEgaQxcE4aVGnb8RI8MlpoSBnkjSBUkmFzhKQyt5DgGNTLranPvxHoibyKkXVx7a291h8GxUlkc2sgY65lyWeHcxYapnrweGcqTVAMnm0STlHlaeEXrXkQlEM0yKzCR1NbqkRmWmGg7oF1ZWmUlrTC+3tO6ZsqL8xETLSKhWnbqCfCI4KQYbEvDUiu6grea8mxkihguNKi8ceJVMxVptBBsC9jCWWKFTcxhQUQcYx6Z4Dw3QbEnDM157jAy/wsnhrhwWYFLFw5ICQlmZuDSbpV2XbqqfHjlwqwCMxfOK3Duwn4F9l24qMCFC8MKDAv4AexPKuGzdX1JQGItpAk+dXNj10763ATTNA6BKXu6ubH6n3GbiOIZ46Xb/H96T2a0Umbxmuf2HnbcW7ctei/ab9r+15fNs3flhTxAj9EzdIo66BU6Qx/RBeohgij6jn6gn7Xf9Uf1J/Wnm9D9vXLPQ3TN6qd/AExbj/Y=</latexit>

p(z|x)
<latexit sha1_base64="erCiicMLVJdqs4W7M4MLmg5TJrU=">AAAFwXiclZTdTtswFMcNoxvrPoDtcjfWKjSYoGoqpCFNSEgUadJ6wSZKC01UOYnTWHXszHYorcmLbLfbQ+1t5oR2JEG7mKVIx+f3P+f4Iz5uTIlUrdbvldVHa7XHT9af1p89f/FyY3Pr1YXkifBwz+OUi4GLJKaE4Z4iiuJBLDCKXIr77uQk4/1rLCTh7FzNYuxEaMxIQDykjGu0uRHvzG/tCKnQDfRNujvabLSarXzAh4a1MBpgMc5GW2vfbZ97SYSZ8iiScmi1YuVoJBTxKE7rdiJxjLwJGuOhMRmKsHR0vvIUbhuPDwMuzMcUzL3lCBUcOpqwOFGYeSYAIkr5FBr//iHM/SX9ueXoLFUuNmMbGggP912i4LmVV5HFAB2aQxECB7k6D8g95jwnZWEi6FKzEEoSxRTD3tcuzI5WYqUIG5eCXM4nCrnyPnsseIBldh+I7n9LECVqBo2E4nI5FMnsVtKqM9/BfTqXGuRyJHyYyaGcRS6n5VRu9H/6iHiCZztKl1FLDx8LFIezkjpIKI2N8W8xw1OPRxFivrYHXqrz381DVA/StEy7Rdqt0k6Rdqr05npB8x+5AgcF+KDqZQFeVuGsAGdVqMICNTOsUFUyLyjmVXhVgFdVOC3AaRWeukvo6lMD69uwgxjB9J2EyPdJ9rrLt6rIZF5J4vuLJCLSfrXChKbDtqM7o79H/rmb2hQHCg5hw4K3t7DRhrYg41BBpxLcL6y9X8783vaIMJ3BH5qnamerGi4b2NGOFyLRzKa7jq4XX5vNuI+HMkQxPrqL3/MFmu4RxrCAZpdHbdN1YJ5gF+qGlX40ZU03s6q966Fx0W5arab15aBxfLDoa+vgDXgLdoAFPoBj8AmcgR7wQAJ+gJ/gV+2kRmpxTdxJV1cWMa9BadT0H/PW+Kw=</latexit>

z~ <latexit sha1_base64="BQ1r9a4mLEbJhupKJFh6aml5+SM=">AAAFuXiclZTfT9swEMcNoxtjv2B73Iu1Cg0mqJoKaUgICWlFmrQ+sInSQhMhx3Ear46d2S6lNfkr9jZtf9j+mzmhHUnQHnZSpPN9vndnO/L5CaNKN5u/l5YfrNQePlp9vPbk6bPnL9Y3Xp4pMZaYdLFgQvZ9pAijnHQ11Yz0E0lQ7DPS80cfMt67IlJRwU/1NCFejIachhQjbUPnrqYsIGaWXq7Xm41mbvC+48ydOpjbyeXGyg83EHgcE64xQ0oNnGaiPYOkppiRdM0dK5IgPEJDMrAuRzFRnsl3nMJNGwlgKKT9uIZ5tJyhw33PUJ6MNeHYJkDEmJhAG9/dh3m8pD91PJOVysXWNqGFcH/XpxqeOnkXVUwwkb0MKUmYq/OEPGLvcVQWjiVbaOZCReOEEdj90oHZlSqiNeXDUpIvxEgjX91VT6QIicr+A2K738aIUT2FVsJIuR2KVYx0lFaD+QnuyvnMIl8gGcBMDtU09gUrl/Lj/9PHFEuRnShdZC0iYihREk1L6nDMWGKdf4s5mWARx4gHxu3j1LhZZ4yY6adpmXaKtFOl7SJtV+n11Zz6obmuwn4B3ut6XoDnVTgtwGkV6qhA7YpoVJXMCopZFV4U4EUVTgpwUoXH/gL65tjCtU3YRpwS9lZBFAQ0e9Xlv6rpaFYpEgTzIjI2QbXDiKWDlmfal3+v/FMndRkJNRzAugNvbmC9BV1Jh5GGXiW5V9h7r1z5nYuptJMhGNin6ma7GiwG1+EWjpBsZMttz6wVX5vLRUAGKkIJObzN3wkkmuxQzomE9pSHLTt1YF5gG5q6kx7YtnaaOdXZdd85azWcZsP5vFc/2pvPtVXwGrwBW8AB78ER+AhOQBdgEIPv4Cf4VTuooVpU+3orXV6a57wCJaupP+1S9io=</latexit>

z2 <latexit sha1_base64="140MI4KVtB7/uRpqHUXODrPUTWQ=">AAAEbnicfZNbi9NAFMdnt1XXetvVBx9EDJaFRUpJF0F9EBZ1QbDiCltbaEqZTE7aYecSMpPehnwGX/Wj+TH8Bk7aCNlp9cCEf87vTM5lMmHCqNK+/2tvv1a/cfPWwe3Gnbv37j84PHr4TcksJdAjksl0EGIFjAroaaoZDJIUMA8Z9MOr9wXvzyBVVIpLvUxgxPFE0JgSrK2rF6xm49PxYdNv+2vztkWnFE1U2sX4qHYZRJJkHIQmDCs17PiJHhmcakoY5I0gU5BgcoUnMLRSYA5qZNbV5t6x9UReLFO7hPbW3uoOg7lSSx7aSI71VLmscO5iw0zHr0eGiiTTIMgmUZwxT0uvaN2LaApEs6UVmKTU1uqRKU4x0XZAu7K0ykpaIb/e07pmKorzkbGWkVQtO/UERERwUgy2pWGhFV3BW03FcmSKGCE1qLxx7FUzFWm0lGwL2MJZYoVNLGBOJOdYRCY4z01Q7AlDc547jMz+wtgQFy4qcOHCASkhwcwMXNqt0q5LV5UPr1y4rMClC2cVOHNhvwL7LpxX4NyFYQWGBfwA9idN4bN1fUkgxVqmJvjUzY1dO+kLE0wyHgJT9nRzY/U/4zYRxZPjhdv8f3pPprRSZvGa5/Yedtxbty16p+03bf/ry+bZu/JCHqAn6Dk6QR30Cp2hj+gC9RBBFH1HP9DP2u/64/rT+rNN6P5euecRumb1kz9QYY/3</latexit>

y <latexit sha1_base64="4cGYRFkWcBo9hzyw+N7dsLcD1Cg=">AAAEbHicfZPritNAFMdnt1XXettV/LQIwWVFpJRUBPWDsKgLghVX3NpCU5bJ5KQddi4hM2mbDnkEv+qz+RS+gpM2QnZaPTDhn/M7k3OZTJgwqrTv/9rZbTSvXb+xd7N16/adu/f2D+5/UzJLCfSJZDIdhlgBowL6mmoGwyQFzEMGg/DyXckHM0gVleJc5wmMOZ4IGlOCtXV9DfLZxf6R3/FX5m2KbiWOUGVnFweN8yCSJOMgNGFYqVHXT/TY4FRTwqBoBZmCBJNLPIGRlQJzUGOzqrXwjq0n8mKZ2iW0t/LWdxjMlcp5aCM51lPlstK5jY0yHb8aGyqSTIMg60RxxjwtvbJxL6IpEM1yKzBJqa3VI1OcYqLteLZlaVeVtEN+tadVzVSUpyNjLSOp2nbmCYiI4KQca1vDQiu6hDeainxsyhghNaiidezVM5VptJRsA9jCWWKFTSxgTiTnWEQmOC1MUO4JQ3NaOIzM/sLYEBcuanDhwiGpIMHMDF3aq9OeS5e1Dy9dmNdg7sJZDc5cOKjBgQvnNTh3YViDYQnfg/1JU/hkXZ8TSLGWqQk+9gpj11b6zASTjIfAlD3dwlj9z7h1RPnkeOE2/5/ekymtlVm+FoW9h1331m2K/vPO647/5cXRydvqQu6hQ/QYPUVd9BKdoA/oDPURQRP0Hf1APxu/mw+bh81H69DdnWrPA3TFmk/+ALL6j1E=</latexit>

Figure 1: DLVMs have layers of stochastic latent variables.

Deep Latent Variable Models (DLVMs, fig. 1) approach this by postulating latent variables z for which the uncertainty in p(z|x) influences the confidence in the target prediction. Recently, efficient inference algorithms have been proposed in the form of variational inference, where an inference neural network is optimized to predict parameters of a variational distribution that approximates an otherwise intractable distribution (Kingma & Welling (2013); Rezende et al. (2014); Alemi et al. (2016); Achille & Soatto (2016)).


<latexit sha1_base64="hAnGZAtKFJW9dFFAVofaA1mIVlg=">AAAFxHiclZTfbtMwFMY9WGGUf9u45MaimtimrWqqSUxCkybRISR6MdC6dmtC5SROY9Wxg+2s67zwIFxwC6/E2+BkLUsycYGlSCfn933nxI583JgSqVqt30v37i/XHjxceVR//OTps+era+unkifCwz2PUy4GLpKYEoZ7iiiKB7HAKHIp7ruTdxnvX2AhCWcnahZjJ0JjRgLiIWVSo9V1O6+hBfZTaMch+bI9Wm20mq18wbuBNQ8aYL6OR2vL322fe0mEmfIoknJotWLlaCQU8ShO63YicYy8CRrjoQkZirB0dN44hRsm48OAC/MwBfNs2aGCfUcTFicKM88YIKKUT6HJ7+7DPF/Sn1iOzkrlYrM2oIFwf9clCp5YeRdZNOjQnIsQOMjVuSHPmCOdlIWJoAvNXChJFFMMe5+7MDtdiZUibFwyuZxPFHLlbfVY8ADL7Jcguvs1QZSoGTQSisvtUCQjpMK0msx3cFvOpQa5HAkfZnIoZ5HLabmUG/2fPiKe4NmO0oVrkeFjgeJwVlIHCaWxCf4tZnjq8ShCzNf2wEu1nXX2ENWDNC3TbpF2q7RTpJ0qvbyYUzfQl1U4KMA7Xc8K8KwKZwU4q0IVFqh5wwpVJVcFxVUVnhfgeRVOC3BahUfuArr6yMD6BuwgRjB9LSHyfZJd8PJfVWRyVSni+/MiItJ+tcOEpsO2ozujv0f+sZvaFAcKDmHDgtfXsNGGtiDjUEGnYu4Xvr1frrxte0SYyeAPzVW1s68aLmbYwaYXItHMXrccXS/eNptxHw9liGJ8cOPf8QWa7hDGsIBmlwdtM3VgXmAL6oaVvjVtzTSzqrPrbnDablqtpvVpr3G4N59rK+AleAU2gQXegEPwARyDHvDAJfgBfoJftfc1WpO15EZ6b2nueQFKq/btD8Xo+cg=</latexit>
q
<latexit sha1_base64="0IOt3cbERNCZOJ4B7qywM+JNpds=">AAAFtniclZTfb9MwEMc9WGGUXxs88mJRTWxoq5oKiUpo0gSdhEQfBlrXbk2oHMdpTB07s911XZb/gVf2n/Hf4GQpSzLxwEmRzvf53p3tyOdGjCrdav1euXd/tfbg4dqj+uMnT589X994cazETGLSx4IJOXSRIoxy0tdUMzKMJEGhy8jAnX5K+eCcSEUFP9KLiDghmnDqU4y0CR2fje0ooOP1RqvZygzedazcaYDcDscbq9e2J/AsJFxjhpQaWa1IOzGSmmJGkro9UyRCeIomZGRcjkKinDjbbgI3TcSDvpDm4xpm0XKG9jtOTHk004RjkwARY2IOTXy3A7N4SX9kOXFaKhMb24QGws6uSzU8srIuqpgQB+YmpCR+ps4Ssoi5xGlZOJNsqcmFioYRI7D/rQfT+1REa8onpSRXiKlGrrqtHknhE5X+BMR2z2aIUb2ARsJIuR0KVYh0kFSD2Qluy7nMIFcg6cFUDtUidAUrl3LD/9OHFEuRnihZZi0jYiJRFCxKan/GWGScf4s5mWMRhoh7sT3ESWynnTFi8TBJyrRXpL0q7RZpt0ovznPq+vFFFQ4L8E7XkwI8qcJFAS6qUAcFalZEo6rksqC4rMLTAjytwnkBzqvwwF1CNz4wsL4Ju4hTwt4oiDyPpk+6/Fc1nV5WinheXkSGsVftMGXJqO3E3fHfK//SS2xGfA1HsGHBqyvYaENb0kmgoVNJHhT2PihXfmtjKs1k8EbmqdrprkbLqbW3hQMkm+ly24nrxddmc+GRkQpQRPZu8nc8ieY7lHMioTnlXttMHZgV2IZxw0o+mLZmmlnV2XXXOW43rVbT+vqusf8xn2tr4BV4DbaABd6DffAZHII+wOAH+Al+getap/a9RmqTG+m9lTznJShZLfoDv470pw==</latexit>

 <latexit sha1_base64="LzSxC62qKq7o4xaWWXmaVVswm34=">AAAFx3iclZTfbtMwFMY9WGGUfxuIK24sqolt2qqmmsQkNGkSnQSiFwOta7cmVI7jNFadONjOus7LBa/BHbfwRLwNTtayJBMXWIp0cn7f+U7syMeNGZWq1fq9dOfucu3e/ZUH9YePHj95urr27ETyRGDSw5xxMXCRJIxGpKeoYmQQC4JCl5G+O3mX8f45EZLy6FjNYuKEaBxRn2KkTGq0+sLOPbTLEpJCWwVEoS9bo9VGq9nKF7wdWPOgAebraLS2/N32OE5CEinMkJRDqxUrRyOhKGYkrduJJDHCEzQmQxNGKCTS0XnvFK6bjAd9LswTKZhnyxXK33M0jeJEkQibAogY41No8jt7MM+X9MeWozOrXGzWOjQQ7u24VMFjK+8iiwU6MEcjBPFzdV6QZ8ypTsrCRLCFZi6UNIwZgb3PXZgdsCRK0WhcKnI5nyjkyhv3WHCfyOyvILbzNUGMqhk0EkbK7VAoQ6SCtJrMd3Bj5zKDXI6EBzM5lLPQ5axs5Yb/pw8pFjzbUbqoWmT4WKA4mJXUfsJYbIJ/iyMyxTwMUeRpe4BTbWedMWJ6kKZl2i3SbpV2irRTpRfnc+r6+qIKBwV4q+tpAZ5W4awAZ1WoggK9vkJVyWVBcVmFZwV4VoXTApxW4aG7gK4+NLC+DjsoooS9lhB5Hs3uePmvKjq5rJh43txEhNqrdpiwdNh2dGf098g/dlObEV/BIWxY8OoKNtrQFnQcKOhUivuFb++XnbdsTIWZDN7QXFU7+6rhYoztb+AAiWb2uunoevG22RH3yFAGKCb71/XbnkDTbRpFRECzy/22mTowN9iEumGlb01bM82s6uy6HZy0m1araX3abRzszufaCngJXoENYIE34AC8B0egBzDQ4Af4CX7VPtR47bx2cS29szSveQ5Kq/btDwNo+yg=</latexit>
p
<latexit sha1_base64="omhpCfmSDgPntJKR3V00EOH65fs=">AAAFuHiclZTfbtMwFMa9scIo/za45MaimtjQVjUVEhVo0gSdhEQvBlrXbk1UHMdprDp2sJ11XZaX4BJ4Md4GJ2tZkokLLFU6Pb/vO8d25ONGjCrdav1eWb2zVrt7b/1+/cHDR4+fbGw+PVEilpj0sWBCDl2kCKOc9DXVjAwjSVDoMjJwpx8yPjgnUlHBj/U8Ik6IJpz6FCNtUsNobOuAaDTeaLSarXzB24G1CBpgsY7Gm2s/bE/gOCRcY4aUGlmtSDsJkppiRtK6HSsSITxFEzIyIUchUU6SbziFWybjQV9I8+Ma5tmyQ/sdJ6E8ijXh2BggYkzMoMnvdWCeL+mPLSfJSuVis7aggbCz51INj628iyoaksDchZTEz9W5Ic+Ya5yWhbFkS81CqGgYMQL7X3owu1FFtKZ8UjK5Qkw1ctVN9UgKn6jsMyC29y1GjOo5NBJGyu1QqEKkg7SazE9wU85lBrkCSQ9mcqjmoStYuZQb/p8+pFiK7ETp0rXMiIlEUTAvqf2YscgE/xZzMsMiDBH3EnuI08TOOmPEkmGalmmvSHtV2i3SbpVenC+o6ycXVTgswFtdTwvwtArnBTivQh0U6PUDqkouC4rLKjwrwLMqnBXgrAoP3SV0k0MD61uwizgl7KWCyPNo9qjLX1XT6WWliOctisgw8aodpiwdtZ2kO/575Z96qc2Ir+EINix4dQUbbWhLOgk0dCrmQWHvg3LlVzam0kwGb2Seqp3tarScW/vbOECymf3dcZJ68bXZXHhkpAIUkf1r/64n0WyXck4kNKfcb5upA/MCOzBpWOk709ZMM6s6u24HJ+2m1Wpan183Dt4v5to6eA5egG1ggTfgAHwER6APMGDgO/gJftXe1r7WJjV6LV1dWXiegdKqyT96XPWP</latexit>

Figure 2: A distribution q is optimized to approximate p .

Variational inference relies on a tractable class of distributions that can be optimized to closely resemble the true distribution (fig. 2), and it's hypothesized that more flexible classes lead to more faithful approximations and thus better performance (Jordan et al. (1999)). To explore this hypothesis we propose a novel tractable class of highly flexible variational distributions. Considering that neural networks with low-precision activations exhibit good performance (Holi & Hwang (1993); Hubara et al. (2016)), we make the modeling assumption that latent variables can be expressed under a strong quantization scheme, without loss of predictive fidelity. If this assumption holds, it becomes tractable to model a scalar latent variable with a flexible multinomial distribution over the values (fig. 3).

By repositioning the variational distribution from a potentially limited description of moments ­ as found in exponential families ­ to a direct expression of probabilities per value, a variety of benefits arise. As the output domain is constrained, the method becomes self-normalizing, relieving the model from poorly scalable batch normalization techniques (Ioffe & Szegedy (2015)). More interesting priors can be explored outside the space of exponential families and the model is able to learn non-linearities per neuron.

true p <latexit sha1_base64="Yr2+pXG+gPJQhFBaHOUHtIe0AvQ=">AAAFvXiclZTfb9MwEMc9WGGUXxs88mJRTWxoq5oKiUloYoJOQqIPA61rtyaaHMdprTp2ZjvrOi/8G0g8wZ/Ff4OTtSzJxAMnRTrf53t3tiOfHzOqdKv1e+nO3eXavfsrD+oPHz1+8nR17dmREonEpIcFE3LgI0UY5aSnqWZkEEuCIp+Rvj/5mPH+OZGKCn6oZzHxIjTiNKQYaRvyXE0utNEyIem3+HS10Wq2coO3HWfuNMDcDk7Xln+4gcBJRLjGDCk1dFqx9gySmmJG0rqbKBIjPEEjMrQuRxFRnsl3ncJ1GwlgKKT9uIZ5tJyhwx3PUB4nmnBsEyBiTEyhjW/vwDxe0h86nslK5WJr69BCuLPtUw0PnbyLKiaYsb0QKUmYq/OEPGLvclIWJpItNHOholHMCOx97cLsWhXRmvJRKckXYqKRr26qx1KERGX/ArHtswQxqmfQShgpt0ORipAep9VgfoKbcj6zyBdIBjCTQzWLfMHKpfzo//QRxVJkJ0oXWYuIGEkUj2cldZgwFlvn32JOplhEEeKBcQc4NW7WGSNmBmlapt0i7VZpp0g7VXpxPqd+aC6qcFCAt7oeF+BxFc4KcFaFelygdkU0qkouC4rLKjwpwJMqnBbgtAr3/QX0zb6F9XXYQZwS9kpBFAQ0e9nlv6rp5LJSJAjmRWRkgmqHCUuHbc90Tv9e+edu6jISajiEDQdeXcFGG7qSjsYaepXkfmHv/XLl1y6m0k6GYGifqpvtargYXrsbeIxkM1tueqZefG0uFwEZqjGKye51/lYg0XSLck4ktKfcbdupA/MCm9A0nPSdbWunmVOdXbedo3bTaTWdL28aex/mc20FvAAvwQZwwFuwBz6BA9ADGJyB7+An+FV7XyM1VuPX0jtL85znoGS16R+9u/hH</latexit>
SQUAD <latexit sha1_base64="J07PUHxlj4Q5TNxyxePwtGamjFk=">AAAFvHiclZTfb9MwEMe9scIovzZ45MWimtjQVjUVEpPQ0IBOQqIPG6xrtyaqHMdprTp2sN11nZd/gwde4N/iv8HJWpZk4oGTIp3v870725HPjxlVutH4vbR8Z6Vy997q/eqDh48eP1lbf3qixERi0sGCCdnzkSKMctLRVDPSiyVBkc9I1x9/THn3nEhFBT/Ws5h4ERpyGlKMtA25riYX2nw96rxvJYO1WqPeyAzedpy5UwNzOxysr/xwA4EnEeEaM6RU32nE2jNIaooZSaruRJEY4TEakr51OYqI8ky26QRu2EgAQyHtxzXMosUMHe56hvJ4ognHNgEixsQU2vjOLsziBf2x45m0VCa2tgEthLs7PtXw2Mm6qHyCGdn7kJKEmTpLyCL2KsdF4USyhWYuVDSKGYGdL22Y3qoiWlM+LCT5Qow18tVN9ViKkKj0VyC2822CGNUzaCWMFNuhSEVIj5JyMDvBTTmfWeQLJAOYyqGaRb5gxVJ+9H/6iGIp0hMli6xFRAwlikezgjqcMBZb599iTqZYRBHigXF7ODFu2hkjZnpJUqTtPG2XaStPW2V6cT6nfmguyrCXg7e6nubgaRnOcnBWhnqUo3ZFNCpLLnOKyzI8y8GzMpzm4LQMD/wF9M2BhdUN2EKcEvZSQRQENH3Yxb+q6fiyVCQI5kVkZIJyhzFL+k3PtAZ/r/xzO3EZCTXsw5oDr65grQldSYcjDb1Scje3926x8isXU2knQ9C3T9VNd9VfzK69TTxCsp4utzxTzb82l4uA9NUIxWTvOn87kGi6TTknEtpT7jXt1IFZgS1oak7y1ra108wpz67bzkmz7jTqztHr2v6H+VxbBc/BC7AJHPAG7INP4BB0AAYx+A5+gl+Vd5WgMq5E19LlpXnOM1CwyvkfQc33DQ==</latexit>

p(z)
<latexit sha1_base64="9eVB2E4xUdHlxkZ7uZH9dPWhHXw=">AAAFvXiclZTdbtMwFMe9scIoXxtccmNRTWxoq5oKiUpoYoJOQqIXA61rtySaHMdprTp2Zrvr2izPgcQVPBZvg5OlLMnEBZYiHZ/f/5zjj/h4EaNKt1q/V1bvrdXuP1h/WH/0+MnTZxubz0+UmEpM+lgwIYceUoRRTvqaakaGkSQo9BgZeJNPKR9cEqmo4Md6HhE3RCNOA4qRNi432nZCpMdeEC+SnfONRqvZyga8a1i50QD5ODrfXPvh+AJPQ8I1Zkgp22pF2o2R1BQzktSdqSIRwhM0IrYxOQqJcuNs1QncMh4fBkKaj2uYecsROui4MeXRVBOOTQBEjIkZNP69Dsz8Jf2x5cZpqkxsxhY0EHb2PKrhsZVVUcWAeGwOREoSZOosIPOYs5yUhVPJlppcqGgYMQL733owPVZFtKZ8VAryhJho5Knb7JEUAVHpXSC2dzFFjOo5NBJGyuVQqNJbSarObAe36TxmkCeQ9GEqh2oeeoKVU3nh/+lDiqVId5Qso5YeMZIoGs9L6mDKWGSMf4s5mWERhoj7sTPESZz9bhixeJgkZdor0l6Vdou0W6VXlzk1P/JVFQ4L8E7V0wI8rcJ5Ac6rUI8L1MyIRlXJoqBYVOFZAZ5V4awAZ1V46C2hFx8aWN+CXcQpYa8VRL5P05ddvlVNJ4tKEt/Pk8gw9qsVJiyx227cPf975F96icNIoKENGxa8voaNNnQkHY01dCvBg8LaB+XMbxxMpekMvm2eqpOuyl42r/1tPEaymU533LhefG0OFz6x1RhFZP8mfteXaLZLOScSml3ut03XgVmCHRg3rOS9KWu6mVXtXXeNk3bTajWtr28bBx/zvrYOXoJXYBtY4B04AJ/BEegDDC7Ad/AT/Kp9qJEaq/Eb6epKHvMClEZt9geffveB</latexit>

z <latexit sha1_base64="S2wxDZmMeuO5qfYacjLaUAhxzuM=">AAAFuniclZTdbtMwFMc9WGGUrw0uubGoJja0VU2FxCSYNEEnIdGLgda1WxNNtuO0Vhw72O66NstbcIUE78Xb4GQtSzJxgaVIx+f3P+f4Iz445kybVuv3yp27q7V799ce1B8+evzk6frGsxMtJ4rQHpFcqgFGmnImaM8ww+kgVhRFmNM+Dj9mvH9BlWZSHJtZTL0IjQQLGEHGus7cCJkxDpJ5er7eaDVb+YC3DWdhNMBiHJ1vrP5wfUkmERWGcKT10GnFxkuQMoxwmtbdiaYxIiEa0aE1BYqo9pJ8ySnctB4fBlLZTxiYe8sRJtjzEibiiaGC2ACIOJdTaP27ezD3l/THjpdkqXKxHZvQQri3i5mBx05eRRcDkrE9DaVokKvzgNxjDzIsCyeKLzULoWZRzCnsfe3C7Ew1NYaJUSkISxkahPVN9ljJgOrsIhDf/TZBnJkZtBJOy+VQpLNLSavOfAc36TC3CEukfJjJoZ5FWPJyKhz9nz5iRMlsR+kyaumRI4Xi8aykDiacx9b4t1jQKZFRhISfuAOSJvnfRhBPBmlapt0i7VZpp0g7VXp5saD2P76swkEB3qp6WoCnVTgrwFkVmnGB2hk1qCqZFxTzKjwrwLMqnBbgtAoP8RLi5NDC+ibsIMEof6Uh8n2WPevyrRoWzitJfH+RREWJX60Q8nTY9pLO+d8j/9xNXU4DA4ew4cCrK9hoQ1ex0dhArxLcL6y9X8782iVM2c7gD+1TdbNVDZeda3+LjJFqZtNtL6kXX5srpE+Heoxiun8dv+MrNN1hQlAF7S7327brwDzBNkwaTvrOlrXdzKn2rtvGSbvptJrOlzeNgw+LvrYGXoCXYAs44C04AJ/AEegBAgT4Dn6CX7X3NVxjtfBaemdlEfMclEbN/AEauvai</latexit>
Figure 3: SQUAD quantizes the domain of z to model a flexible and tractable variational distribution.

1

Under review as a conference paper at ICLR 2019

More concretely, the contributions of this work are as follows:
· We propose a novel variational inference method by leveraging multinomial distributions on quantized latent variables.
· We show that the emerging predicted distributions are multimodal, motivating the need for flexible distributions.
· We demonstrate that the proposed method applied to the information bottleneck objective computes competitive uncertainty over the predictions and that this manifests in better accuracy under strong risk guarantees.

2 BACKGROUND

In this work, we explore deep neural networks for regression and classification. We have data-points (x, y) in a dataset D = {(xi, yi) | i  [1, ..., N ]} and postulate latent variables z that represent the data. We focus on the Information Bottleneck (IB) perspective: first proposed by Tishby et al. (2000), the information bottleneck objective I(y, z; ) - I(x, z; ) is optimized to maximize the mutual information between z and y, whilst minimizing the mutual information between z and x. The objective can be efficiently optimized using a variational inference scheme as shown concur-
rently by both Alemi et al. (2016) and Achille & Soatto (2016). Under the Markov assumption P (z, x, y) = p(z|x)p(y|x)p(x), they derive the following lower bound:

I(y, z; )

-

 I (x,

z; )



L

=

1 N

N

Ep(z|xn)[log q(yn|z)] - DKL[p(z|xn) r(z)],

n=1

(1)

where Ep(zn|xn) is commonly estimated using a single Monte Carlo sample and r(z) is a variational distribution to the marginal distribution of z. In practice r(z) is fixed to a simple distribution such as a spherical Gaussian. Alemi et al. (2016) and Achille & Soatto (2016) continue to show that the
Variational Auto Encoder (VAE) Evidence Lower Bound (ELBO) proposed in Kingma & Ba (2014); Rezende et al. (2014) is a special case of the IB bound when y = x and  = 1:

I(z, x) - I(z, i)  Ep(z|xn)[log q(xn|z)] - DKL[p(z|xn) r(z)],

(2)

where i represents the identity of data-point xi. Interestingly, the VAE perspective considers the bound to optimize a variational distribution q(z|x), whilst the IB perspective prescribes that q(z|x) in the ELBO is not a variational posterior but the true encoder p(z|x), and instead p(y|z) and p(z)
are the distributions approximated with variational counterparts.

From yet another perspective, equation 1 can be interpreted as a domain-translating beta-VAE (Hig-
gins et al. (2016)), where an input image is encoded into a latent space and decoded into the target domain. The Lagrange multiplier  then controls the trade-off between rate and distortion, as argued by Alemi et al. (2017).

In this work, we follow the IB interpretation of the bound in equation 1 and leave the evaluation of our proposed variational inference scheme in other models such as the VAE for further work.

3 METHOD

At the heart of our proposal lies the assumption that neuron networks can be effective under strong
activation quantization schemes. For clarity, we start with presenting the derivation for a single layer information bottleneck. We model the encoder using a mean-field approximation p(z|x) =
k p(zk|x). We individually quantize the domain of each of the K scalar latent variables zk into a small set v = v1, . . . , vC consisting of a small number of C different real values (e.g. C = 11). We use a neural network f(·) with a softmax output over the quantization bins to model the distributions over the values for z:

p(z = vc|x) = f(x)c = softmax(Wcx + bc).

(3)

To optimize the parameters  with Stochastic Gradient Descent (SGD), we need to derive a fully differentiable loss for our model. Providing figure 4 for visual reference, we derive the bound as

2

Under review as a conference paper at ICLR 2019

SQUAD Computational Graph:
x
<latexit sha1_base64="556K3KfvISjTw9nnZ+SMnXd6wz0=">AAAEbHicfZPritNAFMdnt1XXettV/LQIwWVFpJRUBPWDsKgLghVX3NpCU5bJ5KQddi4hM2mbDnkEv+qz+RS+gpM2QnZaPTDhn/M7k3OZTJgwqrTv/9rZbTSvXb+xd7N16/adu/f2D+5/UzJLCfSJZDIdhlgBowL6mmoGwyQFzEMGg/DyXckHM0gVleJc5wmMOZ4IGlOCtXV9DRazi/0jv+OvzNsU3UococrOLg4a50EkScZBaMKwUqOun+ixwammhEHRCjIFCSaXeAIjKwXmoMZmVWvhHVtP5MUytUtob+Wt7zCYK5Xz0EZyrKfKZaVzGxtlOn41NlQkmQZB1onijHlaemXjXkRTIJrlVmCSUlurR6Y4xUTb8WzL0q4qaYf8ak+rmqkoT0fGWkZSte3MExARwUk51raGhVZ0CW80FfnYlDFCalBF69irZyrTaCnZBrCFs8QKm1jAnEjOsYhMcFqYoNwThua0cBiZ/YWxIS5c1ODChUNSQYKZGbq0V6c9ly5rH166MK/B3IWzGpy5cFCDAxfOa3DuwrAGwxK+B/uTpvDJuj4nkGItUxN87BXGrq30mQkmGQ+BKXu6hbH6n3HriPLJ8cJt/j+9J1NaK7N8LQp7D7vurdsU/eed1x3/y4ujk7fVhdxDh+gxeoq66CU6QR/QGeojgiboO/qBfjZ+Nx82D5uP1qG7O9WeB+iKNZ/8Aa7zj1A=</latexit>

normalized activation
z1 <latexit sha1_base64="mep7itZ0m4hEPojuKMEVkDGOogU=">AAAEbnicfZPdihMxFMez26pr/dhdvfBCxMGysEgpUxHUC2FRFwQrrrC1hU4pmcxpGzaTDEmm7TTMM3irj+Zj+AZm2hFm0+qBDP85v5M5H5mECaNK+/6vvf1a/cbNWwe3G3fu3rt/eHT84JsSqSTQI4IJOQixAkY59DTVDAaJBByHDPrh1fuC9+cgFRX8UmcJjGI85XRCCdbW1QtW83FnfNT02/7avG3RKUUTlXYxPq5dBpEgaQxcE4aVGnb8RI8MlpoSBnkjSBUkmFzhKQyt5DgGNTLranPvxHoibyKkXVx7a291h8GxUlkc2sgY65lyWeHcxYapnrweGcqTVAMnm0STlHlaeEXrXkQlEM0yKzCR1NbqkRmWmGg7oF1ZWmUlrTC+3tO6ZsqL8xETLSKhWnbqCfCI4KQYbEvDUiu6grea8mxkihguNKi8ceJVMxVptBBsC9jCWWKFTcxhQUQcYx6Z4Dw3QbEnDM157jAy/wsnhrhwWYFLFw5ICQlmZuDSbpV2XbqqfHjlwqwCMxfOK3Duwn4F9l24qMCFC8MKDAv4AexPKuGzdX1JQGItpAk+dXNj10763ATTNA6BKXu6ubH6n3GbiOIZ46Xb/H96T2a0Umbxmuf2HnbcW7ctei/ab9r+15fNs3flhTxAj9EzdIo66BU6Qx/RBeohgij6jn6gn7Xf9Uf1J/Wnm9D9vXLPQ3TN6qd/AExbj/Y=</latexit>

Legend:
scalar/activation

S

p(z1,k
<latexit sha1_base64="7SiUqDTbifVdPksqW8qBrv+Gq9w=">AAAEgnicfZPdbtMwFMe9LcAoH+tA4oYbi2rSmKopASS4YNIETEKiiCGta6WmqhzHaa06dhQ76YeXl+EWXoi3wWmDlLmFIzn65/yOc87xiYOEUalc9/fO7p5z5+69/fuNBw8fPT5oHj65liJLMeliwUTaD5AkjHLSVVQx0k9SguKAkV4w/VjyXk5SSQW/UouEDGM05jSiGCnjGjWfJcfLkfba0wKewXyE4Q305/nLUbPlnrorg5vCq0QLVHY5Oty78kOBs5hwhRmScuC5iRpqlCqKGSkafiZJgvAUjcnASI5iIod61UABj4wnhJFIzeIKrrz1HRrFUi7iwETGSE2kzUrnNjbIVPRuqClPMkU4XieKMgaVgOVpwJCmBCu2MALhlJpaIZ6gFGFlzmxblnZVSTuIb/e0qpnycmQiUiIUsm0GkRAeYpSUZ91WZK4kXZIzRfliqMsYLhSRReMI1jOVaZQQbAOYwllihEnMyQyLOEY81P5Fof1yTxDoi8JiOP8LI41tOK/BuQ37uIIYMd23aadOOzZd1j68tOGiBhc2zGswt2GvBns2nNXgzIZBDQYl/ETMT5qSr8b1LSEpUiLV/pdOoc3aSk+0P87igDBppltoo/8Zt44onzGa283/p/dkQmtllq9FYe6hZ9+6TXH96tRzT73vb1rnH6obuQ+egxfgGHjgLTgHn8El6AIMbsAP8BP8chznxPGc1+vQ3Z1qz1Nwy5z3fwBy8pU4</latexit>

= vc|x)

1 <latexit sha1_base64="1etOrtQ8bv8ZBtABdaeoSXemM9I=">AAAB53icbVBNS8NAEJ34WetX1aOXxSJ4KokI6q3oxWMLxhbaUDbbSbt2swm7G6GE/gIvHlS8+pe8+W/ctjlo64OBx3szzMwLU8G1cd1vZ2V1bX1js7RV3t7Z3duvHBw+6CRTDH2WiES1Q6pRcIm+4UZgO1VI41BgKxzdTv3WEyrNE3lvxikGMR1IHnFGjZWaXq9SdWvuDGSZeAWpQoFGr/LV7Scsi1EaJqjWHc9NTZBTZTgTOCl3M40pZSM6wI6lksaog3x26IScWqVPokTZkobM1N8TOY21Hseh7YypGepFbyr+53UyE10FOZdpZlCy+aIoE8QkZPo16XOFzIixJZQpbm8lbEgVZcZmU7YheIsvLxP/vHZdc5sX1fpNkUYJjuEEzsCDS6jDHTTABwYIz/AKb86j8+K8Ox/z1hWnmDmCP3A+fwDo6YyI</latexit>
2 <latexit sha1_base64="dplMh1RkaQKwXw38DAT3S4cqL1s=">AAAB53icbVBNS8NAEJ3Ur1q/qh69LBbBU0mKoN6KXjy2YGyhDWWznbRrN5uwuxFK6S/w4kHFq3/Jm//GbZuDtj4YeLw3w8y8MBVcG9f9dgpr6xubW8Xt0s7u3v5B+fDoQSeZYuizRCSqHVKNgkv0DTcC26lCGocCW+Hodua3nlBpnsh7M04xiOlA8ogzaqzUrPXKFbfqzkFWiZeTCuRo9Mpf3X7CshilYYJq3fHc1AQTqgxnAqelbqYxpWxEB9ixVNIYdTCZHzolZ1bpkyhRtqQhc/X3xITGWo/j0HbG1Az1sjcT//M6mYmuggmXaWZQssWiKBPEJGT2NelzhcyIsSWUKW5vJWxIFWXGZlOyIXjLL68Sv1a9rrrNi0r9Jk+jCCdwCufgwSXU4Q4a4AMDhGd4hTfn0Xlx3p2PRWvByWeO4Q+czx/qbIyJ</latexit>

3 <latexit sha1_base64="ZgOBONPC9+QqObn0T7gaBB49z3Q=">AAAB53icbVBNS8NAEJ3Ur1q/qh69LBbBU0lVUG9FLx5bMLbQhrLZTtq1m03Y3Qgl9Bd48aDi1b/kzX/jts1BWx8MPN6bYWZekAiujet+O4WV1bX1jeJmaWt7Z3evvH/woONUMfRYLGLVDqhGwSV6hhuB7UQhjQKBrWB0O/VbT6g0j+W9GSfoR3QgecgZNVZqnvfKFbfqzkCWSS0nFcjR6JW/uv2YpRFKwwTVulNzE+NnVBnOBE5K3VRjQtmIDrBjqaQRaj+bHTohJ1bpkzBWtqQhM/X3REYjrcdRYDsjaoZ60ZuK/3md1IRXfsZlkhqUbL4oTAUxMZl+TfpcITNibAllittbCRtSRZmx2ZRsCLXFl5eJd1a9rrrNi0r9Jk+jCEdwDKdQg0uowx00wAMGCM/wCm/Oo/PivDsf89aCk88cwh84nz/r74yK</latexit>

uniform
noise <latexit sha1_base64="x89H0ZIwpN2v+TLlIEq0E6G/UnI=">AAAB73icbVDLSgNBEOyNrxhfUY9eBoPgKWxEUG9BLx4juCaSLGF2MpsMmccyMyuEJV/hxYOKV3/Hm3/jbLIHTSxoKKq66e6KEs6M9f1vr7Syura+Ud6sbG3v7O5V9w8ejEo1oQFRXOlOhA3lTNLAMstpJ9EUi4jTdjS+yf32E9WGKXlvJwkNBR5KFjOCrZMeezQxjCtZ6Vdrft2fAS2TRkFqUKDVr371BoqkgkpLODam2/ATG2ZYW0Y4nVZ6qaEJJmM8pF1HJRbUhNns4Ck6ccoAxUq7khbN1N8TGRbGTETkOgW2I7Po5eJ/Xje18WWYMZmklkoyXxSnHFmF8u/RgGlKLJ84golm7lZERlhjYl1GeQiNxZeXSXBWv6r7d+e15nWRRhmO4BhOoQEX0IRbaEEABAQ8wyu8edp78d69j3lryStmDuEPvM8f71aQBw==</latexit>
G

softmax

C <latexit sha1_base64="1J8cfU8sKBiQgZnJ7EfYnZqKcOY=">AAAB53icbVBNS8NAEJ3Ur1q/qh69LBbBU0lEUG/FXjy2YGyhDWWznbRrN5uwuxFK6S/w4kHFq3/Jm//GbZuDtj4YeLw3w8y8MBVcG9f9dgpr6xubW8Xt0s7u3v5B+fDoQSeZYuizRCSqHVKNgkv0DTcC26lCGocCW+GoPvNbT6g0T+S9GacYxHQgecQZNVZq1nvlilt15yCrxMtJBXI0euWvbj9hWYzSMEG17nhuaoIJVYYzgdNSN9OYUjaiA+xYKmmMOpjMD52SM6v0SZQoW9KQufp7YkJjrcdxaDtjaoZ62ZuJ/3mdzETXwYTLNDMo2WJRlAliEjL7mvS5QmbE2BLKFLe3EjakijJjsynZELzll1eJf1G9qbrNy0rtNk+jCCdwCufgwRXU4A4a4AMDhGd4hTfn0Xlx3p2PRWvByWeO4Q+czx8ELoya</latexit>

gumbel softmax sample

z^1,1 <latexit sha1_base64="GLgL7pPNKZwDwzcxC9OPG6wvCpg=">AAAEdnicfZNbi9NAFMdnt1XXetvVRx8MlqJIKYkI6oOwqAuCFVfY2kITymQyaYedS8xMehvyOXzVj+VH8c2ZNkJ2Wj0w4Z/zO5NzmUycUSKV7/86OGw0r12/cXSzdev2nbv3jk/uf5WiyBEeIEFFPoqhxJRwPFBEUTzKcgxZTPEwvnxn+XCOc0kEv1CrDEcMTjlJCYLKuKJwBpVelxMddINyctz2e/7GvF0RVKINKjufnDQuwkSggmGuEIVSjgM/U5GGuSKI4rIVFhJnEF3CKR4bySHDMtKbqkuvYzyJl4rcLK68jbe+Q0Mm5YrFJpJBNZMus859bFyo9FWkCc8KhTnaJkoL6inh2RF4CckxUnRlBEQ5MbV6aAZziJQZ1L4s3aqSbsyu9rSpmXB7TiJVIhGya6afYZ4gmNkBdxVeKknW+I0ifBVpG8OFwrJsdbx6JptGCUF3gCmcZkaYxBwvkGAM8kSHZ6UO7Z441melw9D8L0w1cuGyBpcuHKEKIkj1yKX9Ou27dF378NqFqxpcuXBeg3MXDmtw6MJFDS5cGNdgbOF7bH7SHH8yrs8ZzqESuQ4/9ktt1l76TIfTgsWYSnO6pTb6n3HbCPtkcOk2/5/esxmplWlfS3sPA/fW7YrB897rnv/lRfv0bXUhj8BD8Bg8BQF4CU7BB3AOBgCBb+A7+AF+Nn43HzU7zSfb0MODas8DcMWa/h8GyZNa</latexit>
z^1,2 <latexit sha1_base64="5kbhOgWOmpM2CRSlpiHftDzgTj4=">AAAEdnicfZPvi9MwGMdzt6nn/HF3+tIXFsdQZBzdIagvhEM9EJx4ws0N1jLSNN3CpUlN0m1d6N/hW/2z/FN8Z7JV6GXTB1K+fT5P+vxIE2WUSOX7v/b2G80bN28d3G7duXvv/uHR8YOvkucC4QHilItRBCWmhOGBIoriUSYwTCOKh9HVO8uHcywk4exSFRkOUzhlJCEIKuMKgxlUelVOdK97Wk6O2v6JvzZvW/Qq0QaVXUyOG5dBzFGeYqYQhVKOe36mQg2FIojishXkEmcQXcEpHhvJYIplqNdVl17HeGIv4cIspry1t75Dw1TKIo1MZArVTLrMOnexca6SV6EmLMsVZmiTKMmpp7hnR+DFRGCkaGEERIKYWj00gwIiZQa1K0u3qqQbpdd7WtdMmD0nnigec9k1088wixHM7IC7Ci+VJCv8RhFWhNrGMK6wLFsdr57JplGc0y1gCqeZESYxwwvE0xSyWAfnpQ7snijS56XD0PwvTDRy4bIGly4coQoiSPXIpf067bt0VfvwyoVFDRYunNfg3IXDGhy6cFGDCxdGNRhZ+B6bn1TgT8b1OcMCKi508LFfarN20uc6mOZphKk0p1tqo/8Zt4mwzxQu3eb/03s2I7Uy7Wtp72HPvXXbYnB68vrE//Kiffa2upAH4BF4Ap6BHngJzsAHcAEGAIFv4Dv4AX42fjcfNzvNp5vQ/b1qz0NwzZr+HwrQk1s=</latexit>
bin values
v <latexit sha1_base64="eMDXxDKaP/QkD0ooEIy5/WsXBlM=">AAAEbHicfZPritNAFMdnt1XXettV/LQIwWVFpJRUBPWDsKgLghVX3NpCU5bJ9KQddi4hM0mbDnkEv+qz+RS+gpM2QnZaPTDhn/M7k3OZTBgzqrTv/9rZbTSvXb+xd7N16/adu/f2D+5/UzJNCPSJZDIZhlgBowL6mmoGwzgBzEMGg/DyXckHGSSKSnGu8xjGHE8FjSjB2rq+Bll2sX/kd/yVeZuiW4kjVNnZxUHjPJhIknIQmjCs1Kjrx3pscKIpYVC0glRBjMklnsLISoE5qLFZ1Vp4x9Yz8SKZ2CW0t/LWdxjMlcp5aCM51jPlstK5jY1SHb0aGyriVIMg60RRyjwtvbJxb0ITIJrlVmCSUFurR2Y4wUTb8WzL0q4qaYf8ak+rmqkoT0dGWk6katuZxyAmBMflWNsaFlrRJbzRVORjU8YIqUEVrWOvnqlMo6VkG8AWzmIrbGIBcyI5x2JigtPCBOWeMDSnhcNI9hdGhrhwUYMLFw5JBQlmZujSXp32XLqsfXjpwrwGcxdmNZi5cFCDAxfOa3DuwrAGwxK+B/uTJvDJuj7HkGAtExN87BXGrq30mQmmKQ+BKXu6hbH6n3HriPLJ8cJt/j+9xzNaK7N8LQp7D7vurdsU/eed1x3/y4ujk7fVhdxDh+gxeoq66CU6QR/QGeojgqboO/qBfjZ+Nx82D5uP1qG7O9WeB+iKNZ/8Aablj04=</latexit>

 <latexit sha1_base64="nXnFsRVguTsKPTVB6lY96S+Dv54=">AAAB7XicbVBNS8NAEJ3Ur1q/qh69LBbBU0lEUG9FLx4rGltoQ9lsN+3S3U3Y3Qgl9Ed48aDi1f/jzX/jJs1BWx8MPN6bYWZemHCmjet+O5WV1bX1jepmbWt7Z3evvn/wqONUEeqTmMeqG2JNOZPUN8xw2k0UxSLktBNObnK/80SVZrF8MNOEBgKPJIsYwcZKnf49GwlcG9QbbtMtgJaJV5IGlGgP6l/9YUxSQaUhHGvd89zEBBlWhhFOZ7V+qmmCyQSPaM9SiQXVQVacO0MnVhmiKFa2pEGF+nsiw0LrqQhtp8BmrBe9XPzP66UmugwyJpPUUEnmi6KUIxOj/Hc0ZIoSw6eWYKKYvRWRMVaYGJtQHoK3+PIy8c+aV0337rzRui7TqMIRHMMpeHABLbiFNvhAYALP8ApvTuK8OO/Ox7y14pQzh/AHzucPDfGO6g==</latexit>
weighted sum

z^1,k <latexit sha1_base64="0AOktNtPs8psyJbGE8pVa7tzvB8=">AAAEdnicfZPvi9MwGMdzt6nn/HWnL31hcQxFxtGJoL4QDvVAcOIJNzdYy0jTdAtLk5qk27rQv8O3+mf5p/jOZKvQy6YPpHz7fJ70+ZEmyiiRyvd/HRw2mteu3zi62bp1+87de8cn979KnguEB4hTLkYRlJgShgeKKIpHmcAwjSgeRvN3lg8XWEjC2aUqMhymcMpIQhBUxhUGM6j0upzoXndeTo7b/qm/MW9X9CrRBpVdTE4al0HMUZ5iphCFUo57fqZCDYUiiOKyFeQSZxDN4RSPjWQwxTLUm6pLr2M8sZdwYRZT3sZb36FhKmWRRiYyhWomXWad+9g4V8mrUBOW5QoztE2U5NRT3LMj8GIiMFK0MAIiQUytHppBAZEyg9qXpVtV0o3Sqz1taibMnhNPFI+57JrpZ5jFCGZ2wF2FV0qSNX6jCCtCbWMYV1iWrY5Xz2TTKM7pDjCF08wIk5jhJeJpClmsg/NSB3ZPFOnz0mFo8RcmGrlwVYMrF45QBRGkeuTSfp32XbqufXjtwqIGCxcuanDhwmENDl24rMGlC6MajCx8j81PKvAn4/qcYQEVFzr42C+1WXvpMx1M8zTCVJrTLbXR/4zbRthnCldu8//pPZuRWpn2tbT3sOfeul0xeH76+tT/8qJ99ra6kEfgIXgMnoIeeAnOwAdwAQYAgW/gO/gBfjZ+Nx81O80n29DDg2rPA3DFmv4f8F+TlA==</latexit>

sampled activation

z2 <latexit sha1_base64="140MI4KVtB7/uRpqHUXODrPUTWQ=">AAAEbnicfZNbi9NAFMdnt1XXetvVBx9EDJaFRUpJF0F9EBZ1QbDiCltbaEqZTE7aYecSMpPehnwGX/Wj+TH8Bk7aCNlp9cCEf87vTM5lMmHCqNK+/2tvv1a/cfPWwe3Gnbv37j84PHr4TcksJdAjksl0EGIFjAroaaoZDJIUMA8Z9MOr9wXvzyBVVIpLvUxgxPFE0JgSrK2rF6xm49PxYdNv+2vztkWnFE1U2sX4qHYZRJJkHIQmDCs17PiJHhmcakoY5I0gU5BgcoUnMLRSYA5qZNbV5t6x9UReLFO7hPbW3uoOg7lSSx7aSI71VLmscO5iw0zHr0eGiiTTIMgmUZwxT0uvaN2LaApEs6UVmKTU1uqRKU4x0XZAu7K0ykpaIb/e07pmKorzkbGWkVQtO/UERERwUgy2pWGhFV3BW03FcmSKGCE1qLxx7FUzFWm0lGwL2MJZYoVNLGBOJOdYRCY4z01Q7AlDc547jMz+wtgQFy4qcOHCASkhwcwMXNqt0q5LV5UPr1y4rMClC2cVOHNhvwL7LpxX4NyFYQWGBfwA9idN4bN1fUkgxVqmJvjUzY1dO+kLE0wyHgJT9nRzY/U/4zYRxZPjhdv8f3pPprRSZvGa5/Yedtxbty16p+03bf/ry+bZu/JCHqAn6Dk6QR30Cp2hj+gC9RBBFH1HP9DP2u/64/rT+rNN6P5euecRumb1kz9QYY/3</latexit>

y <latexit sha1_base64="4cGYRFkWcBo9hzyw+N7dsLcD1Cg=">AAAEbHicfZPritNAFMdnt1XXettV/LQIwWVFpJRUBPWDsKgLghVX3NpCU5bJ5KQddi4hM2mbDnkEv+qz+RS+gpM2QnZaPTDhn/M7k3OZTJgwqrTv/9rZbTSvXb+xd7N16/adu/f2D+5/UzJLCfSJZDIdhlgBowL6mmoGwyQFzEMGg/DyXckHM0gVleJc5wmMOZ4IGlOCtXV9DfLZxf6R3/FX5m2KbiWOUGVnFweN8yCSJOMgNGFYqVHXT/TY4FRTwqBoBZmCBJNLPIGRlQJzUGOzqrXwjq0n8mKZ2iW0t/LWdxjMlcp5aCM51lPlstK5jY0yHb8aGyqSTIMg60RxxjwtvbJxL6IpEM1yKzBJqa3VI1OcYqLteLZlaVeVtEN+tadVzVSUpyNjLSOp2nbmCYiI4KQca1vDQiu6hDeainxsyhghNaiidezVM5VptJRsA9jCWWKFTSxgTiTnWEQmOC1MUO4JQ3NaOIzM/sLYEBcuanDhwiGpIMHMDF3aq9OeS5e1Dy9dmNdg7sJZDc5cOKjBgQvnNTh3YViDYQnfg/1JU/hkXZ8TSLGWqQk+9gpj11b6zASTjIfAlD3dwlj9z7h1RPnkeOE2/5/ekymtlVm+FoW9h1331m2K/vPO647/5cXRydvqQu6hQ/QYPUVd9BKdoA/oDPURQRP0Hf1APxu/mw+bh81H69DdnWrPA3TFmk/+ALL6j1E=</latexit>

S
softmax

z^1,K <latexit sha1_base64="6mL7EdrwZuFReTSso6dYvuPm8VE=">AAAEdnicfZPvi9MwGMdzt6nn/HF3+tIXFsdQZBydCOoL4VAPhJt4ws0N1jLSNN3CpUlN0m1d6N/hW/2z/FN8Z7JV6GXTB1K+fT5P+vxIE2WUSOX7v/b2G80bN28d3G7duXvv/uHR8YOvkucC4QHilItRBCWmhOGBIoriUSYwTCOKh9HVe8uHcywk4exSFRkOUzhlJCEIKuMKgxlUelVOdK97Xk6O2v6JvzZvW/Qq0QaVXUyOG5dBzFGeYqYQhVKOe36mQg2FIojishXkEmcQXcEpHhvJYIplqNdVl17HeGIv4cIspry1t75Dw1TKIo1MZArVTLrMOnexca6S16EmLMsVZmiTKMmpp7hnR+DFRGCkaGEERIKYWj00gwIiZQa1K0u3qqQbpdd7WtdMmD0nnigec9k1088wixHM7IC7Ci+VJCv8VhFWhNrGMK6wLFsdr57JplGc0y1gCqeZESYxwwvE0xSyWAdnpQ7snijSZ6XD0PwvTDRy4bIGly4coQoiSPXIpf067bt0VfvwyoVFDRYunNfg3IXDGhy6cFGDCxdGNRhZ+AGbn1TgT8b1OcMCKi50cN4vtVk76XMdTPM0wlSa0y210f+M20TYZwqXbvP/6T2bkVqZ9rW097Dn3rptMXhx8ubE//KyffquupAH4BF4Ap6BHngFTsFHcAEGAIFv4Dv4AX42fjcfNzvNp5vQ/b1qz0NwzZr+H29/k3Q=</latexit>

SQUAD-factorized
z1 <latexit sha1_base64="mep7itZ0m4hEPojuKMEVkDGOogU=">AAAEbnicfZPdihMxFMez26pr/dhdvfBCxMGysEgpUxHUC2FRFwQrrrC1hU4pmcxpGzaTDEmm7TTMM3irj+Zj+AZm2hFm0+qBDP85v5M5H5mECaNK+/6vvf1a/cbNWwe3G3fu3rt/eHT84JsSqSTQI4IJOQixAkY59DTVDAaJBByHDPrh1fuC9+cgFRX8UmcJjGI85XRCCdbW1QtW83FnfNT02/7avG3RKUUTlXYxPq5dBpEgaQxcE4aVGnb8RI8MlpoSBnkjSBUkmFzhKQyt5DgGNTLranPvxHoibyKkXVx7a291h8GxUlkc2sgY65lyWeHcxYapnrweGcqTVAMnm0STlHlaeEXrXkQlEM0yKzCR1NbqkRmWmGg7oF1ZWmUlrTC+3tO6ZsqL8xETLSKhWnbqCfCI4KQYbEvDUiu6grea8mxkihguNKi8ceJVMxVptBBsC9jCWWKFTcxhQUQcYx6Z4Dw3QbEnDM157jAy/wsnhrhwWYFLFw5ICQlmZuDSbpV2XbqqfHjlwqwCMxfOK3Duwn4F9l24qMCFC8MKDAv4AexPKuGzdX1JQGItpAk+dXNj10763ATTNA6BKXu6ubH6n3GbiOIZ46Xb/H96T2a0Umbxmuf2HnbcW7ctei/ab9r+15fNs3flhTxAj9EzdIo66BU6Qx/RBeohgij6jn6gn7Xf9Uf1J/Wnm9D9vXLPQ3TN6qd/AExbj/Y=</latexit>

factori -sation

S G
 <latexit sha1_base64="nXnFsRVguTsKPTVB6lY96S+Dv54=">AAAB7XicbVBNS8NAEJ3Ur1q/qh69LBbBU0lEUG9FLx4rGltoQ9lsN+3S3U3Y3Qgl9Ed48aDi1f/jzX/jJs1BWx8MPN6bYWZemHCmjet+O5WV1bX1jepmbWt7Z3evvn/wqONUEeqTmMeqG2JNOZPUN8xw2k0UxSLktBNObnK/80SVZrF8MNOEBgKPJIsYwcZKnf49GwlcG9QbbtMtgJaJV5IGlGgP6l/9YUxSQaUhHGvd89zEBBlWhhFOZ7V+qmmCyQSPaM9SiQXVQVacO0MnVhmiKFa2pEGF+nsiw0LrqQhtp8BmrBe9XPzP66UmugwyJpPUUEnmi6KUIxOj/Hc0ZIoSw6eWYKKYvRWRMVaYGJtQHoK3+PIy8c+aV0337rzRui7TqMIRHMMpeHABLbiFNvhAYALP8ApvTuK8OO/Ox7y14pQzh/AHzucPDfGO6g==</latexit>

z^1,k <latexit sha1_base64="0AOktNtPs8psyJbGE8pVa7tzvB8=">AAAEdnicfZPvi9MwGMdzt6nn/HWnL31hcQxFxtGJoL4QDvVAcOIJNzdYy0jTdAtLk5qk27rQv8O3+mf5p/jOZKvQy6YPpHz7fJ70+ZEmyiiRyvd/HRw2mteu3zi62bp1+87de8cn979KnguEB4hTLkYRlJgShgeKKIpHmcAwjSgeRvN3lg8XWEjC2aUqMhymcMpIQhBUxhUGM6j0upzoXndeTo7b/qm/MW9X9CrRBpVdTE4al0HMUZ5iphCFUo57fqZCDYUiiOKyFeQSZxDN4RSPjWQwxTLUm6pLr2M8sZdwYRZT3sZb36FhKmWRRiYyhWomXWad+9g4V8mrUBOW5QoztE2U5NRT3LMj8GIiMFK0MAIiQUytHppBAZEyg9qXpVtV0o3Sqz1taibMnhNPFI+57JrpZ5jFCGZ2wF2FV0qSNX6jCCtCbWMYV1iWrY5Xz2TTKM7pDjCF08wIk5jhJeJpClmsg/NSB3ZPFOnz0mFo8RcmGrlwVYMrF45QBRGkeuTSfp32XbqufXjtwqIGCxcuanDhwmENDl24rMGlC6MajCx8j81PKvAn4/qcYQEVFzr42C+1WXvpMx1M8zTCVJrTLbXR/4zbRthnCldu8//pPZuRWpn2tbT3sOfeul0xeH76+tT/8qJ99ra6kEfgIXgMnoIeeAnOwAdwAQYAgW/gO/gBfjZ+Nx81O80n29DDg2rPA3DFmv4f8F+TlA==</latexit>

Figure 4: The left diagram visualizes the computational graph of SQUAD at training time, providing a detailed view on how an individual latent variable is sampled. The right diagram visualizes how the proposed matrix-factorization variant improves the parameter efficiency of the model.

follows, starting with the single-data-point loss in equation 1:

L =Ep(z|x)[log q(y|z)] - DKL[p(z|x) r(z)].

(4)

We re-parametrize z as a set of bin-indexing variables sk  {1, . . . , C}:

p(sk = c|x) p(zk = vc|x).

(5)

We model q(y|z) with a softmax layer f(z) = softmax(Wz + b) (abusing notation to indicate element-wise vector indexing with v[s]):

L = Esp[log f (v[s])] - DKL[. . .].

To enable sampling from the discrete variables s, we use the Gumbel-Max trick (Gumbel (1954)), denoted gumb(), re-parameterizing the expectation Esp with uniform noise  U (0, 1):

= E log f v arg max gumb(p(s|x), ) - DKL[. . .].
c

(6)

As the argmax is not differentiable, we approximate this expectation using the Gumbel-Softmax
trick (Maddison et al. (2016); Jang et al. (2016)), which generates samples that smoothly deform into one-hot samples as the softmax temperature  approaches 0:

E

log f

v · softmax gumb(p(s|x), )
c

- DKL[. . .].

(7)

In practice, we anneal  from 1.0 to 0.5 during the training process, as proposed by Yang et al. (2017) to reduce gradient variance initially, at the risk of introducing bias.

For the remainder of this work, we will refer to the latent variables as z in lieu of s, for clarity. To conclude our derivation, we use a fixed SQUAD distribution to model the marginal p(z) := r(z) as shown in figure 5. We can then derive the KL term analytically following the definition for discrete
distributions. Using the fact that the KL divergence is additive for independent variables:

L =E

log f

v · softmax gumb(p(s|x), )
c

-



K k=1

C c=1

p(zk

=

vc|x)

log

p(zk = vc|x) p(zk = vc)

(8)

The derived lower bound is fully differentiable, allowing it to be optimized with common SGDbased methods.

3

Under review as a conference paper at ICLR 2019

At test time, we can approximate the predictive function p(y|x) for a new data-point x by taking T samples from the latent variables z i.e. z^t  p(z|x), and averaging the predictionss for y:

p(y|x) 

q (y  |z)p (z|x )dz



1 T

T

q (y  |z^t ).

t=1

(9)

We improve the flexibility of the proposed model by creating a hierarchical set of latent variables. The joint distribution of L layers of latents is then:

p(z1, . . . , zL|x) = p(zL|zL-1) · · · p(z1|x),

(10)

and is straightforwardly implemented with a simple ancestral sampling scheme. Finally, we have q(y|z1, . . . , zL) = q(y|zL).
Interestingly, the strong quantization proposed in our method can itself be considered an additional information bottleneck, as it exactly upper-bounds the number of bits per latent variable. Such bottlenecks are theorized to have a beneficial effect on generalization (Tishby et al. (2000); Achille & Soatto (2016); Alemi et al. (2017; 2016)), and we can directly control this bottleneck by varying the number of quantization bins.

The computational complexity of the method, as well as the number of model parameters , scale linearly in C, i.e. O(C) (with C the number of quantization bins). It is thus suitable for large-scale inference. We would like to stress that the proposed method differs from work that leverages the Gumbel-Softmax trick to model categorical latent variables: our proposal models continuous scalar latent variables by quantizing their domain and modeling belief with a multinomial distribution. Categorical latent variable models would incur a much larger polynomial complexity penalty of O(C 2 ).

The method is easily integrated into existing deep neural network architectures, and SQUAD layer implementations are provided at github.com/anonymized.

Matrix-factorization variant To improving the tractability of using a large number of quantization bins, we propose a variant of SQUAD that uses a matrix factorization scheme to improve the parameter efficiency. Formally, equation equation 3 becomes:
p(zk = vc|x) = softmax(wk,c(wkx + bk) + bk,c),
with full layer weights W and W respectively of shape (K, B, C) and (|X|, K, B), where K denotes the number of neurons, B the number of factorization, C number of quantization bins and |X| the input dimensionality. The right side of figure 4 provides visual clarification of this variant. To improve the parameter efficiency, we can learn W per layer as well, resulting in shape (|X|, 1, B), which is found to be beneficial for large C in the hyper-parameter search presented in section 5. We depict this alternative model on the right side of figure 4 and will refer to it as SQUAD-factorized. We leave further extensions such as Network-in-Network(Lin et al. (2013)) for future work.

4 RELATED WORK
Outside the realm of DLVMs, other methods have been explored for predictive uncertainty. Lakshminarayanan et al. (2017) propose deep ensembles: straightforward averaging of predictions from a small set of separately adversarially trained DNNs. Although highly scalable, this method requires retraining a model up to 10 times, which can be inhibitively expensive for large datasets.
Gal & Ghahramani (2015b) propose the use of dropout (Srivastava et al. (2014)) at test time and present a Bayesian neural network interpretation of this method. A follow-up work by Gal et al. (2017) explores the use of Gumbel-Softmax to smoothly deform the dropout noise to allow optimization of the dropout rate during training. A downside of MC-dropout is the limited flexibility of the fixed bimodal delta-peak distribution imposed on the weights, which requires a large number of samples for good estimates of uncertainty. van den Oord et al. (2017) propose the use of vector quantization in variational inference, quantizing a multi-dimensional embedding, rather than individual latent variables, and explore this in the context of auto-encoders.

4

Under review as a conference paper at ICLR 2019

r(z) r(z) r(z)

z z z-1.6 -1.2 -.8 -.4 0 .4 .8 1.2 1.6

-1.6 -1.2 -.8 -.4 0 .4 .8 1.2 1.6

-1.4

-.8 -.5 0 .5 .8

1.4

Figure 5: In the IB bound, the marginal p(z) is approximated with a fixed distribution r(z). Using our proposed SQUAD distribution we can we can impose a variety of interesting forms for r(z) via the spacing v and weighting r(z = vc) of the quantization bins. For the values v, we compare linearly spaced bins (left) versus bins with equal probability mass under a normal distribution (right)
. Furthermore, we explore the effect of allowing the bin values to be optimized with SGD on a per
neuron or per layer basis, to allow the model to optimize the quantization scheme with the highest
fidelity. For the prior probabilities, we explore a uniform prior (left) and probability mass of the bins
under a normal distribution (middle).

Figure 6: Risk/coverage curve (with log-axes for discernibility) of 2-layer models on notMNIST. Lines closer to the lower-right are better. As the selective classifier lowers the confidence threshold, coverage increase at the cost of greater classification risk. Area around curves represents 90% confidence bounds computed using 10 initializations/splits.
In the space of learning non-linearities, Su et al. (2017) explore a flexible non-linearity that can assume the form of most canonical activations. More flexible distributions have been explored for distributional reinforcement learning by Dabney et al. (2017) using quantile regression, of which can be seen as a special case of SQUAD where the bin values are learned but have fixed uniform probability. Categorical distributions on scalar variables have been used to model more flexible Bayesian neural network posteriors as by Shayer et al. (2017). The use of a mixture of diracs distribution to approximate a variety of distributions was proposed by Schrempf et al. (2006).
5 RESULTS
Quantifying the quality of uncertainty estimates of models remains an open problem. Various methods have been explored in previous works, such as relative entropy Louizos & Welling (2017); Gal & Ghahramani (2015a), probability calibration, and proper scoring rules Lakshminarayanan et al. (2017). Although interesting in their own right, these metrics do not directly measure a good ranking of predictions, nor indicate applicability in high-risk domains. Proper scoring rules are the exception, but a model with good ranking ability does not necessarily exhibit good performance on proper scoring rules: any score that provides relative ordering suffices and does not have to reflect true calibrated probabilities. In fact, well-ranked confidence scores can be re-calibrated (Niculescu-Mizil & Caruana (2005)) after training to improve performance on proper scoring rules and calibration metrics.
In order to evaluate the applicability of the model in high-risk fields such as medicine, we want to quantify how models perform under a desired risk requirement. We propose to use the selection
5

Under review as a conference paper at ICLR 2019

Table 1: SGR coverage results on Fashion MNIST. We present coverage percentage of the test dataset for three pre-determined risk-guarantees (one per column), where higher coverage is better. The results indicate that SQUAD provides competitive uncertainty, especially at low-risk guarantees. Bayesian approximations via deep ensembles improve coverage all over the board, and for SQUAD in particular. 95% confidence bounds shown in parentheses, optimal results in bold. The 0% coverage for the Gaussian model is a result from too many incorrect predictions having high confidence meaning the 0.995 risk level can not be guaranteed.

Fashion MNIST
Plain MLP Maxout MCdropout Gaussian SQUAD
Deep Ensemble
Plain MLP Ensemble Maxout MCdropout Ensemble Gaussian Ensemble SQUAD Ensemble

cov@risk .5%
29.1 (08.3, 49.8) 41.9 (32.0, 51.7) 00.0 (00.0, 00.0) 42.9 (35.7, 50.1)
cov@risk .5%
58.3 (-, -) 59.1 (-, -) 34.3 (-, -) 61.6 (-, -)

cov@risk 1%
45.9 (41.9, 49.9) 56.5 (54.2, 58.8) 33.5 (31.1, 35.9) 58.3 (55.3, 61.4)
cov@risk 1%
70.2 (-, -) 72.2 (-, -) 47.8 (-, -) 73.1 (-, -)

cov@risk 2%
60.4 (57.2, 63.6) 69.9 (68.5, 71.4) 47.0 (45.6, 48.5) 69.5 (68.0, 71.1)
cov@risk 2%
84.6 (-, -) 86.7 (-, -) 68.4 (-, -) 86.1 (-, -)

with guaranteed risk (SGR) method1 introduced by Geifman & El-Yaniv (2017) to measure this. In summary, the SGR method defines a selective classifier using a trained neural network and can guarantee a user-selected desired risk with high probability (e.g. 99%), by selectively rejecting data points with a predicted confidence score below an optimal threshold.
To limit the influence of hyper-parameters on the comparison, we use the automated optimization method TPE (Bergstra et al. (2011)) for both baselines and our models. The hyper-parameters are optimized for coverage at 2% risk ( = 0.01) on fashionMNIST, and sub-sequentially evaluated on notMNIST. Larger models are evaluated on SVHN.
We compare2 our model against plain MLPs, MCDropout using Maxout activations3 (Goodfellow et al. (2013); Chang & Chen (2015)) and an information bottleneck model using mean-field Gaussian distributions. We evaluate the complementary deep ensembles technique (Lakshminarayanan et al. (2017)) for all methods.
5.1 MAIN RESULTS
We start our analysis by comparing the predictive uncertainty of 2-layer models with 32 latent variables per layer. In figure 6 we visualize the risk/coverage trade-off achieved using the predicted uncertainty as a selective threshold, and present coverage results in table 1. Overall, we find that SQUAD performs significantly better than plain MLPs and deep Gaussian IB models, and we tentatively attribute this to the increased flexibility of the multinomial distribution. Compared to a Maxout MCdropout model with a similar number of weights, SQUAD appears to have a slight --though not significant-- advantage, despite the strong quantization scheme, especially at low risk. Deep ensembles improve results for all methods, which fits the hypothesis that ensembles integrate over a form of weight uncertainty.
1We deviate slightly from Geifman & El-Yaniv (2017) in that we use Softmax Response (SR) ­ the probability taken from the softmax output for the most likely class ­ as the confidence score for all methods. Geifman & El-Yaniv (2017) instead proposed to use the variance of the probabilities for MCdropout, but our experiments showed that SR paints MCdropout in a more favorable light.
2All models are optimized with ADAM (Kingma & Ba (2014)), weight initialization as proposed by He et al. (2015), a weight decay of 10-5 and adaptive learning rate decay scheme -- 10x reduction after 10 epochs of no validation accuracy improvement-- and use early stopping after 20 epochs of no improvement.
3We found this baseline to perform stronger in comparison to conventional ReLU MCdropout models, under equal number of latent variables.
6

Under review as a conference paper at ICLR 2019

Table 2: SQUAD exhibits strong perfromance on a held-out dataset without hyperparameter tuning.

Heldout dataset (notMNIST)
Plain MLP Maxout MCdropout SQUAD
Plain MLP Ensemble Maxout MCdropout Ensemble SQUAD Ensemble

cov@risk .5%
77.4 (72.2, 82.6) 85.7 (84.5, 86.8) 87.1 (85.5, 88.7)
85.9 (-, -) 88.5 (-, -) 90.7 (-, -)

cov@risk 1%
85.5 (83.6, 87.5) 90.6 (90.0, 91.2) 91.1 (90.2, 92.0)
90.6 (-, -) 92.8 (-, -) 93.5 (-, -)

cov@risk 2%
90.3 (89.4, 91.2) 94.2 (93.9, 94.6) 94.5 (94.0, 95.0)
93.5 (-, -) 95.7 (-, -) 96.1 (-, -)

Table 3: Results on SVHN indicate that the quantization scheme imposed by SQUAD models might
hinder performance, but that this is effectively compensated by the SQUAD-factorized variant using a larger amount of bins. Even with T = 4 MC samples at test time, SQUAD performs well.

MLP K=256 (SVHN)
Plain MLP Maxout MCdropout T=100 SQUAD T=100 SQUAD-factorized T=100
Maxout MCdropout T=4 SQUAD T=4 SQUAD-factorized T=4

cov@risk .5%
00.0 (00.0, 00.0) 00.0 (00.0, 00.0) 01.7 (-05.1, 08.4) 0.23 (-0.094, 0.526)
00.0 (00.0, 00.0) 00.0 (00.0, 00.0) 03.9 (-11.7, 19.5)

cov@risk 1%
00.0 (00.0, 00.0) 50.7 (49.2, 52.1) 42.8 (40.6, 45.1) 53.2 (49.8, 56.6)
38.5 (34.7, 42.4) 38.0 (36.1, 39.9) 40.4 (35.7, 45.1)

cov@risk 2%
36.3 (33.5, 39.2) 65.0 (62.6, 67.3) 59.3 (57.9, 60.7) 66.5 (64.0, 68.9)
57.6 (55.0, 60.3) 55.6 (54.8, 56.5) 56.4 (53.2, 59.6)

5.2 NATURAL IMAGES
To explore larger models trained on natural image datasets, we lightly tune hyper-parameters on 256-latent 2-layer models over 100 TPE evaluations. As SVHN contains natural images in color, we anticipate a need for a higher amount of information per variable. We thus explore the effect of the matrix-factorized variant.
As shown in table 3, SQUAD-factorized outperforms the non-factorized variant. Considering the computational cost at the optimum of a 4-neuron factorization (B = 4) with C=37 quantization bins, the model clocks 3.4 million weights. In comparison, the optimum for the presented MCdropout results has C=11, using 9.0 million weights. On an NVIDIA Titan XP, the dropout baseline takes 13s per epoch on average, while SQUAD-factorized spans just 9s.
To evaluate the sample efficiency of the methods, we compare results at T = 4 samples. We find that SQUAD's results suffer less from undersampling than MCdropout. We tentatively attribute the sample efficiency to the flexible approximating posterior on the activations, which is in stark contrast to the rigid approximating distribution that MCdropout imposes on the weights. In conclusion, SQUAD comes out favorably in a resource-constrained environment.
5.3 ANALYSIS OF LATENT VARIABLE DISTRIBUTIONS
In order to evaluate if the proposed variational distribution does not simply collapse into single mode predictions, we want to find out what type of distributions the model predicts over the latent variables. We visualize the forms of predicted distributions in figure 7a. While we capture only a small subset of potential multi-modal behavior, this demonstrates that the model indeed utilizes the distribution to its full potential. To provide an intuition on how these predicted distributions emerge, we present a figure 9 in the appendix.
In figure 8 we visualize one of the non-linearities that the method learns for a 1-dimensional input SQUAD-factorized model. The learned non-linearities resemble "peaked" sigmoid activations, which can be interpreted as a combination of an RBF kernel and sigmoid. This provides food for thought on how non-linearities for conventional neural networks can be designed, and the effect of using such a nonlinearity will be studied in further work.
7

Under review as a conference paper at ICLR 2019
Figure 7: The model uses the flexible variational distribution p(z--x) to its full advantage. Figure (A) visualizes a subset of interesting stereotypical distributions. Figure (B) summarizes distributions predicted by the model similar to stereotypes. Figure (C) shows the frequency of distributions similar to stereotypes (lower KL is closer to stereotypes), demonstrating that these distributions occur frequently.
Figure 8: Predicted distribution over the quantization bins of one z as a function of the input, visualizing the stochastic non-linearity learned by the model, for a 1 dimensional input. The model exhibits peaky sigmoid activations. Left y-axis indicates probability for each bin, right y-axis indicates output value. The red line visualizes the expected value, while the blue line shows the value with maximum likelihood. The gray dots represent Gumbel-Softmax samples with  = 0.5.
8

Under review as a conference paper at ICLR 2019
6 DISCUSSION
As an interesting side effect, the method breaks the gradient flow normally present in deep neural networks. As the input correlates stronger with the learned weights, the output adjusts accordingly. With SQUAD, stronger correlation with learned weights perturbates the belief of values, rather than the values itself. As a benefit, this can potentially decrease the ­ arguably underdefined (Santurkar et al. (2018)) ­ problem of internal covariate shift, as the output domain is constrained. Furthermore, this allows the model to learn the non-linearity. As a downside, this breaks certain assumptions on which best-practice hyperparameter settings have been determined, such as weight initialization and activation functions (He et al. (2015)), and would require further study.
Conclusion In this work, we have proposed a new flexible class of variational distributions and achieve strong performance when applied to a deep variational information bottleneck model. By placing a quantization-based distribution on the activations, we can compute principled uncertainty estimates. We proposed an evaluation scheme motivated by the need in real-world domains to guarantee a minimal risk. The results presented indicate that SQUAD provides an improvement over plain neural networks, Gaussian IB and MCDropout in high-risk domains and that the flexible distribution is used to its full advantage.
REFERENCES
Alessandro Achille and Stefano Soatto. Information dropout: Learning optimal representations through noisy computation. November 2016.
Alexander A Alemi, Ian Fischer, Joshua V Dillon, and Kevin Murphy. Deep variational information bottleneck. December 2016.
Alexander A Alemi, Ben Poole, Ian Fischer, Joshua V Dillon, Rif A Saurous, and Kevin Murphy. An Information-Theoretic analysis of deep Latent-Variable models. November 2017.
James S Bergstra, Re´mi Bardenet, Yoshua Bengio, and Bala´zs Ke´gl. Algorithms for HyperParameter optimization. In J Shawe-Taylor, R S Zemel, P L Bartlett, F Pereira, and K Q Weinberger (eds.), Advances in Neural Information Processing Systems 24, pp. 2546­2554. Curran Associates, Inc., 2011.
Jia-Ren Chang and Yong-Sheng Chen. Batch-normalized maxout network in network. November 2015.
Will Dabney, Mark Rowland, Marc G Bellemare, and Re´mi Munos. Distributional reinforcement learning with quantile regression. October 2017.
Yarin Gal and Zoubin Ghahramani. Bayesian convolutional neural networks with bernoulli approximate variational inference. June 2015a.
Yarin Gal and Zoubin Ghahramani. Dropout as a bayesian approximation: Representing model uncertainty in deep learning. June 2015b.
Yarin Gal, Jiri Hron, and Alex Kendall. Concrete dropout. May 2017.
Yonatan Geifman and Ran El-Yaniv. Selective classification for deep neural networks. In I Guyon, U V Luxburg, S Bengio, H Wallach, R Fergus, S Vishwanathan, and R Garnett (eds.), Advances in Neural Information Processing Systems 30, pp. 4885­4894. Curran Associates, Inc., 2017.
Ian J Goodfellow, David Warde-Farley, Mehdi Mirza, Aaron Courville, and Yoshua Bengio. Maxout networks. February 2013.
E J Gumbel. Statistical theory of extreme values and some practical, applications, national bureau of standards, washington (1954). MR0061342 (15: 811b), 1954.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. In Proceedings of the IEEE international conference on computer vision, pp. 1026­1034. cv-foundation.org, 2015.
9

Under review as a conference paper at ICLR 2019
Irina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick, Shakir Mohamed, and Alexander Lerchner. beta-VAE: Learning basic visual concepts with a constrained variational framework. November 2016.
J L Holi and J N Hwang. Finite precision error analysis of neural network hardware implementations. IEEE Trans. Comput., 42(3):281­290, March 1993.
Itay Hubara, Matthieu Courbariaux, Daniel Soudry, Ran El-Yaniv, and Yoshua Bengio. Quantized neural networks: Training neural networks with low precision weights and activations. September 2016.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In International Conference on Machine Learning, pp. 448­456. jmlr.org, June 2015.
Eric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization with Gumbel-Softmax. November 2016.
Michael I Jordan, Zoubin Ghahramani, Tommi S Jaakkola, and Lawrence K Saul. An introduction to variational methods for graphical models. Mach. Learn., 37(2):183­233, November 1999.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. December 2014.
Diederik P Kingma and Max Welling. Auto-Encoding variational bayes. December 2013.
Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. Simple and scalable predictive uncertainty estimation using deep ensembles. In I Guyon, U V Luxburg, S Bengio, H Wallach, R Fergus, S Vishwanathan, and R Garnett (eds.), Advances in Neural Information Processing Systems 30, pp. 6405­6416. Curran Associates, Inc., 2017.
Min Lin, Qiang Chen, and Shuicheng Yan. Network in network. December 2013.
Christos Louizos and Max Welling. Multiplicative normalizing flows for variational bayesian neural networks. March 2017.
Chris J Maddison, Andriy Mnih, and Yee Whye Teh. The concrete distribution: A continuous relaxation of discrete random variables. November 2016.
Alexandru Niculescu-Mizil and Rich Caruana. Predicting good probabilities with supervised learning. In Proceedings of the 22Nd International Conference on Machine Learning, ICML '05, pp. 625­632, New York, NY, USA, 2005. ACM.
Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and approximate inference in deep generative models. January 2014.
Shibani Santurkar, Dimitris Tsipras, Andrew Ilyas, and Aleksander Madry. How does batch normalization help optimization? (no, it is not about internal covariate shift). May 2018.
O C Schrempf, D Brunn, and U D Hanebeck. Dirac mixture density approximation based on minimization of the weighted cramer-von mises distance. In 2006 IEEE International Conference on Multisensor Fusion and Integration for Intelligent Systems, pp. 512­517, 2006.
Oran Shayer, Dan Levi, and Ethan Fetaya. Learning discrete weights using the local reparameterization trick. October 2017.
Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: A simple way to prevent neural networks from overfitting. J. Mach. Learn. Res., 15: 1929­1958, 2014.
Qinliang Su, Xuejun Liao, and Lawrence Carin. A probabilistic framework for nonlinearities in stochastic neural networks. In I Guyon, U V Luxburg, S Bengio, H Wallach, R Fergus, S Vishwanathan, and R Garnett (eds.), Advances in Neural Information Processing Systems 30, pp. 4486­4495. Curran Associates, Inc., 2017.
10

Under review as a conference paper at ICLR 2019
Figure 9: This figure serves to provide intuition on how a variety of distributions come about in our model. We show the set of weights used to predict the probability for the C bins of a randomly selected latent variable zl=1,k=12 from the first layer in a converged 2-layer SQUAD model (reshaped to a 28x28 squares for comparison with the data). We then present 5 data-points for which the neuron predicts a stereotypical distribution, as visualized in the last bar-plot.
Naftali Tishby, Fernando C Pereira, and William Bialek. The information bottleneck method. April 2000.
Aaron van den Oord, Oriol Vinyals, and Koray Kavukcuoglu. Neural discrete representation learning. November 2017.
Dong Yang, Daguang Xu, S Kevin Zhou, Bogdan Georgescu, Mingqing Chen, Sasa Grbic, Dimitris Metaxas, and Dorin Comaniciu. Automatic liver segmentation using an adversarial Image-toImage network. July 2017.
7 APPENDIX
7.1 EFFECT OF HYPER-PARAMETERS ON COVERAGE: The optimal configuration of hyper-parameters and bin priors have been determined using 700 evaluations selected using TPE. The space of parameters explored is as follows, presented in the hyperopt API for transparency: # Shared C: quniform(2, 10, 1) * 2 + 1, dropout rate: uniform(0.01, .95), lr: loguniform(log(0.0001), log(0.01)), batch_size: qloguniform(log(32), log(512), 1) # SQUAD & Gaussian kl_multiplier: loguniform(log(1e-6), log(0.01)),
11

Under review as a conference paper at ICLR 2019
init_scale: loguniform(log(1e-3), log(20)), # SQUAD use_bin_probs: choice(['uni', 'gaus']), use_bins: choice(['equal_prob_gaus',
'linearly_spaced']), learn_bin_values: choice([
'per_neuron', 'per_layer', 'fixed']), In figure 10 we visualize the pairwise effect of these hyper-parameters on the coverage. The optimal configuration found in for the main SQUAD model are: batch size: 244, KL multiplier: 0.0027, learn bin values: per layer, p(z): uniform, v: linearly spread over (-3.5,3.5), lr: 0.0008, C: 15, initialization scale: 3.214.
Figure 10: This figure visualizes the pairwise relationship between hyper-parameters of SQUAD and the effect on coverage. The top-60 configurations are highlighted. Green values are good, red values are bad. We have filtered on the optimal settings for bin values and prior to reduce clutter.
12


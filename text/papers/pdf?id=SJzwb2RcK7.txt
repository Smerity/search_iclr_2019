Under review as a conference paper at ICLR 2019
ADVERSARIAL DECOMPOSITION OF TEXT REPRESEN-
TATION
Anonymous authors Paper under double-blind review
ABSTRACT
In this paper, we present a method for adversarial decomposition of text representation. This method can be used to decompose a representation of an input sentence into several independent vectors, where each vector is responsible for a specific aspect of the input sentence. We evaluate the proposed method on two case studies: the conversion between different social registers and diachronic language change. We show that the proposed method is capable of fine-grained controlled change of these aspects of the input sentence. For example, our model is capable of learning a continuous (rather than categorical) representation of the style of the sentence, in line with the reality of language use. The model uses adversarial-motivational training and includes a special motivational loss, which acts opposite to the discriminator and encourages a better decomposition. Finally, we evaluate the obtained meaning embeddings on a downstream task of paraphrase detection and show that they are significantly better than embeddings of a regular autoencoder.
1 INTRODUCTION
Despite the recent successes in using neural models for representation learning for natural language text, learning a meaningful representation of input sentences remains an open research problem. A variety of approaches, from sequence-to-sequence models that followed the work of Sutskever et al. (2014) to the more recent proposals (Arora et al., 2017; Nangia et al., 2017; Conneau et al., 2017; Logeswaran & Lee, 2018; Subramanian et al., 2018; Cer et al., 2018) share one common drawback. Namely, all of them encode the input sentence into just one single vector of a fixed size. One way to bypass the limitations of a single vector representation is to use an attention mechanism (Bahdanau et al., 2014; Vaswani et al., 2017). We propose to approach this problem differently and design a method for adversarial decomposition of the learned input representation into multiple components. Our method encodes the input sentence into several vectors, where each vector is responsible for a specific aspect of the sentence.
In terms of learning different separable components of input representation, our work most closely relates to the style transfer work, which has been applied to a variety of different aspects of language, from diachronic language differences (Xu et al., 2012) to authors' personalities (Lipton et al., 2015) and even sentiment (Hu et al., 2017; Fu et al., 2017). The style transfer work effectively relies on the more classical distinction between meaning and form (de Saussure, 1959), which accounts for the fact that multiple surface realizations are possible for the same meaning. For simplicity, we will use this terminology throughout the rest of the paper.
Consider the case when we encode an input sentence into a meaning vector and a form vector. We are then able to perform a controllable change of meaning or form by a simple change applied to these vectors. For example, we can encode two sentences written in two different styles, then swap the form vectors while leaving the meaning vectors intact. We can then generate new unique sentences with the original meaning, but written in a different style.
In the present work, we propose a novel model for this type of decomposition based on adversarialmotivational training and design an architecture inspired by the GANs (Goodfellow et al., 2014) and adversarial autoencoders (Makhzani et al., 2015). In addition to the adversarial loss, we use a special motivator (Albanie et al., 2017), which, in contrast to the discriminator, is used to provide a
1

Under review as a conference paper at ICLR 2019
motivational loss to encourage the model to better decomposition of the meaning and the form, as well as specific aspects of meaning. We make all the code publicly available on GitHub 1.
We evaluate the proposed methods for learning separate aspects of input representation on the following case studies:
1. Learning to separate out a representation of the specific diachronic slice of language. One may express the same meaning using the Early Modern English (e.g. What would she have?) and the contemporary English ( What does she want?)
2. Learning a representation for a social register (Halliday et al., 1968) ­ that is, subsets of language appropriate in a given context or characteristic of a certain group of speakers. These include formal and informal language, the language used in different genres (e.g., fiction vs. newspapers vs. academic texts), different dialects, and even literary idiostyles. We experiment with the registers corresponding to the titles of scientific papers vs. newspaper articles.
2 RELATED WORK
As mentioned above, the most relevant previous work comes from the style transfer research, and it can be divided into two groups:
1. Approaches that aim to generate text in a given form. For example, the task may be to produce just any verse as long as it is in the "style" of the target poet.
2. Approaches that aim to induce a change in either the "form" or the "meaning" of an existing utterance. For example, "Good bye, Mr. Anderson." can be transformed to "Fare you well, good Master Anderson" (Xu et al., 2012)).
An example of the first group is the work by Potash et al. (2015), who trained several separate networks on verses by different hip-hip artists. An LSTM network successfully generated verses that were stylistically similar to the verses of the target artist (as measured by cosine distance on TfIdf vectors). More complicated approaches use language models that are conditioned in some way. For example, Lipton et al. (2015) produced product reviews with a target rating by passing the rating as an additional input at each timestep of an LSTM model. Tang et al. (2016) generated reviews not only with a given rating but also for a specific product. At each timestep a special context vector was provided as input, gated so as to enable the model to decide how much attention to pay to that vector and the current hidden state. Li et al. (2016) used "speaker" vectors as an additional input to a conversational model, improving consistency of dialog responses. Finally, Ficler & Goldberg (2017) performed an extensive evaluation of conditioned language models based on "content" (theme and sentiment) and "style" (professional, personal, length, descriptiveness). Importantly, they showed that it is possible to control both "content" and "style" simultaneously.
Work from the second group can further be divided into two clusters by the nature of the training data: parallel aligned corpora, or non-aligned datasets. The aligned corpora enable approaching the problem of form shift as a paraphrasing or machine translation problem. Xu et al. (2012) used statistical and dictionary-based systems on a dataset of original plays by Shakespeare and their contemporary translations. Carlson et al. (2017) trained an LSTM network on 33 versions of the Bible. Jhamtani et al. (2017) used a Pointer Network (Vinyals et al., 2015), an architecture that was successfully applied to a wide variety of tasks (Merity et al., 2016; Gulcehre et al., 2016; Potash et al., 2017), to enable direct copying of the input tokens to the output. Note that these works use BLEU (Papineni et al., 2002) as the main, or even the only evaluation measure. This is only possible in cases where a parallel corpus is available.
Recently, new approaches that do not require a parallel corpora were developed in both CV (Zhu et al., 2017) and NLP. Hu et al. (2017) succeeded in changing tense and sentiment of sentences with a two steps procedure based on a variational auto-encoder (VAE) (Kingma & Welling, 2013). After training a VAE, a discriminator and a generator are trained in an alternate manner, where the discriminator tries to correctly classify the target sentence attributes. A special loss component
1http://github.com/placeholder
2

Under review as a conference paper at ICLR 2019
forces the hidden representation of the encoded sentence to not have any information about the target sentence attributes. Mueller et al. (2017) used a VAE to produce a hidden representation of a sentence, and then modify it to match the desired form. Unlike Hu et al. (2017), they do not separate the form and meaning embeddings. Shen et al. (2017) applied a GAN to align the hidden representation of sentences from two corpora and force them to do not have any information about the form via adversarial loss. During the decoding, similarly the work by Lipton et al. (2015), special "style" vectors are passed to the decoder at every timestep to produce a sentence with the desired properties. The model is trained using the Professor-Forcing algorithm (Lamb et al., 2016). Kim et al. (2017) worked directly on hidden space vectors that are constrained with the same adversarial loss instead of outputs of the generator, and use two different generators for two different "styles". Finally, Fu et al. (2017) proposed two models for generating sentences with the target properties using an adversarial loss, similarly to Shen et al. (2017) and Kim et al. (2017).
Comparison with previous work In contrast to the proposals of Xu et al. (2012), Carlson et al. (2017), Jhamtani et al. (2017), our solution does not require a parallel corpus. Furthermore, unlike the model by Shen et al. (2017), our model works directly on representation of sentences in the hidden space.
Most importantly, in contrast to the proposals by Mueller et al. (2017), Hu et al. (2017), Kim et al. (2017), Fu et al. (2017), our model produces a representation for both meaning and form and does not treat the form as a categorical (in the vast majority of works, binary) variable. Although the form was represented as dense vectors in previous work, it is still just a binary feature, as they use a single pre-defined vector for each form, with all sentences of the same form assigned the same form vector. In contrast, our work treats form as a truly continuous variable, where each sentence has its own, unique, form vector.
Treating meaning and form not as binary/categorical, but as continuous is more consistent with the reality of language use, since there are different degrees of overlap between the language used by different registers or in different diachronic slices. Indeed, language change is gradual, and the acceptability of expressions in a given register also forms a continuum, so one expects a substantial overlap between the grammar and vocabulary used, for example, on Twitter and by New York Times. To the best of our knowledge, this is the first model that considers linguistic form in the task of text generation as a continuous variable.
One significant consequence of learning a continuous representation for form is that it allows the model to work with a large, and potentially infinite, number of forms. Note that in this case the locations of areas of specific forms in the vector style space would reflect the similarity between these forms. For example, the proposed model could be directly applied to the authorship attribution problem. In this case, each author would have their own area in the form space, and the more similar the authors are in terms of writing style, the closer these areas would be to each other. We performed preliminary experiments on this and report the results in Appendix A.
3 FORMULATION
Let us formulate the problem of decomposition of text representation on an example of controlled change of linguistic form and conversion of Shakespeare plays in the original Early Modern to contemporary English. Let Xa be a corpus of texts xia  X a in Early Middle English f a  F , and Xb be a corpus of texts xib  X b in modern English f b  F . We assume that the texts in both Xa and Xb has the same distribution of meaning m  M. The form f , however, is different and generated from a mixture of two distributions:
f i = iap(f a) + ibp(f b) where f a and f b are two different languages (Early Modern and contemporary English). Intuitively, we say that a sample xi has the form f a if ia > ib, and it has the form f b if ib > ia. The goal of dissociation meaning and form is to learn two encoders Em : X  M and Ef : X  F for the meaning and form correspondingly, and the generator G : M, F  X such that
j  {a, b}, k  {a, b} : G(Em(xk), Ef (xj))  X j
3

Under review as a conference paper at ICLR 2019

That is, the form of a generated sample depends exclusively on the provided f j and can be the in the same domain for two different mu and mv from two samples from different domains X a and X b.
Note that, in contrast to the previously proposals, the form f is not a categorical variable but a continuous vector. This enables fine-grained controllable change of form: the original form f i is changed to reflect the form of the specific target sentence f j with its own unique a and b while preserving the original meaning mi.
An important caveat concerns the core assumption of the similar meaning distribution in the two corpora, which is also made in all other works reviewed in Section 2. It limits the possible use of this approach to cases where the distributions are in fact similar (i.e. parallel or at least comparable corpora are available). It does not apply to many cases that could be analyzed in terms of meaning and form. For example, books for children and scholarly papers are both registers, they have their own form (i.e. specific subsets of linguistic means and structure conventions) ­ but there is little overlap in the content. This would make it hard even for a professional writer to turn a research paper into a fairy tale.

4 METHOD DESCRIPTION

Inspired by Makhzani et al. (2015), Kim et al. (2017), and Albanie et al. (2017), we propose ADNet, a new model for adversarial decomposition of text representation (Figure 1).

Our solution is based on a widely used

Discriminator

Ladv

sequence-to-sequence framework (Sutskever et al., 2014) and consists of four main parts.

m

The encoder E encodes the inputs sequence x into two latent vectors m and f which capture

Encoder

Generator

the meaning and the form of the sentence correspondingly. The generator G then takes these

f two vectors as the input and produces a recon-

Motivator

struction of the original input sequence x^.

Lmotiv

The encoder and generator by themselves will

Figure 1: Overview of ADNet. Encoder encodes the inputs sentences into two latent vectors m and f . The Generator takes them as the input and produces the output sentence. During the training, the Discriminator is used for an adversarial loss that forces m to do not carry any information about the form, and the M otivator is used for a motivational loss that encourages f to carry

likely not achieve the dissociation of the meaning and form. We encourage this behavior in a way similar to Generative Adversarial Networks (GANs) (Goodfellow et al., 2014), which had an overwhelming success the past few years and have been proven to be a good way of enforcing a specific distribution and characteristics on the output of a model.

the needed information about the form.

Inspired by the work of Albanie et al. (2017)

and the principle of "carrot and stick" (Safire,

1995), in contrast to the majority of work that

promotes pure adversarial approach (Goodfel-

low et al., 2014; Shen et al., 2017; Fu et al., 2017; Zhu et al., 2017), we propose two additional

components, the discriminator D and the motivator M to force and motivate the model to learn the

dissociation of the meaning and the form. Similarly to a regular GAN model, the adversarial dis-

criminator D tries to classify the form f based on the latent meaning vector m, and the encoder E

is penalized to make this task as hard as possible.

Opposed to such vicious behaviour, the motivator M tries to classify the form based on the latent form vector f , as it should be done, and encourages the encoder E to make this task as simple as possible. We could apply the adversarial approach here as well and force the distribution of the form vectors to fit a mixture of Gaussians (in this particular case, a mixture of two Guassians) with another discriminator, as it is done by Makhzani et al. (2015), but we opted for the "dualistic" path of two complimentary forces.

4

Under review as a conference paper at ICLR 2019
4.1 ENCODER-DECODER
Both the encoder E and the generator G are modeled with a neural network. Gated Recurrent Unit (GRU) (Chung et al., 2014) is used for E to encode the input sentence x into a hidden vector h = GRU(x).
The vector h is then passed through two different fully connected layers to produce the latent vectors of the form and the meaning of the input sentence:
m = tanh(Wmh + bm) f = tanh(Wf h + bf )
We use E to denote the parameters of the encoder E: Wm, bm, Wf , bf , and the parameters of the GRU unit.
The generator G is also modelled with a GRU unit. The generator takes as input the meaning vector m and the form vector f , concatenates them, and passes trough a fully-connected layer to obtain a hidden vector z that represents both meaning and form of the original input sentence:
z = tanh(Wz[m; f ] + bm) After that, we use a GRU unit to generate the output sentence as a probability distribution over the vocabulary tokens:
T
p(x^) = p(x^t|z, x^1, . . . , x^t-1)
t=1
We use G to denote the parameters of the generator G: Wz, bm, and the parameters of the used GRU. The encoder and generator are trained using the standard reconstruction loss:
Lrec(E, G) = ExXa [- log p(x^|x)] + ExXb [- log p(x^|x)]
4.2 DISCRIMINATOR
The representation of the meaning m produced by the encoder E should not contain any information about the form f . We achieve this by using an adversarial approach. First, we train a discriminator D, consisting of several fully connected layers with ELU activation function (Clevert et al., 2015) between them, to predict the form f of a sentence by its meaning vector: f^D = D(m), where f^ is the score (logit) reflecting the probability of the sentence x to belong to one of the form domains.
Motivated by the Wasserstein GAN (Arjovsky et al., 2017), we use the following loss function instead of the standard cross-entropy:
LD(D) = ExXa [D(Em(x))] - ExXb [D(Em(x))]
Thus, a successful discriminator will produce negative scores f^ for sentences from Xa and positive scores for sentences from Xb. This discriminator is then used in an adversarial manner to provide a learning signal for the encoder and force dissociation of the meaning and form by maximizing LD : Ladv(E) = -advLD, where adv is a hyperparameter reflecting the strength of the adversarial loss. Note that this loss applies to the parameters of the encoder.
4.3 MOTIVATOR
Our experiments showed that it is enough to have just the discriminator D and the adversarial loss Ladv to force the model to dissociate the form and the meaning. However, in order to achieve a better dissociation, we propose to use a motivator M (Albanie et al., 2017) and the corresponding motivational loss. Conceptually, this is the opposite of the adversarial loss, hence the name. As the discriminator D, the motivator M learns to classify the form f of the input sentence. However, its input is not not the meaning vector but the form vector: f^M = M (f ).
The motivator has the same architecture as the discriminator, and the same loss function. While the adversarial loss forces the encoder E to produce a meaning vector m with no information about the form f , the motivational loss encourages E to encode this information in the form vector by minimizing LM : Lmotiv(E ) = motivLM .
5

Under review as a conference paper at ICLR 2019

4.4 TRAINING PROCEDURE
The overall training procedure follows the methods for training GANs (Goodfellow et al., 2014; Arjovsky et al., 2017) and consists of two stages: training the discriminator D and the motivator M , and training the encoder E and the generator G.
In contrast to Arjovsky et al. (2017), we do not train the D and M more than the E and the G. In our experiments we found that simple training in two stages is enough to achieve dissociation of the meaning and the form. Encoder and generator are trained with the following loss function that combines reconstruction loss with the losses from the discriminator and the motivator:
Ltotal(E , G) = Lrec + Ladv + Lmotiv

5 EXPERIMENTAL SETUP

5.1 EVALUATION
Similarly to the evaluation of style transfer in CV (Isola et al., 2017), evaluation of this task is difficult. We follow the approach of Isola et al. (2017); Shen et al. (2017) and recently proposed by Fu et al. (2017) methods of evaluation of "transfer strength" and "content preservation". The authors showed the proposed automatic metrics to a large degree correlate with human judgment and can serve as a proxy. Below we give an overview of these metrics.

Transfer Strength. The goal of this metric is to capture whether the form has been changed successfully. To do that, a classifier C is trained on the two corpora, Xa and Xb to recognize the linguistic "form" typical of each of them. After that, a sentence the form/meaning of which was changed is passed to the classifier. The overall accuracy reflects the degree of success of changing the form/meaning. This approach is widely used in CV (Isola et al., 2017), and was applied in NLP as well (Shen et al., 2017).
In our experiments we used a GRU unit followed by four fully-connected layers with ELU activation functions between them as the classifier.

Content preservation Note that transfer strength by itself does not capture the overall quality of a changed sentence. A extremely overfitted model that produces the same, the most characteristic sentence of one corpus all the time would have a high score according to this metric. Thus, we need to measure how much of the meaning was preserved while changing the form. To do that, Fu et al. (2017) proposed to use a cosine similarity based metric using pretrained word embeddings. First, a sentence embedding is computed by concatenation of max, mean, and average pooling over the timesteps:
v = [max(v1, . . . , vT ); min(v1, . . . , vT ); mean(v1, . . . , vT )]

Next, the cosine similarity score si between the embedding vis of the original source sentence and the target sentence with the changed form vit is computed, and the scores across the dataset are averaged to obtain the total score:

 |Xa|

|Xb| 

11

1

s = 2  |Xa|

si + |Xb|

si

i=1 i=1

5.1.1 CONTINUOUS FORM
The metrics described above treat the form as a categorical (in most cases, even binary) variable. This was not a problem in previous work since the change of form could be done by just inverting the form vector. Our work, in contrast, treats the form as a continuous variable, and, therefore, we cannot just use the proposed metrics directly. To enable a fair comparison, we propose the following procedure.
For each sentence sas in the test set from the corpus Xa we sample k = 10 random sentence from the corpus Xb of the opposite form. After that, we encode them into the meaning mi and form fi

6

Under review as a conference paper at ICLR 2019

vectors,

and

average

the form vectors to obtain a

single form vector favg

=

1 k

k i=1

fi.

We then

generate a new sentence with its original meaning vector ms and the resulting form vector favg, and

use it for evalation. This process enables a fair comparison with the previous works that treat form

as a binary variable.

5.2 DATASETS
We performed an extensive evaluation of the proposed method on several dataset that reflect different changes of meaning, form, or specific aspects of meaning, such as sentiment polarity.

Changing form: register This experiment is conducted with a dataset of titles of scientific papers and news articles published by Fu et al. (2017). This dataset (referred to as "Headlines") contains titles of scientific articles crawled from online digital libraries, such as "ACM Digital Library" and "arXiv". The titles of the news articles are taken from the "News Aggregator Data Set" from UCI Machine Learning Repository (Dheeru & Karra Taniskidou, 2017)

Changing form: language diachrony Diachronic language change is explored with the dataset composed by Xu et al. (2012). It includes the texts of 17 plays by William Shakespeare in the original Early Modern English, and their translations into contemporary English. We randomly permuted all sentences from all plays and sampled the training, validation, and test sets. Note that this is the smallest dataset in our experiments.
Previous work on style transfer for text also included the experiments with changing sentiment polarity (Shen et al., 2017; Fu et al., 2017). We do not report the experiments with sentiment data, since the change in sentiment polarity corresponds to a change in a specific aspect of meaning, rather than form. We therefore believe the comparison with these data would not be instructive.

6 RESULTS AND DISCUSSION

Content preservation Content preservation

0.950 2562,5664, 256 64, 256 0.925 256, 64 0.900 256, 64 0.875 0.850 0.825 0.800

Fu et al ADNet ADNet + Motivator
256, 256 256, 256 64, 256 64, 256

0.20 0.25 Tr0a.n3s0fer str0e.3n5gth 0.40 0.45

0.88 256, 225566, 64
0.86 256, 64 0.84 256, 64

Fu et al ADNet ADNet + Motivator
64, 256

0.82 642,5265, 6256

0.80

256, 256 64, 256

0.78

0.50 0.55 0.60 Tr0a.6n5sfer0s.7tr0eng0t.h75 0.80 0.85 0.90

(a) Shakespeare dataset

(b) Headlines dataset

Figure 2: Transfer strength vs Content preservation (see subsection 5.1) for different combination of the size of the meaning and form vectors. Each point is labeled with "¡meaning vector size¿, ¡form vector size¿".

Probably, the most recent and similar to our work is the model proposed by Fu et al. (2017), in particular the "style-embedding" model. We implemented this model to provide a baseline for comparison.
The classifier used in the transfer strength metric achieves very high accuracy (0.832 and 0.99 for the Shakespeare and Headlines datasets correspondingly). These results concur with the results of Shen et al. (2017) and Fu et al. (2017), and show that the two forms in the corpora are significantly different.
Following Fu et al. (2017), we show the result of different configuration of the size of the form and meaning vectors on Figure 2. Namely, we report combinations of 64 and 256-dimensional vectors. Note that the sizes of the form vector are important. The larger is the form vector, the higher is the

7

Under review as a conference paper at ICLR 2019

transfer strength, but smaller is content preservation. This is consistent with Fu et al. (2017), where they observed a similar behaviour.
It is clear that the proposed method achieves significantly better transfer strength then the previously proposed model. It also has a lower content preservation score, which means that it repeats fewer exact words from the source sentence. Note that a low transfer strength and very high (0~.9) content preservation score means that the model was not able to successfully learn to transfer the form and the target sentence is almost identical to the source sentence. The Shakespeare dataset is the hardest for the model in terms of transfer strength, probably because it is the smallest dataset, but the proposed method performs consistently well in transfer of both form and meaning and, in contrast to the baseline.

Fluency of generated sentences Note that there is no guarantee that the generated sentences would be coherent after switching the form vector. In order to estimate how this switch affects the fluency of generated sentences, we trained a language model on the Shakespeare dataset and calculated the perplexity of the generated sentences using the original form vector and the average of form vectors of k random sentences from the opposite style (see subsubsection 5.1.1). While the perplexity of such sentences does go up, this change is not big (6.89 vs 9.74).

6.1 IMPACT OF THE MOTIVATIONAL TRAINING
To investigate the impact of the motivator, we visualized form and meaning embeddings of 1000 random samples from the Headlines dataset using t-SNE algorithm (Van Der Maaten, 2014) with the Multicore-TSNE library (Ulyanov, 2016). The result is presented in Figure 3.
There are three important observations. First, there is no clear separation in the meaning embeddings, which means that any accurate form transfer is due to the form embeddings, and the dissociation of form and meaning was successful.
Second, even without the motivator the model is able to produce the form embeddings that are clustered into two group. Recall from section 4 that without the motivational loss there are no forces that influence the form embeddings, but nevertheless the model learns to separate them.
However, the separation effect is much more pronounced in the presence of motivator. This explains why the motivator consistently improved transfer strength of ADNet, as shown in Figure 2.

Meaning

Form
15

Meaning
8

30

10 10

6 20

55
00 5
5 10

4 2 10
00 2 4 10

10 15

6 20

15 10 5 0 5 10 15

20 10 0

8

10 20

7.5 5.0 2.5 0.0 2.5 5.0 7.5

Form

10 0

10

(a) Without motivator

(b) With motivator

20

Figure 3: t-SNE visualization of the form and meaning embeddings of 1000 random sentences. Green point represent sentences form news headlines, and red points represent titles of scientific articles.

6.2 QUALITATIVE EVALUATION
Table 1 and Table 2 show several examples of the successful form/meaning transfer achieved by ADNet. Table 1 presentes the results of an experiment that to some extent replicates the approach taken by the authors who treat linguistic form as a binary variable (Shen et al., 2017; Fu et al., 2017). The sentences the original Shakespeare plays were averaged to get the "typical" Early Modern English form vector. This averaged vector was used to decode a sentence from the modern English translation back into the original. The same was done in the opposite direction.
8

Under review as a conference paper at ICLR 2019

Aye, sir. (EME)

 Yes, sir. (CE)

Fare thee well, my lord (EME)

 Fare you well, my lord (CE)

This guy will tell us everything. (CE)

 This man will tell us everything. (EME)

I've done no more to caesar than you will do to me. (CE)  I have done no more to caesar than, you shall do to me. (EME)

Table 1: Decoding of the source sentence from Early Modern English (EME) into contemporary English (CE), and vice versa.

Table 2 illustrates the possibilities of ADNet on fine-grained transfer applied to the change of register. We encoded two sentences in different registers from the Headlines dataset to produce form and meaning embeddings, and then we decoded the first sentence with the meaning embedding of the second, and vice versa. As can be seen from Table 2, the model correctly captures the meaning of sentences and decodes them using the form of the source sentences. Note how the model preserves specific words and the structure of the source sentence. In particular, note how in the first example, the model decided to put the colon after the "crisis management", as the source form sentence has this syntactic structure ("A review:"). This is not possible in the previously proposed models, as they treat form as just a binary variable.

A review: detection techniques for LTE system Situation management knowledge from social media
Security flaw could not affect digital devices, experts say Semantic approach to event processing

Crisis management: media practices in telecommunication management A review study against intelligence internet
Semantic approach approach: current multimedia networks as modeling processes Security flaw to verify leaks

Table 2: Flipping the meaning and the form embeddings of two sentence from different registers. Note the use of the colon in the first example, and the use of the "to"-constructions in the second example, consistent with the form of the source sentences.

6.3 PERFORMANCE OF MEANING EMBEDDINGS ON DOWNSTREAM TASKS
We conducted some experiments to test the assumption that the derived meaning embeddings should improve performance on downstream tasks that require understanding of the meaning of the sentences regardless of their form. We evaluated embeddings produced by the ADNet, trained in the Headlines dataset, on a task of paraphrase detection. We used the SentEval toolkit (Conneau et al., 2017) and the Microsoft Research Paraphrase Corpus (Dolan et al., 2004). The F1 scores on this task for different models are presented in Table 3. Note that all models, except InferSent, are unsupervised. The InferSent model was trained on a big SNLI dataset, consisting of more than 500,000 manually annotated pairs. ADNet achieves the the highest score among the unsupervised systems and outperforms the regular sequence-to-sequence autoencoder with a large gap.

BoW Seq2Seq InferSent Fu et al. (2017) ADNet

80.82 74.68 83.17

78.88

81.38

Table 3: F1 scores on the task of paraphrase detection using the SentEval toolkit (Conneau et al., 2017)

7 CONCLUSION
In this paper, we presented ADNet, a new model that performs adversarial decomposition of text representation. In contrast to previous work, it does not require a parallel training corpus and works directly on hidden representations of sentences. Most importantly, is does not treat the form as a binary variable (as done in most previously proposed models), enabling a fine-grained change of the form of sentences or specific aspects of meaning. We evaluate ADNet on two tasks: the shift of language register and diachronic language change. Our solution achieves superior results, and t-SNE visualizations of the learned meaning and style embeddings illustrate that the proposed motivational loss leads to significantly better separation of the form embeddings.
9

Under review as a conference paper at ICLR 2019
REFERENCES
Samuel Albanie, Se´bastien Ehrhardt, and Joa~o F Henriques. Stopping gan violence: Generative unadversarial networks. arXiv preprint arXiv:1703.02528, 2017.
Martin Arjovsky, Soumith Chintala, and Le´on Bottou. Wasserstein gan. arXiv preprint arXiv:1701.07875, 2017.
Sanjeev Arora, Yingyu Liang, and Tengyu Ma. A simple but tough-to-beat baseline for sentence embeddings. In International Conference on Learning Representation, 2017.
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. arXiv preprint arXiv:1409.0473, 2014.
Keith Carlson, Allen Riddell, and Daniel Rockmore. Zero-shot style transfer in text using recurrent neural networks. arXiv preprint arXiv:1711.04731, 2017.
Daniel Cer, Yinfei Yang, Sheng-yi Kong, Nan Hua, Nicole Limtiaco, Rhomni St John, Noah Constant, Mario Guajardo-Cespedes, Steve Yuan, Chris Tar, et al. Universal sentence encoder. arXiv preprint arXiv:1803.11175, 2018.
Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, and Yoshua Bengio. Empirical evaluation of gated recurrent neural networks on sequence modeling. arXiv preprint arXiv:1412.3555, 2014.
Djork-Arne´ Clevert, Thomas Unterthiner, and Sepp Hochreiter. Fast and accurate deep network learning by exponential linear units (elus). arXiv preprint arXiv:1511.07289, 2015.
Alexis Conneau, Douwe Kiela, Holger Schwenk, Lo¨ic Barrault, and Antoine Bordes. Supervised learning of universal sentence representations from natural language inference data. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pp. 670­680, 2017.
Ferdinand de Saussure. Course in General Linguistics. New York : Philosophical Library, 1959. URL http://archive.org/details/courseingenerall00saus.
Dua Dheeru and Efi Karra Taniskidou. UCI machine learning repository, 2017. URL http: //archive.ics.uci.edu/ml.
Bill Dolan, Chris Quirk, and Chris Brockett. Unsupervised construction of large paraphrase corpora: Exploiting massively parallel news sources. In Proceedings of the 20th international conference on Computational Linguistics, pp. 350. Association for Computational Linguistics, 2004.
Jessica Ficler and Yoav Goldberg. Controlling linguistic style aspects in neural language generation. In Proceedings of the Workshop on Stylistic Variation, pp. 94­104, 2017.
Zhenxin Fu, Xiaoye Tan, Nanyun Peng, Dongyan Zhao, and Rui Yan. Style transfer in text: Exploration and evaluation. arXiv preprint arXiv:1711.06861, 2017.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural information processing systems, pp. 2672­2680, 2014.
Caglar Gulcehre, Sungjin Ahn, Ramesh Nallapati, Bowen Zhou, and Yoshua Bengio. Pointing the unknown words. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), volume 1, pp. 140­149, 2016.
M. A. K. Halliday, A. McIntosh, and P. Stevens. The Linguistic Sciences and Language Teaching. Longmans, Green and Co., London, 1968.
Zhiting Hu, Zichao Yang, Xiaodan Liang, Ruslan Salakhutdinov, and Eric P Xing. Toward controlled generation of text. In International Conference on Machine Learning, pp. 1587­1596, 2017.
Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A Efros. Image-to-image translation with conditional adversarial networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 1125­1134, 2017.
10

Under review as a conference paper at ICLR 2019

Harsh Jhamtani, Varun Gangal, Eduard Hovy, and Eric Nyberg. Shakespearizing modern language using copy-enriched sequence-to-sequence models. arXiv preprint arXiv:1707.01161, 2017.

Yoon Kim, Kelly Zhang, Alexander M Rush, Yann LeCun, et al. Adversarially regularized autoencoders for generating discrete structures. arXiv preprint arXiv:1706.04223, 2017.

Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013.

Alex M Lamb, Anirudh Goyal ALIAS PARTH GOYAL, Ying Zhang, Saizheng Zhang, Aaron C Courville, and Yoshua Bengio. Professor forcing: A new algorithm for training recurrent networks. In Advances In Neural Information Processing Systems, pp. 4601­4609, 2016.

Jiwei Li, Michel Galley, Chris Brockett, Georgios Spithourakis, Jianfeng Gao, and Bill Dolan. A persona-based neural conversation model. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), volume 1, pp. 994­1003, 2016.

Zachary C Lipton, Sharad Vikram, and Julian McAuley. Generative concatenative nets jointly learn to write and classify reviews. arXiv preprint arXiv:1511.03683, 2015.

Lajanugen Logeswaran and Honglak Lee. An efficient framework for learning sentence representations. arXiv preprint arXiv:1803.02893, 2018.

Alireza Makhzani, Jonathon Shlens, Navdeep Jaitly, Ian Goodfellow, and Brendan Frey. Adversarial autoencoders. arXiv preprint arXiv:1511.05644, 2015.

Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models. arXiv preprint arXiv:1609.07843, 2016.

Jonas Mueller, David Gifford, and Tommi Jaakkola. Sequence to better sequence: continuous revision of combinatorial structures. In International Conference on Machine Learning, pp. 2536­ 2544, 2017.

Nikita Nangia, Adina Williams, Angeliki Lazaridou, and Samuel R Bowman. The repeval 2017 shared task: Multi-genre natural language inference with sentence representations. arXiv preprint arXiv:1707.08172, 2017.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting on association for computational linguistics, pp. 311­318. Association for Computational Linguistics, 2002.

Peter Potash, Alexey Romanov, and Anna Rumshisky. Ghostwriter: using an lstm for automatic rap lyric generation. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pp. 1919­1924, 2015.

Peter Potash, Alexey Romanov, and Anna Rumshisky. Here's my point: Joint pointer architecture for argument mining. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pp. 1364­1373, 2017.

William Safire.

On Language ­ Gotcha!

Gang Strikes Again,

1995.

URL https://www.nytimes.com/1995/12/31/magazine/

on-language-gotcha-gang-strikes-again.html.

Tianxiao Shen, Tao Lei, Regina Barzilay, and Tommi Jaakkola. Style transfer from non-parallel text by cross-alignment. In Advances in Neural Information Processing Systems, pp. 6833­6844, 2017.

Efstathios Stamatatos. Authorship attribution using text distortion. In Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers, volume 1, pp. 1138­1149, 2017.

Sandeep Subramanian, Adam Trischler, Yoshua Bengio, and Christopher J Pal. Learning general purpose distributed sentence representations via large scale multi-task learning. arXiv preprint arXiv:1804.00079, 2018.

11

Under review as a conference paper at ICLR 2019

Ilya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to sequence learning with neural networks. In Advances in neural information processing systems, pp. 3104­3112, 2014.

Jian Tang, Yifan Yang, Sam Carton, Ming Zhang, and Qiaozhu Mei. Context-aware natural language generation with recurrent neural networks. arXiv preprint arXiv:1611.09900, 2016.

Dmitry Ulyanov.

Multicore-tsne.

Multicore-TSNE, 2016.

https://github.com/DmitryUlyanov/

Laurens Van Der Maaten. Accelerating t-sne using tree-based algorithms. Journal of machine learning research, 15(1):3221­3245, 2014.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Information Processing Systems, pp. 6000­6010, 2017.

Oriol Vinyals, Meire Fortunato, and Navdeep Jaitly. Pointer networks. In Advances in Neural Information Processing Systems, pp. 2692­2700, 2015.

Wei Xu, Alan Ritter, Bill Dolan, Ralph Grishman, and Colin Cherry. Paraphrasing for style. Proceedings of COLING 2012, pp. 2899­2914, 2012.

Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A Efros. Unpaired image-to-image translation using cycle-consistent adversarial networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 2223­2232, 2017.

A MULTIPLE FORMS AND STYLISTIC SIMILARITIES

(a) Meaning embeddings

(b) Form embeddings

Figure 4: t-SNE visualization of the form and meaning embeddings. Each color corresponds to a different author.

In order to go beyond just two different forms, we experimented with training the model on a set of literature novels from six different authors from Project Gutenberg2 written in two different time periods. A t-SNE visualization of the resulting meaning and form embeddings is presented in Figure 4. Note how form embeddings create a six-pointed star. After further examination, we observed that common phrases (for example, "Good morning" or "Hello!") were embedded into the center of the star, whereas the most specific sentences from a given author were placed into the rays of the star. In particular, some sentences included character names, thus further research is required to mitigate this problem. Stamatatos (2017) provides a promising direction for solving this.

2http://www.gutenberg.org/

12


Under review as a conference paper at ICLR 2019
G-SGD: OPTIMIZING RELU NEURAL NETWORKS IN ITS POSITIVELY SCALE-INVARIANT SPACE
Anonymous authors Paper under double-blind review
ABSTRACT
It is well known that neural networks with rectified linear units (ReLU) activation functions are positively scale-invariant. Conventional algorithms like stochastic gradient descent optimize the neural networks in the vector space of weights, which is, however, not positively scale-invariant. This mismatch may lead to problems during the optimization process. Then, a natural question is: can we construct a new vector space that is positively scale-invariant and sufficient to represent ReLU neural networks so as to better facilitate the optimization process ? In this paper, we provide our positive answer to this question. First, we conduct a formal study on the positive scaling operators which forms a transformation group, denoted as G. We show that the value of a path (i.e. the product of the weights along the path) in the neural network is invariant to positive scaling and prove that the value vector of all the paths is sufficient to represent the neural networks under mild conditions. Second, we show that one can identify some basis paths out of all the paths and prove that the linear span of their value vectors (denoted as G-space) is an invariant space with lower dimension under the positive scaling group. Finally, we design stochastic gradient descent algorithm in G-space (abbreviated as G-SGD) to optimize the value vector of the basis paths of neural networks with little extra cost by leveraging back-propagation. Our experiments show that G-SGD significantly outperforms the conventional SGD algorithm in optimizing ReLU networks on benchmark datasets.
1 INTRODUCTION
Over the past ten years, neural networks with rectified linear hidden units (ReLU) (Hahnloser et al., 2000) as activation functions have demonstrated the power in many important applications, such as information system (Cheng et al., 2016; Wang et al., 2017), image classification (He et al., 2016; Huang et al., 2017), text understanding (Vaswani et al., 2017), etc. These networks are usually trained with Stochastic Gradient Descent (SGD), where the gradient of loss function with respect to the weights can be efficiently computed via back propagation method (Rumelhart et al., 1986).
Recent studies (Neyshabur et al., 2015; LeCun et al., 2015) show that ReLU networks have positively scale-invariant property, i.e., if the incoming weights of a hidden node with ReLU activation are multiplied by a positive constant c and the outgoing weights are divided by c, the neural network with the new weights will generate exactly the same output as the old one for an arbitrary input. Conventional SGD optimizes ReLU neural networks in weight space. However, it is clear that weight vector is not positively scale-invariant. This mismatch may lead to problems during the optimization process (Neyshabur et al., 2015).
Then, a natural question is: can we construct a new vector space that is positively scale-invariant and sufficient to represent ReLU neural networks so as to better facilitate the optimization process ? In this paper, we provide positive answer to this question.
We investigate the positively scale-invariant space to sufficiently represent ReLU neural networks by the following four steps. Firstly, we define the positive scaling operators and show that they form a transformation group (denoted as G). The transformation group G will induce an equivalence relationship called positive scaling equivalence. Then, We found that the values of the paths are invariant to positive scaling operators. Furthermore, we prove that two weight vectors are positively scale-equivalent if and only if the values of the paths in one neural network equal to those in the other
1

Under review as a conference paper at ICLR 2019
neural network, given the signs of some weights unchanged. That is to say, the values of all the paths and can sufficiently represent the ReLU neural networks. After that, we define a generalized addition as the product operator and the generalized multiplication as the power operator. We show that the path vectors are generalized linearly dependent.1 We define the maximal group of paths which are generalized linearly independent as basis path, which corresponds to the basis of the structure matrix constituted by the path vectors. Thus, the values of the basis paths are also positively scale-invariant and can sufficiently to represent the ReLU neural networks. We denote the linear span of the values of basis paths as G-space. In addition, we prove that the dimension of G-space is "H" smaller comparing to the weight space, where H is the total number of hidden units in a multi-layer perceptron (MLP) or feature maps in a convolutional networks (CNN).
To sum up, we find G-space constituted by the values of the basis paths, which is positively scaleinvariant, can sufficiently represent the ReLU neural networks, and has a smaller dimension than the vector space of weights.
Therefore, we propose to optimize the ReLU neural networks in its positively scale-invariant space, i.e., G-space. We design a novel stochastic gradient descent algorithm in G-space (abbreviated as G-SGD) to optimize the ReLU neural networks utilizing the gradient with respect to the values of the basis paths. First, we design skeleton method to construct one group of the basis paths. Then, we develop inverse-chain rule and weight allocation to efficiently compute the gradient of the values of the basis paths by leveraging the back-propagation method. Please note that by using these techniques, there is very little additional computation overhead for G-SGD in comparison with the conventional SGD.
We conduct experiments to show the effectiveness of G-SGD. First, we evaluate G-SGD of training deep convolutional networks on benchmark datasets and demonstrate that G-SGD achieves clearly better performance than baseline optimization algorithms. Second, we empirically test the performance of G-SGD with different degrees of positive scale-invariance. The experimental results show that the higher the positive scale-invariance is, the larger the performance improvement of G-SGD over SGD. This is consistent with that, the positive scale-invariance in weight space will negatively influence the optimization and our proposed G-SGD algorithm can effectively solve this problem.
2 BACKGROUNDS
2.1 RELATED WORKS
There have been some prior works that study the positively scale-invariant property of ReLU networks and design algorithms that are positively scale-invariant. For example, Badrinarayanan et al. (2015) notice the positive scale-invariance in ReLU netowrks, and inspired by this, they design algorithms to normalize gradients by layer-wise weight norm. Du et al. (2018) study the gradient flow in MLP or CNN models with linear, ReLU or Leaky ReLU activation, and prove the squared norms of gradient across different layers are automatically balanced and remained invariant in gradient descent with infinitesimal step size. In our work, we do not care whether the models are balanced or not. Besides, many other optimization algorithms also have positively scale-invariant property such as Newton's method and natural gradient descent. The most related work is Path-SGD (Neyshabur et al., 2015), which also considers the geometry inspired by path norm. This work is different from ours: 1) they regularize the gradient in weight space by path norm while we optimize the loss function directly in a positively scale-invariant space; 2) they do not consider the dependency between paths and it's hard for them to compute the exactly path-regularized gradients. Different from the previous works, we propose to directly optimize the ReLU networks in its positively scale-invariant space, instead of optimizing in the weight space which is not positive scale-invariant. To the best of our knowledge, at the first time, we solve this mismatch by theoretical analysis and an effective and efficient algorithm.
2.2 RELU NEURAL NETWORKS
Let Nw(x) : X  Y denote a L-layer multi-layer perceptron (MLP) with weight w  W  Rm, the input space X  Rd and the output space Y  RK. In the l-th layer (l = 0, и и и , L), there are
1A path vector is represented by one element in {0, 1}m, where m is the number of weights. Please check the details in Section 2.2.
2

Under review as a conference paper at ICLR 2019

hl nodes. It is clear that, h0 = d, hL = K. We denote the il-th node and its value as Oill and olil ,

respectively. We use wl to denote the weight matrix between layer l-1 and layer l, and use wl(il-1, il)

to

denote

the

weight

connecting

nodes

Ol-1 il-1

and

Oill .

The

values

of

the

nodes

are

propagated

as

ol = ((wl)T ol-1), where (и) = max(и, 0) is the ReLU activation function. We use (i0, и и и , iL) to

denote the path starting from input feature node Oi00 to output node OiLL passing though hidden nodes

Oi11 , и и и

,

OL-1 iL-1

.

We can also regard the network structure as a directed graph (O, E), where O = {O1, и и и , OH+d+K } is the set of nodes where H denotes the number of hidden nodes and E = {eij} denote the set of edges in a network where eij denotes the edge pointing to Oj from nodes Oi. We use we, e  E

to denote the weight on edge e. If |E| = m, the weights compose a vector w = (w1, и и и , wm)T .

We define a path as a vector p = (p1, и и и , pm)T and if the edge e is contained in path p, pe = 1;

otherwise pe = 0. The value of path p is defined as vp(w) =

m i=1

wipi

and

the

activation

status

of

path p is defined as ap(x; w) =

H +d+K j=1

I(oj(w, x)

>

0)

и

i:eijE I(peij = 1). We denote the

set composed by all paths as P and the set composed by paths which contain edge connecting the

k-th output node as Pk. Thus, the output can be computed as follows:

Nwk (x) =

vp(w) и ap(w, x) и xi0 .

pP k

(1)

3 POSITIVELY SCALE-INVARIANT SPACE OF RELU NETWORKS

In this section, we first define positive scaling transformation group and the equivalence class induced by this group. Then we study the invariant space under positive scaling transformation group of ReLU networks and study its dimension.

3.1 POSITIVE SCALING TRANSFORMATION GROUP

We formally define the positive scaling operator. We first define a node positive scaling operator gc,O(w) : W  W with constant c > 0 and one hidden node O as

w~ = gc,Oill (w),

where w~l(il-1, il)

=

c и wl(il-1, il) for il-1

=

1, и и и , hl-1; w~l+1(il, il+1)

=

1 c

и

wl+1(il, il+1)

for

il+1 = 1, и и и , hl+1; and values of other elements of w~ are the same with w.

Definition 3.1 (positive scaling operator) Suppose that {O1, и и и , OH} is the set of all the hidden nodes in the network where H denotes the number of hidden nodes. A positive scaling operator g(c1,иии ,cH )(и) : W  W with c1, и и и , cH  R+ is defined as
g(c1,иии ,cH )(и) := gc1,O1  gc2,O2  и и и  gcH ,OH (и),
where  denotes function composition.

We then collect all the g(c1,иии ,cH )(и) together to form a set G := {g(c1,иии ,cH )(и) : c1, и и и , cH  R+}. It is easy to check that G together with the operation "" is a group which is called positive scaling transformation group, and we call the group action of G on W as G-action. (Please refer to Section 8 in Appendix.)
Clearly, if there exists an operator g  G to make w = g(w ), ReLU networks Nw and Nw will generate the same output for any fixed input x. We define the positive scaling equivalence induced by G-action.

Definition 3.2 Consider two ReLU networks with weights w, w  W and the positive scaling transformation group G. We say w and w are positively scale-equivalent if g  G such that w = g(w ), denote as w G w .

Given G-action on W, the equivalence relation "G" partitions W into G-equivalent classes. The following theorem shows that the sufficient and necessary condition for ReLU networks in the same
equivalent class is that they have the same values and activation status of paths.

3

Under review as a conference paper at ICLR 2019

Theorem 3.3 Consider two ReLU neural networks with weights w, w  W. We have that w G w iff for  path p  P and any fixed input x  X , we have vp(w) = vp(w ) and ap(x; w) = ap(x; w).
Invariant variables for a group action are important and widely studied in group theory and geometry. We say a function f : W  R is invariant variable of G-action if f (w) = f (g(w)), g  G. Based on Theorem 3.3, a direct corollary is that values and activation status of paths are invariant variables under G-action. Considering that 1) values of paths are G-invariant variables while the weights aren't; 2) values of paths together with the activation status determines an positive scale-equivalent class, and are sufficient to determine the loss, we propose to optimize the values of paths instead of weights.

3.2 POSITIVELY SCALE-INVARIANT SPACE AND ITS DIMENSION

Although Theorem 3.3 shows the values of paths are invariant variables under G-action, we find that the paths have inner-dependency and therefore their values and activation statuses are not independent (see Figure 1). In order to describe this dependency clearly, we introduce a new representation of paths.

First, we define the addition operation "" and scalarmultiplication operation " " in space (R/{0})m as: w  w = (w1 и w1, и и и , wm и wm),  w = (sgn(w1)|w1|ln a, и и и , sgn(wm)|wm|ln a) where w, w  (R/{0})m,   R+. We call the space (R/{0})m equipped with addition operation "" and scalar-multiplication operation " " is a gen-
eralized linear space (Please refer to Section 8 in
Appendix).

Figure 1: This is a simple ReLU network with

one hidden node. Suppose path values are

vp1 (w) = w1w3, vp2 (w) = w1w4, vp3 (w) =

w2w3, vp4 (w) = w2w4, we can see the inner-

dependency between them, i.e., vp4 (w) =

.vp2 (w)иvp3 (w) vp1 (w)

A

is

the

structure

matrix

of

this

example.

Next we will consider representation of path and operations on path in the generalized linear space. As
described in Section 2.2, each path can be represented as a m-dimensional vector p = (p1, и и и , pm)
that each element equals 0 or 1. In the generalized linear space, considering that 1 and e are the additive identity and multiplicative identity in the field R+ equipped with "" and " ", we assign 1 to pe if pe = 0 and assign e to pe if pe = 1. Then p = (p1, и и и , pm)T becomes a vector that each
element equals 1 or e. Thus the value of path p for can be calculated by the inner product of the
weight vector and the path vector, i.e., vp(w) = w, p , where w = (w1, и и и , wm).

Suppose that P  {1, e}m is the set composed by all paths. We denote the matrix composed by all

paths as A and call it structure matrix of ReLU networks. The size of A is m О n where n is the

number of paths. We observe that the paths in matrix A are not linearly independent, i.e.,some paths

can be linearly represented by other columns. For example in Figure 1, p4 = p2  p3 p1 and the

corresponding

values

of

paths

have

the

relationship:

vp4 (w)

=

.vp2 (w)иvp3 (w) vp1 (w)

Thus

we

need

to

study

the rank of matrix A and find a maximal linearly independent group of paths.

Theorem 3.4 If A is the structure matrix for a ReLU network, then we have rank(A) = m - H, where m is the dimension of weight vector w and H is the total number of hidden nodes for MLP (or feature maps for CNN models) with ReLU respectively.
Definition 3.5 (basis path) A set of paths P0 = {p1, и и и , pm-H } which is a subset of P are called basis paths if p1, и и и , pm-H compose a maximal linearly independent group of column vectors in structure matrix A.

We design an algorithm called skeleton method to identify basis paths efficiently, which will be introduced in Section 4. For given values of basis paths and structure matrix, the values of w can not be determined unless the values of free variables are fixed (Lay (1997)). Assume ws1 , и и и , wsH are selected to be the free variables which are called free skeleton weights, we prove that the activation status can be uniquely determined by the values of paths if signs of free skeleton weights are fixed. Thus, we have the following theorem which is a modification of Theorem 3.3.
Theorem 3.6 Consider two ReLU neural networks with weights w, w  W with the same signs of skeleton weights. We have that w G w iff for p  P0, we have vp(w) = vp(w ).

4

Under review as a conference paper at ICLR 2019

In the following context, we always suppose that w  W have the same signs of free skeleton weights. According to Theorem 3.6 and the linear dependency between values of paths, the loss function can be calculated using values of basis paths if signs of free skeleton weights are fixed. We denote the the loss at training instance (x, y) as l(v; x, y) and propose to optimize the values of basis paths. Considering that values of basis paths are obtained through structure matrix A, the dimension of the space composed by values of basis paths should be equal to rank(A). Then we define the following space.
Definition 3.7 (G-space) The G-space is defined as V := {v = (vp1 , и и и , vpm-H ) : vpj = w, pj , w  (R/{0})m, pj  P0}.
We call the space composed by the values of basis paths G-space, which is invariant under transformation group G, i.e., it is composed by invariant variables under G-action. Besides, we measure the reduction of the dimension for positively scale-invariant space using the invariant ratio H/m, thus we can empirically test how severe this equivalence will influence the optimization in weight space.

4 ALGORITHM: G-SGD

In this section, we will introduce the G-SGD that optimizes ReLU neural network models in the G-space. This novel algorithm makes use of three methods, named Skeleton Method, Inverse-Chain-Rule (ICR) and WeightAllocation (WA), respectively, to calculate the gradients w.r.t. basis path vector and project the updates back to weights efficiently (with little extra computation in comparison with standard SGD).

4.1 SKELETON METHOD

Figure 2: The weights with red color are skele-

Before the calculation of gradients in G-space, we first design an algorithm ton weights.

called skeleton method to construct skeleton weights and basis paths. Due

to space limitation, we only show the MLP case with same number of

hidden nodes in each hidden layer, and put the skeleton method for general case in Appendix.

1. Construct skeleton weights: for weight matrix w2, и и и , wL-1, we select diagonal elements to be the skeleton weights. For weight matrix w1, we select the element w1(i1 mod d, i1) for column i1 with i1 = 1, и и и , h1 to be the skeleton weights. For weight matrix wL, we select the element wL(iL-1, iL-1 mod K) for row iL-1 with iL-1 = 1, и и и , hL-1 to be the skeleton weights. We call the rest weights non-skeleton weights. Figure 2 gives an illustration for skeleton weights in a
MLP network.

2. Construct basis paths: A path which contains at most one non-skeleton weights is a basis path. The proof of this statement could be found in Appendix. For example, in Figure 2, the paths in red color and the paths with only one black weight are basis paths. Beyond that, the paths are non-basis paths.

Once we have basis paths, we can calculate the gradients w.r.t. basis path vector vpi , and iteratively update the model by

vpt+j 1

=

vpt j

-

t



l(v; S  vpj

t

)

,j
v=vt

= 1, и и и

, m - H,

(2)

where St is the mini-batch training data in iteration t. For the calculation of the gradients w.r.t. basis path vector, we introduce inverse-chain-rule method in next section.

4.2 INVERSE-CHAIN-RULE (ICR) METHOD

The basic idea of the Inverse-Chain-Rule method is to connect the gradients w.r.t. weight vector and those w.r.t. basis path vector by exploring the chain rules in both directions. That is, we have,

 vp1 ( l(w; x, y) , и и и , l(w; x, y) ) = ( l(v; x, y) , и и и , l(v; x, y) ) и  w1

иии иии

 vp1 wm

  (3)

w1

wm

 vp1

 vpm-H



и и иvpm-H w1

 vpm-H wm

5

Under review as a conference paper at ICLR 2019

Algorithm 1 G-SGD

Require: initialization w0, learning rate t, training data set D. 1. Construct skeleton weights using skeleton method and construct A~-1 according to ICR and WA

method.

for t = 1, и и и , T do

2.

Implement

feed

forward

process

and

back

propagation

process

to

get

 l(w;S t ) wi

.
w=wt

3.

Calculate

l(v;x,y) v

according

to

Eqn.(3).

4. Calculate weight-ratio using A~-1 according to Eqn.(5).

5. Update the weights as wt+1 = wt и rt+1(w).

end for Ensure: wT .

We

first

compute

the

gradients

w.r.t.

weights,

i.e.,

l(w;x,y) wi

for

i

=

1, и и и

,m

using

standard

back

propagation.

Then we solve Eqn.(3) to obtain the gradients w.r.t.

basis paths, i.e.,

l(v;x,y) vj

for

j = 1, и и и , m - H. We denote matrix at the right side of Eqn.(3) as G. Given the following facts:

(1)

vp we

=

vp we

if the edge e is contained in path p, otherwise

0; (2) according to the skeleton method,

each non-skeleton weight will be contained in only one basis path, which means there is only one

non-zero element in each column corresponding to non-skeleton weights in G, G is sparse and thus

the solution of Eqn.(3) is easy to obtain.

4.3 WEIGHT-ALLOCATION (WA) METHOD

After the values of basis paths is updated by SGD, in a new iteration, we employ ICR again by leveraging BP with the new weight. Thus, we need to project the updates on basis paths back to the updates of weights.

We define the path-ratio of pj at iteration t as Rt(pj) := vptj /vpt-j 1 and the weight-ratio of wi at iteration t as rt(wi) := wit/wit-1. Assume that we have already obtained the path-ratio for all the basis paths Rt+1(pj) according to ICR method and the SGD update rule. Then we want to project the
path-ratios onto the weight-ratios. Because we have vpj = w pj, the weight-ratios obtained after the projection should satisfy the following relationship:

(Rt(p1), и и и , Rt(pm-H )) = (rt(w1), и и и , rt(wm)) A ,

(4)

where the matrix A = (p1, и и и , pm-H ). According to this relationship, we design Weight-Allocation Method to project the path-ratio to weight-ratio as described below. Suppose that w1, и и и , wH are the free skeleton weights. We first add H elements with value 1 at the beginning in vector (Rt(p1), и и и , Rt(pm-H )) to get a new m-dimensional vector. Then we append H columns in matrix A to get a new matrix A~ as A~ = [B, A ] with B = [I, 1]T where I is an identity matrix with size H О H and 1 is the zero matrix with size H О m in generalized linear space. Then it is easy to prove that rank(A~) = m and we can calculate the inverse matrix of A~ and get the weight-ratio as

(rt(w1), и и и , rt(wm)) = (1, и и и , 1, Rt(p1), и и и , Rt(pm-H )) A~-1.

(5)

After the projection, we can see that weight-ratios of free skeleton weights equal 1 which means that
free skeleton weights will not be changed during the training process. According to the skeleton method again, A~ is a sparse matrix and it is easy to calculate its inverse.

Please note by combining the ICR and WA methods, we can obtain the explicit update rule for G-SGD, which is concluded in Algorithm 1. In this way, we obtain the correct gradients. The extra computational complexity of the ICR and WA methods are far lower than that of forward and backward propagation, and can therefore be neglected in practice.

5 EXPERIMENTS
In this section, we first evaluate the performance of G-SGD on training deep convolutional networks and verify that if our proposed algorithm outperforms other baseline methods. Then we investigate

6

Under review as a conference paper at ICLR 2019

the influence of positive scaling invariance on the optimization in weight space, and examine whether optimization in G space brings performance gain. At last, we compare G-SGD with Path-SGD (Neyshabur et al., 2015) and show the necessity of considering the dependency between paths. All experiments are averaged over 5 independent trials if without explicit note.

5.1 DEEP CONVOLUTIONAL NETWORK
In this section, we apply our G-SGD to image classification tasks and conduct experiments on CIFAR-10 and CIFAR-100 (Krizhevsky & Hinton, 2009). In our experiments, we employ the original ResNet architecture described in (He et al., 2016). Specifically, there is no positive scaling invariance across residual blocks since the residual connections break down the structure matrix described in Section 3.2, we target the invariance in each residual block. For better comparison, we also conduct our studies on a stacked deep CNN described in He et al. (2016) (refer to PlainNet), and target the positive scaling invariance across all layers. We train 34 layers ResNet and PlainNet models on the datasets following the training strategies in the original paper, and compare the performance between G-SGD2 and vanilla SGD algorithm. The detailed training strategies could be found in Appendix. In this section, we focus on the performance of different optimization algorithms, and will discuss the combination of G-SGD and regularization in Appendix.

Training loss
Plain-34 SGD

0.94

Test accuracy

100

Plain-34 -SGD ResNet-34 SGD ResNet-34 -SGD

0.93

10 1

CIFAR-10

10 2 0.92

10 3 0

40 80 120 160 0.91 0

40

80 120 160

0.75

100 0.70
10 1

10 2 0.65

CIFAR-100

10 3 0

40 Ep8o0ch 120 160 0.60 0

40 Ep8o0ch 120 160

Figure 3: Training loss and test accuracy w.r.t. the number of effective passes on PlainNet and ResNet.

Table 1: Classification error rate (%) on image classification task.

Plain-34 ResNet-34

SGD G-SGD
SGD G-SGD

C10 7.76 (▒0.17) 7.00 (▒0.10) 7.13 (▒0.22) 6.66 (▒0.13)

C100 36.41(▒0.54) 30.74 (▒0.29) 28.60(▒0.26) 27.74 (▒0.06)

As shown in Figure 3 and Table 1, our G-SGD clearly outperforms SGD on each network and each dataset. To be specific, 1) both the lowest training loss and best test accuracy are achieved by ResNet-34 with G-SGD on both datasets, which indicates that G-SGD indeed helps the optimization of ResNet model; 2) Since G-SGD can eliminate the influence of positive scaling invariance across all layers of PlainNet, we observe the performance gain on PlainNet is larger than that on ResNet. For PlainNet model, G-SGD surprisingly improves the accuracy numbers by 0.8 and 5.7 for CIFAR-10 and CIFAR-100, respectively, which verifies both the improper influence of positive scaling invariance for optimization in weight space and the benefit of optimization in G space. Moreover, Plain-34 trained by G-SGD achieves even better accuracy than ResNet-34 trained by SGD on CIFAR-10, which shows the influence of invariance on optimization in weight space as well.
2Batch normalization is widely used in modern CNN models. Please refer to Appendix for the combination of G-SGD and batch normalization.

7

Under review as a conference paper at ICLR 2019

5.2 THE INFLUENCE OF INVARIANCE
In this section, we study the influence of invariance on the optimization for ReLU Networks. As proved in Section 3, the dimension of weight space is larger than G-space by H, where H is the total number of the hidden nodes in a MLP or the feature maps in a CNN. We define the invariant ratio as H/m. We train several 2-hidden-layer MLP models on Fasion-MNIST (Xiao et al., 2017) with different number of hidden nodes in each layer, and analyze the performance gap  between the models optimized by G-SGD and SGD. The detailed training strategies and network structures could be found in Appendix.

Training loss (x10 2) Test error

0.9 SGD 0.8 -SGD 0.08 0.7 0.06 0.6 0.04 0.5 0.4 0.02
16 32 64 128H256 512 10242048

0.22
0.20 0.020 0.18 0.016
0.012 0.16 0.008 0.14 0.004
0.12 16 32 64 128H256 512 10242048

Figure 4: Training loss and test error on MLPs. The invariant ratio decreases as H increases.

From Figure 4, we can see that, 1) for each number of H, G-SGD clearly outperforms SGD on both training loss and test error, which verifies our claim that optimization loss function in G space is a better choice; 2) as H increases, the invariant ratio decreases and  gradually decreases as well, which provides the evidence for that the positive scaling invariance in weight space indeed improperly influences the optimization.

5.3 COMPARISON WITH PATH-SGD

In this section, we compare the performance of Path-SGD and that of G-SGD. As described in Section 2.1, Path-SGD also consider the positive scaling invariance, but 1) instead of optimizing the loss function in G-space, Path-SGD regularizes optimization by path norm; 2) Path-SGD ignores the dependency among the paths. We extend the experiments in Neyshabur et al. (2015) to G-SGD without unbalance initialization, and conduct our studies on MNIST and CIFAR-10 datasets. The detailed training strategies and description of network structure can be found in Appendix.

Training loss Test error (%) Training loss
Test error

0.20 0.15

MNIST
SGD Path-SGD -SGD

4.00 3.75 3.50

MNIST

0.10 3.25

0.05

3.00 2.75

2.00 CIFAR-10
1.75 1.50 1.25

0.600 CIFAR-10
0.575 0.550 0.525 0.500

0.00 0 100 20E0p(ao)c3h00 400 500 2.50 0 100 20E0p(bo)c3h00 400 500

0 100 20E0p(co)c3h00 400 500

0 100 20E0p(do)c3h00 400 500

Figure 5: Performance of MLP models with Path-SGD and G-SGD.

As shown in Figure 5, while Path-SGD achieves better or equally good test accuracy and training loss than SGD for both MNIST and CIFAR10 datasets, G-SGD achieves even better performance than
Path-SGD, which is consistent with our theoretical analysis that considering the dependency between the paths and optimizing in G-space bring benefit.

6 CONCLUSION
In this paper, we study the G-space for ReLU neural networks and propose a novel optimization algorithm called G-SGD. We study the positive scaling operators which forms a transformation group G and prove that the value vector of all the paths is sufficient to represent the neural networks. Then we show that one can identify basis paths and prove that the linear span of their value vectors (denoted as G-space) is an invariant space with lower dimension under the positive scaling group. We design

8

Under review as a conference paper at ICLR 2019
G-SGD algorithm in G-space by leveraging back-propagation. We conduct extensive experiments to verify the empirical effectiveness of our proposed approach. In the future, we will examine the performance of G-SGD on more large-scale tasks.
REFERENCES
Vijay Badrinarayanan, Bamdev Mishra, and Roberto Cipolla. Symmetry-invariant optimization in deep networks. arXiv preprint arXiv:1511.01754, 2015.
Heng-Tze Cheng, Levent Koc, Jeremiah Harmsen, Tal Shaked, Tushar Chandra, Hrishi Aradhye, Glen Anderson, Greg Corrado, Wei Chai, Mustafa Ispir, et al. Wide & deep learning for recommender systems. In Proceedings of the 1st Workshop on Deep Learning for Recommender Systems, pp. 7Г10. ACM, 2016.
Simon S Du, Wei Hu, and Jason D Lee. Algorithmic regularization in learning deep homogeneous models: Layers are automatically balanced. Advances in Neural Information Processing Systems, 2018.
Richard HR Hahnloser, Rahul Sarpeshkar, Misha A Mahowald, Rodney J Douglas, and H Sebastian Seung. Digital selection and analogue amplification coexist in a cortex-inspired silicon circuit. Nature, 405(6789): 947, 2000.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. In Proceedings of the IEEE international conference on computer vision, pp. 1026Г1034, 2015.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770Г778, 2016.
Geoffrey E Hinton, Nitish Srivastava, Alex Krizhevsky, Ilya Sutskever, and Ruslan R Salakhutdinov. Improving neural networks by preventing co-adaptation of feature detectors. arXiv preprint arXiv:1207.0580, 2012.
Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely connected convolutional networks. In CVPR, volume 1, pp. 3, 2017.
Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. CIFAR, 2009. David C Lay. Linear Algebra and its applications, 1997. Addison Wesley Longman, Inc. ISBN 0-201-76717-1,
1997. Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. Nature, 521(7553):436Г444, 2015. Behnam Neyshabur, Ruslan R Salakhutdinov, and Nati Srebro. Path-sgd: Path-normalized optimization in deep
neural networks. In Advances in Neural Information Processing Systems, pp. 2422Г2430, 2015. David E Rumelhart, Geoffrey E Hinton, and Ronald J Williams. Learning representations by back-propagating
errors. nature, 323(6088):533, 1986. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser,
and Illia Polosukhin. Attention is all you need. In Advances in Neural Information Processing Systems, pp. 5998Г6008, 2017. Ruoxi Wang, Bin Fu, Gang Fu, and Mingliang Wang. Deep & cross network for ad click predictions. In Proceedings of the ADKDD'17, pp. 12. ACM, 2017. Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms. 2017. Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. arXiv preprint arXiv:1605.07146, 2016. Shuxin Zheng, Qi Meng, Huishuai Zhang, Wei Chen, Nenghai Yu, and Tie-Yan Liu. Capacity control of relu neural networks by basis-path norm. arXiv preprint arXiv:1809.07122, 2018.
9

Under review as a conference paper at ICLR 2019

APPENDIX: G-SGD: OPTIMIZING RELU NEURAL NETWORKS IN ITS POSITIVELY SCALE-INVARIANT SPACE
The Appendix document is composed of examples of skeleton weights and basis paths for different MLP structures, proofs of propositions, lemmas and theorems and the additional information about the experiments in the paper Optimization of ReLU Neural Networks using G-Stochastic Gradient Descent .
7 NOTATIONS

Notations
m
H
n
m-H W  Rm
w = (w1, и и и , wm) wl
wl(il-1, il) Oill
E = {eij }

Table 2: Notations

Object

dimension of weight space

total number of hidden nodes or feature maps

total number of paths

total number of basis paths and dimension of G-space

weight vector space

weight vector with m =

L l=1

hl-1 hl

for

MLP

weight matrix at layer l with size hl-1 О hl for MLP

weight element in matrix wl at position (il-1, il)

the il-th hidden node at layer l the set of edges in neural network model

Index
l il (iL, iL-1, и и и , i0) p pi sj

Table 3: Index
Range {1, и и и , L} {1, и и и , hl} il  [hl], l  [L]
P {p1, и и и , pm-H } = P0 {s1, и и и , sH }  {1, и и и , m}

Object index of layer index of hidden nodes at l-layer explicit index of path
path basis path free skeleton weight

Table 4: Mathematical Notations

Notation # / 

Meaning the number of
division function composition

8 SOME CONCEPTS IN ABSTRACT ALGEBRA
Definition 8.1 (Transformation group) Suppose that G is a set of transformations, and  is an operation defined between the elements of G. If G satisfies the following conditions: 1) (operational closure) for any two elements g1, g2  G, it has g1  g2  G; 2) (associativity) for any three elements g1, g2, g3  G, it has (g1  g2)  g3 = g1  (g2  g3); 3) (unit element) there exists unit element e  G, so that for any element g  G, there is g  e = g; 4) (inverse element) for any element g  G, there exists an inverse element g-1  G of g such that g  g-1 = e. Then, G together with the operation "" is called a transformation group.
Definition 8.2 (Group action) If G is a group and W is a set, then a (left) group action G,W of G on W is a function G,W : G О W  W that satisfies the following two axioms (where we denote (g, w) as g и w): 1) (identity) e и w = w; 2) (compatibility) (g  h) и w = g  (h и w) for all g, h  G and all w  W.
10

Under review as a conference paper at ICLR 2019

We call the space (R/{0})m equipped with addition operation "" and scalar-multiplication operation " " is a generalized linear space. Please note then it is not a vector space because R/{0} together with  and  where a  b = sgn(a и b)eln |a|иln |b| is not a field. The distributive law is not satisfied.
However, we still define this space to help us the identify the independent group of values of paths.

9 GENERAL SKELETON METHODS AND EXAMPLES OF MLP MODELS

Algorithm 2 skeleton method
Require: w1, и и и , wL. 1. Skeleton incoming weight: For matrix w1, get the elements w1((j mod h0), j), j  [h1]. 2. Skeleton outgoing weight: For matrix w2, get the elements w2(j, (j mod h2)), j  [h1].
3. Basis path set: Construct set S1 = jh=1 1{{(i0, j, (j mod h2))|i0  [h0]}  {(j mod h0), j, i2)|i2  [h2]}}
4. Partition set S into Sj1 = {(i0, i1, j)|i0  [h0], i1  [h1]}, j  [h2] according to the third element in the triple. for l = 2, ..., L - 1 do
5. (sub)-basis incoming path: For matrix wl, get elements wl((j mod hl-1), j) j  [hl]. Select pls,j which is one of the path passed Ojl and composed by skeleton weights. 6. (sub)-basis outgoing weight: For matrix wl+1, get the elements wl+1(j, (j mod hl+1)), j  [hl]. 7. Basis path set: Construct set
Sl = hj=l 1{(s, (j mod hl+1)))|s  Sjl }  {(psl,j , il+1)|il+1  [hl+1]}} 8. Partition set Sl into Sjl = {(i0, и и и il, j)|i0  [h0], и и и , il  [hl], }, j  [h2] according to the last element (indexed by j) in the array.
end for Ensure: Set SL-1.

In this section, we will introduce the skeleton method in Algorithm 2 in a recursive way for general case. The algorithm will take L - 1 round. When it takes the l-th round, it regards the network as a l-layer network with layer 1 to layer l and regards the node O1l , и и и , Ohl l as the output nodes. When it takes the l + 1 iteration, it regards nodes O1l , и и и , Ohl l as hidden nodes and identify the skeleton incoming weight and skeleton outgoing weight. Thus at each round, the number of index
for the basis paths will be added by 1. The logic for the construction of basis paths is that: 1) select
one incoming skeleton weight (or incoming basis path only composed by skeleton weights) and
one outgoing skeleton weight for each hidden node. 2) the paths which contain no more than one
non-skeleton weight is basis paths.

Next, we use some examples to explain the types of skeleton weights, basis paths constructed by skeleton method in the main paper. We call the basis path which only contains skeleton weights all-basis paths and basis path which contains one non-skeleton weight skip-basis paths.

First, we consider MLP models with the same number of hidden nodes of each layer Hinton et al.
(2012). Fig.6(a) shows an example of MLP model. It only displays the skeleton weights. We can
see that the number of basis paths is three which equals the number of hidden nodes of one layer. If the number of hidden nodes is h1 = и и и = hL-1 = h, by using skeleton method in Algorithm 1, the number of skeleton weights is H = hL and the number of all-basis paths equals the number of
hidden nodes of one layer, which is h. Thus we have

#(basis paths) =#(skip-basis paths) + #(all-basis paths) =#(non-skeleton weights) + #(all-basis paths) =#weights - #(skeleton weights) + #(hidden nodes of one layer)
L
= hl-1hl - hL + h
l=1
=h0h + (L - 2)h2 + hhL - (L - 1)h.

(6) (7) (8) (9)
(10)
(11)

11

Under review as a conference paper at ICLR 2019

Second, we consider MLP models with decreasing number of hidden nodes, i.e., h1  h2  и и и  hL-1. Fig.6(b) shows an example. We can see that the number of basis paths is five which equals the largest number of hidden nodes,i.e., h1. We can see that because the hl > hl-1, some nodes (e.g., O12) will have multiple incoming weights (e.g., w2(1, 1), w2(5, 1)) which are skeleton outgoing weight for the front hidden nodes. For these nodes, they select one of skeleton weights to be the
skeleton incoming weights, which is displayed using red full line (e.g.,w2(1, 1)). Others are displayed using red dotted line (e.g., w2(5, 1)). Thus the path which contains only skeleton weights and less than one dotted skeleton weight is a all-basis path. We have

#(basis paths) =#(skip-basis paths) + #(all-basis paths) =#(non-skeleton weights) + #(all-basis paths) =#weights - #(skeleton weights) + #(hidden nodes of between the first and second layer)
LL
= hl-1hl - ( hl + h1) + h1
l=1 l=1 L L-1
= hl-1hl - hl.
l=1 l=1

(12) (13) (14) (15)
(16)
(17)

(a) (b) (c) Figure 6: Examples

Third, we consider MLP models with unbalanced number of hidden nodes of each layer. If hl-1 < hl and hl > hl+1 for one layer l with l  2, there exists hidden nodes whose the skeleton incoming

weight and skeleton outgoing weight are both dotted,e.g., O42 in Fig.6(b). In this case, the number of

all-basis paths is h1 +

L-2 l=1

(max{hl,

hl+1}

-

hl).

It

is

because

that

each

node

must

be

passed

by

at least one all-basis paths. Thus for the example showed in Fig.6(b), although both the incoming

skeleton weight and the outgoing skeleton weight of O42 are dotted, the path (1, 1, 4, 1, 1) passed O42 is all-basis path. In this case, the number of all-basis paths is h1 + Ll=-12(max{hl, hl+1} - hl).

#(basis paths)

=#(skip-basis paths) + #(all-basis paths)

=#(non-skeleton weights) + #(all-basis paths)

=#weights - #(skeleton weights) + #(all-basis paths)

L L-2

L-2

= hl-1hl - (h1 + max{hl, hl+1} + hL-1) + (h1 + (max{hl, hl+1} - hl))

l=1 l=1

l=1

L L-1

= hl-1hl - hl.

l=1 l=1

(18) (19) (20) (21)
(22)
(23)

10 PROOFS OF THEORETICAL RESULTS IN SECTION 3
In this section, we will provide proofs of the lemma and theorems in Section 3 of the main paper. 12

Under review as a conference paper at ICLR 2019

10.1 PROOF OF THEOREM 3.3

Theorem 3.3: Consider two ReLU neural networks with weights w, w  W. We have that w  w iff for  path p and  input x  X , we have vp(w) = vp(w ) and ap(w, x) = ap(w , x).

Proof: The sufficiency is trivial according to the representation of Nwk (x), which is shown in Eqn(1) in the main paper.

For the necessity, if w  w , then there exist a positive scaling operator g(и) to make g(w ) = w.

We use il to denote the node index of nodes in layer-l and il  [hl]. Then we have wl(il-1, il) =

1 cil-l-11

и clil

и wl(il-1, il)

for

l

=

2, и и и

, L - 1,

because

each

weight

may

be

modified

by

the

operators

of its connected two nodes gcil-l-11 ,Oil-l-11 and g .clil ,Oill Thus vp(w) = vp(w ) is satisfied because

L
vp(w ) = wl(il-1, il) = ci11 w1(i0, i1) и
l=1

L-1 1 cl-1
l=2 il-1

и cill wl(il-1, il)и

L

= wl(il-1, il) = vp(w).

l=1

и

1 cL-1
iL-1

wL(iL-1, iL)

(24) (25)

Next we need to prove that ap(w, x) = ap(w , x) is also satisfied. Because the value of the activation is determined by the sign of ol, l = {1, и и и , L - 1}, we just need to prove that

[olw,1(x), и и и , owl ,hl (x)] = [cl1 и owl ,1(x), и и и , chl l и olw ,hl (x)],
where clil , il  [hl], l  [L - 1] are positive numbers. We prove it by induction.
(1) For o1 of a L-layer MLP (L > 2): Suppose that (и) is a ReLU activation function. For the i1-th hidden node, we have

o1w,i1 = 

h0
w1(i0, i1)xi0
i0 =1

=

h0
c1i1 и w1(i0, i1)xi0
i0 =1

= ci11 и 

h0
w1(i0, i1)xi0
i0 =1

= c1i1 и o1w ,i1 . (26)

(2) For ol of the L-layer MLP (l > 2): Suppose that

[owj ,1(x), и и и , ojw,hj (x)] = [cj1 и ojw ,1(x), и и и , chj j и owj ,hj (x)], j = {1, и и и , l - 1}.

Then we have

olw,il

=



 hl-1 
il-1 =1

wl(il-1,

 il)olw-,i1l-1 (x)

=



 hl-1 
il-1 =1

1 cl-1
il-1

и cill

и

wl (il-1 ,

il)

и

cl-1 il-1



и

ol-1 w ,il-1

(x)

=

clil

и olw ,il .

(27)

Thus we finish the proof.

10.2 PROOF OF THEOREM 3.4 AND THEOREM 3.6

In order to prove Theorem 3.4 and Theorem 3.6, we need to prove that there exist a group of paths which are independent and can represent other paths, and the activation status can be calculated using values of basis paths and signs of free skeleton weights. In order to simplify the proof, we leverage the basis paths constructed by skeleton method. We only show the proof of the following lemma, from which we can easily get Theorem 3.4 and Theorem 3.6.

Lemma 10.1 The paths selected by skeleton method are basis paths and ap(w, x) can be calculated using signs of free skeleton weights and the values of basis paths in a recursive way.

Proof sketch: Let us consider the matrix A = (p1, и и и , pm-H ) composed by basis paths constructed

by skeleton method:

A=

I0 B1 B2

(28)

13

Under review as a conference paper at ICLR 2019

There is an identity matrix with size z О z where z is the number of skip skeleton paths. This identity

matrix means that wi is a non-skeleton weight and is contained in pi, i  z. Thus, through the

row transformation of the matrix, B1 can be transformed to zero matrix. According to skeleton

method, column vectors in B2 are independent because skeleton weight will only appear in one

all-basis path. Thus the independent property has been proved. Furthermore, by leveraging the

structure of matrix A , it is easily to check that for a non-skeleton path p, it can be calculated as

p=

z i=1

ipi

-

m-H j=z+1

j pj

where

i

=

0

or

1

and

j

=

0, 1, 2 и и и

,L

-

1.

More

specifically,

if p contains wi, i  z, then i = 1; otherwise, i = 0.

For the second statement, because the activation status is determined by all the oill (x), we just need to prove the sign of oill (x) is determined by the value of basis path vector v. For each hidden node Oill , there exist only one basis path which passes it and only contain skeleton weights (We call the basis path which contains only skeleton weights all-basis path). We denote the value of all-basis

path which passes Oill as vpa (Oill ) = w1(Oill ) и

L j=2

wj

(Oill

),

where

wj

(Oill

)

denotes

the

skeleton

weight of pa(Oill ) at the j-th layer which is also an skeleton outgoing weight for one hidden node. We will prove oill (x) can be calculated as

oill (x) =

1

L j=l+1

wj

(Oill

)

и

Fill (v; x),

(29)

where Fill (v; x) is a function which is determined by v and the input x. If Eqn(29) is satisfied, the sign of olil (x) can be determined as following:

sgn(olil (x)) = sgn(wl+1(Oill )) и и и sgn(wL(Oill )) и sgn(Fill (v; x)).

(30)

Next we prove Eqn(29) by induction.

(1) For l = 1,

oi11 (x)

=

h0
w1(i0, i1)) и xi0
i0 =1

=

h0 i0 =1

vps (w1(i0, i1))

L j=2

wj

(Oi11

)

и xi0

=

L j=2

1 wj (Oi11 )

h0 i0 =1

ps(w1(i0,

i1)))

и

xi0 ,

(31)

where vps (w1(i0, i1))) is the value of basis path which contains w1(i0, i1) and wj(Oi11 ) is the

outgoing skeleton weight (free skeleton weight) of Oi11 . It means that Eqn(29) is satisfied with

Fi11 (v; x) =

h0 i0 =1

vps

(w1(i0,

i1)))

и

xi0

.

(2)

For

l

>

1,

suppose

that

ol-1
il-1

(x)

=

L j=l

1 wj (Oil-l-11 )

и

F l-1
il-1

(v

;

x),

hl-1

olil (x) =

wl(il-1,

il)

и

ol-1 il-1

il-1 =1

(32)

=

hl-1 il-1 =1

vps (wl(il-1, il)) j=l+1 wj (Oill )

и

ol-1 il-1

(33)

hl-1
=
il-1 =1

vps (wl(il-1, il))

L j=l+1

wj (Oill

)

и

w1 (Oil-l-11

)

и

l-1 j=1

wj

(Oil-l-11

)

и

L j=l

1 wj (Oil-l-11 )

и

F l-1 il-1

(v;

x)

(34)

=

1

L j=l+1

wj

(Oill

)

hl-1 il-1 =1

vps (wl(il-1, il))

w1(Oil-l-11 ) и

l-1 j=1

wj

(Oil-l-11

)

и

L j=l

1 wj (Oil-l-11 )

и

F l-1 il-1

(v;

x)

(35)

=

1

L j=l+1

wj

(Oill

)

hl-1 il-1 =1

vps (wl(il-1, il)) vpa (Oil-l-11 )

и

F l-1 il-1

(v;

x)

=

1

L j=l+1

wj

(Oill

)

и

Fill (v; x).

Thus we have finished the proof the second statement.

(36) (37)

14

Under review as a conference paper at ICLR 2019

11 APPENDIX INFORMATION OF THE EXPERIMENTS

11.1 UPDATE RULE OF G-SGD

Suppose that pi with i = 1, и и и , z is the basis path containing one non-basis edge (denoted as wi), and pj with j = z + 1, и и и , m - H is the basis path containing basis edges only, and wj is its basis edge at layer 1.
First, according to the ICR Method, we can get the update rule of value of skeleton paths as below,

vit+1

=

vit

-

t

wt i и wit vit

vjt+1

=

vjt

-

wjt t

и

(wt j

-

pi :wj

vt i

и

)vit
wjt

vj

Combined with the weight allocation method, we can get the update rule of G-SGD as follows:

wjt+1

=

wjt

-

t

и

wt j

и

(wjt )2

-

wjt и (vjt )2

pi:wj wit и wit

wit+1

=

wjt1

-  иt

wt j1 и(wit)2 (vit )2

Rt(pj : wi)

,

wkt+1 = wkt , wk = wi, wj

where Rt(pj : wi) is ratio of the basis path pj which contains non-basis weight wi.

11.2 SKELETON METHOD FOR RESNET AND ICR METHOD FOR BATCH NORMALIZATION

For ResNet, we implement the skeleton method to construct skeleton weights and basis paths in each residual block. Because of the skip-connection, there is an identity weight which doesn't change during the optimization. Thus, if the skip-connected weight connects node O, there isn't a positive scaling operator gO,c  G to make w  gO,c(w). So we can't construct basis paths for the whole network. The invariance of ResNet only exists in each residual block. In this sense, the equivalence of invariance is less severe than other neural network structure.

Because of the existence of the batch-normalization layers, the output of neural network with BN is

o^ij l

=

olij -х 2+

,

where

olij

means

the

i-th

output

in

layer-l

which

is

calculated

using

the

i-th

sample,

х

=

1 b

b i0 =1

olij

is

the

expectation

of

oij , i

=

1, и и и

,b

and

2

=

1 b

bi=1(oilj - х)2 is the variance.

Assume that olij = wloli-1 and the inputs oil-1 has expectation 0 and variance 1 (It can be roughly

satisfied for neural networks with BN.), we have 2  wl . Thus the loss function of the NN with

BN layer can be approximately represented as l(, wOin ; x, y) =

f (;x,y) O wOin

where wOin denotes

the incoming weights of node O and f (; x, y) denotes a function which is only related to .

Thus inverse-chain-rule for NN with BN layer can be approximated by the following equations. If w is an incoming weight of node O, we have

l(w; x, y)  z1 l(; x, y) и vi и 1 ,

w

i=1 vi

w wOin

(38)

which results in

l(w; x, y) и w

wOin

 z1 l(; x, y) и pi .

i=1 pi

w

(39)

Then

we

use

l(w;x,y) w

и

wOin

to w in the ICR methods.

15

Under review as a conference paper at ICLR 2019
11.3 INITIALIZATION METHOD OF SKELETON WEIGHTS
According to our analysis in the main paper, only the signs of skeleton weights matter the optimization. Thus we need to determine the signs of skeleton weights before training process. For the absolute value of skeleton weights, we can see from section 10.1 that different absolute value of skeleton weights well determine different scale of learning rate. Although our theoretical results show that the absolute value of skeleton weights can be randomly set, we choose them to be 1 for easier learning rate tuning and robustness.
In order to verify how signs of skeleton weights influence the performance. We test the performance for various combination of signs for them on image classification task (see section 5.2). Results shows that there are no differences for them. A intuitive explanation is that the selected network model is over-parameterized and the approximation ability will not be influenced by signs of skeleton weights. For simplicity, we initialize the value of skeleton weights as 1.
11.4 DETAILED TRAINING STRATEGIES IN SECTION 5.1
In this section, we extend our G-SGD to deep convolutional networks. CIFAR-10 and CIFAR-100 have been used in the experiment. We apply random crop to the input image by size of 32 with padding 4, and normalize each pixel value to [0,1]. We then apply random horizontal flipping to the image. The mini-batch size of 128 is used in this experiment. The training is conducted for 64k iterations. We follow the learning rate schedule strategy in the original paper (He et al., 2016), specifically, the initial learning rates of vanilla SGD and G-SGD are set to 1.0 and then divided by 10 after 32k and 48k iterations. The ResNet implementation can be found in https://github.com/pytorch/vision/ and the models are initialized by the default methods in PyTorch.
11.5 THE COMBINATION OF G-SGD AND REGULARIZATION
The optimization algorithms achieve better generalization performance on test dataset by combining with proper regularization methods. In the previous experiments, we focus on the difference performance of optimization algorithms. In this section, we conduct the experiments to investigate the combination of G-SGD and regularization. In weight space, weight norm is widely used as regularization for ReLU networks (He et al., 2016; Huang et al., 2017). Recently, (Zheng et al., 2018) propose the basis path norm in G-space. In this section, we reproduce the experiments in (He et al., 2016; Zheng et al., 2018) on SGD regularized by weight norm (SGD+WD) and G-SGD regularized by basis path norm (G-SGD+BPR), and extend them on CIFAR-100 dataset. The learning rate of 1.0 is widely used to train ResNet model and its variants on CIFAR dataset, hence we employ it in our experiment as well. We do a wide range grid search for the hyper-parameter  for weight decay and basis path regularization from {0.1, 0.2, 0.5} О 10-, where   {3, 4, 5, 6}, and report the best performance based on the CIFAR-10 validation set. We use the same hyper-parameter on CIFAR-100 dataset.
11.6 DETAILED TRAINING STRATEGIES IN SECTION 5.2
In this section, our aim is to verify the influence of invariance to optimization in weight space. We train several 2-hidden-layer MLP models with different invariant ratio (i.e. H/m) on Fasion dataset. The original size of input image is 28 О 28. We normliazed the input data, and to reduce the dimensions of input feature, we downsample the image to 7 О 7 by using average pooling. The network structue is followed by [49:h:h:10] where h is the number of hidden nodes in each layer. The detailed model properties are shown in table 5. All models are initialized by (He et al., 2015) except the skeleton weights which is mentioned in Section 4 without explicit note. We use the learning rate of 0.01 and mini-batch size of 64 for vanilla SGD and G-SGD, and train each model for 100 epochs.
As shown in Figure 7 and 8, the G-SGD achieves clearly better performance than SGD with combining regularization method on all models and all datasets. To be specific, the test accuracy of 94.29% is gained by ResNet-34 trained by SGD and weight decay on CIFAR-10 dataset (the number reported in (He et al., 2016) is 91.25%), while G-SGD with basis path regularization improves the test accuracy to 94.67%. On CIFAR-100 dataset, the test accuracy of ResNet-34 trained by SGD and weight decay is 74.39% (the ResNet-34 result on CIFAR-100 hasn't been reported in He et al. (2016), a result of ResNet-110 with similar training strategy on this dataset is reported in Zagoruyko & Komodakis
16

Under review as a conference paper at ICLR 2019

Table 5: Network information in Section 5.2.

#hidden nodes 8

16 32 64 128 256 512 1024

#weights

536

1200

2912

7872

23936 80640 292352 1108992

H

16

32

64

128

256

512

1024

2048

invariant ratio 1.49О10-2 1.33О10-2 1.10О10-2 8.13О10-3 5.35О10-3 3.17О10-3 1.75О10-3 9.23О10-4

PlainNet34

Training Loss
SGD

0.95

100

SGD+WD -SGD

0.94

10 1 -SGD+BPR

0.93

10 2

0.92

10 3 0 20 40 60 80 100 120 140 160

80

0.95

100 0.94 10 1 10 2 0.93

10 3 0.92

0 20 40 60 Ep8o0ch 100 120 140 160

80

Test Accuracy
100 120 140 160 100 Epo1c2h0 140 160

ResNet34

Figure 7: Training loss and test accuracy w.r.t. the number of effective passes on CIFAR-10 dataset.

(2016) which is 74.84%), while G-SGD with basis path regularization improves the test accuracy to 75.20%. The experimental results verify our analysis again that optimization in G-space is a better choice.
11.7 DETAILED TRAINING STRATEGIES IN SECTION 5.3
Path-SGD (Neyshabur et al., 2015) also notice the positive scale invariance in neural networks with linear or ReLU activation. Instead of optimizing the loss function in G-space, they use path norm as regularizer to the gradient in weight space. Meanwhile, the dependency among all paths hasn't been noted, which leads to the computation overhead of the gradient of path norm is very high. In section 5.3, we extend the experiments in (Neyshabur et al., 2015) to G-SGD on MNIST and CIFAR-10 dataset. A 5-hidden-layer MLP model is used in this experiment with 64 units in each layer. We do grid search for the learning rate of each algorithm from 1.0 О 10-, where  is an integer between 0 to 10. We report the best result for each algorithm. The mini-batch size of 64 is used, and the input images of gray scale are normalized to [0, 1].

17

Under review as a conference paper at ICLR 2019

PlainNet34

100 10 1 10 2
0
100 10 1 10 2 10 3 0

Training Loss

SGD SGD+WD -SGD -SGD+BPR

0.80 0.75 0.70

0.65

20 40 60 80 100 120 140 160 0.60 80 0.80 0.78 0.76 0.74 0.72
20 40 60 Ep8o0ch 100 120 140 160 0.70 80

Test Accuracy
100 120 140

100 Epo1c2h0

140

160 160

ResNet34

Figure 8: Training loss and test accuracy w.r.t. the number of effective passes on CIFAR-100 dataset.

18


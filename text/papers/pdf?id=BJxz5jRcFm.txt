Under review as a conference paper at ICLR 2019
TANGENT-NORMAL ADVERSARIAL REGULARIZATION FOR SEMI-SUPERVISED LEARNING
Anonymous authors Paper under double-blind review
ABSTRACT
The ever-increasing size of modern datasets combined with the difficulty of obtaining label information has made semi-supervised learning of significant practical importance in modern machine learning applications. In comparison to supervised learning, the key difficulty in semi-supervised learning is how to make full use of the unlabeled data. In order to utilize manifold information provided by unlabeled data, we propose a novel regularization called the tangent-normal adversarial regularization, which is composed by two parts. The two parts complement with each other and jointly enforce the smoothness along two different directions that are crucial for semi-supervised learning. One is applied along the tangent space of the data manifold, aiming to enforce local invariance of the classifier on the manifold, while the other is performed on the normal space orthogonal to the tangent space, intending to impose robustness on the classifier against the noise causing the observed data deviating from the underlying data manifold. Both of the two regularizers are achieved by the strategy of virtual adversarial training. Our method has achieved state-of-the-art performance on semi-supervised learning tasks on both artificial dataset and practical datasets.
1 INTRODUCTION
The recent success of supervised learning (SL) models, like deep convolutional neural networks, highly relies on the huge amount of labeled data. However, though obtaining data itself might be relatively effortless in various circumstances, to acquire the annotated labels is still costly, limiting the further applications of SL methods in practical problems. Semi-supervised learning (SSL) models, which requires only a small part of data to be labeled, does not suffer from such restrictions. The advantage that SSL depends less on well-annotated datasets makes it of crucial practical importance and draws lots of research interests. The common setting in SSL is that we have access to a relatively small amount of labeled data and much larger amount of unlabeled data. And we need to train a classifier utilizing those data. Comparing to SL, the main challenge of SSL is how to make full use of the huge amount of unlabeled data, i.e., how to utilize the marginalized input distribution p(x) to improve the prediction model i.e., the conditional distribution of supervised target p(y|x). To solve this problem, there are mainly three streams of research.
The first approach, based on probabilistic models, recognizes the SSL problem as a specialized missing data imputation task for classification problem. The common scheme of this method is to establish a hidden variable model capturing the relationship between the input and label, and then applies Bayesian inference techniques to optimize the model (Kingma et al., 2014; Zhu et al., 2003; Rasmus et al., 2015). Suffering from the estimation of posterior being either inaccurate or computationally inefficient, this approach performs less well especially in high-dimensional dataset (Kingma et al., 2014).
The second line tries to construct proper regularization using the unlabeled data, to impose the desired smoothness on the classifier. One kind of useful regularization is achieved by adversarial training (Goodfellow et al., 2014b), or virtual adversarial training (VAT) when applied to unlabeled data (Miyato et al., 2016; 2017). Such regularization leads to robustness of classifier to adversarial examples, thus inducing smoothness of classifier in input space where the observed data is presented. The input space being high dimensional, though, the data itself is concentrated on a underlying manifold of much lower dimensionality (Cayton, 2005; Narayanan & Mitter, 2010; Chapelle et al., 2009;
1

Under review as a conference paper at ICLR 2019

Rifai et al., 2011). Thus directly performing VAT in input space might overly regularize and does potential harm to the classifier. Another kind of regularization called manifold regularization aims to encourage invariance of classifier on manifold (Simard et al., 1998; Belkin et al., 2006; Niyogi, 2013; Kumar et al., 2017; Rifai et al., 2011), rather than in input space as VAT has done. Such manifold regularization is implemented by tangent propagation (Simard et al., 1998; Kumar et al., 2017) or manifold Laplacian norm (Belkin et al., 2006; Lecouat et al., 2018), requiring evaluating the Jacobian of classifier (with respect to manifold representation of data) and thus being highly computationally inefficient.
The third way is related to generative adversarial network (GAN) (Goodfellow et al., 2014a). Most GAN based approaches modify the discriminator to include a classifier, by splitting the real class of original discriminator into K subclasses, where K denotes the number of classes of labeled data (Salimans et al., 2016; Odena, 2016; Dai et al., 2017; Qi et al., 2018). The features extracted for distinguishing the example being real or fake, which can be viewed as a kind of coarse label, have implicit benefits for supervised classification task. Besides that, there are also works jointly training a classifier, a discriminator and a generator (Li et al., 2017).
Our work mainly follows the second line. We firstly sort out three important assumptions that motivate our idea:

The manifold assumption The observed data presented in high dimensional space is with high
probability concentrated in the vicinity of some underlying manifold of much lower di-
mensionality (Cayton, 2005; Narayanan & Mitter, 2010; Chapelle et al., 2009; Rifai et al., 2011). We denote the underlying manifold as M. We further assume that the classification task concerned relies and only relies on M (Rifai et al., 2011).
The noisy observation assumption The observed data x can be decomposed into two parts as x = x0 + n, where x0 is exactly supported on the underlying manifold M and n is some noise independent of x0 (Bengio et al., 2013; Rasmus et al., 2015). With the assumption that the classifier only depends on the underlying manifold M, the noise part might have undesired influences on the learning of the classifier.
The semi-supervised learning assumption If two points x1, x2  M are close in manifold distance, then the conditional probability p(y|x1) and p(y|x2) are similar (Belkin et al., 2006; Rifai et al., 2011; Niyogi, 2013). In other words, the true classifier, or the true condition distribution p(y|X) varies smoothly along the underlying manifold M.

Inspired by the three assumptions, we introduce a novel regularization

called the tangent-normal adversarial regularization (TNAR), which is composed by two parts. The tangent adversarial regularization (TAR) induces the smoothness of the classifier along the tangent space of the underlying manifold, to enforce the invariance of the classifier along

x <latexit sha1_base64="3yfXwZgeZQAuEFBzg4J1LtazqaA=">AAACLHicbVDRStxAFJ1oW21qa7SPfbm4LCgtSyKCgghSX/pSUXDdhU0aJrMTd3AyCTM3siHkg/rirwilDxXx1e9wds1Dqz1w4XDOvdx7T1JIYdD3b52FxVev3ywtv3Xfrbz/sOqtrZ+bvNSM91kucz1MqOFSKN5HgZIPC81plkg+SC6PZv7gimsjcnWGVcGjjF4okQpG0Uqxd9QNa5jG4gtUsQibuBYHQfPj2O1WcADp5nQfQpxwpFtuF8KM4oRRWX9v3Km1p7EPn0FB7HX8nj8HvCRBSzqkxUns/QrHOSszrpBJaswo8AuMaqpRMMkbNywNLyi7pBd8ZKmiGTdRPX+2ga5VxpDm2pZCmKt/T9Q0M6bKEts5u9c892bi/7xRieleVAtVlMgVe1qUlhIwh1lyMBaaM5SVJZRpYW8FNqGaMrT5ujaE4PnLL8n5di/we8HpTufwaxvHMvlENsgmCcguOSTfyAnpE0Z+khvyh9w6185v5865f2pdcNqZj+QfOA+PHW+jzw==</latexit>

=

x0 + n

r? <latexit sha1_base64="fjEyY9J5CGfUlRvp4tP75ZZaAts=">AAACOHicbVBNSxxBEO0xH+qYxDU5eimyLCgJy4wICiJIvOSiMZBVYWcz9PTWuI09PUN3jewwzM/KxZ+RW/DiQRGv+QXpXfeQaB40vH6viqp6SaGkpSD45c09e/7i5fzCor/06vWb5dbK22Obl0ZgT+QqN6cJt6ikxh5JUnhaGORZovAkOd+f+CcXaKzM9TeqChxk/EzLVApOTopbXzpRDeNYfoQqllET13I3bL4f+p0KdiFdG+9ARCMkvu53IMo4jQRX9UHjfmNXMI4D+ADaN3EdFWiKBuJWO+gGU8BTEs5Im81wFLd+RsNclBlqEopb2w+DggY1NySFwsaPSosFF+f8DPuOap6hHdTTwxvoOGUIaW7c0wRT9e+OmmfWVlniKie728feRPyf1y8p3R7UUhcloRYPg9JSAeUwSRGG0qAgVTnChZFuVxAjbrggl7XvQggfn/yUHG90w6Abft1s732axbHAVtl7tsZCtsX22Gd2xHpMsB/sit2wW+/Su/buvPuH0jlv1vOO/QPv9x86hahy</latexit>

rk
<latexit sha1_base64="WedWKiSa3rKJviC9jqjnds1JrKQ=">AAACPHicbVBNSxxBEO0xxo8x6iY55lK4LCjKMiOCgSBIvOSSYEhWhZ116Omt2W3s6Rm6a8IOw/ywXPwRueXkxYMh5Jpzetc9JJoHDa/fq6KqXlIoaSkIvnsLTxafLi2vrPprz9Y3NlvPX5zZvDQCeyJXublIuEUlNfZIksKLwiDPEoXnydXJ1D//gsbKXH+mqsBBxkdaplJwclLc+tSJapjEcg+qWEZNXMujsLn84HcqOIJ0e/IGIhoj8R2/A1HGaSy4qt837jdxBZM4gF3QvonrqOCGK4WqgbjVDrrBDPCYhHPSZnOcxq1v0TAXZYaahOLW9sOgoEHNDUmhsPGj0mLBxRUfYd9RzTO0g3p2fAMdpwwhzY17mmCm/t1R88zaKktc5XR/+9Cbiv/z+iWlrwe11EVJqMX9oLRUQDlMk4ShNChIVY5wYaTbFcTYhSDI5e27EMKHJz8mZ/vdMOiGHw/ax2/ncaywV2yLbbOQHbJj9o6dsh4T7Cu7YXfsh3ft3Xo/vV/3pQvevOcl+wfe7z+eO6ow</latexit>

x0 <latexit sha1_base64="UNhdvP2qo+KQA6IoxLC3arqMDFU+uf5oEh2b4YdHw38rKt8TZ6s4oJa7lDVtSdjH1OqVX8aIQK2T3QpSMvCEROilcVK7MWDn+XYJkySUYw=">AAACBPM2nX3icbVZBDBNSax9gttxMABxEFFIHz2X0l5vjTRa16eZLoK8qoqj6XVbJp2eq5tI1ymcr7vxNGT81WTE6WpKiMGguTkCIgFNokZzLGZbiCn5kQFJJUpyISuWiAaHgFEhCGZYTkLwrb1Hl5PZ6pECapFOUFS5wRmWVhBM2/d5AgjkDZ54LYDbETrmas+l8vik1tMbKtyR7F/R62Gan8hSZW1DUoH+nh0rd+sBFUIjbz2osAq5EQFM+80fJm8CWft9l93y5ykvDmWof/30wpIrlzmcq0JMdvXbcePDbw4mQSrgU+4JXFzvnk+fvaxnsIYZvy9l+vSeGBc1hXDuQ+lmRULnhQOR9YgUeBq4PNctdP91e07GVbW52yWbidnF3dRFbk2Zp/ka+VUgCSB4fGjPLu3S8g3v8fHxNt7f0l+5z4jw8k+Y/n9ORP7NzZ9m601o12vrnKPfj/ztus0Hrbfgrujf92Fs4syi71lWuezPl3n73e053j8+3xnqxmXMzF+DU0pRXsOpjUTsr25Cjpk9rVMyJDelgCCZpKcp8vTLyhP1gYRzF6tcyUYLRAFUMSfymkObCUfFjD6JggFfCCkE0g8Kih7lLH7QGzC+mgyRsgaLFSWTFWBLRJ4XhkJTGFzpGzQE0U/rbMeXFPZM2/7rrTU4+CoWyzuMRvgtQU7WR5aCqCvkNqT7qSOIC6qdxogYYyK5HZFaTGrBBpaTN/AO2TEdqzLMUZpMQWSsC2IBOIkE5WVNPxdLioDhCqTrcb91Y8ABb5a1ONaWbyAj+wclpSGXyMShSxYQ17TnKWitPDxADSTUhJ1qFJCEU4xlIaVWRr0uldhh6KPEsX2nFpVf+Bxf9USbcee92Ub3NrXsS2yVda2A5FeoWwj7bT8gkv1a9L+ddsixUzIwTjUcAGXaUOS4SBzP7AyL7QGcgv32tUXXsPVN0RbSxMNBmE7jgRkOYsRuOtzZRKjPpmxlzT/N5xz82iZ6pfKa2dSdukIg27tVAY0JrhWHNh+LI4B5Qt9eoYpfkstUp1ve2k3VO497iX4jRyuoYDqKDZB0HBh9RbSnWONous3rTt2HUpJjrdXxNLZzlDzmPthyoe2N1Bv7V1GKsO6aVw23f1LMQPyjSo/8cLUkN+BWGiUvi2WRQYldtI4ZS1uD2EsH6ulzXDdWEZVa/EsBSj3LXwaOTJrVHQT5lZ6AaKeghQCq0KI5VKZouWMz5WZwsTIMGtfdgjCH4ma/0KDGJvawNcCT0khcn0JkIChNzicRA6LxohGuwcjoYX0oaqPSKcbLD3BQlEMRYbDfk4nGJg/+NlA1C3WywCHQbjxawE8mdLZhqF3eCEoxHjRYSD3ITWbwc0Gkn32/VgJ9+5ND6fR7dz7kupoXWldbmxBldNpuELn34cwS1OMuNfYKOYAXp2J6TpKnMCadRvWM2nFL/NX9u42EmFqEESIeJyWNcRV3YAtJUljHSiDWd9iSCtydsBL9EzQv9gXiohXmCIvX48il1BauXiTer+j5yv2QfYmn91KTp3HS45xOrn8Oz23sPja0Du16q5KpNmSo5LLkq6ElC9dLCxQLHOU/4xHI4Zy+fR1S8nzUKxEMjuH84Kl4KxWadZIo2sTzl4WpG<WD/0dOKmlhpaDg0tlaeMxTMBlsRiLCtF>GBhBdYcGwCru/rot8XyX9yMKue+daVMzGMpC4h3Kl42NHc+md67eh6EGr48Q/NG8XA/fk1+n+S/EwT4YUnDanyI8Z+96Mrevu+ttfF/1hfAztE7u3xqwWnD/6h93uMwj6aBjmJSC3FX8y5ghzBhX3yp4SEBDJQv+fH8QJIBMXwe3Fmy/T4gzZQ+pySuTUD4ANeAHbgk7nK5V+/1lS1O7Wfd/p4CJK9f1/znrXWXvmznwp3j3/7nPtDn/MFL3jml1Z3bGDj83jB5rg6mq5UFosNDy=8p<U5/87yl/af/te7wexCO50i+tqag>oAE=</latexit>

M <latexit sha1_base64="jZjtDIteAF0fbOtpX/k4oqqXCsw=">AAACHXicbVDLSsNAFJ3UV42vqks3g6WgICURQUEKRTdulAq2FpoaJtNJOzh5MHMjDSE/4sZfceNCERduxL9x+lj4OnDhcM693HuPFwuuwLI+jcLM7Nz8QnHRXFpeWV0rrW+0VJRIypo0EpFse0QxwUPWBA6CtWPJSOAJdu3dno786zsmFY/CK0hj1g1IP+Q+pwS05JYOKk6Ghy7fw6nLndzNeM3Oby7MSopr2N8ZHmMHBgzIrukEBAaUiOw8d0tlq2qNgf8Se0rKaIqGW3p3ehFNAhYCFUSpjm3F0M2IBE4Fy00nUSwm9Jb0WUfTkARMdbPxdzmuaKWH/UjqCgGP1e8TGQmUSgNPd45OVL+9kfif10nAP+pmPIwTYCGdLPITgSHCo6hwj0tGQaSaECq5vhXTAZGEgg7U1CHYv1/+S1r7Vduq2pcH5frJNI4i2kLbaAfZ6BDV0RlqoCai6B49omf0YjwYT8ar8TZpLRjTmU30A8bHF+U0n/U=</latexit>

manifold. And the normal adversarial regularization (NAR) penalizes

the deviation of the classifier along directions orthogonal to the tangent space, to impose robustness on the classifier against the noise carried in observed data. The two regularization terms enforce different aspects of the classifier's smoothness and jointly improve the generalization performance, as demonstrated in Section 4.
To realize our idea, we have two challenges to conquer: how to estimate the underlying manifold and how to efficiently perform TNAR.
For the first issue, we take advantage of the generative models equipped with an extra encoder, to characterize coordinate chart of manifold (Kumar et al., 2017; Lecouat et al., 2018; Qi et al., 2018). More specifically, in this work we choose variational autoendoer (VAE) (Kingma & Welling, 2013) and localized GAN (Qi et al., 2018) to estimate the underlying manifold from data.
For the second problem, we develop an adversarial regularization ap-

Figure 1: Illustration for tangent-normal adversarial regularization. x = x0 + n is the observed data, where x0 is exactly supported on the underlying manifold M and n is the noise independent of x0. r is the adversarial perturbation along the tangent space to induce invariance of the classifier on manifold; r is the adversarial perturbation along the normal space to impose robustness on the classifier against noise n.

proach based on virtual adversarial training (VAT) (Miyato et al., 2017).

Different from VAT, we perform virtual adversarial training in tangent space and normal space sep-

arately as illustrated in Figure 1, which leads to a number of new technical difficulties and we will

2

Under review as a conference paper at ICLR 2019

elaborate the corresponding solutions later. Compared with the traditional manifold regularization methods based on tangent propagation (Simard et al., 1998; Kumar et al., 2017) or manifold Laplacian norm (Belkin et al., 2006; Lecouat et al., 2018), our realization does not require explicitly evaluating the Jacobian of classifier. All we need is to calculate the derivative of matrix vector product, which only costs a few times of back or forward propagation of network.

2 BACKGROUND

2.1 NOTATIONS
We denote the labeled and unlabeled dataset as Dl = {(xl, yl)} and Dul = {xul} respectively, thus D := Dl  Dul is the full dataset. The output of classification model is written as p(y|x, ), where  is the model parameters to be trained. We use (·, ·) to represent supervised loss function. And the regularization term is denoted as R with specific subscript for distinction. The observed space of x is written as RD. And the underlying manifold of the observed data x is written as M = Rd, d D. We use z for the manifold representation of data x. We denote the decoder, or the generator, as x = g(z) and the encoder as z = h(x), which form the coordinate chart of manifold together. If not stated otherwise, we always assume x and z correspond to the coordinate of the same data point in observed space RD and on manifold M, i.e., g(z) = x and h(x) = z. The tangent space of M at point x is TxM = Jzg(Rd) = Rd, where Jzg is the Jacobian of g at point z. The tangent space TxM is also the span of the columns of Jzg. For convenience, we define J := Jzg.
The perturbation in the observed space RD is denoted as r  RD, while the perturbation on the manifold representation is denoted as   Rd. Hence the perturbation on manifold is g(z + ) - g(z)  RD. When the perturbation  is small enough for the holding of the first order Taylor's expansion, the perturbation on manifold is approximately equal to the perturbation on its tangent space, g(z + ) - g(z)  J ·   TxM. Therefore we say a perturbation r  RD is actually on manifold, if there is a perturbation   Rd, such that r = J · .

2.2 VIRTUAL ADVERSARIAL TRAINING

VAT (Miyato et al., 2017) is an effective regularization method for SSL. The virtual adversar-
ial loss introduced in VAT is defined by the robustness of the classifier against local perturbation in the input space RD. Hence VAT imposes a kind of smoothness condition on the classifier. Mathematically, the virtual adversarial loss in VAT for SSL is L(Dl, Dul, ) := E(xl,yl)Dl (yl, p(y|xl, )) + ExDRvat(x, ), where the VAT regularization Rvat is defined as Rvat(x; ) := max r 2 dist(p(y|x, ), p(y|x + r, )), where dist(·, ·) is some distribution distance measure and controls the magnitude of the adversarial example. For simplicity, define

F (x, r, ) := dist(p(y|x, ), p(y + r, )).

(1)

Then Rvat arg max r

= max r 2 F (x, r, ).  F (x, r, ). Once we have

The so called virtual adversarial r, the VAT loss can be optimized

example is r := with the objective as

L(Dl, Dul, ) = E(xl,yl)Dl (yl, p(y|xl, )) + ExDF (x, r, ).

To obtain the virtual adversarial example r, Miyato et al. (2017) suggested to apply second order

Taylor's expansion to F (x, r, ) around r = 0 as

F (x, r, )



1 2

rT

H

r,

(2)

where H := 2rF (x, r, )|r=0 denotes the Hessian of F with respect to r. The vanishing of the first two terms in Taylor's expansion occurs because that dist(·, ·) is a distance measure with minimum

zero and r = 0 is the corresponding optimal value, indicating that at r = 0, both the value and the

gradient of F (x, r, ) are is an eigenvalue problem

zero. Therefore and the direction

for small of r can

enough , be solved

r by

 arg power

max r 2 iteration.

1 2

rT

H

r,

which

2.3 GENERATIVE MODELS FOR DATA MANIFOLD
We take advantage of generative model with both encoder h and decoder g to estimate the underlying data manifold M and its tangent space TxM. As assumed by previous works (Kumar et al., 2017;

3

Under review as a conference paper at ICLR 2019

Lecouat et al., 2018), perfect generative models with both decoder and encoder can describe the data
manifold, where the decoder g(z) and the encoder h(x) together serve as the coordinate chart of manifold M. Note that the encoder is indispensable for it helps to identify the manifold coordinate z = h(x) for point x  M. With the trained generative model, the tangent space is given by TxM = Jzg(Rd), or the span of the columns of J = Jzg.

In this work, we adopt VAE (Kingma & Welling, 2013) and localized GAN (Qi et al., 2018) to learn the targeted underlying data manifold M as summarized below.

VAE VAE (Kingma & Welling, 2013) is a well known generative model consisting of both encoder and decoder. The training of VAE is by optimizing the variational lower bound of log likelihood,

log p(x, )  Ezq(z|x,) log p(x|z, ) - KL(q(z|x, ) p(z)).

(3)

Here p(z) is the prior of hidden variable z, and q(z|x, ), p(x|z, ) models the encoder and decoder in VAE, respectively. The derivation of the lower bound with respect to  is well defined thanks to the reparameterization trick, thus it could be optimized by gradient based method. The lower bound
could also be interpreted as a reconstruction term plus a regularization term (Kingma & Welling, 2013). With a trained VAE, the encoder and decoder are given as h(x) = arg maxz q(z|x) and g(z) = arg maxx q(x|z) accordingly.

Localized GAN Localized GAN (Qi et al., 2018) suggests to use a localized generator G(x, z) to

replace the global generator g(z) in vanilla GAN Goodfellow et al. (2014a). The key difference

between localized GAN and previous generative model for manifold is that, localized GAN learns a

distinguishing local coordinate chart for each point x  M, which is given by G(x, z), rather than

one global coordinate chart. To model the local coordinate chart in data manifold, localized GAN re-

quires the localized generator to satisfy two more regularity conditions: 1) locality: G(x, 0) = x, so

that G(x, z) is localized around x; 2) orthogonmality:

 G(x,z) z

T

 G(x,z) z

=

I, to ensure

G(x, z)

is non-degenerated. The two conditions are achieved by the following penalty during training of

localized GAN:

Rlocalized GAN := µ1 G(x, 0) - x 2 + µ2

G(x, z) z

T

G(x, z

z)

-

I

2
.

Since G(x, z) defines a local coordinate chart for each x separately, in which the latent encode of x is z = 0, there is no need for the extra encoder to provide the manifold representation of x.

3 METHOD

In this section we elaborate our proposed tangent-normal adversarial regularization (TNAR) strategy. The TNAR loss to be minimized for SSL is
L(Dl, Dul, ) := E(xl,yl)Dl yl, p(y|xl, ) +1ExDRtangent(x, )+2ExDRnormal(x, ). (4)
The first term in Eq. (4) is a common used supervised loss. Rtangent and Rnormal is the so called tangent adversarial regularization (TAR) and normal adversarial regularization (NAR) accordingly, jointly forming the proposed TNAR. We assume that we already have a well trained generative model for the underlying data manifold M, with encoder h and decoder g, which can be obtained as described in Section 2.3.

3.1 TANGENT ADVERSARIAL REGULARIZATION

Vanilla VAT penalizes the variety of the classifier against local perturbation in the input space RD (Miyato et al., 2017), which might overly regularize the classifier, since the semi-supervised
learning assumption only indicates that the true conditional distribution varies smoothly along the underlying manifold M, but not the whole input space RD (Belkin et al., 2006; Rifai et al., 2011;
Niyogi, 2013). To avoid this shortcoming of vanilla VAT, we propose the tangent adversarial regu-
larization (TAR), which restricts virtual adversarial training to the tangent space of the underlying manifold TxM, to enforce manifold invariance property of the classifier.

Rtangent(x; ) :=

max

F (x, r, ),

r 2 ,rTxM=Jz g(Rd)

(5)

4

Under review as a conference paper at ICLR 2019

where F (x, r, ) is defined as in Eq. (1). To optimize Eq. (5), we first apply Taylor's expansion to

F (x, r, )

so

that

Rtangent(x; )



max

r

2

,rTxM=Jz g(Rd)

1 2

rT

H

r,

where

the

notations

and

the

derivation are as in Eq. (2). We further reformulate Rtangent as

maximize
rRD

1 2

rT

H

r,

s.t. r 2  , r = J . (  Rd, J := Jzg  RD×d, H  RD×D) (6)

Or equivalently,

maximize 1 T JT HJ,

Rd

2

s.t. T J T J   2.

(7)

This is a classic generalized eigenvalue problem, the optimal solution  of which could be obtained

by power iteration and conjugate gradient (and scaling). The iteration framework is as

v  JT HJ;

µ  (J T J )-1v;

  µ/ µ 2 .

(8)

Now we elaborate the detailed implementation of each step in Eq. (8).

Computing JT HJ. Note that z = h(x), x = g(z). Define r() := g(z + ) - g(z). For F x, r(),  = dist(p(y|x, ) p(y|x + r(), )), we have 2F (x, r(), ) = (Jz+g)T r2F (x, r(), )(Jz+g) + 2g(z + ) · rF (x, r(), ). While on the other hand, since dist(·, ·) is some distance measure with minimum zero and r(0) = 0 is the corresponding optimal value, we have F (x, r(0), ) = 0, rF (x, r(0), ) = 0. Therefore, 2F (x, r(0), ) = (Jzg)T r2F (x, r(0), )Jzg = JT HJ. Thus the targeted matrix vector product could be efficiently computed as JT HJ = 2F (x, r(0), ) ·  =  F (x, r(0), ) ·  . Note that F (x, r(0), ) ·  is a scalar, hence the gradient of which could be obtained by back propagat-
ing the network for once. And it only costs twice back propagating for the computation of JT HJ.

Solving JT Jµ = v. Similarly, define K() := g(z + ) - g(z) T g(z + ) - g(z) . We have
2K() = (Jz+g)T Jz+g + 2g(z + ) · K(). Since K(0) = 0, we have 2K(0) = (Jzg)T Jzg = JT J. Thus the matrix vector product JT Jµ could be evaluated similarly as JT Jµ =  K(0) · µ . The extra cost for evaluating JT Jµ is still back propagating the net-
work for twice. Due to JT J being positive definite (g is non-degenerated), we can apply several steps of conjugate gradient to solve JT Jµ = v efficiently.

By iterating Eq. (8), we obtain the optimal solution  of Eq. (7). The desired optimal solution is then r = J / J , using which we obtain Rtangent(x; ) = F (x, r , ).

Compared with manifold regularization based on tangent propagation (Simard et al., 1998; Kumar et al., 2017) or manifold Laplacian norm (Belkin et al., 2006; Lecouat et al., 2018), which is computationally inefficient due to the evaluation of Jacobian, our proposed TAR could be efficiently implemented, thanks to the low computational cost of virtual adversarial training.

3.2 NORMAL ADVERSARIAL REGULARIZATION

Motivated by the noisy observation assumption indicating that the observed data contains noise driving them off the underlying manifold, we come up with the normal adversarial regularization (NAR) to enforce the robustness of the classifier against such noise, by performing virtual adversarial training in the normal space. The mathematical description is

Rnormal(x; ) :=

r

max F (x, r, ) 
2 ,rTxM

r

max
2 ,rTxM

1 2

rT

H r.

(9)

Note that TxM is spanned by the coloums of J = Jzg, thus rTxM  JT · r = 0. Therefore we could reformulate Eq. (9) as

maximize
rRD

1 2

rT

H

r,

s.t. r 2  , JT · r = 0.

(10)

However, Eq. (10) is not easy to optimize since JT · r cannot be efficiently computed. To overcome this, instead of requiring r being orthogonal to the whole tangent space TxM, we take a step back to

5

Under review as a conference paper at ICLR 2019

demand r being orthogonal to only one specific tangent direction, i.e., the tangent space adversarial perturbation r . Thus the constraint JT · r = 0 is relaxed to (r )T · r = 0. And we further replace
the constraint by a regularization term,

maximize 1 rT Hr - rT (r rT )r,

rRD

2

s.t. r 2  ,

(11)

where  is a hyperparameter introduced to control the orthogonality of r.

Since Eq. (11) is again an eigenvalue problem, and we can apply power iteration to solve it. Note

that a small identity matrix  r

I

is

needed

to

be

added

to

keep

1 2

H

- r

rT

+

r

I semi-

positive definite, which does not change the optimal solution of the eigenvalue problem. The power

iteration is as

r

1 2

H

r

-

(r

)T r

r+

r

r.

(12)

And the evaluation of Hr is by Hr = r rF (x, 0, ) · r , which could be computed efficiently. After finding the optimal solution of Eq. (11) as r, the NAR becomes Rnormal(x, ) = F (x, r, ).
Finally, as in (Miyato et al., 2017), we add entropy regularization to our loss function. It ensures neural networks to output a more determinate prediction and has implicit benefits for performing virtual adversarial training, Rentropy(x, ) := - y p(y|x, ) log p(y|x, ). Our final loss for SSL is

L(Dl, Dul, ) :=E(xl,yl)Dl yl, p(y|xl, ) + 1ExDRtangent(x, ) + 2ExDRnormal(x, ) + 3ExDRentropy(x, ).

(13)

The TAR inherits the computational efficiency from VAT and the manifold invariance property from traditional manifold regularization. The NAR causes the classifier for SSL being robust against the off manifold noise contained in the observed data. These advantages make our proposed TNAR, the combination of TAR and NAR, a reasonable regularization method for SSL, the superiority of which will be shown in the experiment part in Section 4.

4 EXPERIMENTS
To demonstrate the advantages of our proposed TNAR for SSL, we conduct a series of experiments on both artificial and real dataset. The compared methods for SSL include: 1) SL: supervised learning using only the labeled data; 2) VAT: vanilla VAT (Miyato et al., 2017); 3) TNAR-VAE: the proposed TNAR method, with the underlying manifold estimated by VAE; 4) TNAR-LGAN: the proposed TNAR method, with the underlying manifold estimated by localized GAN; 5) TNARManifold: the proposed TNAR method with oracle underlying manifold for the observed data, only used for artificial dataset; 6) TNAR-AE: the proposed TNAR method, with the underlying manifold estimated roughly by autoendoer, only used for artificial dataset. 7) TAR: the tangent adversarial regularization, used in ablation study. 8) NAR: the normal adversarial regularization, used in ablation study. If not stated otherwise, all the above methods contain entropy regularization term.
4.1 TWO-RINGS ARTIFICIAL DATASET
We introduce experiments on a two-rings artificial dataset to show the effectiveness of our proposed methods intuitively. In this experiments, there is 3, 000 unlabeled data (gray dots) and 6 labeled data (blue dots), 3 for each class. The detailed construction could be found in Appendix.
The performance of each compared methods is shown in Table 1, and the corresponding classification boundary is demonstrated in Figure 2. The TNAR under true underlying manifold (TNARManifold) perfectly classifies the two-rings dataset with merely 6 labeled data, while the other methods fail to predict the correct decision boundary. Even with the underlying manifold roughly approximated by an autoendoer, our approach (TNAR-AE) outperforms VAT in this artificial dataset. However, the performance of TNAR-AE is worse than TNAR-Manifold, indicating that the effectiveness of TNAR relies on the quality of estimating the underlying manifold.

6

Under review as a conference paper at ICLR 2019

1.0

0.5

SL

0.0

VAT TNAR-AE

TNAR-Manifold

-0.5

Table 1: Classification errors (%) of compared methods on two-ring artificial dataset. We test with and without entropy regularization in each method and report the best one. In VAT and TNAR-AE, without entropy regularization is better; For TNAR-Manifold, adding entropy regularization is better.

-1.0

-1.0 -0.5

0.0

0.5

1.0

Figure 2: The decision boundaries of compared methods on two-rings artificial dataset. Gray dots distributed on two rings: the unlabeled data. Blue dots (3 in each ring): the labeled data. Colored curves: the decision boundaries found by compared methods.

Model SL VAT TNAR-AE TNAR-Manifold TNAR-Manifold (ent)

Error (%) 32.95 23.80 12.45 9.90 0

4.2 FASHIONMNIST
We also conduct experiments on FashionMNIST dataset1. There are three sets of experiments with the number of labeled data being 100, 200 and 1, 000, respectively. The details about the networks are in Appendix.
The corresponding results are shown in Table 2, from which we observe at least two phenomena. The first is that our proposed TANR methods (TNAR-VAE, TNAR-LGAN) achieve lower classification errors than VAT in all circumstances with different number of labeled data. The second is that the performance of our method depends on the estimation of the underlying manifold of the observed data. In this case, TNAR-VAE brings larger improvement than TNAR-LGAN, since VAE produces better diverse examples according to our observation. As the development of generative model capturing more accurate underlying manifold, it is expected that our proposed regularization strategy benefits more for SSL.
Table 2: Classification errors (%) of compared methods on FashionMNIST dataset.

Method VAT
TNAR/TAR/NAR-LGAN TNAR/TAR/NAR-VAE

100 labels 27.69
23.65/24.87/28.73 23.35/26.45/27.83

200 labels 20.85
18.32/19.16/24.49 17.23/20.53/24.81

1000 labels 14.51
13.52/14.09/15.94 12.86/14.02/15.44

Figure 3: The perturbations and adversarial examples in the tangent space and the normal space. Note that the perturbations is actually too small to distinguish easily, thus we show the scaled perturbations. First row: FashionMNIST dataset; Second row: CIFAR-10 dataset. From left to right: original example, tangent adversarial perturbation, normal adversarial perturbation, tangent adversarial example, normal adversarial example.
1https://github.com/zalandoresearch/fashion-mnist
7

Under review as a conference paper at ICLR 2019

4.3 ABLATION STUDY
We conduct ablation study on FashionMNIST datasets to demonstrate that both of the two regularization terms in TNAR are crucial for SSL. The results are reported in Table 2. Removing either tangent adversarial regularization (NAR) or normal adversarial regularization (TAR) will harm the final performance, since they fail to enforce the manifold invariance or the robustness against the off-manifold noise. Furthermore, the adversarial perturbations and adversarial examples are shown in Figure 3. We can easily observe that the tangent adversarial perturbation focuses on the edges of foreground objects, while the normal space perturbation mostly appears as certain noise over the whole image. This is consistent with our understanding on the role of perturbation along the two directions that capture the different aspects of smoothness.

4.4 CIFAR-10 AND SVHN
There are two classes of experiments for demonstrating the effectiveness of TNAR in SSL, SVHN with 1, 000 labeled data, and CIFAR-10 with 4, 000 labeled data. The experiment setups are identical with Miyato et al. (2017). We test two kinds of convolutional neural networks as classifier (denoted as "small" and "large") as in Miyato et al. (2017). Since it is difficult to obtain satisfying VAE on CIFAR-10, we only conduct the proposed TNAR with the underlying manifold identified by Localized GAN (TNAR-LGAN) for CIFAR-10. Note that in Miyato et al. (2017), the authors applied ZCA as pre-processing procedure, while other compared methods do not use this trick. For fair comparison, we only report the performance of VAT without ZCA. More detailed experimental settings are included in Appendix.

Table 3: Classification errors (%) of compared methods on SVHN / CIFAR-10 dataset.

Method VAT (small) VAT (large) ALI (Dumoulin et al., 2016) Improved GAN (Salimans et al., 2016) Tripple GAN (Li et al., 2017) FM GAN (Kumar et al., 2017) LGAN (Qi et al., 2018) TNAR-VAE (small) TNAR-VAE (large) TNAR-LGAN (small) TNAR-LGAN (large)

SVHN 1,000 labels 4.37 4.23 7.41 8.11 5.77 4.39 4.73 3.93 3.84 4.10 3.93

CIFAR-10 4,000 labels 15.67 15.29 17.99 18.63 16.99 16.20 14.23 13.63 13.53

In Table 3 we report the experiments results on CIFAR-10 and SVHN, showing that our proposed TNAR outperforms other state-of-the-art SSL methods on both SVHN and CIFAR-10, demonstrating the superiority of our proposed TNAR.
5 CONCLUSION
We present the tangent-normal adversarial regularization, a novel regularization strategy for semisupervised learning, composing of regularization on the tangent and normal space separately. The tangent adversarial regularization enforces manifold invariance of the classifier, while the normal adversarial regularization imposes robustness of the classifier against the noise contained in the observed data. Experiments on artificial dataset and multiple practical datasets demonstrate that our approach outperforms other state-of-the-art methods for semi-supervised learning. The performance of our method relies on the quality of the estimation of the underlying manifold, hence the breakthroughs on modeling data manifold could also benefit our strategy for semi-supervised learning, which we leave as future work.
8

Under review as a conference paper at ICLR 2019
REFERENCES
Mikhail Belkin, Partha Niyogi, and Vikas Sindhwani. Manifold regularization: A geometric framework for learning from labeled and unlabeled examples. Journal of machine learning research, 7 (Nov):2399­2434, 2006.
Yoshua Bengio, Li Yao, Guillaume Alain, and Pascal Vincent. Generalized denoising auto-encoders as generative models. In Advances in Neural Information Processing Systems, pp. 899­907, 2013.
Lawrence Cayton. Algorithms for manifold learning. Univ. of California at San Diego Tech. Rep, 12(1-17):1, 2005.
Olivier Chapelle, Bernhard Scholkopf, and Alexander Zien. Semi-supervised learning (chapelle, o. et al., eds.; 2006)[book reviews]. IEEE Transactions on Neural Networks, 20(3):542­542, 2009.
Zihang Dai, Zhilin Yang, Fan Yang, William W Cohen, and Ruslan R Salakhutdinov. Good semisupervised learning that requires a bad gan. In Advances in Neural Information Processing Systems, pp. 6510­6520, 2017.
Vincent Dumoulin, Ishmael Belghazi, Ben Poole, Olivier Mastropietro, Alex Lamb, Martin Arjovsky, and Aaron Courville. Adversarially learned inference. arXiv preprint arXiv:1606.00704, 2016.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural information processing systems, pp. 2672­2680, 2014a.
Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples. arXiv preprint arXiv:1412.6572, 2014b.
Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013.
Diederik P Kingma, Shakir Mohamed, Danilo Jimenez Rezende, and Max Welling. Semi-supervised learning with deep generative models. In Advances in Neural Information Processing Systems, pp. 3581­3589, 2014.
Abhishek Kumar, Prasanna Sattigeri, and Tom Fletcher. Semi-supervised learning with gans: Manifold invariance with improved inference. In Advances in Neural Information Processing Systems, pp. 5540­5550, 2017.
Samuli Laine and Timo Aila. Temporal ensembling for semi-supervised learning. arXiv preprint arXiv:1610.02242, 2016.
Bruno Lecouat, Chuan-Sheng Foo, Houssam Zenati, and Vijay R Chandrasekhar. Semi-supervised learning with gans: Revisiting manifold regularization. arXiv preprint arXiv:1805.08957, 2018.
Chongxuan Li, Kun Xu, Jun Zhu, and Bo Zhang. Triple generative adversarial nets. arXiv preprint arXiv:1703.02291, 2017.
Takeru Miyato, Andrew M Dai, and Ian Goodfellow. Adversarial training methods for semisupervised text classification. arXiv preprint arXiv:1605.07725, 2016.
Takeru Miyato, Shin-ichi Maeda, Masanori Koyama, and Shin Ishii. Virtual adversarial training: a regularization method for supervised and semi-supervised learning. arXiv preprint arXiv:1704.03976, 2017.
Hariharan Narayanan and Sanjoy Mitter. Sample complexity of testing the manifold hypothesis. In Advances in Neural Information Processing Systems, pp. 1786­1794, 2010.
Partha Niyogi. Manifold regularization and semi-supervised learning: Some theoretical analyses. The Journal of Machine Learning Research, 14(1):1229­1250, 2013.
Augustus Odena. Semi-supervised learning with generative adversarial networks. arXiv preprint arXiv:1606.01583, 2016.
9

Under review as a conference paper at ICLR 2019
Guo-Jun Qi, Liheng Zhang, Hao Hu, Marzieh Edraki, Jingdong Wang, and Xian-Sheng Hua. Global versus localized generative adversarial nets. In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018.
Antti Rasmus, Mathias Berglund, Mikko Honkala, Harri Valpola, and Tapani Raiko. Semisupervised learning with ladder networks. In Advances in Neural Information Processing Systems, pp. 3546­3554, 2015.
Salah Rifai, Yann N Dauphin, Pascal Vincent, Yoshua Bengio, and Xavier Muller. The manifold tangent classifier. In Advances in Neural Information Processing Systems, pp. 2294­2302, 2011.
Alan E Robinson, Paul S Hammon, and Virginia R de Sa. Explaining brightness illusions using spatial filtering and local response normalization. Vision research, 47(12):1631­1644, 2007.
Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training gans. In Advances in Neural Information Processing Systems, pp. 2234­2242, 2016.
Patrice Y Simard, Yann A LeCun, John S Denker, and Bernard Victorri. Transformation invariance in pattern recognitiontangent distance and tangent propagation. In Neural networks: tricks of the trade, pp. 239­274. Springer, 1998.
Jost Tobias Springenberg, Alexey Dosovitskiy, Thomas Brox, and Martin Riedmiller. Striving for simplicity: The all convolutional net. arXiv preprint arXiv:1412.6806, 2014.
Xiaojin Zhu, Zoubin Ghahramani, and John D Lafferty. Semi-supervised learning using gaussian fields and harmonic functions. In Proceedings of the 20th International conference on Machine learning (ICML-03), pp. 912­919, 2003.
A TWO-RINGS DATASET
The underlying manifold for two-rings data is given by M = M+  M-, where M+ = (x1, x2) x21 + x22 = 0.92 and M- = (x1, x2) x21 + x22 = 1.12 represent two different
classes. The observed data is sampled as x = x0 + n, where x0 is uniformly sampled from M and n  N (0, 2-2). We sample 6 labeled training data, 3 for each class, and 3, 000 unlabeled training data, as shown in Figure 2.
B EXPERIMENTS DETAILS ON FASHIONMNIST
In FashionMNIST2 experiments, we preserver 1, 00 data for validation from the original training dataset. That is, we use 100/200/1, 000 labeled data for training and the other 100 labeled data for validation. For pre-processing, we scale images into 0  1. The classification neural network is as following. (a, b) means the convolution filter is with a × a shape and b channels. The max pooling layer is with stride 2. And we apply local response normalization (LRN) (Robinson et al., 2007). The number of hidden nodes in the first fully connected layer is 512.
Conv(3, 32)  ReLU  Conv(3, 32)  ReLU  MaxPooling  LRN  Conv(3, 64)  ReLU  Conv(3, 64)  ReLU  MaxPooling  LRN  FC1  ReLU  FC2
For the labeled data, the batch size is 32, and for the unlabeled data, the batch size is 128. All networks are trained for 12, 000 updates. The optimizer is ADAM with initial learning rate 0.001, and linearly decay over the last 4, 000 updates. The hyperparameters tuned is the magnitude of the tangent adversarial perturbation ( 1), the magnitude of the normal adversarial perturbation ( 2) and the hyperparameter  in Eq. (11). Other hyperparameters are all set to 1. We tune  from {1, 0.1, 0.01, 0.001}, and 1, 2 randomly from [0.05, 20].
2https://github.com/zalandoresearch/fashion-mnist
10

Under review as a conference paper at ICLR 2019

Table 4: The structure of convolutional neural networks for experiments on CIFAR-10 and SVHN, based on Springenberg et al. (2014); Salimans et al. (2016); Laine & Aila (2016). All the convolutional layers and fully connected layers are followed by batch normalization except the fully connected layer on CIFAR-10. The slopes of all lReLU functions in the networks are 0.1.

Conv-Small on SVHN Conv-Small on CIFAR-10

Conv-Large

32×32 RGB image

3×3 conv. 64 lReLU 3×3 conv. 64 lReLU 3×3 conv. 64 lReLU

3×3 conv. 96 lReLU 3×3 conv. 96 lReLU 3×3 conv. 96 lReLU

3×3 conv. 128 lReLU 3×3 conv. 128 lReLU 3×3 conv. 128 lReLU

2×2 max-pool, stride 2 dropout, p = 0.5

3×3 conv. 128 lReLU 3×3 conv. 128 lReLU 3×3 conv. 128 lReLU

3×3 conv. 192 lReLU 3×3 conv. 192 lReLU 3×3 conv. 192 lReLU

3×3 conv. 256 lReLU 3×3 conv. 256 lReLU 3×3 conv. 256 lReLU

2×2 max-pool, stride 2 dropout, p = 0.5

3×3 conv. 128 lReLU 1×1 conv. 128 lReLU 1×1 conv. 128 lReLU

3×3 conv. 192 lReLU 1×1 conv. 192 lReLU 1×1 conv. 192 lReLU

3×3 conv. 512 lReLU 1×1 conv. 256 lReLU 1×1 conv. 128 lReLU

global average pool, 6×6  1×1

dense 128  10

dense 192 10

dense 128 10

10-way softmax

The encoder of the VAE for identify the underlying manifold is a LeNet-like one, with two convolutional layers and one fully connected layer. And the decoder is symmetric with the encoder, except using deconvolutional layers to replace convolutional layer. The latent dimensionality is 128. The localized GAN for identify the underlying manifold is similar as stated in Qi et al. (2018). And the implementation is modified from https://github.com/z331565360/Localized-GAN. We change the latent dimensionality into 128.
We tried both joint training the LGAN with the classifier, and training them separately, observing no difference.
C EXPERIMENTS DETAILS ON SVHN AND CIFAR-10
In SVHN3 and CIFAR-104 experiments, we preserve 1, 000 data for validation from the original training set. That is, we use 1, 000/4, 000 labeled data for training and the other 1, 000 labeled data for validation. The only pre-processing on data is to scale the pixels value into 0  1. We do not use data augmentation. The structure of classification neural network is shown in Table 4, which is identical as in Miyato et al. (2017).
For the labeled data, the batch size is 32, and for the unlabeled data, the batch size is 128. For SVHN, all networks are trained for 48, 000 updates. And for CIFAR-10, all networks are trained for 200, 000 updates. The optimizer is ADAM with initial learning rate 0.001, and linearly decay over the last 16, 000 updates. The hyperparameters tuned is the magnitude of the tangent adversarial perturbation ( 1), the magnitude of the normal adversarial perturbation ( 2) and the hyperparameter  in Eq. (11). Other hyperparameters are all set to 1. We tune  from {1, 0.1, 0.01, 0.001}, and 1, 2 randomly from [0.05, 20].
3http://ufldl.stanford.edu/housenumbers/ 4https://www.cs.toronto.edu/~kriz/cifar.html
11

Under review as a conference paper at ICLR 2019 The VAE for identify the underlying manifold for SVHN is implemented as in https:// github.com/axium/VAE-SVHN. The only modification is we change the coefficient of the regularization term from 0.01 to 1. The localized GAN for identify the underlying manifold for SVHN and CIFAR-10 is similar as stated in Qi et al. (2018). And the implementation is modified from https://github.com/z331565360/Localized-GAN. We change the latent dimensionality into 512 for both SVHN and CIFAR-10.
D MORE ADVERSARIAL EXAMPLES
More adversarial perturbations and adversarial examples in tangent space and normal space are shown in Figure 4 and Figure 5.
Figure 4: The perturbations and adversarial examples in tangent space and normal space for FashionMNIST dataset. Note that the perturbations is actually too small to distinguish easily, thus we show the scaled perturbations. From left to right: original example, tangent adversarial perturbation, normal adversarial perturbation, tangent adversarial example, normal adversarial example.
12

Under review as a conference paper at ICLR 2019
Figure 5: The perturbations and adversarial examples in tangent space and normal space for CIFAR-10 dataset. Note that the perturbations is actually too small to distinguish easily, thus we show the scaled perturbations. From left to right: original example, tangent adversarial perturbation, normal adversarial perturbation, tangent adversarial example, normal adversarial example.
13


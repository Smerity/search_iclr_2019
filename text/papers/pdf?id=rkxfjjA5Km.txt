Under review as a conference paper at ICLR 2019

AUTO-ENCODING KNOCKOFF GENERATOR FOR FDR CONTROLLED VARIABLE SELECTION
Anonymous authors Paper under double-blind review

ABSTRACT
A new statistical procedure (Model-X Cande`s et al. (2018)) has provided a way to identify important factors using any supervised learning method controlling for FDR. This line of research has shown great potential to expand the horizon of machine learning methods beyond the task of prediction, to serve the broader needs in scientific researches for interpretable findings. However, the lack of a practical and flexible method to generate knockoffs remains the major obstacle for wide application of Model-X procedure. This paper fills in the gap by proposing a model-free knockoff generator which approximates the correlation structure between features through latent variable representation. We demonstrate our proposed method can achieve FDR control and better power than two existing methods in various simulated settings and a real data example for finding mutations associated with drug resistance in HIV-1 patients.

1 INTRODUCTION

In the past decades, the machine learning methods have achieved great advancement in improving prediction accuracy. However, prediction accuracy is not the sole quest of 'big data' analysis. A universal aim across a lot of scientific disciplines is to identify the causes of certain outcome. For example, in new drug development, a personalized medicine strategy can be formed by identifying the gene markers that is biological linked to certain drug response or resistance. In electronic health record analysis, the goal is to identify 'actionable' factors for the purpose of reducing cost and improving the quality of care. In epidemiology or sociology studies, it is of interest to identify protection or risk factors, especially those that can be changed through public policy or civil planning to improve public welfare. In these scientific endeavors, the need is to identify the important factors associated certain outcome, so that further confirmatory investigation (e.g. randomized international studies) could be conducted to expand knowledge or change future actions for a better outcome. For this purpose, we are interested in procedures that control the Type I error, which is the chance of a false discovery. With a large amount of factors to test, controlling the chance of any false discovery (family-wise error) is too stringent. Therefore the objective is usually relaxed to control the False Discovery Rate (FDR) (Benjamini & Hochberg, 1995).

A recent breakthrough in the statistical theory, i.e. the Model-X framework(Barber & Cande`s, 2015; Cande`s et al., 2018), provided a general solution. It can be incorporated with any machine learning models to select true signals associated with the outcome, with rigorous control for FDR. This line of research showed light on expanding the horizon of supervised learning methods beyond prediction. However, the implementation of Model-X requires the generation of the so called 'knockoffs', which has very limited existing methods. The goal of our paper is to fill in the gap by proposing a model-free method for generating knockoffs that is suitable for any data type. It can also be efficiently implemented leveraging on the power of of the recent development in deep generative models. Consequently, with the creation of such a generator, we equipped every supervised learning method the ability to conduct FDR controlled variable selection.

We consider the following problem (Cande`s et al., 2018). There are N i.i.d. samples (X(i), Y i)

from a population, where the predictors are random variables X = (X1, . . . , Xp). The outcome Y only depend on a subset of predictors S  {1, 2, . . . , p}, i.e. given {Xj, j  S}, Y is independent

of the other Xj's. The smallest set S that satisfy this requirement is considered as true factors.

The goal is to find a good estimator S^ for S with control of the F DR = E

#{j:jS^\S} #{j:jS^}1

. Barber

1

Under review as a conference paper at ICLR 2019

& Cande`s (2015) first proposed the method for linear regression with Gaussian errors, where the design matrix X is assumed to be fixed. Cande`s et al. (2018) greatly generalized it to the Model-X framework. In contrast to the traditional statistical models focusing on the conditional distribution Y |X1, . . . , Xp, Model-X makes no assumption for the conditional distribution, but assume X has a known distribution for the purpose of generating knockoffs X~ that satisfies Definition 1.1.
Definition 1.1 Model-X knockoffs for the family of random variables X = (X1, . . . , Xp) are a new family of random variables X~ = (X~1, . . . , X~p) constructed with the following two properties: (1) for any subset S  {1, . . . , p}, (X, X~ )swap(S) =d (X, X~ ); (2) X~  Y |X if there is a response Y. (2) is guaranteed if X~ is constructed without looking at Y.
Proposition 1 (Cande`s 2018) The random variables X~ = (X~1, . . . , X~p) are model-X knockoffs for X = (X1, . . . , Xp) if and only if (Xj, X-j, X~j, X~-j) =d (X~j, X-j, Xj, X~-j), for any j  {1, . . . , p}. And Y X~ |X.

|=

To implement Model-X with any machine learning models, one need to define the model-specific feature statistics Wj = wj([X, X~ ], y) that has the flip-sign property,

Wj ([X, X~ ]swap(S), y) = (1 - 21{jS})Wj ([X, X~ ], y).

According to Thm 3.4 in Cande`s et al. (2018), the following procedure (Knockoff) can estimate

the S^ controlling for modified FDR, i.e.

E

#{j:jS^\S} #{j:jS^}+q-1

 q. First, compute the threshold

 = min

t

>

0

:

#{j:Wj -t} #{j:Wj t}



q

, then the selected set is

(Knockoff): S^ = {j : Wj   }.

(1)

More conservatively, the following procedure (Knockoff+) can control for the FDR, choose a thresh-

old  + = min

t

>

0

:

1+#{j:Wj -t} #{j:Wj t}



q

, and select the variables in the set

(Knockoff+): S^ = {j : Wj   +}.

(2)

An example of the feature statistics is the signed max lambda statistics in L1 penalized regressions Barber & Cande`s (2015): for the concatenated design matrix [X, X~ ], define ^() =

arg minb Loss(y, [XX~ ]b) +  b L1 . For each feature Xj and its knockoff X~j, let Zj = sup{ :

^2j-1() = 0} and Z~j = sup{ : ^2j() = 0}. The signed lambda statistics is defined as Wj = (Zj  Z~j)sign(Zj - Z~j). Various feature statistics has been proposed for common learn-
ing methods (Cande`s et al., 2018; Gimenez et al., 2018), however the current knockoff generation

methods are still limited to make Model-X generally applicable on any real data set.

Although Cande`s et al. (2018) provided a general algorithm for generating Model-X knockoffs,
i.e. the Sequential Conditional Independent Pairs algorithm, it needs to sample from the conditional distribution of X~j from f (Xj|X-j, X~1:j-1) which becomes intractable with slightly complex distributional assumption for X. Cande`s et al. (2018) also proposed a second-order approach by matching first two moments of (X, X~ )swap(S) and (X, X~ ), which satisfies Def. 1.1 when X is from Gaussian distribution. Sesia et al. (2018) proposed an algorithm to sample knockoffs when X is from Hid-
den Markov Model. Gimenez et al. (2018) proposed algorithm that applicable for X from simple
Bayesian network models (such as mixture Gaussian) with distributional assumptions for certain
conditional probabilities which need to be estimated and sampled from.

In this paper, we relaxes the distributional assumptions largely from all the existing methods. Our procedure assumes there is a (multivariate) latent variable Z, conditional on which X's are mutually independent. We propose a procedure to generate X's knockoff from conditional distributions of Z|X and X|Z. And we also provide a FDR bound when these estimated conditional probabilities is compatible with the distribution of X. In practice, we assume Z and the conditional distributions are from parametric families, which is adaptive to different real data application, and the parameters can be approximated by deep neural networks. In this manner, the algorithms developed in the line of research on variational autoencoder, provide a flexible and practical tool for implementing our proposed Knockoff generation method. In the next session, we provide the formal derivation and justification for this approach.

2

Under review as a conference paper at ICLR 2019
2 A MODEL-FREE KNOCKOFF GENERATOR WITH LATENT ENCODING VARIABLES.
The random vector of covariates X = (X1, X2, . . . , Xp) are from pX (x) (X can be continuous, discrete or mixed ). There exists a vector of continuous latent random variables Z from distribution pZ (z). Each component of the covariates Xj  p (fi(Z)), j = 1, . . . , p, such that j's are mutually independent given Z. For example, when the predictor is continuous, Xj  N (fj(Z), j2); and when the predictor is binary, Xj  Bernoulli(fj(Z)). Given the observed samples {x(i)}Ni=1 from pX (x), to estimate pZ (z) and pX|Z may not be an identifiable problem. However, we just need to find one Z that approximately satisfy the above condition in practice for our purpose of knockoff generation. Then we propose to generate knockoff X~ as Algorithm 1.
Algorithm 1 Auto-Encoding Knockoff Generator. 1: Assume two working models from parametric families: a) an encoder QZ|X (z|x; ) to approx-
imate pZ|X (z|x); b) a decoder QX|Z (x|z; f, q ) to approximate pX|Z (x|z). 2: Train model parameters to get estimates ^, f^. And let q be the working noise distribution to
generate X~ given f^(Z~), and is element-wise independent q = j q j . 3: Generate Z~ from QZ|X (z|X; ^). 4: Generate X~ from QX|Z (x|Z~; f^, q ).
Algorithm 1 can be efficiently implemented with but not limited to the existing algorithms developed in the line of research on Variational Autoencoder (VAE) (Kingma & Welling, 2014; Rezende et al., 2014; Maddison et al., 2017; Jang et al., 2017). For example, in the common VAE implementation, Z is assumed tobe from its prior N (0, Iq), the working model QZi|X is assumed to be from N (µi(X), i2(X)), and QXi|Z is N (fi(Z), 2), where µi(·) and i2(·) and f (·) are functions to be approximated by neural networks. The q in this case is i.i.d. normal distribution with infinitesimal variance. So in practice X~ is generated as f^(Z~). Another example is to approximate discrete Z's by the concrete distribution (Jang et al., 2017; Maddison et al., 2017). In this case X~ is generated from Bernoulli(f^(Z~)) and = X~ - f^(Z~) are element-wise independent. These two methods provide efficient algorithms for implementing Algorithm 1 in specific cases. However Algorithm 1 and the following justification are more general. We do not assume Z's from independent prior distribution, and Z|X do not need to be element-wise independent either. See more discussions in Section 5 paragraph 2.
Notice that the Step 3 in Algorithm 1 is different from the deep generative model, the latter generate new image by first generating a Z from the predetermined prior distribution pZ(z). However, in step 3 of our algorithm, the new Z~ is generated from the conditional distribution QZ|X . The following Theorem 2.1 justifies why we propose procedure as so.
Theorem 2.1 For any vector of random variables Z, such that conditional on Z, Xi's are mutually independent. If Z~ is from pZ|X (·|X), then X~ from pX|Z (·|Z~) is model-X knockoffs.
Proof: For any j  1, . . . , p and sets Aj, Bj, A-j, B-j,
P (Xj  Aj , X~j  Bj , X-j  A-j , X~-j  B-j ) = pX|Z (Aj , A-j |Z)pX|Z (Bj , B-j |Z)dF (Z)
= pXj|Z (Aj |Z)pX-j|Z (A-j |Z)pXj|Z (Bj |Z)pX-j|Z (B-j |Z)dF (Z)
= pXj|Z (Bj |Z)pX-j|Z (A-j |Z)pXj|Z (Aj |Z)pX-j|Z (B-j |Z)dF (Z)
= pX|Z (Bj , A-j |Z)pX|Z (Aj , B-j |Z)dF (Z) = P (Xj  Bj , X~j  Aj , X-j  A-j , X~-j  B-j )
We have shown Xj and X~j are exchangeable in the joint distribution, then X~ is the Model-X knockoff by Proposition 1.
3

Under review as a conference paper at ICLR 2019

To provide the FDR bound for the knockoffs generated from the estimated encoder and decoder, we do not assume there exists a Z satisfies the condition of Theorem 2.1 exactly. Instead, we assume the working models and observed distribution of X are approximately compatible as in the following theorem. We then apply the results per Barber et al. (2018) to show FDR control, where they showed the FDR derived from approximate knockoffsis bounded by a function of the observed KL divergence between (Xj, X~j, X-j, X~-j) and (X~j, Xj, X-j, X~-j) as below:

K Lj

=

log

P (Xj , X~j , X-j , X~-j ) P (X~j , Xj , X-j , X~-j )

n
= log
i=1

P (Xij , X~ij , Xi,-j , X~i,-j ) P (X~ij , Xij , Xi,-j , X~i,-j )

Theorem 2.2 Assume the observed marginal distribution pX (x) and the working models (QZ|X , QX|Z , q estimated in Algorithm 1) are approximately compatible as following: there exists a ran-
dom vector Z from certain distribution pZ (z) and an  0, such that

sup | log
x

q (x) p^(x)

|  an and sup | log
z,x

pZ|X^ (z|x) QZ|X (z|x)

|  an.

Where the density of ^ = X - f^(Z) is denoted as p^, considering f^ as a fixed function. And denote X^ = f^(Z) + as a random variable generated from the decoder QX|Z (x|z = Z; f^, q ).

Then the FDR can be controlled at q exp{8na2n + 8 n log(p)an}.

Proof: Consider X^~ which is another independent sample from QX|Z (x|z = Z; q ) when sampling
X^ using the same Z. Since q is element-wise independent, conditions in Theorem 2.1 holds, thus X^~ is a model-X knockoff of X^ and

log

P (X^j  Aj , X~^j  Bj , X^-j  A-j , X~^-j  B-j ) P (X^j  Bj , X^~j  Aj , X^-j  A-j , X~^-j  B-j )

Notice that by our assumptions

= 0.

sup | log
x

pX^ (x) pX (x)

|  sup | log
x,z

pX^ |Z (x|z) pX|Z (x|z)

| = sup | log
x,z

q (x - f^(z)) p^(x - f^(z))

|  an

sup | log
x,x~

pX^~ |X~ (x~|x) pX~ |X (x~|x)

| = sup | log
x,z,x~

q (x~ - f^(z))pZ|X^ (z|x)dz q (x~ - f^(z))QZ|X (z|x)dz

|  sup | log
z,x

pZ|X^ (z|x) QZ|X (z|x)

|  an

So supx,x~ | log

p(X^ ,X~^ )(x,x~) p(X,X~ )(x,x~)

|  2an and we can bound the log likelihood ratio by

log

P (Xj  Aj , X~j  Bj , X-j  A-j , X~-j  B-j ) P (Xj  Bj , X~j  Aj , X-j  A-j , X~-j  B-j )

=

log

P (X^j  Aj , X~^j  Bj , X^-j  A-j , X^~-j  B-j ) P (X^j  Bj , X^~j  Aj , X^-j  A-j , X~^-j  B-j )

+ log

P (Xj  Aj , X~j  Bj , X-j  A-j , X~-j  B-j ) P (X^j  Aj , X^~j  Bj , X^-j  A-j , X^~-j  B-j )

+ log

P (X^j  Bj , X~^j  Aj , X^-j  A-j , X^~-j  B-j )) P (Xj  Bj , X~j  Aj , X-j  A-j , X~-j  B-j )

 0 + 2an + 2an = 4an

So we can bound

max
j=1,··· ,p

K

Lj



8na2n

+

8

n log(p)an.

And FDR has the bound in the statement by applying Lemma 2 in Barber et al. (2018). Specifically, if an = o((n log(p))-1/2), then FDR will be asymptotically bounded by q + o(1).

4

Under review as a conference paper at ICLR 2019
3 SIMULATION
In the simulation, we compare three knockoff generation schemes: a) our proposed method implemented through VAE b) the fixed knockoff generation method in Barber & Cande`s (2015) c) the second order matching in Cande`s et al. (2018). We demonstrate the performance of these methods in two simulation scenarios. In both scenarios, the sample size is 200, and number of potential predictors is 100, where the first m variables are the true signals. The coefficients  for the true predictors are alternating  and -, where  is the magnitude of the signal. In Figures 12, the power and FDR are drawn as curves with respect to . Results based on 100 replications are presented for m = 10, 20, Gaussian and binary outcomes separately. In the Gaussian case, the error term is standard normally distributed. In the logistic regression case, binary outcome follows Bernoulli distribution with probabilities 1/(1 + exp(-X)). Setting 1. The first setting generates continuous predictors: a) Generate independent uniform (0, 1) distributed 200 by 200 random matrix. b) Let C be the Cholesky decomposition of the correlation matrix with 0.1 on the off diagonal entries and compute Z = U C. c) Even column X2i is generated as Z4i + 0.5  Z43i+1, and odd column X2i+1 is generated as Z4i+2 - 0.5Z42i+2 + 0.5 exp(Z4i+3), where i = 0, . . . , 49 following the python array indexing. d) Rescale X's to be within range [0, 1] by subtracting the column min and divided by column max - min. This generation scheme is an arbitrary representation of a set of lightly correlated non-normal distributed continuous variables. Empirically, the pairwise correlation X ranges from -0.3 to 0.6 and the absolute value of the correlation has mean 0.07. The results of this setting are presented in Figure 1. The signal to noise ratios of the Gaussian case approximately has the range of 0.25 - 0.32 for m = 10 and 0.7 - 0.852 for setting with m = 20. We present the results controlled for F DR = 0.1, because of the page limit, the FDR = 0.1 binary case has low power ( although the comparison of methods are the same), thus we present results of binary outcome for FDR = 0.2. Setting 2. The second setting generates categorical predictors:a) Generate independent standard Gaussian distributed 200 by 100 random matrix. b) Let C be the Cholesky decomposition of the correlation matrix with 0.1 on the off diagonal entries. Let Z = U C. c) X is the categorized Z matrix, where Xij = 1(Zij > 0). Approximately, the signal to noise ratio for the Gaussian case is 22 for m = 10 and 42 for m = 20. We present results for FDR controlled at 0.1.
Results According to Figures 1 2, our proposed method implemented through VAE has FDR controlled below the predetermined threshold, and it demonstrates higher power than its two competitors. Since the data were not Gaussian, the second-order matching method has the lowest power. The assumptions of the Fixed knockoff generations holds for the Gaussian cases, thus the power of increases to about 0.4 for setting 1 and 0.55 for setting 2 with large signal  = 10 and with m = 20. Cases with more true signals m = 20 demonstrate better power for Gaussian outcomes, since we control for the proportion of false discoveries, more errors are allowed with increasing number of true signals. In the binary case, although the Gaussian error assumption for using the fixed knockoff generation does not hold, FDR is still controlled in all three methods. However the powers are low.
Implementation details. For both settings, we implemented our method with VAE Kingma & Welling (2014), where the latent variables are multivariate normal with 300 dimensions. The models were trained with batch size 25 and for 20 epochs with the default `adam' optimizer in `keras'Chollet et al. (2015). The architecture of VAE for setting 1 is as following, the encoder networks for µz and log() have two hidden layers with 500 and 400 neurons with `tanh' activation and L2 regularization with tuning parameter 0.2. The activation for the output layer is `linear'. The decoder network has two hidden layers with the same specifications, followed by a batch-normalization layer and output with linear activation. The architecture of VAE for setting 2 is as following, the encoder networks has two hidden layers with 500 and 400 neurons with `relu' activation and L2 regularization with tuning parameter 0.3. The activation for the output layer is `linear'. The decoder is an one layer network with the `sigmoid' activation function and then threshold by 0.5.
For all simulation settings and methods, we are using the signed max lambda statistics in Barber & Cande`s (2015) (see definition on page 2). We implemented the L1 penalized linear and logistic regression with R package 'glmnet' with a finer grid of tuning parameter to break ties in Zj and Z~j. We used the more conservative Knockoff+ as in (2).
5

Under review as a conference paper at ICLR 2019

(a) Gaussian m = 10, FDR= 0.1

(b) Gaussian m = 20, FDR= 0.1

(c) Binary m = 10, FDR= 0.2

(d) Binary m = 20, FDR= 0.2

Figure 1: Simulation performance for setting 1

4 REAL DATA EXPERIMENT
In the real data experiment, we demonstrate the performance of our proposed method on generating knockoffs for sparse genetic mutation data. The dataset and scientific problem is from a study aiming at detecting mutations associated with drug resistance in patients with Human Immunodeficiency Virus Type 1 (HIV-1) Rhee et al. (2006). Due to the space limit, here we present results for the 7 protease inhibitors. There is no ground truth for which subset of mutations caused the drug resistance. Nevertheless, there is a set of 73 treatment-selected mutations (TSMs) identified from a separate study Rhee et al. (2005), which are selected as the mutations marginally correlated with the patient treatment history with protease inhibitors. Thus to evaluate the performance of the knockoffs, we first simulates the outcome for which we know the true signal, and evaluate the FDR control and power as shown in Figure 3, and then we used the real data outcome and compared our selected mutations with the TSM. Notice that the TSM is not drug specific and can not be considered as ground truth, by showing number of selected mutations in TSM set we are not showing the FDR control but demonstrate reproducibility across studies.
The normal distributed Z does not fit the discrete and sparse nature of the genetic mutation data. Thus we adopted the Categorical Variation Autoencoder (CAT-VAE) Jang et al. (2017); Maddison et al. (2017), where in our implementation, the latent variables Z are 20 Gumbel-Softmax distributed variable with temperature 1, each with 10 categories; both the encoder and decoder has one hidden layer of dimension 200.
Figure 3 demonstrates the results for simulated outcomes using the real data X. Overall speaking, our proposed method still achieves the FDR control and demonstrates the highest power among

6

Under review as a conference paper at ICLR 2019

(a) Gaussian m = 10, FDR= 0.1

(b) Gaussian m = 20, FDR= 0.1

(c) Binary m = 10, FDR= 0.1

(d) Binary m = 20, FDR= 0.1

Figure 2: Simulation performance for setting 2

the three. The correlation is smaller than the previous simulation settings (since the X is a sparse matrix), so the other two methods has better power. Controlled for a FDR level of 0.2, all three methods achieve a high power of 80% very large signal at  = 10 in the Gaussian case. For the binary case, our proposed method shows greater advantage in achieving more than 2-times the power of the other two methods.

Table 1 presents the number of selected and matched selection of mutations with the 73 TSMs. Here we present results controlled by both Knockoff+ (as in (2)) and Knockoff (as in (1)) controlled at FDR level 0.2. Similar to the simulated settings, our proposed method select more mutations than the other methods, the average proportion of selected mutation that is in the TSM set for our proposed method is 75.3%, which demonstrate reproducibility across studies. With less conservative Knockoff procedure, Fixed and Second-Order methods are able to select more mutations, where our proposed method seems be less sensitive to choice of Knockoff or Knockoff+.

Implementation details For these analysis, the CAT-VAE knockoffs are generated for 186 muta-

tions with ( 4) occurrence in the data set and the number of patients is 846. Since each drug

resistance outcome is observed in a subset of samples, when conduct analysis for each drug, we

only consider mutations with  2 occurrence. Following analysis in Rhee et al. (2006), the outcome

is a continuous variable, which is the log transformed drug susceptibility. For results in Figure 3,

the linear signal is simulated as following: in each replication, randomly chose 50 predictors from

186 mutations , and compute µ as sum of xj's with alternating signs. The Gaussian outcome is µ

plus

a

standard

normal

noise

and

the

binomial

outcome

is

generated

from

Bernoulli(

1 1+exp(-µi )

).

7

Under review as a conference paper at ICLR 2019

(a) Gaussian, FDR= 0.2

(b) Binary, FDR= 0.2

Figure 3: Capture of simulated signals with real data generated knockoffs

Table 1: Number of TRM mutations/ total number of selected mutations controlled for FDR at 0.2

Knockoff+ Knockoff

Sample Size CAT-VAE Fixed
Second-Order CAT-VAE Fixed
Second-Order

APV 767 28/28 25/25 0 32/40 30/35 24/24

ATV 328 8/31 2/10 0 9/38 0 4/11

IDV 825 34/49 0 0 34/49 14/14 4/4

LPV 515 27/37 0 0 28/38 0 17/24

NFV 842 42/54 0 5/5 42/54 26/26 29/29

RTV 793 38/48 35/44 5/5 38/48 34/43 35/42

SQV 824 39/52 23/25 0 38/50 25/27 17/17

5 DISCUSSION AND FUTURE DIRECTIONS
The connection between representative learning and Model-X FDR control framework has two directions. One direction is that the VAE algorithms provide a way to efficiently implement the knock generating methods proposed in this paper. The other direction is that the Model-X can also enhance the interpretability of representative learning by equipping it with the ability for finite sample FDR controlled feature selection, see example in Gimenez et al. (2018). With the natural connection between our proposed algorithm and the deep generative model for images, it is a promising future direction to investigate how to construct feature statistics to interpret image features.
The proposed knockoff generating method opens a venue for the development of new algorithms. For example, the VAE and CAT-VAE algorithms both assume Z|X are independent, this can be relaxed since our method does not assume this. Some minor changes also worth further investigation, for example, instead of implementing the existing VAE algorithm with infinitesimal q , we can add a small noise to f^(Z~) for generating X~ , with predetermined or estimated variance.
For real data applications, the existing VAE algorithms has already offered a large class of working knockoff generation models to choose from. An open question is how to evaluate the goodness of knockoffs and pick a better model. One evaluation approach is from the performance in FDR control and power in simulated settings. To this end, one can assume a Bayesian prior for the signals patterns of the data, for each knockoff model estimate the FDR and power for a class of signal patterns from several simulated settings. Another approach is to evaluate the knockoff directly. This is a largely unsolved problem. An insight from Theorem 2.2 is the FDR is controlled when q and the density of X - f^(Z) are close. Therefore it is more desirable that X - f^(Z~) is element-wise independent.
To sum up, the method we proposed in the paper provides a practical method for model free knockoff generation. It will stimulate broad further research and software development towards a general applicable tool for the scientific community to identify important factors with rigorous control for false discovery.
8

Under review as a conference paper at ICLR 2019
REFERENCES
Rina Foygel Barber and Emmanuel J. Cande`s. Controlling the false discovery rate via knockoffs. Ann. Statist., 43(5):2055­2085, 10 2015.
Rina Foygel Barber, Emmanuel J. Cande`s, and Richard J. Samworth. Robust inference with knockoffs. arXiv:1801.03896 [stat.ME], 2018.
Yoav Benjamini and Yosef Hochberg. Controlling the false discovery rate: A practical and powerful approach to multiple testing. Journal of the Royal Statistical Society. Series B (Methodological), 57(1):289­300, 1995.
Emmanuel Cande`s, Yingying Fan, Lucas Janson, and Jinchi Lv. Panning for gold: model-x knockoffs for high dimensional controlled variable selection. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 80(3):551­577, 2018.
Franc¸ois Chollet et al. Keras. https://keras.io, 2015. Jaime Roquero Gimenez, Amirata Ghorbani, and James Zou. Knockoffs for the mass: new feature
importance statistics with false discovery guarantees. arXiv:1807.06214 [stat.ML], 2018. Eric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization with gumbel-softmax. ICLR,
2017. Diederik P. Kingma and Max Welling. Auto-encoding variational bayes. ICLR, 2014. Chris J. Maddison, Andriy Mnih, and Yee Whye Teh. The concrete distribution: A continuous
relaxation of discrete random variables. ICLR, 2017. Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and
approximate inference in deep generative models. 2014. Soo-Yon Rhee, W. Jeffrey Fessel, Andrew R. Zolopa, Leo Hurley, Tommy Liu, Jonathan Taylor,
Dong Phuong Nguyen, Sally Slome, Daniel Klein, Michael Horberg, Jason Flamm, Stephen Follansbee, Jonathan M. Schapiro, and Robert W. Shafer. Hiv-1 protease and reverse-transcriptase mutations: Correlations with antiretroviral therapy in subtype b isolates and implications for drugresistance surveillance. The Journal of Infectious Diseases, 192(3):456­465, 2005. Soo-Yon Rhee, Jonathan Taylor, Gauhar Wadhera, Asa Ben-Hur, Douglas L. Brutlag, and Robert W. Shafer. Genotypic predictors of human immunodeficiency virus type 1 drug resistance. Proceedings of the National Academy of Sciences, 103(46):17355­17360, 2006. M Sesia, C Sabatti, and E J Cande`s. Gene hunting with hidden markov model knockoffs. Biometrika, 2018.
9


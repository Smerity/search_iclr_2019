Under review as a conference paper at ICLR 2019
textTOvec: DEEP CONTEXTUALIZED NEURAL AUTOREGRESSIVE MODELS OF LANGUAGE WITH DISTRIBUTED COMPOSITIONAL PRIOR
Anonymous authors Paper under double-blind review
ABSTRACT
We address two challenges of probabilistic topic modelling in order to better estimate the probability of a word in a given context, i.e., P (word|context) : (1) No Language Structure in Context: Probabilistic topic models ignore word order by summarizing a given context as a "bag-of-word" and consequently the semantics of words in the context is lost. In this work, we incorporate language structure by combining a neural autoregressive topic model (TM) with a LSTM based language model (LSTM-LM) in a single probabilistic framework. The LSTM-LM learns a vector-space representation of each word by accounting for word order in local collocation patterns, while the TM simultaneously learns a latent representation from the entire document. In addition, the LSTM-LM models complex characteristics of language (e.g., syntax and semantics), while the TM discovers the underlying thematic structure in a collection of documents. We unite two complementary paradigms of learning the meaning of word occurrences by combining a topic model and a language model in a unified probabilistic framework, named as ctx-DocNADE. (2) Limited Context and/or Smaller training corpus of documents: In settings with a small number of word occurrences (i.e., lack of context) in short text or data sparsity in a corpus of few documents, the application of TMs is challenging. We address this challenge by incorporating external knowledge into neural autoregressive topic models via a language modelling approach: we use word embeddings as input of a LSTM-LM with the aim to improve the wordtopic mapping on a smaller and/or short-text corpus. The proposed DocNADE extension is named as ctx-DocNADEe. We present novel neural autoregressive topic model variants coupled with neural language models and embeddings priors that consistently outperform state-of-theart generative topic models in terms of generalization (perplexity), interpretability (topic coherence) and applicability (retrieval and classification) over 6 long-text and 8 short-text datasets from diverse domains.
1 INTRODUCTION
Probabilistic topic models, such as LDA (Blei et al., 2003), Replicated Softmax (RSM) (Salakhutdinov & Hinton, 2009) and Document Neural Autoregressive Distribution Estimator (DocNADE) (Larochelle & Lauly, 2012; Zheng et al., 2016; Lauly et al., 2017) are often used to extract topics from text collections, and predict the probabilities of each word in a given document belonging to each topic. Subsequently, they learn latent document representations that can be used to perform natural language processing (NLP) tasks such as information retrieval (IR), document classification or summarization. However, such probabilistic topic models ignore word order and represent a given context as a bag of its words, thereby disregarding semantic information.
To motivate our first task of extending probabilistic topic models to incorporate word order and language structure, assume that we conduct topic analysis on the following two sentences:
Bear falls into market territory and Market falls into bear territory
When estimating the probability of a word in a given context (here: P ("bear"|context)), traditional topic models do not account for language structure since they ignore word order within the context
1

Under review as a conference paper at ICLR 2019

retmebcxdrisrmlcatolparsilfeooloodkraoadnrlaciletednrltetekyss

Coarse Granuality: Global View Topic due to Word distributions across documents

Fine Granuality: Local View

falling falls 0.83

rise

fall0.75 0.77

drop 0.61 0.54 tumble

Nearest Neighbors of fall

Deal with stock index falls

0.47

0.36 0.53 0.64 0.61

Brace for market share drops

Cosine similarity in Word Embedding space

Figure 1: (left): A topic-word distribution due to global exposure, obtained from the matrix W as row-vector. (middle): Nearest neighbors in semantics space, represented by W in its column vectors. (right): BoW and cosine similarity illustration in distributed embedding space.

and are based on "bag-of-words" (BoWs) only. In this particular setting, the two sentences have the same unigram statistics, but are about different topics. On deciding which topic generated the word "bear" in the second sentence, the preceding words "market falls" make it more likely that it was generated by a topic that assigns a high probability to words related to stock market trading, where "bear territory" is a colloquial expression in the domain. In addition, the language structure (e.g., syntax and semantics) is also ignored. For instance, the word "bear" in the first sentence is a proper noun and subject while it is an object in the second. In practice, topic models also ignore functional words such as "into", which may not be appropriate in some scenarios.
Recently, Peters et al. (2018) have shown that a deep contextualized LSTM-based language model (LSTM-LM) is able to capture different language concepts in a layer-wise fashion, e.g., the lowest layer captures language syntax and topmost layer captures semantics. However, in LSTM-LMs the probability of a word is a function of its sentence only and word occurrences are modelled in a fine granularity. Consequently, LSTM-LMs do not capture semantics at a document level.
Similarly, while bi-gram LDA based topic models (Wallach, 2006; Wang et al., 2007) and n-gram based topic learning (Lauly et al., 2017) can capture word order in short contexts, they are unable to capture long term dependencies and language concepts. In contrast, DocNADE (Larochelle & Lauly, 2012) learns word occurrences across documents i.e., coarse granularity (in the sense that the topic assigned to a given word occurrence equally depends on all the other words appearing in the same document); however since it is based on the BoW assumption all language structure is ignored. In language modeling, Mikolov et al. (2010) have shown that recurrent neural networks result in a significant reduction of perplexity over standard n-gram models.
Contribution 1: We introduce language structure into neural autoregressive topic models via a LSTM-LM, thereby accounting for word ordering (or semantic regularities), language concepts and long-range dependencies. This allows for the accurate prediction of words, where the probability of each word is a function of global and local contexts, modelled via DocNADE and LSTM-LM, respectively. The proposed neural topic model is named as contextualized-Document Neural Autoregressive Distribution Estimator (ctx-DocNADE) and offers learning complementary semantics by combining joint word and latent topic learning in a unified neural autoregressive framework. For instance, Figure 1 (left and middle) shows the complementary topic and word semantics, based on TM and LM representations of the term "fall". Observe that the topic captures the usage of "fall" in the context of stock market trading, attributed to the global view.
While this is a powerful approach for incorporating language structure and word order in particular for long texts and corpora with many documents, learning from contextual information remains challenging in settings with short texts and few documents, since (1) limited word co-occurrences or little context (2) significant word non-overlap in such short texts and (3) small training corpus of documents lead to little evidence for learning word co-occurrences. However, distributional word representations (i.e. word embeddings) (Pennington et al., 2014) have shown to capture both the semantic and syntactic relatedness in words and demonstrated impressive performance in NLP tasks.
For example, assume that we conduct topic analysis over the two short text fragments: Deal with stock index falls and Brace for market share drops. Traditional topic models with "BoW" assumption will not be able to infer relatedness between word pairs such as (falls, drops) due to the lack of word-overlap and small context in the two phrases. However, in the distributed embedding space, the word pairs are semantically related as shown in Figure 1 (left).
2

Under review as a conference paper at ICLR 2019

v1 v2
U

vi

vD

hD1N hD2N hDi N hDDN ... ...

...
v1 vi-1

vi

DocNADE

W
vD

Coarse Granuality: Global View Word occurrences across document

vi
U

Fine Granuality: Local View

x1 x2

x x Word i i+1 occurences

x... i-1

in collocation patterns

DocNADE (DN)
W

Latent Topics

hi hDi N

hLi M
LSTM-LM
=W Embedding layer or
=W+E

v1 v2 ... vi-i vi v vi+1 ... D-1 vD ci = x1, x2,...,xi-1

ctx-DocNADEe

Figure 2: (left): DocNADE for the document v. (right): ctx-DocNADEe for the observable corresponding to vi  v. Blue colored lines signify the connections that share parameters. The observations (double circle) for each word vi are multinomial, where vi is the index in the vocabulary of the ith word of the document. hDi N and hLi M are hidden vectors from DocNADE and LSTM models, respectively for the target word vi. Connections between each input vi and hidden units hDi N are shared. The symbol v^i represents the autoregressive conditionals p(vi|v<i), computed using hi which is a weighted sum of hDi N and hLi M in ctx-DocNADEe.

Related work such as Sahami & Heilman (2006) employed web search results to improve the information in short texts and Petterson et al. (2010) introduced word similarity via thesauri and dictionaries into LDA. Das et al. (2015) and Nguyen et al. (2015) integrated word embeddings into LDA and Dirichlet Multinomial Mixture (DMM) (Nigam et al., 2000) models. However, these works are based on LDA-based models without considering language structure, e.g. word order. In addition, DocNADE outperforms LDA and RSM topic models in terms of perplexity and IR.
Contribution 2: We incorporate distributed compositional priors in DocNADE: we use pre-trained word embeddings via LSTM-LM to supplement the multinomial topic model (i.e., DocNADE) in learning latent topic and textual representations on a smaller corpus and/or short texts. Knowing similarities in a distributed space and integrating this complementary information via a LSTM-LM, a topic representation is much more likely and coherent.
Taken together, we combine the advantages of complementary learning and external knowledge, and couple topic- and language models with pre-trained word embeddings to model short and long text documents in a unified neural autoregressive framework, named as ctx-DocNADEe. Our approach learns better textual representations, which we quantify via generalizability (e.g., perplexity), interpretability (e.g., topic extraction and coherence) and applicability (e.g., IR and classification).
To illustrate our two contributions, we apply our modeling approaches to 6 long-text and 8 short-text datasets from diverse domains and demonstrate that our approach consistently outperforms state-ofthe-art generative topic models. Our learned representations, result in a gain of: (1) 4.6% (.790 vs .755) in topic coherence, (2) 6.5% (.615 vs .577) in precision at retrieval fraction 0.02, and (3) 4.4% (.662 vs .634) in F 1 for text classification, averaged over 6 long-text and 8 short-text datasets.
When applied to short-text and long-text documents, our proposed modeling approaches generate contextualized topic vectors, which we name textTOvec.
2 NEURAL AUTOREGRESSIVE TOPIC MODELS
Generative models are based on estimating the probability distribution of multidimensional data, implicitly requiring modeling complex dependencies. Restricted Boltzmann Machine (RBM) (Hinton et al., 2006) and its variants (Larochelle & Bengio, 2008) are probabilistic undirected models of binary data. RSM (Salakhutdinov & Hinton, 2009) and its variants (Gupta et al., 2018b) are generalization of the RBM, that are used to model word counts. However, estimating the complex probability distribution of the underlying high-dimensional observations is intractable. To address this challenge, NADE (Larochelle & Murray, 2011) decomposes the joint distribution of binary ob-
3

Under review as a conference paper at ICLR 2019

servations into autoregressive conditional distributions, each modeled using a feed-forward network. Unlike for RBM/RSM, this leads to tractable gradients of the data negative log-likelihood.

2.1 DOCUMENT NEURAL AUTOREGRESSIVE TOPIC MODEL (DOCNADE)

An extension of NADE and RSM, DocNADE (Larochelle & Lauly, 2012) models collections of documents as orderless sets of words (BoW approach), thereby disregarding any language structure. In other words, it is trained to learn word representations reflecting the underlying topics of the documents only, ignoring syntactical and semantic features as those encoded in word embeddings (Bengio et al., 2003; Mikolov et al., 2013; Pennington et al., 2014; Peters et al., 2018).

DocNADE Lauly et al. (2017) represents a document by transforming its BoWs into a sequence

v = [v1, ..., vD] of size D, where each element vi  {1, 2, ..., K} corresponds to a multinomial

observation (representing a word from a vocabulary of size K). Thus, vi is the index in the vocab-

ulary of the ith word of the document v. DocNADE models the joint distribution p(v) of all words

vi by decomposing it as p(v) =

D i=1

p(vi|v<i),

where

each

autoregressive

conditional

p(vi|v<i)

for the word observation vi is computed using the preceding observations v<i  {v1, ..., vi-1} in a

feed-forward neural network for i  {1, ...D},

hDi N (v<i) = g(e + W:,vk ) and p(vi = w|v<i) =
k<i

exp(bw + Uw,:hDi N (v<i)) w exp(bw + Uw ,:hDi N (v<i))

(1)

where, g(·) is an activation function, U  RK×H is a weight matrix connecting hidden to output, e  RH and b  RK are bias vectors, W  RH×K is a word representation matrix in which a

column W:,vi is a vector representation of the word vi in the vocabulary, and H is the number of hidden units (topics). The log-likelihood of any document v of any arbitrary length is given by:

LDN (v) =

D i=1

log

p(vi

|v<i

).

Note that the past word observations v<i are orderless due to

BoWs, and may not correspond to the words preceding the ith word in the document itself.

Algorithm 1 Computation of log p(v)
Input: A training document v Input: Word embedding matrix E Output: log p(v) 1: a  e 2: q(v) = 1 3: for i from 1 to D do 4: compute hi and p(vi|v<i) 5: q(v)  q(v)p(vi|v<i) 6: a  a + W:,vi 7: log p(v)  log q(v)

model DocNADE ctx-DocNADE ctx-DocNADEe

hi
hiDN  g(a) hi  hiDN hiLM  LSTM(ci, embedding = W) hi  hiDN +  hLi M hLi M  LSTM(ci, embedding = W + E) hi  hiDN +  hiLM

p(vi|v<i) equation 1 equation 2 equation 2

Table 1: Computation of hi and p(vi|v<i) in DocNADE, ctx-DocNADE and ctx-DocNADEe models, correspondingly used in estimating log p(v) (Algorithm 1).

2.2 DEEP CONTEXTUALIZED DOCNADE WITH DISTRIBUTIONAL SEMANTICS

We propose two extensions of the DocNADE model: (1) ctx-DocNADE: introducing language structure via LSTM-LM and (2) ctx-DocNADEe: incorporating external knowledge via pre-trained word embeddings E, to model short and long texts. The unified network(s) account for the ordering of words, syntactical and semantic structures in a language, long and short term dependencies, as well as external knowledge, thereby circumventing the major drawbacks of BoW-based representations.

Similar to DocNADE, ctx-DocNADE models each document v as a sequence of multinomial ob-
servations. Let [x1, x2, ..., xN ] be a sequence of N words in a given document, where xi is represented by an embedding vector of dimension, dim. Further, for each element vi  v, let ci = [x1, x2, ..., xi-1] be the context (preceding words) of ith word in the document. Unlike in DocNADE, the conditional probability of the word vi in ctx-DocNADE (or ctx-DocNADEe) is a function of two hidden vectors: hDi N (v<i) and hiLM (ci), stemming from the DocNADE-based and LSTM-based components of ctx-DocNADE, respectively:

hi(v<i) = hiDN (v<i) +  hiLM (ci) and p(vi = w|v<i) =

exp(bw + Uw,:hi(v<i)) w exp(bw + Uw ,:hi(v<i))

(2)

where hiDN (v<i) is computed as in DocNADE (equation 1) and  is the mixture weight of the LM

component, which can be optimized during training (e.g., based on the validation set). The second

4

Under review as a conference paper at ICLR 2019

short-text Data Train Val Test |RV| |FV| L C Domain 20NSshort 1.3k 0.1k 0.5k 1.4k 1.4k 13.5 20 News
TREC6 5.5k 0.5k 0.5k 2k 2295 9.8 6 Q&A R21578title 7.3k 0.5k 3.0k 2k 2721 7.3 90 News Subjectivity 8.0k .05k 2.0k 2k 7965 23.1 2 Senti
Polarity 8.5k .05k 2.1k 2k 7157 21.0 2 Senti TMNtitle 22.8k 2.0k 7.8k 2k 6240 4.9 7 News
TMN 22.8k 2.0k 7.8k 2k 12867 19 7 News AGnewstitle 118k 2.0k 7.6k 5k 17125 6.8 4 News

long-text

Data Train Val Test |RV| |FV| L C Domain

20NSsmall 0.4k 0.2k 0.2k 2k 4555 187.5 20 News

Reuters8 5.0k 0.5k 2.2k 2k 7654 102 8 News

20NS 7.9k 1.6k 5.2k R21578 7.3k 0.5k 3.0k SiROBs 27.0k 1.0k 10.5k

2k 33770 107.5 20 2k 11396 128 90 3k 9113 39 22

News News Indus

AGNews 118k 2.0k 7.6k 5k 34071 38 4 News

Table 2: Data statistics: Short/long texts and/or small/large corpora from diverse domains. Symbols- Avg: average, L: avg text length (#words), |RV | and |F V |: size of reduced (RV) and full vocabulary (FV), C: number of classes, Senti: Sentiment, Indus: Industrial, `k':thousand and : multi-label. For short-text, L<25.

term hLi M is a context-dependent representation and output of an LSTM layer at position i - 1

over input sequence ci, trained to predict the next word vi. The LSTM offers history for the ith

word via modeling temporal dependencies in the input sequence, ci. The conditional distribution

for each word vi is estimated by equation 2, where the unified network of DocNADE and LM

combines global and context-dependent representations. Our model is jointly optimized to maximize

the pseudo log likelihood, log p(v) 

D i=1

log

p(vi|v<i

).

In the weight matrix W of DocNADE, each row vector Wj,: is a distribution over vocabulary of size K, representing the jth topic and each column vector W:,vi is a vector for the word vi. To obtain complementary semantics, we exploit this property and expose W to both global and local
influences by sharing W in the DocNADE and LSTM-LM componenents. Thus, the embedding
layer of LSTM-LM component represents the column vectors.

ctx-DocNADE, in this realization of the unified network the embedding layer in the LSTM component is randomly initialized. This extends DocNADE by accounting for the ordering of words and language concepts via context-dependent representations for each word in the document.

ctx-DocNADEe, the second version extends ctx-DocNADE with distributional priors, where the embedding layer in the LSTM component is initialized by the sum of a pre-trained embedding matrix E and the weight matrix W. Note that W is a model parameter; however E is a static prior.

Algorithm 1 and Table 1 show the log p(v) for a document v in three different settings: DocNADE, ctx-DocNADE and ctx-DocNADEe. In the DocNADE component, since the weights in the matrix W are tied, the linear activation a can be re-used in every hidden layer and computational complexity reduces to O(HD), where H is the size of each hidden layer. In every epoch, we run an LSTM over the sequence of words in the document and extract hidden vectors hiLM , corresponding to ci for every target word vi. Therefore, the computational complexity in ctx-DocNADE or ctx-DocNADEe is O(HD + N), where N is the total number of edges in the LSTM network (Hochreiter & Schmidhuber, 1997; Sak et al., 2014). The trained models can be used to extract a textTOvec representation, i.e., h(v) = hDN (v) +  hLM (cN +1) for the text v of length D, where hDN (v) = g(e + kD W:,vk ) and hLM (cN+1) = LSTM(cN+1, embedding = W or (W + E)).
ctx-DeepDNEe: DocNADE and LSTM can be extended to a deep, multiple hidden layer architecture by adding new hidden layers as in a regular deep feed-forward neural network, allowing for improved performance. In the deep version, the first hidden layer is computed in an analogous fashion to DocNADE variants (equation 1 or 2). Subsequent hidden layers are computed as:

hiD,dN (v<i) = g(ed + Wi,d · hi,d-1(v<i)) or hiL,dM (ci) = deepLST M (ci, depth = d)

for d = 2, ...n, where n is the total number of hidden layers (i.e., depth) in the deep feed-forward and LSTM networks. For d=1, the hidden vectors hiD,1N and hLi,1M correspond to equations 1 and 2.
The conditional p(vi = w|v<i) is computed using the last layer n, i.e., hi,n = hDi,nN +  hLi,nM .

3 EVALUATION

We apply our modeling approaches to 8 short-text and 6 long-text datasets of varying size with single/multi-class labeled documents from public as well as industrial corpora. See the appendices

5

Under review as a conference paper at ICLR 2019

Model
glove(RV) glove(FV)
doc2vec Gauss-LDA glove-DMM glove-LDA DocNADE(RV) DocNADE(FV)
DeepDNE ctx-DocNADE ctx-DocNADEe ctx-DeepDNEe

20NSshort IR F 1
.236 .493 .236 .488 .090 .413 .080 .118 .183 .213 .160 .320 .290 .440 .290 .440 .100 .080 .296 .440 .306 .490 .278 .416

TREC6 IR F 1 .480 .798 .480 .785 .260 .400 .325 .202 .370 .454 .300 .600 .550 .804 .546 .791 .479 .629 .595 .817 .599 .824 .606 .804

R21578title IR F 1
.587 .356 .595 .356 .518 .176 .367 .012 .273 .011 .387 .052 .657 .313 .654 .302 .630 .221 .641 .300 .656 .308 .647 .244

Subjectivity IR F 1
.754 .882 .775 .901 .571 .763 .558 .676 .738 .834 .610 .805 .820 .889 .848 .907 .865 .909 .874 .910 .874 .917 .878 .920

Polarity IR F 1 .543 .715 .553 .728 .510 .624 .505 .511 .515 .585 .517 .607 .560 .699 .576 .724 .503 .531 .591 .725 .605 .740 .591 .723

TMNtitle IR F 1 .513 .693 .545 .736 .190 .582 .408 .472 .445 .590 .260 .412 .524 .664 .525 .688 .536 .661 .560 .687 .595 .726 .576 .694

TMN IR F 1 .638 .736 .643 .813 .220 .720 .713 .692 .551 .666 .428 .627 .652 .759 .687 .796 .671 .783 .692 .793 .698 .806 .687 .796

AGnewstitle IR F 1
.588 814 .612 .830 .265 .600 .516 .752 .540 .652 .547 .687 .656 .819 .678 .821 .682 .825 .691 .826 .703 .828 .689 .826

Avg IR F 1 .542 .685 .554 .704 .328 .534 .434 .429 .451 .500 .401 .513 .588 .673 .600 .683 .558 .560 .617 .688 .630 .705 .620 .688

Table 3: State-of-the-art comparison: IR (i.e, IR-precision at 0.02 fraction) and classification F 1 for short texts, where Avg: average over the row values, the bold and underline: the maximum for IR and F1, respectively.

for the data description and example texts. Table 2 shows the data statistics, where 20NS and R21578 signify 20NewsGroups and Reuters21578, respectively.
Baselines: We compare the performance of our proposed models ctx-DocNADE and ctx-DocNADEe, to baseline methods based on (1) word representation: glove (Pennington et al., 2014), where a document is represented by summing the embedding vectors of it's words, (2) document representation: doc2vec (Le & Mikolov, 2014), (3) neural BoW topic model: DocNADE, (3) topic models, including pre-trained word embeddings: Gauss-LDA (GaussianLDA) (Das et al., 2015), and glove-DMM, glove-LDA (Nguyen et al., 2015).

TMNtitle Reuters8 Subjectivity 20NSshort 20NS AGnewstitle

Model 20NSsmall Reuters8 20NS

R21578 SiROBs AGnews

IR F 1 IR F 1 IR F 1 IR F 1 IR F 1 IR F 1

glove(RV) .214 .442 .845 .830 .200 .608 .644 .316 .273 .202 .725 .870

glove(FV) .238 .494 .837 .880 .253 .632 .659 .340 .285 .217 .737 .890

doc2vec .200 .450 .586 .852 .216 .691 .524 .215 .282 .226 .387 .713

Gauss-LDA .090 .080 .712 .557 .142 .340 .539 .114 .232 .070 .456 .818

glove-DMM .060 .134 .623 .453 .092 .187 .501 .023 .226 .050 - -

DocNADE(RV) .270 .530 .884 .890 .366 .644 .723 .336 .374 .298 .787 .882

DocNADE(FV) .299 .509 .879 .907 .427 .727 .715 .340 .382 .308 .794 .888

ctx-DocNADE .313 .526 .880 .898 .472 .732 .714 .315 .386 .309 .791 .890

ctx-DocNADEe .327 .524 .883 .900 .486 .745 .721 .332 .390 .311 .796 .894

Avg IR F 1 .483 .544 .501 .575 .365 .524 .361 .329
-.567 .596 .582 .613 .592 .611 .601 .618

Model

PPL

DocNADE 980

ctx-DocNADE 968

ctx-DocNADEe 966

DocNADE 283

ctx-DocNADE 276

ctx-DocNADEe 272

DocNADE 1437

ctx-DocNADE 1430

ctx-DocNADEe 1427

Model

PPL

DocNADE 846

ctx-DocNADE 822

ctx-DocNADEe 820

DocNADE 1375

ctx-DocNADE 1358

ctx-DocNADEe 1361

DocNADE 646

ctx-DocNADE 656

ctx-DocNADEe 648

Table 4: IR-precision at fraction 0.02 and classification F 1 for long texts Table 5: Generalization: PPL

Experimental Setup: DocNADE is often trained on a reduced vocabulary (RV) after pre-processing (e.g., ignoring functional words, etc.); however, we also investigate training it on full text/vocabulary (FV) (Table 2) and compute document representations to perform different evaluation tasks. The FV setting preserves the language structure, required by LSTM-LM, and allows a fair comparison of DocNADE+FV and ctx-DocNADE variants. We use the glove embedding of 200 dimensions. All the baselines and proposed models (ctx-DocNADE, ctx-DocNADEe and ctx-DeepDNEe) were run in the FV setting over 200 topics to quantify the quality of the learned representations. To better initialize the complementary learning in ctx-DocNADEs, we perform a pre-training for 10 epochs with  set to 0. See the appendices for the experimental setup and hyperparameters for the following tasks, including the ablation over  on validation set.

3.1 GENERALIZATION: PERPLEXITY (PPL)

To evaluate the generative performance of the topic models, we estimate the log-probabilities for the

test documents and compute the average held-out perplexity (P P L) per word as, P P L = exp -

1 z

z t=1

1 |vt |

log p(vt)

, where z

and |vt| are the

total number of

documents and words in a document

vt. For DocNADE, the log-probability log p(vt) is computed using LDN (v); however, we ignore

the mixture coefficient, i.e., =0 (equation 2) to compute the exact log-likelihood in ctx-DocNADE

versions. The optimal  is determined based on the validation set. Table 5 quantitatively shows the

PPL scores, where the complementary learning with  = 0.01 (optimal) in ctx-DocNADE achieves

lower perplexity than the baseline DocNADE for both short and long texts, e.g., (822 vs 846) and

(1358 vs 1375) on AGnewstitle and 20NS datasets, respectively in the FV setting.

6

Under review as a conference paper at ICLR 2019

Precision (%)

Precision (%)

0.0005 0.001 0.002 0.005 0.01 0.02 0.05
0.1 0.2 0.3 0.5 00..00000...000000000015152 0.01 0.02 0.05 0.1 0.2 0.3 0.5 0.8 1.0 00..000...00000...000000000000015251215 0.1 0.2 0.3 0.5 0.8 1.0

0.5

0.46

ctx-DocNADEe ctx-DocNADE

0.42 DocNADE

0.38 glove

0.34

0.3

0.26

0.22

0.18

0.14

0.1

Fraction of Retrieved Documents (Recall)

(a) IR: 20NSshort

0.75

0.7

0.65

0.6

0.55

0.5

0.45

ctx-DocNADEe ctx-DocNADE

0.4 DocNADE(FV)

0.35 DocNADE(RV)

0.3 glove(FV)

0.25 glove(RV)

Fraction of Retrieved Documents (Recall)
(d) IR: TMNtitle

0.9

0.85

0.8

0.75

0.7 ctx-DocNADEe

0.65 ctx-DocNADE DocNADE(FV)
0.6 DocNADE(RV)

0.55

glove(FV) glove(RV)

0.5

Fraction of Retrieved Documents (Recall)

(b) IR: Subjectivity

0.8

0.75

0.7

0.65

0.6

0.55 ctx-DocNADEe 0.5 ctx-DocNADE

0.45 DocNADE(FV)

0.4

DocNADE(RV) glove(FV)

0.35 glove(RV)

0.3

Fraction of Retrieved Documents (Recall)

(e) IR: AGnewstitle

0.66 ctx-DocNADEe 0.64 ctx-DocNADE
DocNADE(FV) 0.62 DocNADE(RV) 0.6 glove(FV)
glove(RV) 0.58
0.56
0.54
0.52
0.5

Fraction of Retrieved Documents (Recall)

(c) IR: Polarity

0.7 ctx-DocNADEe

0.6 ctx-DocNADE DocNADE(FV)

0.5

DocNADE(RV) glove(FV)

glove(RV) 0.4

0.3

0.2

0.1 Fraction of Retrieved Documents (Recall)
(f) IR: 20NS

Figure 3: Retrieval performance (IR-precision) on 6 datasets at different fractions

00..00000...000000000015512 0.01 0.02 0.05 0.1 0.2 0.3 0.5
00..00000...000000000015521 0.01 0.02 0.05 0.1 0.2 0.3 0.5
00..000...00000000000012515 0.01 0.02 0.05 0.1 0.2 0.3 0.5

Data
20NSshort TREC6
R21578title Polarity TMNtitle TMN

glove-DMM W10 W20 .512 .575 .410 .475 .364 .458 .637 .363 .633 .778 .705 .444

glove-LDA DocNADE W10 W20 W10 W20 .616 .767 .669 .779 .551 .736 .699 .818 .478 .677 .701 .812 .375 .468 .610 .742 .651 .798 .712 .822 .550 .683 .642 .762

ctx-DNE ctx-DNEe W10 W20 W10 W20 .682 .794 .696 .801 .714 .810 .713 .809 .713 .802 .723 .834 .611 .756 .650 .779 .716 .831 .735 .845 .639 .759 .709 .825

Data glove-DMM DocNADE W10 W20 W10 W20
Subjectivity .538 .433 .613 .749 AGnewstitle .584 .678 .731 .757
20NSsmall .578 .548 .508 .628 Reuters8 .372 .302 .583 .710 20NS .458 .374 .606 .729 Avg (all) .527 ..452 .643 .755

ctx-DNE ctx-DNEe W10 W20 W10 W20 .629 .767 .634 .771 .739 .858 .746 .865 .546 .667 .565 .692 .584 .710 .592 .714 .615 .746 .631 .759 .654 .772 .672 .790

Table 6: Average coherence for short and long texts over 200 topics in FV setting, where DocNADE  DNE

3.2 INTERPRETABILITY: TOPIC COHERENCE
We compute topic coherence (Chang et al., 2009; Newman et al., 2009; Gupta et al., 2018a) to assess the meaningfulness of the underlying topics captured. We choose the coherence measure proposed by Ro¨der et al. (2015) , which identifies context features for each topic word using a sliding window over the reference corpus. Higher scores imply more coherent topics.
We use the gensim module (radimrehurek.com/gensim/models/coherencemodel.html, coherence type = c v) to estimate coherence for each of the 200 topics (top 10 and 20 words). Table 6 shows average coherence over 200 topics, where the higher scores in ctx-DocNADE compared to DocNADE (.772 vs .755) suggest that the contextual information and language structure help in generating more coherent topics. The introduction of embeddings in ctx-DocNADEe boosts the topic coherence, leading to a gain of 4.6% (.790 vs .755) on average over 11 datasets. Note that the proposed models also outperform the baselines methods glove-DMM and glove-LDA. Qualitatively, Table 7 illustrates an example topic from the 20NSshort text dataset for DocNADE, ctx-DocNADE and ctx-DocNADEe, where the inclusion of embeddings results in a more coherent topic.
3.3 APPLICABILITY: TEXT RETRIEVAL AND CATEGORIZATION
Text Retrieval: We perform a document retrieval task using the short-text and long-text documents with label information. We follow the experimental setup similar to Lauly et al. (2017), where all test documents are treated as queries to retrieve a fraction of the closest documents in the original training set using cosine similarity measure between their textTOvec representations (section
7

Under review as a conference paper at ICLR 2019

2.2). To compute retrieval precision for each fraction (e.g., 0.0001, 0.005, 0.01, 0.02, 0.05, etc.), we average the number of retrieved training documents with the same label as the query. For multi-label datasets, we average the precision scores over multiple labels for each query. Since, Salakhutdinov & Hinton (2009) and Lauly et al. (2017) have shown that RSM and DocNADE strictly outperform LDA on this task, we solely compare DocNADE with our proposed extensions.

ctxDocNADE -DocNADEe

DocNADE ctx-DocNADE ctx-DocNADEe

vga, screen, computer, color, svga, graphics

computer, sell, screen, offer, bar, macintosh,

color, powerbook, vga, card,

san, windows,

sold, cars, terminal, forsale, utility, monitor,

svga, offer

gov, vesa computer, processor

.554 .624

.667

Query :: "emerging economies move ahead nuclear plans" #match

#IR1 :: imf sign lifting japan yen

YES

#IR2 :: japan recovery takes hold debt downgrade looms YES

#IR3 :: japan ministers confident treasuries move

YES

#IR1 :: nuclear regulator back power plans

NO

#IR2 :: defiant iran plans big rise nuclear

NO

#IR3 :: japan banks billion nuclear operator sources

YES

Table 7: A topic of 20NS dataset with coherence Table 8: Illustration of the top-3 retrievals for an input query
Table 3 and 4 show the retrieval precision scores for the short-text and long-text datasets, respectively at retrieval fraction 0.02. Observe that the introduction of both pre-trained embeddings and language/contextual information leads to improved performance on the IR task noticeably for short texts. We also investigate topic modeling without pre-processing and filtering certain words, i.e. the FV setting and find that the DocNADE(FV) or glove(FV) improves IR precision over the baseline RV setting. Therefore, we opt for the FV in the proposed extensions. On an average over the 8 shorttext and 6 long-text datasets, ctx-DocNADEe reports a gain of 7.1% (.630 vs .588) (Table 3) 6.0% (.601 vs .567) (Table 4), respectively in precision compared to DocNADE(RV). In addition, the deep variant (d=3) with embeddings, i.e., ctx-DeepDNEe shows competitive performance on TREC6 and Subjectivity datasets. Figures (3a, 3b, 3c) and (3d, 3e and 3f) illustrate the average precision for the retrieval task on 6 datasets. Observe that ctx-DocNADEe outperforms the baselines at all fractions with a gain of 6.5% (.615 vs .577) in precision at fraction 0.02.
Text Categorization: We perform text categorization to measure the quality of our textTovec representations. We consider the same experimental setup as in the retrieval task and extract textTOvec of 200 dimension for each document, learned during the training of ctx-DocNADE variants. To perform text categorization, we employ a logistic regression classifier with L2 regularization. While, ctx-DocNADEe and ctx-DeepDNEe make use of glove embeddings, they are evaluated against the topic model baselines with embeddings. For the short texts (Table 3), the glove leads DocNADE in classification performance, suggesting a need for distributional priors in the topic model. Therefore, the ctx-DocNADEe reports a gain of 4.8% (.705 vs .673) and 3.6%(.618 vs .596) in F 1, compared to DocNADE(RV) on an average over the short (Table 3) and long (Table 4) texts, respectively. In result, a gain of 4.4% (.662 vs .634) overall.

3.4 QUALITATIVE INSPECTION OF LEARNED REPRESENTATIONS
To further interpret the topic models, we analyze the meaningful semantics captured via topic extraction. Table 7 shows a topic extracted using 20NS dataset that could be interpreted as computers, which are (sub)categories in the data, confirming that meaningful topics are captured. Observe that the ctx-DocNADEe extracts a more coherent topic due to embedding priors. To qualitatively inspect the contribution of word embeddings and textTOvec representations in topic models, we analyse the text retrieved for each query using the representations learned from DocNADE and ctxDoocNADEe models. Table 8 illustrates the retrieval of the top 3 texts for an input query, selected from TMNtitle dataset, where #match is YES if the query and retrievals have the same class label. Observe that ctx-DocNADEe retrieves the top 3 texts, each with no unigram overlap with the query.

3.5 CONCLUSION
In this work, we have shown that accounting for language concepts such as the ordering of words in neural autoregressive topic models helps to better estimate the probability of a word in a given context. To this end, we have combined a topic- (i.e., DocNADE) and a neural language (e.g., LSTM) model in a single probabilistic framework. This facilitates learning a latent representation from the entire document whilst accounting for the word order in the collocation patterns. We further augment this complementary learning with external knowledge by introducing word embeddings.

8

Under review as a conference paper at ICLR 2019
REFERENCES
Yoshua Bengio, Re´jean Ducharme, Pascal Vincent, and Christian Jauvin. A neural probabilistic language model. Journal of machine learning research, 3(Feb):1137­1155, 2003.
D. Blei, A. Ng, and M. Jordan. Latent dirichlet allocation. pp. 993­1022, 2003.
Jonathan Chang, Jordan Boyd-Graber, Chong Wang, Sean Gerrish, and David M. Blei. Reading tea leaves: How humans interpret topic models. In In Neural Information Processing Systems (NIPS), 2009.
Rajarshi Das, Manzil Zaheer, and Chris Dyer. Gaussian lda for topic models with word embeddings. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), volume 1, pp. 795­804, 2015.
Pankaj Gupta, Subburam Rajaram, Hinrich Schu¨tze, and Bernt Andrassy. Deep temporal-recurrentreplicated-softmax for topical trends over time. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), volume 1, pp. 1079­1089, New Orleans, USA, 2018a. Association of Computational Linguistics.
Pankaj Gupta, Subburam Rajaram, Hinrich Schu¨tze, and Bernt Andrassy. Deep temporal-recurrentreplicated-softmax for topical trends over time. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pp. 1079­1089. Association for Computational Linguistics, 2018b. URL http://aclweb.org/anthology/N18-1098.
Geoffrey E. Hinton, Simon Osindero, and Yee Whye Teh. A fast learning algorithm for deep belief nets. Neural Computation, 18:1527­1554, 2006.
Sepp Hochreiter and Ju¨rgen Schmidhuber. Long short-term memory. Neural computation, 9(8): 1735­1780, 1997.
Hugo Larochelle and Yoshua Bengio. Classification using discriminative restricted boltzmann machines. In Proceedings of the 25th international conference on Machine learning, pp. 536­543. ACM, 2008.
Hugo Larochelle and Stanislas Lauly. A neural autoregressive topic model. In Proceedings of the Advances in Neural Information Processing Systems 25 (NIPS 2012). NIPS, 2012.
Hugo Larochelle and Iain Murray. The neural autoregressive distribution estimator. In Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics, pp. 29­37, 2011.
Stanislas Lauly, Yin Zheng, Alexandre Allauzen, and Hugo Larochelle. Document neural autoregressive distribution estimation. Journal of Machine Learning Research, 18(113):1­24, 2017. URL http://jmlr.org/papers/v18/16-017.html.
Quoc V. Le and Tomas Mikolov. Distributed representations of sentences and documents. pp. 1188­ 1196, 2014.
Toma´s Mikolov, Martin Karafia´t, Luka´s Burget, Jan C ernocky`, and Sanjeev Khudanpur. Recurrent neural network based language model. In Eleventh Annual Conference of the International Speech Communication Association, 2010.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient estimation of word representations in vector space. In Workshop Track of the 1st International Conference on Learning Representations (ICLR 2013), 2013.
David Newman, Sarvnaz Karimi, and Lawrence Cavedon. External evaluation of topic models. In Proceedings of the 14th Australasian Document Computing Symposium, 2009.
9

Under review as a conference paper at ICLR 2019
Dat Quoc Nguyen, Richard Billingsley, Lan Du, and Mark Johnson. Improving topic models with latent feature word representations. Transactions of the Association for Computational Linguistics, 3:299­313, 2015.
Kamal Nigam, Andrew Kachites McCallum, Sebastian Thrun, and Tom Mitchell. Text classification from labeled and unlabeled documents using em. Machine learning, 39(2-3):103­134, 2000.
Jeffrey Pennington, Richard Socher, and Christopher D. Manning. Glove: Global vectors for word representation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, EMNLP 2014, October 25-29, 2014, Doha, Qatar, A meeting of SIGDAT, a Special Interest Group of the ACL, pp. 1532­1543, 2014. URL http://aclweb.org/ anthology/D/D14/D14-1162.pdf.
Matthew Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. Deep contextualized word representations. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pp. 2227­2237. Association for Computational Linguistics, 2018. URL http://aclweb.org/anthology/N18-1202.
James Petterson, Wray Buntine, Shravan M Narayanamurthy, Tibe´rio S Caetano, and Alex J Smola. Word features for latent dirichlet allocation. In Advances in Neural Information Processing Systems, pp. 1921­1929, 2010.
Michael Ro¨der, Andreas Both, and Alexander Hinneburg. Exploring the space of topic coherence measures. In Proceedings of the WSDM. ACM, 2015.
Mehran Sahami and Timothy D Heilman. A web-based kernel function for measuring the similarity of short text snippets. In Proceedings of the 15th international conference on World Wide Web, pp. 377­386. AcM, 2006.
Has¸im Sak, Andrew Senior, and Franc¸oise Beaufays. Long short-term memory recurrent neural network architectures for large scale acoustic modeling. In Fifteenth annual conference of the international speech communication association, 2014.
Ruslan Salakhutdinov and Geoffrey Hinton. Replicated softmax: an undirected topic model. In Proceedings of the Advances in Neural Information Processing Systems 22 (NIPS 2009), pp. 1607­1614. NIPS, 2009.
Hanna M Wallach. Topic modeling: beyond bag-of-words. In Proceedings of the 23rd international conference on Machine learning, pp. 977­984. ACM, 2006.
Xuerui Wang, Andrew McCallum, and Xing Wei. Topical n-grams: Phrase and topic discovery, with an application to information retrieval. In icdm, pp. 697­702. IEEE, 2007.
Yin Zheng, Yu-Jin Zhang, and Hugo Larochelle. A deep and autoregressive approach for topic modeling of multimodal data. In IEEE transactions on pattern analysis and machine intelligence, pp. 1056­1069. IEEE, 2016.
A DATA DESCRIPTION
We use 14 different datasets: (1) 20NSshort: We take documents from 20NewsGroups data, with document size less (in terms of number of words) than 20. (2) TREC6: a set of questions (3) Reuters21578title: a collection of new stories from nltk.corpus. We take titles of the documents. (4) Subjectivity: sentiment analysis data. (5) Polarity: a collection of positive and negative snippets acquired from Rotten Tomatoes (6) TMNtitle: Titles of the Tag My News (TMN) news dataset. (7) AGnewstitle: Titles of the AGnews dataset. (8) Reuters8: a collection of news stories, processed and released by (9) Reuters21578: a collection of new stories from nltk.corpus. (10) 20NewsGroups: a collection of news stories from nltk.corpus. (11) RCV1V2 (Reuters): www.ai.mit.edu/projects/jmlr/papers/ volume5/lewis04a/lyrl2004_rcv1v2_README.htm (12) 20NSsmall: We sample 20 document for training from each class of the 20NS dataset. For validation and test, 10 document
10

Under review as a conference paper at ICLR 2019

Label: training Instructors shall have tertiary education and experience in the operation and maintenance of the equipment or sub-system of Plant. They shall be proficient in the use of the English language both written and oral. They shall be able to deliver instructions clearly and systematically. The curriculum vitae of the instructors shall be submitted for acceptance by the Engineer at least 8 weeks before
the commencement of any training.
Label: maintenance The Contractor shall provide experienced staff for 24 hours per Day, 7 Days per week, throughout the Year,
for call out to carry out On-call Maintenance for the Signalling System.
Label: cables Unless otherwise specified, this standard is applicable to all cables which include single and multi-core cables
and wires, Local Area Network (LAN) cables and Fibre Optic (FO) cables.
Label: installation The Contractor shall provide and permanently install the asset labels onto all equipment supplied under this Contract. The Contractor shall liaise and co-ordinate with the Engineer for the format and the content of the labels. The Contractor shall submit the final format and size of the labels as well as the installation layout of the labels on the respective equipment, to the Engineer for acceptance.
Label: operations, interlocking It shall be possible to switch any station Interlocking capable of reversing the service into "Auto-Turnaround Operation". This facility once selected shall automatically route Trains into and out of these stations, independently of the ATS system. At stations where multiple platforms can be used to reverse
the service it shall be possible to select one or both platforms for the service reversal.
Table 9: SiROBs data: Example Documents (Requirement Objects) with their types (label).

Hyperparameter learning rate hidden units iterations
activation function 

Search Space [0.001] [200] [2000] sigmoid
[1.0, 0.8, 0.5, 0.3, 0.1, 0.01, 0.001]

Table 10: Hyperparameters in Generalization in the DocNADE and ctx-DocNADE variants for 200 topics

for each class. (13) TMN: The Tag My News (TMN) news dataset. (14) Sixxx Requirement OBjects (SiROBs): a collection of paragraphs extracted from industrial tender documents (our industrial corpus). The SiROBs is our industrial corpus, extracted from industrial tender documents. The documents contain requirement specifications for an industrial project for example, railway metro construction. There are 22 types of requirements i.e. class labels (multi-class), where a requirement is a paragraph or collection of paragraphs within a document. We name the requirement as Requirement Objects (ROBs). Some of the requirement types are project management, testing, legal, risk analysis, financial cost, technical requirement, etc. We need to classify the requirements in the tender documents and assign each ROB to a relevant department(s). Therefore, we analyze such documents to automate decision making, tender comparison, similar tender as well as ROB retrieval and assigning ROBs to a relevant department(s) to optimize/expedite tender analysis. See some examples of ROBs from SiROBs corpus in Table 9.
B EXPERIMENTAL SETUP
B.1 EXPERIMENTAL SETUP AND HYPERPARAMETERS FOR GENERALIZATION TASK
See Table 10 for hyperparameters used in generalization.
11

Under review as a conference paper at ICLR 2019

Hyperparameter retrieval fraction
learning rate hidden units activation function
iterations 

Search Space [0.02] [0.001] [200] tanh [2000]
[1.0, 0.8, 0.5, 0.3, 0.1, 0.01, 0.001]

Table 11: Hyperparameters in the Document Retrieval task.

Dataset

Model

20NSshort Subjectivity
TMNtitle AGnewstitle
Reuters-8 20NS

ctx-DocNADE ctx-DocNADEe ctx-DocNADE ctx-DocNADEe ctx-DocNADE ctx-DocNADEe ctx-DocNADE ctx-DocNADEe ctx-DocNADE ctx-DocNADEe ctx-DocNADE ctx-DocNADEe

1.0 899.04 890.3
1898.1 1877.7 1296.1 1279.2 336.1 323.3 1282.1 1247.1



0.1 0.01

829.5

-

828.8

-

977.8 966.5

975.02 964.2

1482.7 -

1480.2 -

- 865

- 862.9

- 311.9

- 310.2

1209.3 1207.2

- 1206.1

0.001 -
966.9 -
1488.9 1485.3
-

Table 12:  for Generalization task: Ablation over validation set

B.2 EXPERIMENTAL SETUP AND HYPERPARAMETERS FOR IR TASK
We set the maximum number of training passes to 1000, topics to 200 and the learning rate to 0.001 with tanh hidden activation. For model selection, we used the validation set as the query set and used the average precision at 0.02 retrieved documents as the performance measure. Note that the labels are not used during training. The class labels are only used to check if the retrieved documents have the same class label as the query document. To perform document retrieval, we use the same train/development/test split of documents discussed in data statistics (experimental section) for all the datasets during learning.
See Table 11 for the hyperparameters in the document retrieval task.
B.3 EXPERIMENTAL SETUP FOR DOC2VEC MODEL
We used gensim (https://github.com/RaRe-Technologies/gensim) to train Doc2Vec models for 12 datasets. Models were trained with distributed bag of words, for 1000 iterations using a window size of 5 and a vector size of 500.
B.4 CLASSIFICATION TASK
We used the same split in training/development/test as for training the Doc2Vec models (also same split as in IR task) and trained a regularized logistic regression classifier on the inferred document vectors to predict class labels. In the case of multilabel datasets (R21578,R21578title, RCV1V2), we used a one-vs-all approach. Models were trained with a liblinear solver using L2 regularization and accuracy and macro-averaged F1 score were computed on the test set to quantify predictive power.
B.5 EXPERIMENTAL SETUP FOR GLOVE-DMM AND GLOVE-LDA MODELS
We used LFTM (https://github.com/datquocnguyen/LFTM) to train glove-DMM and glove-LDA models. Models were trained for 200 iterations with 2000 initial iterations using 200 topics. For short texts we set the hyperparameter beta to 0.1, for long texts to 0.01; the mixture
12

Under review as a conference paper at ICLR 2019

Dataset
20NSshort Subjectivity Polarity TMNtitle TMN AGnewstitle 20NSsmall Reuters-8 20NS R21578 SiROBs AGnews

Model
ctx-DocNADE ctx-DocNADEe ctx-DocNADE ctx-DocNADEe ctx-DocNADE ctx-DocNADEe ctx-DocNADE ctx-DocNADEe ctx-DocNADE ctx-DocNADEe ctx-DocNADE ctx-DocNADEe ctx-DocNADE ctx-DocNADEe ctx-DocNADE ctx-DocNADEe ctx-DocNADE ctx-DocNADEe ctx-DocNADE ctx-DocNADEe ctx-DocNADE ctx-DocNADEe ctx-DocNADE ctx-DocNADEe

1.0 0.264 0.277 0.874 0.868 0.587 0.602 0.556 0.604 0.683 0.696 0.665 0.686 0.352 0.381
0.503 0.524 0.714 0.715 0.409 0.41 0.786 0.795

 0.8 0.5 0.265 0.265 0.277 0.278 0.874 0.873 0.868 0.874 0.588 0.591 0.603 0.601 0.557 0.559 0.604 0.6 0.689 0.692 0.698 0.698 0.668 0.678 0.688 0.695 0.356 0.366 0.381 0.375
0.87 0.872 0.873 0.506 0.513 0.521 0.518 0.714 0.714 0.715 0.715 0.409 0.408 0.411 0.411 0.789 0.792 0.796 0.8

0.3 0.265 0.276 0.874 0.87 0.587 0.599 0.568 0.6 0.694 0.7 0.689 0.696 0.37 0.353 0.87
0.512 0.511 0.714 0.714 0.408 0.409 0.797 0.799

Table 13:  for IR task: Ablation over validation set at retrieval fraction 0.02

parameter lambda was set to 0.6 for all datasets. The setup for the classification task was the same as for doc2vec; classification was perfromed using relative topic proportions as input (i.e. we inferred the topic distribution of the training and test documents and used the relative distribution as input for the logistic regression classifier). Similarly, for the IR task, similarities were computed based on the inferred relative topic distribution.
C ABLATION OVER THE MIXTURE WEIGHT 
C.1  FOR GENERALIZATION TASK
See Table 12.
C.2  FOR IR TASK
See Table 13.

13


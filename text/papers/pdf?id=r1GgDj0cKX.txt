Under review as a conference paper at ICLR 2019
PRUNING IN TRAINING: LEARNING AND RANKING SPARSE CONNECTIONS IN DEEP CONVOLUTIONAL NETWORKS
Anonymous authors Paper under double-blind review
ABSTRACT
This paper proposes a Pruning in Training (PiT) framework of learning to reduce the parameter size of networks. Different from existing works, our PiT framework employs the sparse penalties to train networks and thus help rank the importance of weights and filters. Our PiT algorithms can directly prune the network without any fine-tuning. The pruned networks can still achieve comparable performance to the original networks. In particular, we introduce the (Group) Lasso-type Penalty (L-P /GL-P), and can alternatively implement them using (Group) Split LBI (S-P / GS-P)­ an regularization solution path with corresponding penalties to regularize the networks, and a pruning strategy proposed by Fu et al. (2016b) is used to help prune the network. We conduct the extensive experiments on MNIST, Cifar-10, and miniImageNet. The results validate the efficacy of our proposed methods. Remarkably, on MNIST dataset, our PiT framework can produce a small network which only has 17.5% parameter size of LeNet-5, and achieves the 98.47% recognition accuracy.
1 INTRODUCTION
The expressive power of Deep Convolutional Neural Networks (DNNs) comes from the millions of parameters, which are optimized by various algorithms such as Stochastic Gradient Descent (SGD), and Adam Kingma & Ba (2015). However, one has to strike a trade-off between the representation capability and computational cost, caused by the plenty of parameters in the real world applications, e.g., robotics, self-driving cars, and augmented reality. Pruning significant number of parameters would be essential to reduce the computational complexity and thus facilitate a timely and efficient fashion on a resource-limited platform, e.g. devices of Internet of Things (IoT). In addition, it has long been conjectured that the state-of-the-art DNNs may be too complicated for most specific tasks; and we may have the free lunch of "reducing 2× connections without losing accuracy and without retraining" Han et al. (2015b).
To compress DNNs, recent efforts had been made on learning the DNNs of small size. They either reduce the number and size of weights of parameters of original networks, and fine-tune the pruned networks Abbasi-Asl & Yu (2017); Yang et al. (2018), or distill the knowledge of large model Hinton et al. (2014), or directly learning the compact and lightweight small DNNs, such as ShuffleNet Ma et al. (2018), MobileNet Howard et al. (2017), and SqueezeNet Iandola et al. (2017). Note that, (1) to efficiently learn the compressed DNNs, previous works had to introduce additional computational cost in fine-tuning, or training the updated networks; (2) it is not practical nor desirable to learn the tailored, or bespoke networks for any applications, beyond computer vision tasks.
To this end, the center idea of this paper is to propose a Pruning in Training (PiT) framework that enables pruning networks in the training process. Particularly, the sparsity regularizers, including lasso-type, and split LBI penalties are applied to train the networks. Such regularizers not only encourage the sparsity of DNNs, i.e., fewer (sparse) connections with non-zero values, but also can accelerate the speed of DNNs convergence. Furthermore, in the learning process, we can iteratively compute the regularization path of layer-wise parameters of DNNs. The parameters can be ranked by the regularization path in a descending order, as Fu et al. (2016a). The parameters in the high rank are in the high priority of not being pruned.
1

Under review as a conference paper at ICLR 2019
More importantly, our PiT can learn the sparse structures of DNNs, and utilize the functionality of filters and connection weights (in fully connected layers). In the optimal cases, the weights (or filters) of each layer should be learned fully orthogonal to each other and thus formulate an orthogonal basis. The orthogonal constraint may be only enforced as the initialization (e.g., SVD Jia (2017) and Saxe et al. (2014)), or via the other regularization tricks, such as dropout preventing co-adaption Srivastava et al. (2014), or batch normalization reducing the internal covariate shift of hidden layers Ioffe & Szegedy (2015). Therefore, our PiT can help uncover redundant information in a network by compressing less important filters and weights, and facilitate pruning out more interpretable networks.
2 RELATED WORKS
The deeper and wider deep CNN architectures can enable the superior performance on various tasks, and yet cause the prohibitively expensive computation cost. To efficiently train the networks, the regularization is usually applied to the weight parameters (Sec. 2.1). It is also essential to prune networks to reduce the size of networks (Sec. 2.2)
2.1 NETWORK REGULARIZATION
Due to large number of parameters, the deep networks require large amount of memory and computational resources, and are inclined to overfit the training data. To alleviate this problem, it is essential to regularize the networks in training stage; such as dropout Srivastava et al. (2014) preventing the co-adaptation, and adding L2 or L1 regularization to weights. In particular, the L1 regularization enforces the sparsity on the weights and results in a compact, memory-efficient network with slightly sacrificing the prediction performance Collins & Kohli (2014). Further, group sparsity regularization Yuan & Lin (2006) can also been applied to deep networks with desirable properties. Alvarez et al. Alvarez & Salzmann (2016) utilized a group sparsity regularizer to automatically decide the optimal number of neuron groups. The structured sparsity Wen et al. (2016a); Yoon & Hwang (2017) has also been investigated to exert good data locality and group sparsity. Different from these works, the (Group) Split LBI penalty is for the first time, introduced to regularize the networks. This regularization term can not only enforce the structured sparsity, but also can efficiently compute the solution paths of each variable.
2.2 NETWORK PRUNING
Compressing the networks involves the pruning and compressing the weights and filters of DNNs. The common strategies include (1) matrix decomposition methods Jaderberg et al. (2014); Zhang et al. (2016; 2015); Tai et al. (2016) by decomposing the weight matrix of DNNs as a low-rank product of two smaller matrices; (2) low-precision weights methods Zhu et al. (2017); Zhou et al. (2017) by learning to store low-precision weights of DNNs; and (3) pruning methods Han et al. (2015b); Li et al. (2017) directly removing weights of connections, or neurons.
Our framework is one of pruning methods. Previous pruning works, iteratively prune the weights or neurons, and fine-tune the network Han et al. (2015b); Guo et al. (2016). Remarkably, network regularization is of significant important in pruning methods. The sparse properties of features maps and/or weights of DNNs exerted by network regularization, are utilized in Wen et al. (2016b); Lebedev & Lempitsky (2016). Luo et al. Luo et al. (2017) adopt the statistics information from next layer to guide and save the importance of filters of the current layer. Molchanov et al. Molchanov et al. (2017) employed Taylor expansion to approximate the change of cost function which can be further utilized as the criterion in pruning network parameters. A LASSO-based channel selection strategy is investigated in He et al. (2017). Abbasi-Asl et al. Abbasi-Asl & Yu (2017) defined a filter importance index of greedy pruning the network. Comparing with all the methods, our framework is different in two points: (1) Criterion of importance of weights and filters. We rank the importance of weights and filters by their solution paths computed by sparse regularizers, rather than designing the elaborated metrics as previous works Abbasi-Asl & Yu (2017); Yang et al. (2018). Specifically, our algorithm is a process of solving the discrete partial differential equations; and our framework can result in the solution paths of optimizing the weights and filters, whose importance are ranked,
2

Under review as a conference paper at ICLR 2019

according to the selected order in the path, as Fu et al. (2016b). (2) Pruning in training: once DNNs are trained, we simply prune out less important weights/filter by a threshold.

3 METHODOLOGY
In this section, the Residual Network (ResNet) structureHe et al. (2016) is employed to elaborate our framework. Our algorithms can be used in the other DNNs, e.g. Lenet-5.
3.1 NOTATION
We adopt the notations of ResNet strucutre, in which the output of the ith block Oi can be represented as:
Oi = F (x, {W }i) + Wix
where x the input of the first layer of the ith block, {W }i and Wi respresent the filter weights in the ith block and the shortcut weight matrix, respectively. The function F(·) represent the multiple convolutional layers. Denote the weight matrix of the first convolutional layer as Wconv1 and that of the fully connected layer as Wfc. Suppose there are I blocks, then we denote all the parameters of the network as  := {Wconv1, {W }1, ..., {W }I , W1, ..., WI , Wfc} and -W := \W for W  . Our key objective is to train a sparse DNN of less parameters, and yet comparable performance to the non-sparse DNN. The training function of DNN is defined as,

minL (; X , y) +  · P ()


(1)

where P (·) is the penalty function of parameters . If we use (X , y) as the sample set of the dataset; then in classification task, the loss function is the cross-entropy function as

L(; X , y) = - 1 N N

K
yn,k log{pn,k()},

n=1 k=1

(2)

where N and K are the number of samples and classes and pn,k () denotes the probability of the nth sample belongs to class k. Generally, we can use Stochastic Gradient Descent (SGD) algorithm
to update ; and the algorithm is summarized in algorithm 1.

Algorithm 1 SGD for ResNet
1: Input: Learning rate , X and y 2: Initialize: k = 0, k is initialized randomly.
3: Iteration 4: k+1 = k - L(k) 5: Output: {k+1}

3.2 REGULARIZATION ON ONE LAYER

One direct intuition is to adopt the sparsity regularization on the parameters, or those of the one layer of the network, such as Liu et al. (2015); Wen et al. (2016b). To reduce the number of connection weights, one can consider different types of regularization, including (1) Lasso-type penalty (L1), (2) Group-Lasso-type penalty Yuan & Lin (2006); (3) An iterative regularization path with structural sparsity (e.g., elastic net Zou & Hastie (2005), and Split LBI Huang et al. (2016)): here we employ the Split LBI which learns the structural sparsity via variable splitting and Linearized Bregman Iteration (LBI), due to the computational efficiency of the LBI, and model selection consistency,
Lasso-type penalty can be directly implemented on the fully connection layer i as,

P (W ) = W 1;

(3)

3

Under review as a conference paper at ICLR 2019

Group-Lasso-type penalty Yuan & Lin (2006) aims at regularizing the groups of parameters , and W (g) is a group of partial weights in ,

G

P (W ) =

W (g) 2

g=1

(4)

where W (g) 2 =

|W (g)| i=1

Wi(g)

2
, and

W (g)

is the number of weights in W (g); G is the

total number of groups.

3.3 OPTIMIZATION

This Split LBI algorithm Huang et al. (2016) introduces an augmented variable  which is enforced

sparsity

and

kept

close

to

W,

by

variable

splitting

term

1 2

-W

22. Then the objective function

turns to:

L(W,

;

-W

,X,

y)

=

L(W ;

-W ,

X,

y)

+

1 2

-W

22, ( > 0)

To enforce the sparsity of , we here implement the LBI algorithm on the W , and the algorithm can be summarized in algorithm 2, where

1

ProxL (Z )

=

arg

min
W

2

W -Z

2 2

+

P

(W

)

(5)

The 5th-8th lines are Split LBI algorithm, which returns a regularization path of
{-k W , W k, W k, k}. It starts from the null model with 0 = 0, and tends to select more and more variables as the algorithm evolves, until over-fitted. At each step, the sparse estimator W k is the projection of W k onto the subset of the support set of k. The remainder of the projection is affected by weak signals with small magnitude and mostly the ones mainly affected by random noise. Particularly, we highlight several points,

· The  is the damping factor, which enjoys the low bias with larger value, however, at the sacrifice of high computational cost. The  is the step size. In Huang et al. (2016), it has been proved that the  is the inverse scale with  and should be small enough to ensure the statistical property. In our scenario, we set it to 0.01/.
· The tk = k is the regularization parameter, which plays the similar role with  in Lasso. It's the trade-off between underfiting and overfiting, which can be determined via the loss/accuracy on the validation dataset.

· The  controls the difference between W and W . In Huang et al. (2016), it has been proved that larger value of  can enjoy better model selection consistency, however may suffer from the larger parameter estimation error. In Sun et al. (2017); Zhao et al. (2018), it has been proved that as long as  0, the dense estimator W can enjoy better prediction error by leveraging weak signals. We will discuss it in the next subsection.
· Each component of the closed form solution W  Rp1×p2 in equation 5 can be simplified as,

 max(0, 1 - 1/ Z(g) 2)Z(g)  max(0, 1 - 1/|Zi|)Zi

P defined in equation 4 for g  {1, ..., G} P defined in equation 3 for i  {1, ..., p1 × p2} (6)

3.4 PRUNING STRATEGY
The pruning algorithm is inspired by the Fu et al. (2016b). Particularly, it has been pointed out in Zhao et al. (2018) that the dense estimator can be orthogonally decomposed into three parts: strong signals which correspond to non-zero elements in W , weak signals and random noise. Due to the ability to leverage additional weak signals as long as  is large enough, it has been proved theoretically and experimentally that, the dense estimator outperforms the sparse estimator in prediction.

4

Under review as a conference paper at ICLR 2019
Algorithm 2 SGD for ResNet with Split LBI 1: Input: Learning rate ,  > 0, step size of LBI , damping factor  > 0, X and y 2: Initialize: k = 0, k is initialized randomly, k = Zk = 0 3: Iteration 4: -k+W1 = -k W - -W L(-k W , W k, k)
# LBI update 5: W k+1 = W k - W L(-k W , W k, k) 6: Zk+1 = Zk - L(-k W , W k, k) 7: k+1 = ProxJ (Zk+1) 8: W k+1 = W k  1{i  Sk+1} i,j Sk+1 = supp(k) 9: Output: {k-+W1, W k+1, W k+1, k+1}
This inspires us to sequentially consider all available solutions for all sparse variables along the Regularization Path (RP) by gradually decreasing the values of regularization coefficients. Specifically, we can order the parameter set  according to the magnitude values of weights W . Following this order, we identify the top r% of weights in r. The complementary set 1-r = \r can be pruned. Compared to the pruning methods in Han et al. (2015a), we can prune the weights in the training process and do not need to fine-tune the weights.
3.5 REGULARIZATION ON MULTIPLE LAYERS
Furthermore, one can easily extend algorithm 2 to prune at L (L > 1) layers. We take the Split LBI as an example; the other two methods can also be directly applied to multiple layers. The corresponding algorithm is described in algorithm 3.
4 EXPERIMENTS
Algorithm 3 SGD for ResNet with Split LBI on multiple layers 1: Input: Learning rate ,  > 0, step size of LBI , damping factor  > 0, X and y 2: Initialize: k = 0, k is initialized randomly, k1 = Z1k = 0, ..., kL = ZLk = 0 3: Iteration 4: k-+W1 = -{W1k,...,WLk} - -{W1k,...,WLk} L(-{W1k,...,WLk}, {W1k, ..., WLk}, {1k, ..., Lk })
# LBI update at L layers 5: For l = 1, ..., L 6: Wlk+1 = Wlk - W L(-{W1k,...,WLk}, {W1k, ..., WLk}, {k1, ..., Lk }) 7: Zlk+1 = Zlk - L(-{W1k,...,WLk}, {W1k, ..., WLk}, {1k, ..., kL}) 8: lk+1 = ProxJ (Zlk+1) 9: Wlk+1 = Wlk  1{i  Slk+1} i,j Slk+1 = supp(kl ) 10: End 11: Output: { ,-{W1k+1,...,WLk+1} {W1k+1, ..., WLk+1}, {W1k+1, ..., WLk+1}, {k1+1, ..., Lk+1}}
We conduct the experiments on three datasets, namely, MNIST, CIFAR10, and MiniImageNet. We use the standard supervised training and testing splits on all datasets, except MiniImageNet, whose setting is splitted by ourselves, and will be released. The classification accuracy is reported on each dataset.
Competitors. We compare three methods of pruning networks. (1) Plain: we train a plain network and use the L2- penalty P (W ) = W 2. For all layers, we set the coefficient  as 5e - 4 in Eq (1). We prune the trained network by ranking the weights and filters, in term of their magnitude values in the descending order. This pruning strategy can be taken as a simplified version of our pruning algorithm in Sec. 3.4. (2) Rand. We randomly remove the weights or filters in the networks. This is a naive baseline. (3) Ridge-Penalty (R-P) Han et al. (2015b): We use the same ranking methodology
5

Under review as a conference paper at ICLR 2019

(%) Plain Rand R-P GL-P GS-P
(%) Plain Rand R-P L-P S-P

100 99.17 99.11 99.12 99.05 99.00
100 99.10 99.09 99.13 99.10 99.13

25 12.5 6.25 3.13

60.87 43.72 61.05

29.65 30.07 46.91

20.82 18.35 30.34

20.82 24.12 30.34

74.29 47.28 28.58 28.58 85.09 32.58 22.88 22.88
(1) Pruning the conv.c3 layer 25 12.5 6.25 3.13

96.73 91.56 96.39

95.65 71.05 95.31

89.60 51.92 91.29

78.40 33.65 82.75

98.89 98.89 98.89 98.89 98.73 98.61 98.23 96.75
(3) Pruning the fc.c6 layer

1.57 20.82 22.62 30.34 28.58 22.88
1.57 64.17 29.91 68.35 98.89 92.53

(%) Plain Rand R-P GL-P GS-P
(%) Plain Rand R-P L-P S-P

100 25 12.5 6.25 3.13

99.12 99.19 99.16

80.46 62.23 75.47

62.61 37.71 60.31

45.49 23.58 37.97

32.34 18.58 26.11

98.95 98.95 90.29 60.37 32.91 98.97 98.96 98.67 68.27 42.10
(2) Pruning the conv.c5 layer 100 25 12.5 6.25 3.13

99.11 99.15 99.13

98.36 66.29 98.62

94.52 50.76 96.50

78.15 34.61 84.04

68.72 24.05 67.08

99.10 99.11 99.11 99.11 99.09 99.03 99.00 99.00 99.01 99.03
(4) Pruning the fc.f7 layer

1.57 21.30 14.36 18.11 20.31 24.95
1.57 47.35 19.78 56.34 96.47 95.41

Table 1: Pruning one layer in LeNet-5 on MNIST dataset (Top-1 Accuracy).

to rank the weights and filters by L2 regularization path. For that particular layer that we want to do the pruning, the coefficient  would be finally set as 1e - 3.
We also compare two variants of our PiT framework. (4) Lasso-type penalty or Group-Lasso-type penalty (L-P / GL-P): the L-P is used to prune the weights of fully connected layers, and we employ the GL-P to directly remove the filters of convolutional layers. (5) Split LBI or Group Split LBI penalty(S-P / GS-P): the split BLI penalty is utilized to prune the weights. Accordingly, we have the Group Split LBI penalty by regularizing the groups of filter parameters as Yuan & Lin (2006). Note that all the results are trained for one time; and we do not have fine-tuning step after the pruning.
4.1 LETNET ON MNIST
The handwritten digits MNIST dataset is widely used to experimentally evaluate the machine learning methods. We use the standard supervised split and LeNet-5 LeCun et al. (1998) which is composed of 3 convolutional layers and 2 fully connected layers. All the models are trained and get converged in 50 epochs. Note that each experiment is repeated for five times, and the averaged results are reported. In the experiments, we consider saving the portion of 100%, 50%, 25%, 12.5%, 6.25%, 3.13%, and 1.57% of original parameters on each layer. Please refer to the Appendix for more detailed results.
Pruning each layer. The results are shown in Tab. 1. We employ our PiT algorithms to prune each individual layers of LeNet-5, while we keep the parameters of the other layers unchanged. We have the following observations:
(1) On two fully connected layers (fc.f6 and fc.f7), both the L-P and S-P of our PiT framework work very well. For example, on the fc.f7 layer, our S-P only has 1.57% of the parameters on these layers. Surprisingly, our performance is only 0.03% lower than that of the original network. In contrast, we compare the pruning results with the baseline: Plain, Rand, and R-P. There is significant performance dropping with the more parameters pruned. This shows the efficacy of our PiT framework.
(2) On the convolutional layer (conv.c5), our L-P and S-P layers also achieve remarkable results. Note that the conv.c5 layer has 48k out of 60k number of parameters in Lenet-5. We show that our S-P saves 12.5% of total parameters of this layer (i.e., 42k number of parameters have been removed on this layer) and the results get only dropped by 0.3%. This demonstrates that our PiT framework indeed can save the relatively important weights and filters, and effectively do the network pruning.
(3) The conv.c3 layer is another convolutional layer in LeNet-5. We found that this layer is very important to maintain a good performance of overall network. Nevertheless, the results of our pruning L-P and S-P are still better than the other baselines.
Pruning two layers. Totally, the LeNet-5 has 60k parameters, while the conv.c5 and fc.f6 have 48k and 10k number of parameters respectively. That means these two layers have the most number of
6

Under review as a conference paper at ICLR 2019

(%) 100 25 12.5 6.25 3.13 1.57

fc.f6 + fc.f7 Com-Rat(%)

Plain Rand R-P L-P S-P
-- (%)

99.03 33.72 46.29 42.29 57.24 33.83 99.02 37.25 25.84 17.13 11.09 11.27 99.14 67.35 44.32 66.56 45.14 31.83 99.10 98.58 98.58 98.58 98.58 98.60 99.05 98.71 98.71 98.71 98.68 98.32 91.83 87.74 85.70 84.68 84.17 83.91
100 25 12.5 6.25 3.13 1.57

conv.c5+fc.f6 Com-Rat(%)

Plain Rand R-P GL-P / L-P GS-P / S-P
--

99.16 99.16 99.08 98.96 98.69 52.91

70.04 40.24 83.21 97.92 98.47 29.37

51.58 22.76 50.68 97.92 98.47 17.60

32.83 15.30 34.07 73.44 88.77 11.71

14.54 12.06 25.39 28.08 50.71 8.77

20.84 10.36 12.87 16.84 28.14 7.30

Table 2: Pruning two layers in LeNet-5 on the MNIST dataset. Each column indicates the percentage of parameter saved on these two layers. Com-Rat, is short for the compression ratio of the total network, i.e., the ratio of saved parameters divided the total number of parameters of LeNet-5.

(%) Plain Rand R-P GL-P GS-P
(%) Plain Rand R-P GL-P GS-P
(%) Plain Rand R-P GL-P GS-P

100 25 12.5 1.57

92.96 93.58 93.75

28.91 17.04 82.35

29.21 15.14 55.53

11.76 13.97 21.19

93.54 93.53 93.30 89.20 93.17 93.23 93.27 93.27
(1) Pruning Block#1.0 100 25 12.5 1.57

93.48 93.50 93.78

49.77 31.60 64.74

40.31 35.15 51.29

36.75 37.40 49.65

93.50 93.50 93.50 93.51 93.50 93.16 93.27 93.29
(4) Pruning the Block#2.1 100 25 12.5 1.57

93.08 93.44 93.55

82.70 48.88 88.63

54.18 40.89 69.91

38.29 37.38 47.40

93.65 93.65 93.65 93.68 93.61 93.50 93.45 93.41
(7) Pruning the Block#4.0

(%) Plain Rand R-P GL-P GS-P
(%) Plain Rand R-P GL-P GS-P
(%) Plain Rand R-P GL-P GS-P

100 25 12.5 1.57

93.44 92.90 93.95

37.32 25.63 64.03

20.60 30.82 46.21

18.84 34.33 27.67

93.57 93.57 93.57 93.57 93.52 93.53 93.53 93.49 (2) Pruning the Block#1.1 100 25 12.5 1.57

93.41 93.11 93.61

55.48 37.42 74.24

22.94 36.66 57.27

35.36 37.10 38.32

93.66 93.66 93.67 93.61 93.59 93.63 93.48 93.42 (5) Pruning the Block#3.0 100 25 12.5 1.57

93.37 93.57 93.63

90.01 82.86 88.20

85.29 79.16 87.16

75.53 76.73 70.77

93.79 93.79 93.79 93.77 93.61 93.93 93.83 93.89 (8) Pruning the Block#4.1

(%) Plain Rand R-P GL-P GS-P
(%) Plain Rand R-P GL-P GS-P

100 25 12.5 1.57

93.42 93.44 93.35

41.69 14.39 54.27

18.36 13.42 32.17

27.28 11.43 32.51

93.60 93.62 93.58 93.61 93.27 93.18 93.26 93.24 (3) Pruning the Block#2.0
100 25 12.5 1.57

93.11 93.84 93.73

51.73 46.47 54.31

34.17 46.62 67.49

18.86 43.55 56.04

93.54 93.54 93.54 93.54 93.82 93.46 93.33 93.26 (6) Pruning the Block#3.1

Table 3: Pruning each block in ResNet-18 on Cifar-10 dataset. Note that each block has two CNN layers.

parameters. In this case, we utilize our PiT algorithms to prune both fc.f6 + fc.f7, and conv.c5+fc.f6 layers. The results are reported in Tab. 2. We can show that our PiT framework can still efficiently compress the network while preserve significant performance.
The best compressed model. When we prune the conv.c5 and fc.f6 layers, our model can achieve the best and efficient performance. With only 17.60% parameter size of original LeNet-5, our model can beat the performance as high as 98.47%. Remarkably, our PiT framework has not done any fine-tuning and re-training the pruned network by any other dataset. This suggests that our PiT can indeed uncover the important weights and filters. Our best models will be downloaded online.
7

Under review as a conference paper at ICLR 2019

Block

(%) 100 25 12.5 6.25 3.13 1.57

#4.0 + #4.1 Com-Rat(%)

Plain Rand R-P GL-P GS-P
--

93.13 93.28 93.46 94.03 92.90 62.29

75.87 11.82 78.00 93.96 92.92 43.44

20.64 10.28 27.48 93.96 92.88 34.02

10.01 10.08 10.02 93.92 92.93 29.30

10.00 10.12 10.00 93.20 92.85 26.95

10.00 10.15 10.00 92.11 92.70 25.77

Block

(%) 100 25 12.5 6.25 3.13 1.57

#3.1 + #4.0+#4.1 Com-Rat(%)

Plain Rand R-P GL-P GS-P
--

93.13 92.94 93.78 92.65 91.55 56.91

89.22 12.55 46.10 92.64 91.54 35.36

51.70 10.00 14.67 92.47 91.69 24.59

10.68 10.00 10.01 92.17 91.07 19.20

10.00 10.00 16.05 91.43 90.95 16.51

10.00 10.00 10.43 90.96 91.29 15.16

Block

(%) 100 25 12.5 6.25 3.13 1.57

#3.0+#3.1+#4.0+#4.1 Com-Rat(%)

Plain Rand R-P GL-P GS-P
--

92.97 92.83 93.96 91.15 90.61 52.88

10.00 13.94 18.63 91.15 89.19 29.32

10.00 12.67 10.00 90.18 88.47 17.53

10.00 14.72 10.00 87.33 88.26 11.64

10.00 12.57 10.00 81.82 88.40 8.70

10.00 13.64 10.00 73.77 87.94 7.23

Table 4: Pruning multiple blocks in ResNet-18 on Cifar-10 dataset. (Chance-level = 10%). ComRat, is short for compression ratio of the total network, as in Tab. 2.

4.2 RESNET-18 ON CIFAR10 DATASET
The CIFAR-10 dataset consists of 60,000 images of size 32 × 32 in 10 classes, with 6000 images per class on average. There are 50,000 training images and 10,000 test images. We use the standard supervised split; and ResNet-18 is employed as the classification network. All the models are trained and get converged in 40 epochs. Note that each experiment is repeated for five times, and the averaged results are reported. We still show the results which have 100%, 50%, 25%, 12.5%, 6.25%, 3.13%, and 1.57% parameter size of original networks on each layer.
Pruning one Residual Block. The results are shown in Tab. 3. In this table, we apply our PiT algorithm on one residual block while the other layers are unchanged. We draw several conclusions,
(1) Our PiT framework (i.e., GS-P and GL-P) can efficiently train and prune the network. From Block #3.0 ­ Block #4.1, surprisingly the pruned network with 1.57% of original parameter size of ResNet-18, can also achieve almost the same recognition accuracies as the non-pruned ResNet-18. From Block #1.0 ­ Block #2.1, the smallest pruned ratio of PiT can still hit significant high performance if compared with the other competitors. This reflects the efficacy of our pruning algorithm. In particular, in the training process, our PiT framework is optimized to learn and select the important weights or filters; and our PiT can thus conduct a direct dimension reduction of these parameters.
(2) By the increased ratio of pruned parameters, the R-P method can also have better performance than Rand, and Plain methods. This shows that our pruning algorithm also works in the general cases. However, the R-P is not enforcing the sparse constraints in learning the weight parameters of network. Thus it has inferior performance to two PiT methods.
Pruning multiple blocks. The ResNet-18 totally has around 10.95M parameters. Block #4.1, #4.0, #3.1, #3.0 have 4.7M , 3.5M , 1.2M , and 0.88M parameters. These blocks have the most number of parameters; and we prune these multiple blocks. The results are shown in Tab. 4. Note that even only 1.57% parameter size of those layers are saved, our PiT algorithms (GL-P, and GS-P) can still remain remarkable high recognition accuracy. Again it shows the efficacy of our PiT framework.
8

Under review as a conference paper at ICLR 2019

(%)
Plain Rand R-P
L-P / GL-P S-P / GS-P

100
83.92 82.36 82.79
81.09 78.95

50
13.53 17.90 13.72
81.09 77.81

25
8.12 6.52 7.10
76.43 73.92

12.5
5.32 6.38 6.38
75.06 70.65

6.25
5.29 7.90 6.29
68.42 68.67

3.13
5.92 9.58 6.52
55.25 67.58

1.57
6.31 8.67 5.76
33.49 65.17

Table 5: Top 5 accuracy on miniImagenet by pruning ResNet-18, the fully connected layer, Block#4.0 and #4.1 layers.

4.3 RESNET-18 ON miniIMAGENET DATASET
The miniImageNet dataset is a subset of ImageNet and is composed of 60,000 images in 100 categories. In each category, we take 500 images as training set and other 100 as testing set. We also use the ResNet-18 structure on miniImageNet. All the models are trained and get converged in 50 epochs. Note that each experiment is repeated for five times, and the averaged results are reported. In term of the analysis in Sec. 4.2, we prune the fully connected layer, Block #4.0, and #4.1. The results are shown in Tab. 5.
When no parameters are pruned, the R-P can achieve better results than our PiT algorithms. These results make sense, since the ridge penalty does not enforce the sparsity to the network1. However, with the increased ratio of parameters pruned, the performance of R-P gets degraded dramatically. In contrast, the results of our methods in PiT framework get decreased very slow. For example, when only 50% are saved in all the layers, the Top-5 accuracies are reduced by only 0% and 1.1% for L-P / GL-P, and S-P / GS-P respectively. Remarkably, if we only save 1.57% of original parameters on those layers, the S-P / GS-P can still is as high as 65.17, which is only 13.78% performance dropped. Again, note that all the methods have not done any fine-tuning step, and only been trained in one round. That means our S-P / GS-P can indeed select the most expressive weights or filters, and thus reduce the size of networks.
4.4 DISCUSSION AND FUTURE WORK
As the experiments shown in these three datasets, our PiT indeed can learn to prune networks without fine-tuning. We give some further discussion and highlight the potential future works,
1. In all our experiments, our L-P / GL-P, and S-P / GS-P are applied to, at most, four layers in one network. Theoretically, our PiT algorithms should be able to be directly applied to any layers of DNNs, since PiT only adds some sparse penalties in the loss functions. However, in practice, we found that the network training algorithm, i.e., SGD in Alg. 3, is unstable, if we apply the sparse penalties more than four layers. It will take much more time and training epochs to get the networks converged.
2. Essentially, our PiT presents a feature selection algorithm, which can dynamically learn the importance of weights and filters in the learning process; mostly importantly, we donot need any fine-tuning step, which, we believe, will destroy values and properties of selected weights and filters. Therefore, it would be very interesting to analyze the statistical properties of selected features in each layer.
3. Theoretically, we can not guarantee the orthogonality of weights and filters in the trained model. Empirically, we adapt some strategies. For example, the weights and filters of each layer can be orthogonally initialized; and we apply the common regularization tricks, e.g., dropout, and batch normalization. These can help decorrelate the learned parameters of the same layers. Practically, our PiT framework works well in selecting the important parameters and prune the networks as shown in the experiments. We also visualize the correlation between removed and none removed filters in the Appendix.
4. It is a conjecture that the capacity of DNNs may be too large to learn a small dataset; and it is essential to do network pruning. However, it is also an open question as how to numerically measure the capacity of DNNs and the complexity of one dataset.
1In practice, ridge regression may have better performance than lasso.

9

Under review as a conference paper at ICLR 2019
5 CONCLUSION
This paper proposes a Pruning in Training (PiT) framework. We add the sparse penalties in training the networks, and the weights and filters can be ranked via their learned parameter values. The networks can thus be directly pruned via the ranked weight order. Our framework can show good results on several deep learning benchmark datasets.
REFERENCES
Reze Abbasi-Asl and Bin Yu. Structural compression of convolutional neural networks based on greedy filter pruning. In arxiv, 2017. 1, 2.2
Jose M Alvarez and Mathieu Salzmann. Learning the number of neurons in deep networks. In NIPS, 2016. 2.1
Maxwell Collins and Pushmeet Kohli. Memory bounded deep convolutional networks. In arXiv preprint arXiv:1412.1442, 2014, 2014. 2.1
Yanwei Fu, Timothy M Hospedales, Tao Xiang, Jiechao Xiong, Shaogang Gong, Yizhou Wang, and Yuan Yao. Robust subjective visual property prediction from crowdsourced pairwise labels. IEEE transactions on pattern analysis and machine intelligence, 38(3):563­577, 2016a. 1
Yanwei Fu, Timothy M. Hospedales, Jiechao Xiong, Tao Xiang, Shaogang Gong, Yuan Yao, and Yizhou Wang. Robust estimation of subjective visual properties from crowdsourced pairwise labels. IEEE TPAMI, 2016b. (document), 2.2, 3.4
Yiwen Guo, Anbang Yao, and Yurong Chen. Dynamic network surgery for efficient dnns. In NIPS, 2016. 2.2
Song Han, Huizi Mao, and William J Dally. Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding. In ICLR, 2015a. 3.4
Song Han, Jeff Pool, John Tran, and William Dally. Learning both weights and connections for efficient neural network. In NIPS, 2015b. 1, 2.2, 4
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770­778, 2016. 3
Yihui He, Xiangyu Zhang, and Jian Sun. Channel pruning for accelerating very deep neural networks. In ICCV, 2017. 2.2
Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. In NIPS 2014 Deep Learning Workshop, 2014. 1
Andrew G. Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto, and Hartwig Adam. Mobilenets: Efficient convolutional neural networks for mobile vision applications. In arxiv, 2017. 1
Chendi Huang, Xinwei Sun, Jiechao Xiong, and Yuan Yao. Split lbi: An iterative regularization path with structural sparsity. advances in neural information processing systems. Advances In Neural Information Processing Systems, pp. 3369­3377, 2016. 3.2, 3.3, 3.3
Forrest N. Iandola, Song Han, Matthew W. Moskewicz, Khalid Ashraf, William J. Dally, and Kurt Keutzer. Squeezenet: Alexnet-level accuracy with 50x fewer parameters and ¡0.5mb model size. In ICLR, 2017. 1
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In ICML, 2015. 1
Max Jaderberg, Andrea Vedaldi, and Andrew Zisserman. Speeding up convolutional neural networks with low rank expansions. In BMVC, 2014. 2.2
10

Under review as a conference paper at ICLR 2019
Kui Jia. Improving training of deep neural networks via singular value bounding. In arXiv:1611.06013v3, 2017. 1
Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015. 1
Vadim Lebedev and Victor Lempitsky. Fast convnets using group-wise brain damage. In CVPR, 2016. 2.2
Yann LeCun, Leon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. 86(11):2278­2324, 1998. 4.1
Hao Li, Asim Kadav, Igor Durdanovic, Hanan Samet, and Hans Peter Graf. Pruning filters for efficient convnets. In ICLR, 2017. 2.2
Baoyuan Liu, Min Wang, Hassan Foroosh, Marshall Tappen, and Marianna Pensky. Sparse convolutional neural networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 806­814, 2015. 3.2
Jian-Hao Luo, Jianxin Wu, and Weiyao Lin. ThiNet: A filter level pruning method for deep neural network compression. In ICCV, 2017. 2.2
Ningning Ma, Xiangyu Zhang, Hai-Tao Zheng, and Jian Sun. Shufflenet v2: Practical guidelines for efficient cnn architecture design. In arXiv:1807.11164v1, 2018. 1
Pavlo Molchanov, Stephen Tyree, Tero Karras, Timo Aila, and Jan Kautz. Pruning convolutional neural networks for resource efficient transfer learning. In ICLR, 2017. 2.2
Andrew M. Saxe, James L. McClelland, and Surya Ganguli. Exact solutions to the nonlinear dynamics of learning in deep linear neural networks. In ICLR, 2014. 1
Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: a simple way to prevent neural networks from overfitting. The Journal of Machine Learning Research, 15(1):1929­1958, 2014. 1, 2.1
Xinwei Sun, Lingjing Hu, Yuan Yao, and Yizhou Wang. Gsplit lbi: Taming the procedural bias in neuroimaging for disease prediction. In International Conference on Medical Image Computing and Computer-Assisted Intervention, pp. 107­115. Springer, 2017. 3.3
Cheng Tai, Tong Xiao, Yi Zhang, Xiaogang Wang, et al. Convolutional neural networks with lowrank regularization. In ICLR, 2016. 2.2
Wei Wen, Chunpeng Wu, Yandan Wang, Yiran Chen, and Hai Li. Learning the number of neurons in deep networks. In NIPS, 2016a. 2.1
Wei Wen, Chunpeng Wu, Yandan Wang, Yiran Chen, and Hai Li. Learning structured sparsity in deep neural networks. In NIPS, 2016b. 2.2, 3.2
He Yang, Guoliang Kang, Xuanyi Dong, Yanwei Fu, and Y. Yang. Soft filter pruning for accelerating deep convolutional neural networks. In IJCAI 2018, 2018. 1, 2.2
Jaehong Yoon and Sung Ju Hwang. Combined group and exclusive sparsity for deep neural networks. In ICML, 2017. 2.1
Ming Yuan and Yi Lin. Model selection and estimation in regression with grouped variables. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 68(1):49­67, 2006. 2.1, 3.2, 3.2, 4
Xiangyu Zhang, Jianhua Zou, Xiang Ming, Kaiming He, and Jian Sun. Efficient and accurate approximations of nonlinear convolutional networks. In CVPR, 2015. 2.2
Xiangyu Zhang, Jianhua Zou, Kaiming He, and Jian Sun. Accelerating very deep convolutional networks for classification and detection. IEEE Transactions on Pattern Analysis and Machine Intelligence, 38(10):1943­1955, 2016. 2.2
11

Under review as a conference paper at ICLR 2019

(a) Correlation between pruned and remaining filters

(b) Correlation between remaining filters

Figure 1: Correlation matrix of conv.c5 before and after the pruning. Note that we randomly select 15 filters to visualize in (a).

Bo Zhao, Xinwei Sun, Yanwei Fu, Yuan Yao, and Yizhou Wang. Msplit lbi: Realizing feature selection and dense estimation simultaneously in few-shot and zero-shot learning. arXiv preprint arXiv:1806.04360, 2018. 3.3, 3.4
Aojun Zhou, Anbang Yao, Yiwen Guo, Lin Xu, and Yurong Chen. Incremental network quantization: Towards lossless cnns with low-precision weights. ICLR, 2017. 2.2
Chenzhuo Zhu, Song Han, Huizi Mao, and William J Dally. Trained ternary quantization. ICLR, 2017. 2.2
Hui Zou and Trevor Hastie. Regularization and variable selection via the elastic net. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 67(2):301­320, 2005. 3.2

6 APPENDIX
We visualize the correlation matrix of conv.c5 of LeNet-5 on MNIST dataset, in Fig. 1. We randomly select 1000 images from the testing set of MNIST. In Fig. 1(a), each row is corresponding to one pruned filter, and each column is corresponding to the remaining filter. We find that most of them have lower correlation with one exception. In Fig. 1(b), we show the correlation between the remaining filters.

12

Under review as a conference paper at ICLR 2019

Ratio 100%

25%

12.5%

6.25%

3.13%

1.57%

Plain Rand R-P

99.17±0.02 99.11±0.01 99.12±0.04

60.87±14.00 43.72±5.74 61.05±7.46

29.65±4.89 30.07±7.48 46.91±6.36

20.82±3.32 18.35±4.58 30.34±6.82

20.82±3.32 24.12±5.75 30.34±6.82

20.82±3.32 22.62±6.43 30.34±6.82

L-P 99.05±0.04 S-P 99.00±0.01
Ratio 100%

74.29±1.06 47.28±12.12 28.58±3.22

85.09±5.76 32.58±2.65 22.88±6.89

(a) Pruning the conv.c3 layer

25%

12.5%

6.25%

28.58±3.22 22.88±6.89
3.13%

28.58±3.22 22.88±6.89
1.57%

Plain Rand R-P

99.12±0.02 99.19±0.01 99.16±0.06

80.46±5.46 62.23±10.12 75.47±8.11

62.61±9.05 37.71±4.34 60.31±5.19

45.49±0.97 23.58±6.96 37.97±2.99

32.34±1.53 18.58±4.70 26.11±2.13

21.30±4.83 14.36±3.27 18.11±1.05

L-P 98.95±0.04 S-P 98.97±0.07
Ratio 100%

98.95±0.04 90.29±1.30 60.37±5.45

98.96±0.08 98.67±0.15 68.27±11.22

(a) Pruning the conv.c5 layer

25%

12.5%

6.25%

32.91±3.35 42.10±7.83
3.13%

20.31±1.10 24.95±8.92
1.57%

Plain Rand R-P

99.10±0.02 99.09±0.01 99.13±0.05

96.73±1.05 91.56±3.89 96.39±0.48

95.65±1.76 71.05±6.08 95.31±0.79

89.60±3.49 51.92±8.87 91.29±3.46

78.40±5.48 33.65±6.17 82.75±5.59

64.17±6.93 29.91±8.49 68.35±6.29

L-P S-P
Ratio

99.10±0.03 99.13±0.03
100%

98.89±0.05 98.89±0.05 98.89±0.05

98.73±0.10 98.61±0.15 98.23±0.40

(a) Pruning the fc.c6 layer

25%

12.5%

6.25%

98.89±0.05 96.75±0.88
3.13%

98.89±0.05 92.53±3.17
1.57%

Plain Rand R-P

99.11±0.06 99.15±0.02 99.13±0.08

98.36±0.25 66.29±10.47 98.62±0.23

94.52±2.08 50.76±8.21 96.50±1.13

78.15±7.25 34.61±8.66 84.04±13.42

68.72±15.72 24.05±5.86 67.08±15.75

47.35±10.88 19.78±7.93 56.34±2.33

L-P 99.10±0.09 99.11±0.08 99.11±0.08 99.11±0.08 99.09±0.10 96.47±1.91 S-P 99.03±0.05 99.00±0.04 99.00±0.05 99.01±0.05 99.03±0.04 95.41±3.89
(a) Pruning the fc.f7 layer

Table 6: Pruning one layer in LeNet-5 on MNIST dataset.

13

Under review as a conference paper at ICLR 2019

Ratio 100% 25% 12.5% 6.25% 3.13% 1.57%

Plain Rand R-P

92.96 93.58 93.75

28.91 17.04 82.35

29.21 15.14 55.53

18.09 15.38 31.01

16.48 14.38 25.18

11.76 13.97 21.19

L-P S-P
Ratio

93.54 93.17
100%

93.53 93.30 92.61 91.29 93.23 93.27 93.30 93.26 (a) Pruning the Block#1.0 25% 12.5% 6.25% 3.13%

89.20 93.27
1.57%

Plain Rand R-P

93.44 92.90 93.95

37.32 25.63 64.03

20.60 30.82 46.21

13.94 29.31 30.97

15.93 33.40 28.50

18.84 34.33 27.67

L-P S-P
Ratio

93.57 93.52
100%

93.57 93.57 93.57 93.57 93.53 93.53 93.49 93.51 (a) Pruning the Block#1.1 25% 12.5% 6.25% 3.13%

93.57 93.49
1.57%

Plain Rand R-P

93.42 93.44 93.35

41.69 14.39 54.27

18.36 13.42 32.17

24.32 12.60 28.09

27.18 12.41 32.83

27.28 11.43 32.51

L-P S-P
Ratio

93.60 93.27
100%

93.62 93.58 93.57 93.59 93.18 93.26 93.26 93.26 (a) Pruning the Block#2.0 25% 12.5% 6.25% 3.13%

93.61 93.24
1.57%

Plain Rand R-P

93.48 93.50 93.78

49.77 31.60 64.74

40.31 35.15 51.29

33.81 35.75 45.81

37.05 37.92 47.68

36.75 37.40 49.65

L-P S-P
Ratio

93.50 93.50
100%

93.50 93.50 93.50 93.51 93.16 93.27 93.28 93.32 (a) Pruning the Block#2.1 25% 12.5% 6.25% 3.13%

93.51 93.29
1.57%

Plain Rand R-P

93.41 93.11 93.61

55.48 37.42 74.24

22.94 36.66 57.27

21.70 34.00 41.14

30.32 36.42 37.44

35.36 37.10 38.32

L-P S-P
Ratio

93.66 93.59
100%

93.66 93.67 93.68 93.66 93.63 93.48 93.46 93.45 (a) Pruning the Block#3.0 25% 12.5% 6.25% 3.13%

93.61 93.42
1.57%

Plain Rand R-P

93.11 93.84 93.73

51.73 46.47 54.31

34.17 46.62 67.49

21.04 44.79 54.03

19.12 40.30 42.84

18.86 43.55 56.04

L-P S-P
Ratio

93.54 93.82
100%

93.54 93.54 93.54 93.54 93.46 93.33 93.29 93.25 (a) Pruning the Block#3.1 25% 12.5% 6.25% 3.13%

93.54 93.26
1.57%

Plain Rand R-P

93.08 93.44 93.55

82.70 48.88 88.63

54.18 40.89 69.91

45.27 38.45 47.08

39.31 36.97 49.69

38.29 37.38 47.40

L-P S-P
Ratio

93.65 93.61
100%

93.65 93.65 93.65 93.66 93.50 93.45 93.38 93.44 (a) Pruning the Block#4.0 25% 12.5% 6.25% 3.13%

93.68 93.41
1.57%

Plain Rand R-P

93.37 93.57 93.63

90.01 82.86 88.20

85.29 79.16 87.16

79.24 77.26 79.41

77.75 76.65 75.00

75.53 76.73 70.77

L-P 93.79 93.79 93.79 93.80 93.77 93.77 S-P 93.61 93.93 93.83 93.84 93.93 93.89
(a) Pruning the Block#4.1 14

Table 7: Pruning one block in ResNet-18 on Cifar-10 dataset.


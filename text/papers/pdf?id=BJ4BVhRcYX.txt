Under review as a conference paper at ICLR 2019
INTERPRETABLE CONVOLUTIONAL FILTER PRUNING
Anonymous authors Paper under double-blind review
ABSTRACT
The sophisticated structure of Convolutional Neural Network (CNN) allows for outstanding performance, but at the cost of intensive computation. As significant redundancies inevitably present in such a structure, many works have been proposed to prune the convolutional filters for computation cost reduction. Although extremely effective, most works are based only on quantitative characteristics of the convolutional filters, and highly overlook the qualitative interpretation of individual filter's specific functionality. In this work, we interpreted the functionality and redundancy of the convolutional filters from different perspectives, and proposed a functionality-oriented filter pruning method. With extensive experiment results, we proved the convolutional filters' qualitative significance regardless of magnitude, demonstrated significant neural network redundancy due to repetitive filter functions, and analyzed the filter functionality defection under inappropriate retraining process. Such an interpretable pruning approach not only offers outstanding computation cost optimization over previous filter pruning methods, but also interprets filter pruning process.
1 INTRODUCTION
The great success of Convolutional Neural Network (CNN) is benefited from its advanced algorithm and architecture, which utilize inter-connected multi-layer network structure to hierarchically abstract the data feature for recognition tasks Krizhevsky & et al (2012). However, this complex mechanism offers CNN outstanding performance at the price of intensive computation cost. Therefore, many CNN optimization works have been proposed to alleviate this computation cost Han & et al (2015), Jaderberg & et al (2014), etc.
While those network redundancy leveraging works are mostly based on quantitative evaluation, the qualitative analysis based on network interpretation is highly overlooked. With the neural network interpretation, CNN can be well analyzed regarding its hierarchical structure and individual component's functionality, and therefore no longer a "black-box". However, most neural network interpretation works is merely considered as a post-analysis approach, and very few works have utilized it to analysis and guide the optimization directly (Yosinski et al. (2015)).
Considering the absence of the network interpretation analysis and optimization, in this work we utilized different CNN interpretation techniques to analysis the convolutional filter functionality and optimize the filter pruning method. In this work, we have the following major contributions:
· We utilized CNN visualization techniques and backward-propagation gradients analysis to interpret the convolutional filter functionality, and demonstrate significant network redundancy due to repetitive filter functions rather than insignificant magnitude;
· We designed a hierarchical filter functionality-oriented pruning method to explore the interpretable neural network optimization;
· We observed the filter functionality transission with model tuning, and reveals the inappropriate pruning and retraining methods may significantly defect the filter functionality and cause unnecessary network reconstruction effort.
Experiment results with various CNN models and image dataset show that, the proposed filter interpretation approach can effectively analysis the convolutional filters' the functionality and similarity. The proposed functionality-oriented pruning methods also achieves outstanding performance compared to traditional filter pruning methods with better training efficiency and interpretability
1

Under review as a conference paper at ICLR 2019

Conv 5-2 Conv 1-1 Conv 3-1 Conv 4-1 Conv1-2 Conv3-2 Conv4-2 Conv5-2

Figure 1: The illustration of visualized patterns of the convolutional filters.

Figure 2: Filter similarity matrices.

2 RELATED WORK
Convolutional Filter Pruning It is well known that the major computation cost in a CNN comes from the convolutional layers. Therefore, most CNN optimization works fall into convolutional filter pruning or compact CNN training with convolutional filter constraints: Li et al. (2016) ranked the convolutional filters with their absolute weight sum as the filter significance indication and pruned the insignificant filters for better computation efficiency, which is well-recognized and 1 ranking based prunning; Hu et al. (2016) evaluated the filter significance calculating the occupation of zero valued weights for pruning guidance; He et al. (2017) also applied Lasso-regression in the filter pruning for batched processing. The filter significance is also evaluated with the filter feature maps: Polyak & Wolf (2015) pruned the filters with small feature map values; Luo et al. (2017) evaluate filter's significance by testing its impact on feature map reconstruction errors after being pruned.
Most of these works are based on quantitative analysis on the filter significance. However, such an approach is already questioned by some recent works: Huang et al. (2018) proved that certain redundancy also exists in filters with large "significance"; while Ye et al. (2018) correspondingly demonstrated that filters with small values also significantly affect the neural network performance. Therefore, rather than merely evaluating the convolutional filters in a quantitative approach, we analysis the filter functionality in a qualitative manner to guide the neural network optimization.
Convolutional Filter Visualization As the convolutional filters are designed to capture certain input features, the semantics of the captured feature can effectively indicate the functionality of each filter. However, as the convolutional filter are embodied by matrices, the functionality is hard to directly interpret, which significantly hinders the qualitative neural network optimization.
Recently, many CNN visualization techniques have been proposed to qualitatively analysis the CNN in perspectives of network structure and component functionality, such as Activation Maximization (AM) (Yosinski et al. (2015)), Network Inversion (Mahendran & Vedaldi (2015)), Deconvolutional Neural Networks (Zeiler & Fergus (2014)), and Network Dissection based Visualization (Zhou et al. (2016)). Among those methods, AM offers the most efficient and effective interpretation for the convolutional filter functionality, which is defined by each filter's maximum activation pattern. In this work, we utilize the AM as our major visualization tool for convolutional filter analysis.
3 CONVOLUTIONAL FILTER FUNCTIONALITY INTERPRETATION AND FILTER REDUNDANCY ANALYSIS
Different from previous quantitative filter significance analysis, the convolutional filter functionality is qualitatively interpreted in this paper. Based on the functionality interpretation, the neural network redundancy is further analyzed regarding the filter functionality. For preliminary demonstration, we adopt a VGG-16 model trained on CIFAR-10 dataset for intuitive result demonstration.
2

Under review as a conference paper at ICLR 2019

3.1 INTERPRETABLE CONVOLUTIONAL FILTER FUNCTIONALITY ANALYSIS

To qualitatively interpret the convolutional filter functionality, both AM visualization and backwardpropagation gradients analysis are utilized. AM visualization interprets a filter's feature extraction preference from the neural network inputs, while the backward-propagation gradients evaluate the filters' contribution to classification outputs.

In AM visualization, the feature extraction preference of a filter Fil from layer Ll is represented by a synthesized input image X that causes the highest activation of Fil (i.e. the convolutional feature map value). Mathematically, the synthesis process of such an input can be formulated as:

V (Fil) = arg max Ail(X),
X

X  X +  · Ali(X) , X

(1)

where Ali(X) is the activation of filter Fil from an input image X,  is the gradient ascent step size. With X initialized as an input image of random noises, each pixel of this input is iteratively changed
along the Ail(X)/X increment direction to achieve the highest activation. Eventually, X demonstrates a specific visualized pattern V (Fil), which contains the filter's most sensitive input features with certain semantics, and represents the filter's functional preference for feature extraction.

Fig. 1 demonstrates several filters' visualized patterns from different layers. By interpreting the content of these patterns, it is clear to see that the filters in lower layers are more sensitive to fundamental colors and shapes. While with the layer depth increasing, the patterns tend to show specific objects and eventually recognizable class targets.

In the backward-propagation gradients analysis, the gradients indicate the impact of each convolutional filter to individual classification targets:

1N i,yj = N
n=1

P (yj) Ali(xn)

,

(2)

where P (yj) is the output probability of a sample image xn to the class yj, and Ali(xn) is the activation of filter Fil for each test image xn. P (yj)/Ai(xn) is the backward-propagation gradient of class yj respective to the filter's activation, which indicates the dedicated contribution of filter Fil to class yj. With preliminary experiments, we also find that some filters' visualized patterns also
constrain obvious class objects, which indicates large impact to certain classes yj. Therefore, the
connection between filters and dedicated class outputs can be also determined.

Associating these two analysis methods, namely the CNN visualization and backward-propagation gradients analysis, the filters' functionalities can be well interpreted in a qualitative manner.

3.2 FUNCTION SIMILARITY BASED FILTER REDUNDANCY ANALYSIS

Based on the above analysis, the convolutional filter functionality can be well interpreted regarding the input feature extraction preference and output contribution. These qualitative interpretations also guide certain neural network redundancy analysis, where filters with similar functionalities may indicate unnecessary structural repetition. Such repetition can be well identified among the filters with similar visualized patterns, as shown in the Fig. 1: as denoted by the red blocks, significant functionality similarities present among the convolutional filters in each layer.

To quantitatively determinate possible filter functionality repetition, a similarity degree between

filter Fk(c,l) and Fi(c,l) is formulated regarding both AM visualized patterns and the back-propagation gradients:

SD[V (Fi(c,l)), V (Fk(c,l))] = V (Fi(c,l)) - V (Fk(c,l)) 2 · i - k) 2,

(3)

where V (Fi(c,l)) - V (Fk(c,l)) 2 calculates the Euclidean distance between two visualized patterns, which indicate feature extraction preference similarity 1. And i - k) 2 calculates the similar-

ity between the two filter's contribution to specific output classes. To equally evaluate these two

criterion, both vectores are normalized, and the summation is utilized as the filter similarity degree.

1Although image similarity analysis tools like SSIM are more specialized, they cannot be applied into low layer filters with extremely small resolutions. Therefore, the Euclidean distance is adopted for its generality.

3

Under review as a conference paper at ICLR 2019

31 ... 31 ...

25 ... 25 ...

Pruning Rate
Unclustered
Pruning Rate

15 ... 15 ...

66

Contribution Index

... 12

Contribution Index

...

(a) Proposed functionality-oriented filter pruning.

(b) 1 ranking based filter pruning.

Figure 3: Case study of the hierarchical filter functionality-oriented and filter 1 ranking based filter pruning on the Conv5 1 of VGG-16. The convolutional filters are shown by their visualized patterns,
and aligned according to the visualization based filter clustering.

According to the qualitative analysis of Eq.3, a set of matrices filter similarity degrees are constructed as shown in Fig.2. The matrix axes represent the filter index in each layer (e.g. 512x512 for filter Conv5 2), and each cross-point indicates the similarity degree SD between two filters. The magnitudes of the similarity degrees are denoted by a color range, where the minimum is denoted as red and the maximum is denoted as white. From Fig.2, it can be observed that the filter similarity widely present in different layers. Meanwhile, significant filter similarity exit in the shallowest and deepest layers of the neural network (e.g. Conv1 1 and Conv5 2). Those high similarity degrees indicate considerable functionality repetition among the convolutional filters, which might be leveraged for effective neural network optimization.
In later sections, the filter functionality evaluation approach is widely applied for network redundancy analysis and filter pruning guidance.
4 HIERARCHICAL FILTER FUNCTIONALITY-ORIENTED PRUNING
Based on the convolutional filter functionality and redundancy analysis, we further propose a novel hierarchical filter pruning method oriented by the qualitative functionality interpretation.
4.1 PRUNING METHOD OVERVIEW
The proposed functionality-oriented filter pruning method is designed in a hierarchical manner to address the functionality redundancy in different neural network component levels.
The method has the following major stages: (1) Filter Clustering ­ Based on the aforementioned filter similarity analysis with AM visualization and the backpropagation gradients, the filters with similar functionalities in each layer are clustered into multiple groups. The filters in the same cluster are considered to have repetitive functionalities; (2) Filter Level Pruning ­ Inside each cluster, the filters' relative contribution to the output classes are qualitative evaluated by the backpropagation gradients analysis. The contribution index of each filter can be considered as the filter significance regarding its functionality; (3) Cluster Level Pruning ­ Inside each layer, the relative cluster pruning rate is based on the cluster volume size to guide the filter pruning in each cluster; (4) Layer Level Pruning ­ Guided by the layer sensitive to pruning, the relative pruning ratio for each layer is calculated to guide the pruning in each layer and therefore each cluster.
Fig. 3(a) illustrate an intuitive example for the filter clustering in the layer Conv5 1, where each row represents one cluster in the layer, and the horizontal axis indicates the filter's contribution index. Given a pruning rate for the model, the relative pruning rate for each layer and cluster can be determined, and the filters with least classification contribution will be pruned first (marked by green crosses). Meanwhile, we also demonstrate 1 ranking based filter pruning in the Fig. 3(b). Considering 1 ranking's quantitative filter significance evaluation, the pruned filters (marked by red crosses) have no interpretable selection patterns and no functionality correlations. Moreover,
4

Under review as a conference paper at ICLR 2019

Algorithm 1 Visualization based k-means Filter Clustering
Input: CNN, Number of layers L, and Number of filters in each layer Il Output: L Clusters 1: for each Layer Ll do Listl=[]; 2: for each F ilter Fil do Generate the pattern: V (Fil)=Xil; Listl.append(V (Fil)); 3: for c in range (1, Il/2) do Grid search all possible cluster numbers: Cl=kmeans.cluster(Listl,
c); 4: for Clc in Cl do Merge the single filter clusters: Cllocked = []; 5: if Len(Ccl)== 1 then Cllocked.append(Ccl); k=k-1. 6: Select the maximal c as the optimal cluster number: c=Max(c) return Ccl

comparing to 1 ranking based filter pruning, the proposed functionality-oriented pruning requires extremely small effort for model retraining, which will be further explained in later sections.
In the next section, the details for each processing stage will be presented.

4.2 HIERARCHICAL FILTER PRUNING SCHEME

Filter Clustering SD derived from Eq. 3 demonstrates the functionality similarity of any two convolutional filter. While to evaluate all the filters' similarity in a comprehensive neural network model, we applies k-means algorithm in each layer to cluster the filters.
As described in Alg. 1, we first utilize AM to obtain the visualized pattern of each convolutional filter. Then, we apply the k-means algorithm to SD values of all the convolutional filters in each individual layer. To determine proper cluster amount in each layer (i.e. k), a grid search is conducted from one to half of the total filter number. It is worth noting that there are inevitably certain amount of filters with extremely minimal similarity with others, which are also group together and won't be considered for pruning due to their possible instinct functionalities.
Each of the convolutional filter clusters can be considered as an interpretable functionality node. However, they can be also considered as neural network redundancy units that extract repetitive features from the inputs, and possibly defect the neural network computation efficiency.
Filter Level Pruning Ideally, the filters with the exactly same functionality can be substituted to each other, and each cluster can reserve only one filter and prune out the rest. However, due to the complex mechanism of neural networks, the filters with similar functionalities may have distinct contribution to the classification results. Therefore a relative filter contribution evaluation inside the cluster is necessary to determine the pruning significance for each filter.
To quantitatively identify the filters' contribution variation, the backward-propagation gradients is utilized to rank the filters in each cluster. Given a layer l with Il filters, the neural network output could be formulated by any layer l's output feature maps in the following format:

Z(F, Al) = F L(... (F l+1  Al + bl+1)...) + bL,

(4)

where F is the filter weights and bias in every layer. Al is the output feature maps (activation maps) of layer l. Due to the high complexity of F and multiple layers' combination, an approximate function Z is needed. Here we use Z's first-order Taylor expansion for approximating:

Z (Al

+

)

=

Z (Al )

+

 Z (Al ) Al

·

.

(5)

In our pruning method, when filter i in the l - 1 layer is pruned, the filter's output feature map is corresponding set to zero, i.e. changing the ith dimension of Al to zero. Therefore, the influence on
Z can be qualitatively evaluated as:

 Z (Ali ) Ali

· ,

where



=

Ali



0.

(6)

5

Under review as a conference paper at ICLR 2019

Before pruning, each filter Fi(c,l) in the cluster Ccl of layer l is firstly ranked by the contribution index, I(Fi(c,l)), which is calculated by examining the backward-propagation gradients. Specifically,

I (Fi(c,l) )

=

1 N

N

n=1

Z(F, Al) Ali(xn)

,

(7)

where the Z(F, Al) is the neural network output loss of a sample image n, and the Ail(xn) is the feature map of filter i for each test image n.

As shown in Fig. 3, the filters in each cluster are ranked with contribution index. When a specific filter pruning amount is assigned to the cluster, filters with small contribution will be firstly pruned.

Cluster Level Pruning As filter clusters are supposed to bear certain inner redundancy, they can perform filter pruning independently. Hence a parallel pruning operation can be well executed in each cluster as shown in Fig. 3. However, the significant variance of filter distribution also presents in each cluster. In each layer, a few clusters may have significantly larger volumes, while there are also a certain amount of filters that can't be well clustered. Therefore, a dedicated cluster level scheme is necessary to balance the pruning rates between clusters. Considering the cluster volume, we define an adaptive pruning rate of Rc(clt,l) = length(Ccl). The length(Ccl) calculates the cluster volume size, since larger cluster volume demonstrates more filter redundancy. And a larger Rc(clt,l) value will lead significantly more aggressive cluster level pruning. Therefore, we can see different filter pruning amounts between clusters in the Fig. 3.

Layer Level Pruning As the functionalities of each layer are relatively independent, all the layers can be pruned in a parallel manner with minor accuracy defection. In our pruning method, we derive a hyper-parameter P r based on each layers' accuracy sensitive to the pruning. Specifically, we prune filters in each layer independently and evaluate the pruned model's accuracy on the test set without retraining. P r is the neural network accuracy drop by pruning certain amount filters in each layer. Since each layer has different accuracy sensitivity to pruning, we empirically determine the same accuracy drop P r for all layers. By setting the hyper-parameter P r, we can find a certain amount of filters r to prune for each layer, corresponding amount of filters can be well assigned to each cluster as r · Rc(clt,l). And in each cluster, the filters to be pruned can be well identified by the contribution index of I(Fk(c,l)).
Leveraging the convolutional filters' qualitative functionality analysis, our proposed method is expected to leverage the neural network interpretability for more accurate redundant filter allocation, faster pruning speed, and optimal computation efficiency.

5 EXPERIMENTS
In this section, we will evaluate the functionality-oriented filter pruning method and testify the interpretable filter functionality that the method derived from.
5.1 EXPERIMENT SETUP
The visualization analysis and filter pruning method are implemented in Caffe environment (Jia et al. (2014)). The evaluation is performed on our designed ConvNet and VGG-16 on CIFAR-10, VGG-16 and ResNet-32 on ImageNet.
In evaluations on CIFAR-10, a data argumentation procedure is processed firstly through horizontal flip and random crop, and a 4 pixel padded training data set is generated. The whole training data is utilized in calculating the contribution index in Eq.7 for more accurate estimation. Our designed ConvNet is designed based on the classic AlexNet model, in which the convolutional filter has the size of 3x3 and the number as the same as the original model 2.
In evaluations on ImageNet, 10 classes images out of 1000 classes, namely ImageNet-10 is utilized in this work. And each class contains 1300 training images and 50 validation images. The VGG-16 and ResNet-32 on ImageNet-10 is pre-trained with a batch size of 128 and a learning rate of 1e-3,
2The detailed neural network structure of the ConvNet and the VGG-16 are demonstrated in the Appendix.

6

Under review as a conference paper at ICLR 2019

Accuracy

Accuracy

Our Proposed Method

1 ConvNet-Cifar10 1

VGG-Cifar10

1

VGG-ImageNet10

ResNet-ImageNet10

0.98

0.8
0.6
0.4 Conv1
Conv2 Conv3
0.2 Conv4
Conv5
0 1 ConvNet-Cifar10

0.8 Conv 1_1

Conv 1_2

Conv 2_1

0.6 Conv 2_2
Conv 3_1

Conv 3_2

0.4

Conv 3_3 Conv 4_1

Conv 4_2

0.2

Conv 4_3 Conv 5_1

Conv 5_2

Conv 5_3

0

1 VGG-Cifar10

0.8 Conv 1_1

Conv 1_2

Conv 2_1

0.6 Conv 2_2
Conv 3_1

Conv 3_2

0.4

Conv 3_3 Conv 4_1

Conv 4_2

0.2

Conv 4_3 Conv 5_1

Conv 5_2

Conv 5_3
0

1 VGG-ImageNet10

0.96 0.94 0.92 0.9 0.98

Conv _2 Conv _3 Conv _8 Conv _9 Conv _14 Conv _15 Conv _20 Conv _21 Conv _26 Conv _27
ResNet-ImageNet10

0.8

0.8 Conv 1_1
Conv 1_2

0.8 Conv 1_1
Conv 1_2

0.96

Conv 2_1

Conv 2_1

Conv _2

0.6

0.6 Conv 2_2

0.6 Conv 2_2

Conv _3

Conv 3_1 Conv 3_2

Conv 3_1 Conv 3_2

0.94

Conv _8 Conv _9

0.4 Conv1
Conv2
0.2 Conv3
Conv4

0.4

Conv 3_3 Conv 4_1

Conv 4_2

0.2

Conv 4_3 Conv 5_1

Conv 5_2

0.4

Conv 3_3 Conv 4_1

Conv 4_2

0.2

Conv 4_3 Conv 5_1

Conv 5_2

0.92

Conv _14 Conv _15 Conv _20 Conv _21 Conv _26

Conv5

Conv 5_3

Conv 5_3

Conv _27

0 0 0 0.9

10 30 50 70 90 10 30 50 70 90 10 30 50 70 90

10 30 50 70 90

Filter Pruned (%)

Filter Pruned (%)

Filter Pruned (%)

Filter Pruned (%)

Figure 4: Sensitivity of each individual layer to pruning methods.

L1 Based Method

achieving 98.6% and 97.1% testing accuracy respectively. As the same, the train data set is fully utilized to calculate the contribution index.
The ResNets-32 for ImageNet have three stages of residual blocks for feature maps with sizes of 56x56, 28x28, and 14x14. The three stages contain 3, 4, and 2 residual blocks respectively, and each residual block has three convolutional layers. The first convolutional layer of ResNets-32 is numbered as 1 and the layer number is defined to increase from shallower to deeper layers. Note that the projection layer located in the junction of two adjacent stages is neither numbered nor pruned as was done in other previous works. In each residual block, only the first two convolutional layers are pruned to keep the input and output feature maps to be identical. Hence, only filters from layer 2, 3, 5,... ,9 in stage 1, layer 11, 12,... 18 in stage 2, and layer 23, 24, 26, 27 in stage 3 are pruned.
In the experiment, the well trained neural network models are pruned by both proposed methods and 1 are compared from the perspectives of layer-wise pruning, model-wise pruning, filter training behavior, as well as the overall performance. The retraining process is executed on 20 epochs (i.e. 1/8 original training epochs) with a constant learning rate of 0.001.
5.2 LAYER-WISE PRUNING ANALYSIS
We demonstrate the effectiveness of our proposed method by comparing the accuracy defection sensitivity of individual layers to the proposed pruning method and 1 (Li et al. (2016)) based pruning.
The first row of Fig. 4 shows the layer-wise network accuracy degradation under different pruning ratio according to our method, while the the accuracy degradation caused by the 1 based method is demonstrated in the second row.
The following phenomenon is observed: (1) The proposed pruning has a slower accuracy degradation rate compared with the 1 ranking based pruning method in majority of layers, especially in the scenario of VGG-16 on the CIFAR-10; (2) In the proposed pruning, the accuracy degradation is slight and degradation rate is small before more than 50% repetitive convolutional filters are pruned; (3) The proposed method can slow down the accuracy degradation of some representative layers of ResNet-32 with the increasing of pruning rate. The reason is the ResNet contains much less filters

7

Under review as a conference paper at ICLR 2019

Table 1: CIFAR-10 Pruning Configuration

Pruning network Rate

ConvNet prune A A-L1 prune B B-L1
VGG-16 prune A A-L1 prune A B-L1

0 40.2% 40.2% 64.5% 64.5%
0 42.9% 42.9% 66.6% 66.6%

*Baseline accuracy

FLOPs (x108)
8.53 5.34 5.34 2.1 2.1
3.31 1.85 1.85 1.03 1.03

FLOPs reduction
37% 37% 63% 63%
41% 41% 68% 68%

prune accuracy
87.88% 83.29% 65.61% 53.72%
88.1% 82.4% 72.1% 60.2%

retrain accuracy
90.05%* 90.04% 89.42% 88.57% 87.88%
90.2%* 90.3% 89.82 % 89.9% 89.73 %

compared to other neural network model. In addition, the deeper layers of ResNet are more sensitive to pruning than the shallower layers, which is different with the other network models.
As such, our proposed method demonstrates significant pruning stability and robustness in the layerwise pruning. It also means the functionality redundant filters can be more accurately identified and pruned through the proposed interpretive approach.

5.3 MODEL-WISE PRUNING ANALYSIS
In this section, we further analyze the sensitivity to pruning of different neural network models. The performance of the proposed method is compared with both the 1 and the activation based methods. In the activation based filter pruning method, filters with small output activation are removed Polyak & Wolf (2015)).
Fig. 5 shows the model-wise pruning results of different network models under different pruning rate in the three pruning methods that are named as Cluster, L1, Activation respectively. Here, all convolutional layers in a network model are pruned simultaneously. The results depict that (1) The proposed method outperforms the 1 and activation based methods on all models, especially when the neural networks pruning rate is between 20% and 40%; (2) The proposed method has comparable performance with the 1 based method when the pruning rate is smaller than 10%.
The above results demonstrate that our proposed method can be applied on full model compression with less accuracy degradation. The overall performance is discussed in the next section.

5.4 OVERALL PERFORMANCE EVALUATION
In this section, the overall pruning performance of our proposed method and the 1 based method is evaluated and compared, which are depicted as prune and L1 respectively in Table 1 and Table 2. In the evaluation, a retraining procedure is performed to further recover the accuracy. Table 1 shows the

Accuracy

ConvNet-Cifar10 1

0.9 0.8

0.7 Cluster

0.6

L1 Activation

0.5 0 10 20 30 40 50

Filter Pruned (%)

VGG-Cifar10

VGG-ImageNet10

Cluster L1 Activation 0 10 20 30 40 50
Filter Pruned (%)

Cluster L1 Activation 0 10 20 30 40 50
Filter Pruned (%)

Figure 5: Model-wise pruning analysis.

ResNet-ImageNet10
Cluster L1 Activation 0 10 20 30 40 50
Filter Pruned (%)

8

Under review as a conference paper at ICLR 2019

performance in FLOPs reduction, accuracy after pruning, and accuracy after retraining of ConvNet and VGG on the CIFAR-10 dataset, and Table 2 shows the results of VGG and ResNet-32 on the ImageNet-10 dataset. In the evaluations, two pruning configuration is utilized. In prune A and AL1, a small amount of filters are pruned and the original accuracy can be completely recovered by retraining. prune B and B-L1 indicates the scenarios that filters are pruned aggressively to maximize the FLOPs reduction with optimal accuracy. The pruning rate is the percentage of the pruned over the total filters in a neural network model, and the comparison is executed on the same pruning rate.
Table 1 and Table 2 demonstrates that our proposed method introduce less accuracy degradation than the previous 1 based method. In addition, the accuracy caused by the proposed pruning can be easier recovered through retraining with better accuracy. For example, in the evaluations of VGG on the CIFAR-10 dataset, the proposed method can achieve 41% FLOPs reduction with retraining accuracy even higher than the original value, however, the accuracy loss caused in the 1 based pruning can not be well recovered. As is shown in Table 2, large accuracy loss occurs in the VGG on the ImageNet-10 dataset in the two pruning methods. And the proposed method induces less accuracy degradation and higher retraining accuracy compared with the 1 based pruning. Table 2 also shows that it is hard to recover the accuracy drop through retraining in the pruning of the ResNet on the ImageNet-10 dataset, and hence acceptable pruning rate in this scenario is relatively small.
5.5 NETWORK RETRAINING ANALYSIS
In most state-of-the-art works for neural network compression, the retraining is a necessary operation to compensate the inaccurately pruned filters and enhance the network performance. In this section, we evaluate the retraining operation quantitatively and qualitatively.
In Fig. 6, we first quantitatively explore the retraining process in terms of the model accuracy recovery. As shown in Fig. 6, the soiled lines represent the pruned model accuracy recovery based on our proposed method whereas the dot lines represent the 1 based method. We can observed that: (1) The models pruned by our method always demonstrate more quickly accuracy recovery. (2) To recover to the same accuracy, our method requires less retraining iterations.
Meanwhile, we also qualitatively explore the retraining process by the AM. Here we mainly focus on the functionality transition during network retraining process. As shown in Fig. 7, we randomly select one filter that has not been pruned by both our method and 1 based method and its original function analysis pattern is showed in the first column. Then we use the same visualization methods to visualize the pattern during different retraining iterations, e.g. every 100 iterations. The AM patterns show that, regardless the retraining iterations, the remained filter function of our proposed pruning method remains unchanged. However, the 1 based method changes the original filter functionality during retraining process. This means that the retraining process of the 1 based method rebuilds the filter functionality, which therefore needs more retraining iterations to restore the model accuracy. The reasons behind that are 1 based method may partially destruct the original neural

Table 2: ImageNet-10 Pruning Configuration

network

Pruning Rate

VGG-16 prune A A-L1 prune B B-L1
ResNet-32 prune A A-L1 prune B B-L1

0 28.3% 28.3% 40.9% 40.9%
0 21.5% 21.5% 30.2% 30.2%

*Baseline accuracy

FLOPs (x1010)
1.54 0.84 0.84 0.57 0.57
2.31 1.73 1.73 1.55 1.55

FLOPs reduction
45% 45% 63% 63%
25% 25% 33% 33%

prune accuracy
51.9% 40.7% 25.1% 21.2%
94.75% 92.87% 91.25% 90.37%

retrain accuracy
98.6%* 96.8% 95.7% 94.3% 93.2%
97.12%* 97.50% 96.25% 96.25% 94.75 %

9

Accuracy

Under review as a conference paper at ICLR 2019

Accuracy

0.9 0.88 0.86 0.84 0.82 0
0.9 0.8 0.7 0.6 0.5
0

ConvNet Cluster_0.5 VGG Cluster_0.5

L1_0.5 L1_0.5

20 40 60 80 100 120 140 160 180 200 Retrain Iterations

ConvNet Cluster_2.5 VGG Cluster_1.0

L1_2.5 L1_1.0

20 40 60 80 100 120 140 160 180 200 Retrain Iterations

(a) CIFAR-10

Accuracy

Accuracy

1.0

0.8

0.6 0.4 0.2 0
0.98

VGG Cluster_0.5 VGG Cluster_1.5

L1_0.5 L1_1.5

20 40 60 80 100 120 140 160 180 200 Retrain Iterations

0.96

0.94

0.92

ResNet Cluster_2.5 ResNet Cluster_1.0

L1_2.5 L1_1.0

0.90 0 100 200 300 400 500 600 700 800 9001000

Retrain Iterations

(b) ImageNet-10

Figure 6: Pruned model accuracy recovery by retraining.

network's functionality composition, which then need more iterations to reconstruct and balance the functionality composition.
That's also the reason why our pruning method causes significantly less accuracy drop and requires less retraining efforts since our pruning method induces less influence to the neural network functionality composition. Therefore, with more interpretable and accurate redundant filter functionality identification and pruning in our method, the costly retraining process will become less necessary, as our retraining analysis results demonstrate.

6 CONCLUSION
In this work, through activation maximization based visualization and gradient based filter functionality analysis, we firstly show that convolutional neural network filter redundancy exists in the form of functionality repetition. In other words, the functional repetitive filters could be effectively pruned from neural network to provide computation redundancy. Based on such motivation, in this paper, we propose an interpretable filter pruning method by first clustering filters with same functions together and then reducing the repetitive filters with smallest significance and contribution. Extensive experiments on CIFAR and ImageNet demonstrate the superior performance of our interpretable pruning method over state-of-the-art method, e.g. 1 ranking based pruning. By further analyzing the functionality changing of remaining filters in the retraining process, we show that 1 ranking based pruning actually partially destructs original neural network's functionality composition. By contrast, our method shows consistent neuron functionality during retraining process, denoting less harm to original network composition. This reveals the underlying reasons of our methods' better performance than L1 method.

Retrain Iterations 0 300 400 500 600 700 800 900 1000
Cat

Figure 7: Filter functionality transformation during retraining. 10

Under review as a conference paper at ICLR 2019
REFERENCES
Song Han and et al. Learning both weights and connections for efficient neural network. In Advances in neural information processing systems, pp. 1135­1143, 2015.
Yihui He, Xiangyu Zhang, and Jian Sun. Channel pruning for accelerating very deep neural networks. In Proceedings of the International Conference on Computer Vision, 2017.
Hengyuan Hu, Rui Peng, Yu-Wing Tai, and Chi-Keung Tang. Network trimming: A data-driven neuron pruning approach towards efficient deep architectures. arXiv preprint arXiv:1607.03250, 2016.
Qiangui Huang, Kevin Zhou, Suya You, and Ulrich Neumann. Learning to prune filters in convolutional neural networks. arXiv preprint arXiv:1801.07365, 2018.
Max Jaderberg and et al. Speeding up convolutional neural networks with low rank expansions. arXiv preprint arXiv:1405.3866, 2014.
Yangqing Jia, Evan Shelhamer, Jeff Donahue, Sergey Karayev, Jonathan Long, Ross Girshick, Sergio Guadarrama, and Trevor Darrell. Caffe: Convolutional architecture for fast feature embedding. In Proceedings of the 22nd ACM international conference on Multimedia, pp. 675­678. ACM, 2014.
Alex Krizhevsky and et al. Imagenet classification with deep convolutional neural networks. In Proceedings of the Advances in Neural Information Processing Systems, pp. 1098­1105, 2012.
Hao Li, Asim Kadav, Igor Durdanovic, Hanan Samet, and Hans Peter Graf. Pruning filters for efficient convnets. arXiv preprint arXiv:1608.08710, 2016.
Jian-Hao Luo, Jianxin Wu, and Weiyao Lin. Thinet: A filter level pruning method for deep neural network compression. arXiv preprint arXiv:1707.06342, 2017.
Aravindh Mahendran and Andrea Vedaldi. Understanding deep image representations by inverting them. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 5188­5196, 2015.
Adam Polyak and Lior Wolf. Channel-level acceleration of deep face representations. IEEE Access, 3:2163­2175, 2015.
Jianbo Ye, Xin Lu, Zhe Lin, and James Z Wang. Rethinking the smaller-norm-less-informative assumption in channel pruning of convolution layers. arXiv preprint arXiv:1802.00124, 2018.
Jason Yosinski, Jeff Clune, Anh Nguyen, Thomas Fuchs, and Hod Lipson. Understanding neural networks through deep visualization. arXiv preprint arXiv:1506.06579, 2015.
Matthew D Zeiler and Rob Fergus. Visualizing and understanding convolutional networks. In European conference on computer vision, pp. 818­833. Springer, 2014.
Bolei Zhou, Aditya Khosla, Agata Lapedriza, Aude Oliva, and Antonio Torralba. Learning deep features for discriminative localization. In Proceedings of the Computer Vision and Pattern Recognition, pp. 2921­2929, 2016.
11

Under review as a conference paper at ICLR 2019

7 APPENDIX

7.1 NETWORK CONFIGURATION
Table 3 and Table 4 show the detailed network structure and pruning configuration of our ConvNet and the VGG-16 on CIFAR10 respectively. The cluster distribution is the total number of clusters in each convolutional layers.
Table 3: ConvNet on CIFAR10 Pruning Configuration

layer

cluster

base prune A A- 1 prune B B- 1

output distribution filter filter filter filter filter

Pr Conv1 1 Conv2 1 Conv3 1 Conv4 1 Conv5 1

32x32 16x16 8x8 4x4 2x2

15 20 26 27 46

0.5 64 19 128 8 256 21 512 146 512 398

- 2.5 19 36 8 38 21 64 146 329 398 483

36 38 64 329 483

Table 4: VGG-16 on CIFAR10 Pruning Configuration

layer

cluster

base prune A A- 1 prune B B- 1

output distribution filter filter filter filter filter

Pr Conv1 1 Conv1 2 Conv2 1 Conv2 2 Conv3 1 Conv3 2 Conv3 3 Conv4 1 Conv4 2 Conv4 3 Conv5 1 Conv5 2 Conv5 3

32x32 32x32 16x16 16x16 8x8 8x8 8x8 4x4 4x4 4x4 2x2 2x2 2x2

15 17 20 31 26 24 14 27 28 38 46 61 40

0.5 64 19 64 3 128 8 128 17 256 21 256 18 256 36 512 146 512 313 512 223 512 398 512 395 512 218

-1 19 36 3 19 8 38 17 26 21 64 18 64 36 77 146 329 313 424 223 438 398 483 395 432 218 384

36 19 38 26 64 64 77 329 424 438 483 432 384

7.2 FILTER FUNCTIONALITY TRANSITION DURING RETRAINING
Retrain Iterations
0 300 400 500 600 700 800 900 1000
Cat
Ship Cluster_Based
Deer L1_Based

VGG_0.5_Conv5_1

Figure 8: Filter functionality transition during retraining. 12


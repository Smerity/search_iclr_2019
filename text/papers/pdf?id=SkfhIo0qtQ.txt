Under review as a conference paper at ICLR 2019
VOLUMETRIC CONVOLUTION: AUTOMATIC REPRESENTATION LEARNING IN UNIT BALL
Anonymous authors Paper under double-blind review
ABSTRACT
Convolution is an efficient technique to obtain abstract feature representations using hierarchical layers in deep networks. Although performing convolution in Euclidean geometries is fairly straightforward, its extension to other topological spaces--such as a sphere (S2) or a unit ball (B3)--entails unique challenges. In this work, we propose a novel `volumetric convolution' operation that can effectively convolve arbitrary functions in B3. We develop a theoretical framework for volumetric convolution based on Zernike polynomials and efficiently implement it as a differentiable and an easily pluggable layer for deep networks. Furthermore, our formulation leads to derivation of a novel formula to measure the symmetry of a function in B3 around an arbitrary axis, that is useful in 3D shape analysis tasks. We demonstrate the efficacy of proposed volumetric convolution operation on a possible use-case i.e., 3D object recognition task.
1 INTRODUCTION
Convolution-based deep neural networks have performed exceedingly well on 2D representation learning tasks (Krizhevsky et al., 2012; He et al., 2016). The convolution layers perform parameter sharing to learn repetitive features across the spatial domain while having lower computational cost by using local neuron connectivity. However, most state-of-the-art convolutional networks can only work on Euclidean geometries and their extension to other topological spaces e.g., spheres, is an open research problem. Remarkably, the adaptation of convolutional networks to spherical domain can advance key application areas such as robotics, geoscience and medical imaging.
Some recent efforts have been reported in the literature that aim to extend convolutional networks to spherical signals. Initial progress was made by Boomsma & Frellsen (2017), who performed conventional planar convolution with a careful padding on a spherical-polar representation and its cube-sphere transformation (Ronchi et al., 1996). A recent pioneering contribution by Cohen et al. (2018) used harmonic analysis to perform efficient convolution on the surface of the sphere (S2) to achieve rotational equivariance. These works, however, do not systematically consider radial information in a 3D shape and the feature representations are learned at specified radii. Specifically, Cohen et al. (2018) estimated similarity between spherical surface and convolutional filter in S2, where the kernel can be translated in S2. In this paper, we propose a novel approach to perform volumetric convolutions inside unit ball (B3) that explicitly learns representations across the radial axis. Although we derive generic formulas to convolve functions in B3, we experiment on one possible use case in this work, i.e., 3D shape recognition. In comparison to closely related spherical convolution approaches, modeling and convolving 3D shapes in B3 entails two key advantages: `volumetric convolution' can capture both 2D texture and 3D shape features and can handle non-polar 3D shapes. We develop the theory of volumetric convolution using orthogonal Zernike polynomials (Canterakis, 1999), and use careful approximations to efficiently implement it using low computational-cost matrix multiplications. Our experimental results demonstrate significant boost over spherical convolution and that confirm the high discriminative ability of features learned through volumetric convolution.
Furthermore, we derive an explicit formula based on Zernike Polynomials to measure the axial symmetry of a function in B3, around an arbitrary axis. While this formula can be useful in many function analysis tasks, here we demonstrate one particular use-case with relevance to 3D shape recognition. Specifically, we use the the derived formula to propose a hand-crafted descriptor that
1

Under review as a conference paper at ICLR 2019
accurately encodes the axial symmetry of a 3D shape. Moreover, we decompose the implementation of both volumetric convolution and axial symmetry measurement into differentiable steps, which enables them to be integrated to any end-to-end architecture.
Finally, we propose an experimental architecture to demonstrate the practical usefulness of proposed operations. We use a capsule network after the convolution layer as it allows us to directly compare feature discriminability of spherical convolution and volumetric convolution without any bias. In other words, the optimum deep architecture for spherical convolution may not be the same for volumetric convolution. Capsules, however, do not deteriorate extracted features and the final accuracy only depends on the richness of input shape features. Therefore, a fair comparison between spherical and volumetric convolutions can be done by simply replacing the convolution layer.
It is worth pointing out that the proposed experimental architecture is only a one possible example out of many possible architectures, and is primarily focused on three factors: 1) Capture useful features with a relatively shallow network compared to state-of-the-art. 2) Show richness of computed features through clear improvements over spherical convolution. 3) Demonstrate the usefulness of the volumetric convolution and axial symmetry feature layers as fully differentiable and easily pluggable layers, which can be used as building blocks for end-to-end deep architectures.
The main contributions of this work include:
· Development of the theory for volumetric convolution that can efficiently model functions in B3. · Implementation of the proposed volumetric convolution as a fully differentiable module that can
be plugged into any end-to-end deep learning framework. · The first approach to perform volumetric convolution on 3D objects that can simultaneously model
2D (appearance) and 3D (shape) features. · A novel formula to measure the axial symmetry of a function defined in B3, around an arbitrary
axis using Zernike polynomials. · An experimental end-to-end trainable framework that combines hand-crafted feature representation
with automatically learned representations to obtain rich 3D shape descriptors.
The rest of the paper is structured as follows. In Sec. 2 we introduce the overall problem and our proposed solution. Sec. 3 presents an overview of 3D Zernike polynomials. Then, in Sec. 4 and Sec. 5 we derive the proposed volumetric convolution and axial symmetry measurement formula respectively. Sec. 6.2 presents our experimental architecture, and in Sec. 7 we show the effectiveness of the derived operators through extensive experiments. Finally, we conclude the paper in Sec. 8.
2 PROBLEM DEFINITION
Convolution is an effective method to capture useful features from uniformly spaced grids in Rn, within each dimension of n, such as gray scale images (R2), RGB images (R3), spatio-temporal data (R3) and stacked planar feature maps (Rn). In such cases, uniformity of the grid within each dimension ensures the translation equivariance of the convolution. However, for topological spaces such as S2 and B3, it is not possible to construct such a grid due to non-linearity. A naive approach to perform convolution in B3 would be to create a uniformly spaced three dimensional grid in (r, , ) coordinates (with necessary padding) and perform 3D convolution. However, the spaces between adjacent points in each axis are dependant on their absolute position and hence, modeling such a space as a uniformly spaced grid is not accurate.
To overcome these limitations, we propose a novel volumetric convolution operation which can effectively perform convolution on functions in B3. It is important to note that ideally, the convolution in B3 should be a signal on both 3D rotation group and 3D translation. However, since Zernike polynomials do not have the necessary properties to automatically achieve translation equivariance, we stick to 3D rotation group in this work and refer to this operation as convolution from here onwards. Fig. 1 shows the analogy between planar convolution and volumetric convolution. In Sec. 3, we present an overview of 3D Zernike polynomials that will be later used in Sec. 4 to develop volumetric convolution operator.
2

Under review as a conference paper at ICLR 2019

3 3D ZERNIKE POLYNOMIALS

3D Zernike polynomials are a complete and orthogonal set of basis functions in B3, that exhibits a `form invariance' property under 3D rotation (Canterakis, 1999). A (n, l, m)th order 3D Zernike
basis function is defined as,

Zn,l,m = Rn,l(r)Yl,m(, )

(1)

where Rn,l is the Zernike radial polynomial (Appendix D.3), Yl,m(, ) is the spherical harmonics function (Appendix D.1), n  Z+, l  [0, n], m  [-l, l] and n - l is even. Since 3D Zernike polynomials are orthogonal and complete in B3, an arbitrary function f (r, , ) in B3 can be approximated
using Zernike polynomials as follows.

n l

f (, , r) =

n,l,m(f )Zn,l,m(, , r)

n=0 l=0 m=-l

(2)

where n,l,m(f ) could be obtained using,

1 2 

n,l,m(f ) =

f (, , r)Zn,l,mr2sindrdd

00 0

(3)

where  denotes the complex conjugate. In Sec. 4, we will derive the proposed volumetric convolution.

4 VOLUMETRIC CONVOLUTION OF FUNCTIONS IN B3

When performing convolution in B3, a critical problem which arises is that several rotation operations exist for mapping a point p to a particular point p . For example, using Euler angles, we can decompose a rotation into three rotation operations R(, ) = R()yR()zR()y, and the first rotation R()y can differ while mapping p to p (if y is the north pole). However, if we enforce the kernel function to be symmetric around y, the function of the kernel after rotation would only depend on p and p . This observation is important for our next derivations because we can then uniquely
define a 3D rotation on kernel in terms of azimuth and polar angles.

Let the kernel be symmetric around y and f (, , r), g(, , r) be the functions of object and kernel respectively. Then we define volumetric convolution as,

1 2 

f g(, ) := f (, , r), (,)(g(, , r)) =
00

f (, , r), (,)(g(, , r)) sin dddr
0
(4)

where (,) is an arbitrary rotation, that aligns the north pole with the axis towards (, ) direction ( and  are azimuth and polar angles respectively). Eq. 4 is able to capture more complex patterns

compared to spherical convolution due to two reasons: 1) the inner product integrates along the radius

and 2) the projection onto spherical harmonics forces the function into a polar function, that can

result in information loss.

In Sec. 4.1 we derive differentiable relations to compute 3D Zernike moments for functions in B3.

4.1 SHAPE MODELING OF FUNCTIONS IN B3 USING 3D ZERNIKE POLYNOMIALS

Instead of using Eq. 3, we derive an alternative method to obtain the set {n,l,m}. The motivations are

two fold: 1) ease of computation and 2) the completeness property of 3D Zernike Polynomials ensures

that limn f - n l m n,l,mZn,l,m = 0 for any arbitrary function f . However, since n should be finite in the implementation, aforementioned property may not hold, leading to increased

distance between the Zernike representation and the original shape. Therefore, minimizing the recon-

struction error

(,,r)S3 f¯(, , r) - f (, , r) where f¯(, , r) =

N n

l

m n,l,mZn,l,m,

pushes the set {n,l,m} inside frequency space, where {n,l,m} has a closer resemblance to the

corresponding shape. Following this conclusion, we derive the following method to obtain {n,l,m}.

3

Under review as a conference paper at ICLR 2019

Figure 1: Analogy between planar and volumetric convolutions. Top (left to right): image, kernel and planar convolution. Bottom (left to right): 3D object, 3D kernel and volumetric convolution. In planar convolution the kernel translates and inner product between the image and the kernel is computed in (x, y) plane. In volumetric convolution a 3D rotation is applied to the kernel and the inner product is computed between 3D function and 3D kernel over B3.

Since Yl,m(, ) = (-1)m

2l+1 4

(l-m)! (l+m)!

Plm(cos)eim

,

where

Plm(·)

is

the

associated

Legendre

function (Appendix D.2), it can be deduced that, Yl,-m(, ) = (-1)mYl,m(, ). Using this

relationship we obtain Zn,l,-m(, ) = (-1)mZn,l,m(, ) and hence approximate Eq. 2 as,

n l

f (, , r) =

An,l,mRe{Zn,l,m} + Bn,l,mImg{Zn,l,m}

(5)

n=0 l=0 m=0

where Re{Zn,l,m} and Img{Zn,l,m} are real and imaginary components of Zn,l,m respectively. In matrix form, this can be rewritten as,

f (, , r) = U a + V b = f (, , r) = Xc

(6)

where c is the set of 3D Zernike moments n,l,m. Eq. 6 can be interpreted as an overdetermined linear system, with the set n,l,m as the solution. To find the least squared error solution to the Eq. 6 we use the pseudo inverse of X. Since this operation has to be differentiable to train the model

end-to-end, a common approach like singular value decomposition cannot be used here. Instead, we

use an iterative method to calculate the pseudo inverse of a matrix (Li et al., 2011). It has been shown that Vn converges to A+ where A+ is the Moore-Penrose pseudo inverse of A if,

Vn+1 = Vn(3I - AVn(3I - AVn)), n  Z+

(7)

for a suitable initial approximation V0. They also showed that a suitable initial approximation would be V0 = AT with 0 <  < 2/(AAT ), where (·) denotes the spectral radius. Empirically, we choose  = 0.001 in our experiments. Next, we derive the theory of volumetric convolution within

the unit ball.

4.2 CONVOLUTION IN B3 USING 3D ZERNIKE POLYNOMIALS

We formally present our derivation of volumetric convolution using the following theorem. A short version of the proof is then provided. Please see Appendix A for the complete derivation.

Theorem 1: Suppose f, g : X - R3 are square integrable complex functions defined in B3 so that f, f <  and g, g < . Further, suppose g is symmetric around north pole and
 (, ) = Ry()Rz() where R  SO(3). Then,

1 0

2 0

 0

f (, , r), (,)(g(, , r)) sin dddr



4 3

 n=0

n l=0

l
n,l,m(f )n,l,0(g)Yl,m(, )
m=-l

(8)

where n,l,m(f ), n,l,0(g) and Yl,m(, ) are (n, l, m)th 3D Zernike moment of f , (n, l, 0)th 3D

Zernike moment of g, and spherical harmonics function respectively.

Proof: Completeness property of 3D Zernike Polynomials ensures that it can approximate an arbitrary function in B3, as shown in Eq. 2. Leveraging this property, Eq. 4 can be rewritten as,

n l

n

l

f  g(, ) =

n,l,m(f )Zn,l,m, (,)(

n ,l ,m (g)Zn ,l ,m )

n=0 l=0 m=-l

n =0 l =0 m =-l

(9)

4

Under review as a conference paper at ICLR 2019

However, since g(, , r) is symmetric around y, the rotation around y should not change the function.

This ensures,

g(r, , ) = g(r,  - , )

(10)

and hence,

n

l n
n ,l ,m (g)Rn ,l (r)Yl ,m =

l
n ,l ,m (g)Rn ,l (r)Yl ,m e-im 

n =0 l =0 m =-l

n =0 l =0 m =-l

(11)

This is true, if and only if m = 0. Therefore, a symmetric function around y, defined inside the unit

sphere can be rewritten as,

n

n ,l ,0(g)Zn ,l ,0

(12)

n =0 l =0

which simplifies Eq. 9 to,

n l

n

f  g(, ) =

n,l,m(f )Zn,l,m, (,)(

n ,l ,0(g)Zn ,l ,0)

n=0 l=0 m=-l

n =0 l =0

(13)

Using the properties of inner product, Eq. 13 can be rearranged as,

n n l

f  g(, ) =

n,l,m(f )n ,l ,0(g) Zn,l,m, (,)(Zn ,l ,0)

n=0 l=0 n =0 l =0 m=-l

(14)

Using the rotational properties of Zernike polynomials, we obtain (see Appendix A for our full

derivation),

4  n l

f  g(, ) = 3

n,l,m(f )n,l,0(g)Yl,m(, )

n=0 l=0 m=-l

(15)

Since we can calculate n,l,m(f ) and n,l,0(g) easily using Eq. 6, f  g(, ) can be found using

a simple matrix multiplication. It is interesting to note that, since the convolution kernel does not

translate, the convolution produces a polar shape, which can be further convolved­if needed­using the

l

relationship f  g(, ) =

4 2l+1

f^(l, m)g^(l, m)Y(l,m)(, ) where, f^(l, m) and g^(l, m)

l m=-l

are the (l, m)th frequency components of f and g in spherical harmonics space. Next, we present a

theorem to show the equivariance of volumetric convolution with respect to 3D rotation group.

4.3 EQUIVARIANCE TO 3D ROTATION GROUP
One key property of the proposed volumetric convolution is its equivariance to 3D rotation group. To demonstrate this, we present the following theorem.
Theorem 1: Suppose f, g : X - R3 are square integrable complex functions defined in B3 so that f, f <  and g, g < . Also, let ,, be a 3D rotation operator that can be decomposed into three Eular rotations Ry()Rz()Ry() and , another rotation operator that can be decomposed into Ry()Rz(). Suppose ,, (g) = ,(g). Then, (,,)(f )  g(, ) = (,)(f  g)(, ), where  is the volumetric convolution operator.
The proof to our theorem can be found in Appendix B. The intuition behind the theorem is that if a 3D rotation is applied to a function defined in B3 Hilbert space, the output feature map after volumetric convolution exhibits the same rotation. The output feature map however, is symmetric around north pole, hence the rotation can be uniquely defined in terms of azimuth and polar angles.

5 AXIAL SYMMETRY OF FUNCTIONS IN B3
In this section we present the following proposition to obtain the axial symmetry measure of a function in B3, around an arbitrary axis using 3D Zernike polynomials.

5

Under review as a conference paper at ICLR 2019

Figure 2: Kernel representations of spherical convolution
(left) vs Volumetric convolution (right). In volumetric convolution, the shape is modeled and convolved in B3
which allows encoding non-polar 3D shapes with texture. In contrast, spherical convolution is performed in S2 that
can handle only polar 3D shapes with uniform texture.

Proposition: Suppose g : X - R3 is a square integrable complex function defined in B3 such that g, g < . Then, the power of projection of g in to S = {Zi} where S is the set of Zernike basis functions that are symmetric around an axis towards (, ) direction is given by,

nl

||sym(,)||=

|| n,l,mYm,l(, )||2

n l=0 m=-l

where  and  are azimuth and polar angles respectively.

The proof to our proposition is given in Appendix C.

(16)

6 A CASE STUDY: 3D OBJECT RECOGNITION

6.1 3D OBJECTS AS FUNCTIONS IN B3

A 2D image is a function on Cartesian plane, where a unique value exists for any (x, y) coordinate.
Similarly, a polar 3D object can be expressed as a function on the surface of the sphere, where any
direction vector (, ) has a unique value. To be precise, a 3D polar object has a boundary function in the form of f : S2  [0, ].

Translation of the convolution kernel on (x, y) plane in 2D case, extends to movements on the surface of the sphere in S2. If both the object and the kernel have polar shapes, this task can be tackled by projecting both the kernel and the object onto spherical harmonic functions (Appendix E). However, this technique suffers from two drawbacks. 1) Since spherical harmonics are defined on the surface of the unit sphere, projection of a 3D shape function into spherical harmonics approximates the object to a polar shape, which can cause critical loss of information for non-polar 3D shapes. This is frequently the case in realistic scenarios. 2) The integration happens over the surface of the sphere, which is unable to capture patterns across radius.

These limitations can be addressed by representing and convolving the shape function inside the unit ball (B3). Representing the object function inside B3 allows the function to keep its complex shape information without any deterioration since each point is mapped to unique coordinates (r, , ), where r is the radial distance,  and  are azimuth and polar angles respectively. Additionally, it
allows encoding of 2D texture information simultaneously. Figure 2 compares volumetric convolution
and spherical convolution. Since we conduct experiments only on 3D objects with uniform surface values, in this work we use the following transformation to apply a simple surface function f (, , r)
to the 3D objects:

f (, , r) = r, if surface exists at (, , r) 0, otherwise

(17)

6.2 AN EXPERIMENTAL ARCHITECTURE
We implement an experimental architecture to demonstrate the usefulness of the proposed operations. While these operations can be used as building-tools to construct any deep network, we focus on three key factors while developing the presented experimental architecture: 1) Shallowness: Volumetric convolution should be able to capture useful features compared to other methodologies with less number of layers. 2) Modularity: The architecture should have a modular nature so that a fair comparison can be made between volumetric and spherical convolution. We use a capsule network after the convolution layer for this purpose. 3) Flexibility: It should clearly exhibit the usefulness

6

Under review as a conference paper at ICLR 2019

View Angles

Feature Generation

Figure 3: Experimental ar-

CapsNet chitecture: An object is

first mapped to three view

angles. For each angle, ax-

CBP ial symmetry and volumet-

ric convolution features are generated for P + and P -.

These two features are then

Max-Pool

Classification separately combined using

CapsNet compact bilinear pooling.

Finally, the features are

fed to two individual capCBP sule networks, and the de-

cisions are max-pooled.

1-D ConvNet

of axial symmetry features as a hand-crafted and fully differentiable layer. The motivation is to demonstrate one possible use case of axial symmetry measurements in 3D shape analysis.

The proposed architecture consists of four components. First, we obtain three view angles, and

later generate features for each view angle separately. We optimize the view angles to capture

complimentary shape details such that the total information content is maximized. For each viewing angle `k', we obtain two point sets Pk+ and Pk- consisting of tuples denoted as:

Pk+ = {(xi, yi, zi) : yi > 0}, and Pk- = {(xi, yi, zi) : yi < 0},

(18)

such that y denotes the horizontal axis. Second, the six point sets are volumetrically convolved

with kernels to capture local patterns of the object. The generated features for each point set are

then combined using compact bilinear pooling. Third, we use axial symmetry measurements to

generate additional features. The features that represent each point set are then combined using

compact bilinear pooling. Fourth, we feed features from second and third components of the overall

architecture to two independent capsule networks and combine the outputs at decision level to obtain

the final prediction. The overall architecture of the proposed scheme is shown in Fig. 3.

6.3 OPTIMUM VIEW ANGLES
We use three view angles to generate features for better representation of the object. First, we translate the center of mass of the set of (x, y, z) points to the origin. The goal of this step is to achieve a general translational invariance, which allows us to free the convolution operation from the burden of detecting translated local patterns. Subsequently, the point set is rearranged as an ordered set on x and z and a 1D convolution net is applied on y values of the points. Here, the objective is to capture local variations of points along the y axis, since later we analyze point sets P + and P - independently. The trained filters can be assumed to capture properties similar to ny/xn and ny/zn, where n is the order of derivative. The output of the 1D convolution net is rotation parameters represented by a 1 × 9 vector r = {r1, r2, · · · , r9}. Then, we compute R1 = Rx(r1)Ry(r2)Rz(r3), R2 = Rx(r4)Ry(r5)Rz(r6) and R3 = Rx(r7)Ry(r8)Rz(r9) where R1, R2 and R3 are the rotations that map the points to three different view angles.
After mapping the original point set to three view angles, we extract the Pk+ and Pk- point sets from each angle k that gives us six point sets. These sets are then fed to the volumetric convolution layer to obtain feature maps for each point set. We then measure the symmetry around four equi-angular axes using Eq. 16, and concatenate these measurement values to form a feature vector for the same point sets.

6.4 FEATURE FUSION USING COMPACT BILINEAR POOLING
Compact bilinear pooling (CBP) provides a compact representation of the full bilinear representation, but has the same discriminative power. The key advantage of compact bilinear pooling is the significantly reduced dimensionality of the pooled feature vector.
We first concatenate the obtained volumetric convolution features of the three angles, for P + and P - separately to establish two feature vectors. These two features are then fused using compact bilinear

7

Under review as a conference paper at ICLR 2019

pooling (Gao et al., 2016). The same approach is used to combine the axial symmetry features. These fused vectors are fed to two independent capsule nets.
Furthermore, we experiment with several other feature fusion techniques and present results in Sec. 7.2.

6.5 CAPSULE NETWORK
Capsule Network (CapsNet) (Sabour et al., 2017) brings a new paradigm to deep learning by modeling input domain variations through vector based representations. CapsNets are inspired by so-called inverse graphics, i.e., the opposite operation of image rendering. Given a feature representation, CapsNets attempt to generate the corresponding geometrical representation. The motivation for using CapsNets in the network are twofold: 1) CapsNet promotes a dynamic `routing-by-agreement' approach where only the features that are in agreement with high-level detectors are routed forward. This property of CapsNets does not deteriorate extracted features and the final accuracy only depends on the richness of original shape features. It allows us to directly compare feature discriminability of spherical and volumetric convolution without any bias. For example, using multiple layers of volumetric or spherical convolution hampers a fair comparison since it can be argued that the optimum architecture may vary for two different operations. 2) CapsNet provides an ideal mechanism for disentangling 3D shape features through pose and view equivariance while maintaining an intrinsic co-ordinate frame where mutual relationships between object parts are preserved.
Inspired by these intuitions, we employ two independent CapsNets in our network for volumetric convolution features and axial symmetry features. In this layer, we rearrange the input feature vectors as two sets of primary capsules--for each capsule net--and use the dynamic routing technique proposed by Sabour et al. (2017) to predict the classification results. The outputs are then combined using max-pooling, to obtain the final classification result. For volumetric convolution features, our architecture uses 1000 primary capsules with 10 dimensions each. For axial symmetry features, we use 2500 capsules, each with 10 dimensions. In both networks, decision layer consist of 12 dimensional capsules.

6.6 HYPERPARAMETERS

We use n = 5 to implement Eq. 15 and three iterations to calculate the Moore-Penrose pseudo inverse

using

Eq.

7.

We

use

a

decaying

learning

rate

lr

=

0.1

×

gstep
0.9 3000

,

where

gstep

is

incremented

by

one

per each iteration. For training, we use the Adam optimizer with 1 = 0.9, 2 = 0.999, = 1 × 10-8

where parameters refer to the usual notation. All these values are chosen empirically. Since we

have decomposed the theoretical derivations into sets of low-cost matrix multiplications, specifically

aiming to reduce the computational complexity, the GPU implementation is highly efficient. For

example, the model takes less than 15 minutes for an epoch during the training phase for ModelNet10,

with a batchsize 2, on a single GTX 1080Ti GPU.

7 EXPERIMENTS
In this section, we discuss and evaluate the performance of the proposed approach. We first compare the accuracy of our model with relevant state-of-the-art work, and then present a thorough ablation study of our model, that highlights the importance of several architectural aspects. We use ModelNet10 and ModelNet40 datasets in our experiments. Next, we evaluate the robustness of our approach against loss of information and finally show that the proposed approach for computing 3D Zernike moments produce richer representations of 3D shapes, compared to the conventional approach.
7.1 COMPARISON WITH THE STATE-OF-THE-ART
Table 1 illustrates the performance comparison of our model with state-of-the-art. The model attains an overall accuracy of 92.17% on ModelNet10 and 86.5% accuracy on ModelNet40, which is on par with state-of-the-art. We do not compare with other recent work, such as Kanezaki et al. (2016); Qi et al. (2016); Sedaghat et al. (2016); Wu et al. (2016); Qi et al. (2016); Bai et al. (2016); Maturana & Scherer (2015) that show impressive performance on ModelNet10 and ModelNet40. These are not comparable

8

Under review as a conference paper at ICLR 2019

Method
SO-Net (Li et al., 2018) Kd-Networks (Klokov & Lempitsky, 2017) VRN (Brock et al., 2016) Pairwise (Johns et al., 2016) MVCNN (Su et al., 2015) Ours PointNet (Qi et al., 2017) ECC (Simonovsky & Komodakis, 2017) DeepPano (Shi et al., 2015) 3DShapeNets (Wu et al., 2015) PointNet (Garcia-Garcia et al., 2016)

Trainable layers
11FC 15KD 45Conv 23Conv 60Conv + 36FC 1Conv + 2Caps 2ST + 5Conv 4Conv + 1FC 4Conv + 3FC 4-3DConv + 2FC 2Conv + 2FC

ModelNet10
95.7% 94.0% 93.11% 92.8% 92.17% 85.45% 83.5% 77.6%

ModelNet40
93.4% 91.8% 90.8% 90.7% 90.1% 86.5% 86.2% 83.2% 77,63% 77% -

Table 1: Comparison with state-of-the-art methods on ModelNet10 and ModelNet40 datasets (ranked according to performance). Ours achieve a competitive performance with the least network depth.

with our proposed approach, as we propose a shallow, single model without any data augmentation, with a relatively low number of parameters. Furthermore, our model reports these results by using only a single volumetric convolution layer for learning features. Fig. 4 demonstrates effectiveness of our architecture by comparing accuracy against the number of layers in state-of-the-art models.

7.2 ABLATION STUDY

Table 2: Ablation study of the proposed architecture on ModelNet10 dataset

Table 2 depicts the performance comparison between several variants of our model. To highlight the effectiveness of the learned optimum view points, we replace the optimum view point layer with three fixed orthogonal view points. This modification causes an accuracy drop of 6.57%, emphasizing that the optimum view points indeed depends on the shape. Another interesting-- perhaps the most important--aspect to study is the performance of the proposed volumetric convolution against spherical convolution. To this end, we replace the volumetric convolution layer of our model with spherical convolution and compare the results. It can be seen that our volumetric convolution scheme outperforms spherical convolution by a significant margin of 12.56%, indicating that volumetric convolution captures shape properties more effectively.

Method
Final Architecture (FA) FA + Orthogonal Rotation FA - VolCNN + SphCNN
FA -MaxPool + MeanPool FA + Feature Fusion (Axial + Conv)
Axial Symmetry Features VolConv Features SphConv Features
FA - CapsNet + FC layers FA - CBP + Feature concat FA - CBP + MaxPool FA - CBP + Average-pooling

Accuracy
92.17% 85.60% 79.53%
87.27% 86.53%
66.73% 85.3% 71.6%
87.3 % 90.7% 90.3% 85.3 %

Furthermore, using mean-pooling instead of max-

pooling, at the decision layer drops the accuracy to 87.27%. We also evaluate performance of using a sin-

100

gle capsule net. In this scenario, we combine axial symmetry features with volumetric convolution features

SO-Net

KD-Net

Ours

Pairwise

VRN

Accuracy

using compact bilinear pooling (CBP), and feed it a sin-

90

gle capsule network. This variant achieves an overall accuracy of 86.53%, is a 5.64% reduction in accuracy

DeepPano 3DShapeNet

compared to the model with two capsule networks. Moreover, we compare the performance of two fea-

80 PointNet

ture categories--volumetric convolution features and axial symmetry features--individually. Axial symmetry features alone are able to obtain an accuracy of

10 20 30 40 Number of trainable layers

66.73%, while volumetric convolution features reach a Figure 4: Accuracy vs number of trainable

significant 85.3% accuracy. On the contrary, spherical layers trend (ModelNet10)

convolution attains an accuracy of 71.6%, which again highlights the effectiveness of volumetric

convolution.

Then we compare between different choices that can be applied to the experimental architecture. We first replace the capsule network with a fully connected layer and achieve an accuracy of 87.3%. This is perhaps because capsules are superior to a simple fully connected layer in modeling viewpoint invariant representations. Then we try different substitutions for compact bilinear pooling and

9

Under review as a conference paper at ICLR 2019

Accuracy Reconstruction error

Proposed approach Conventional approach
80 0.6
60 0.4
40
20 0.2

5% 10% 20% 30% Percentage loss of points

50%

2345 n

Figure 5: The robustness of the proposed model Figure 6: The mean reconstruction error Vs `n'.

against missing data. The accuracy drop is less Our Zernike frequencies computation approach

than 30% at a high data loss rate of 50%.

has far less error than the conventional approach.

achieve 90.7%, 90.3% and 85.3% accuracies respectively for feature concatenation, max-pooling and average-pooling. This justifies the choice of compact bilinear pooling as a feature fusion tool. However, it should be noted that these choices may differ depending on the architecture.

7.3 ROBUSTNESS AGAINST INFORMATION LOSS
One critical requirement of a 3D object classification task is to be robust against various information loss. To demonstrate the effectiveness of our proposed features in this aspect, we randomly remove data points from the objects in validation set, and evaluate model performance. The results are illustrated in Fig. 5. The model shows no performance loss until 20% of the data is lost, and only gradually drops to an accuracy level of 66.5 at a 50% data loss, which implies strong robustness against random information loss.

7.4 EFFECTIVENESS OF THE PROPOSED METHOD FOR CALCULATING 3D ZERNIKE MOMENTS

In Sec. 4.1, we proposed an alternative method to calculate 3D Zernike moments (Eq. 5, 6), instead

of the conventional approach (Eq. 3). We hypothesized that moments obtained using the former has

a closer resemblance to the original shape, due to the impact of finite number of frequency terms.

In this section, we demonstrate the validity of our hypothesis through experiments. To this end, we

compute moments for the shapes in the validation set of ModelNet10 using both approaches, and

compare the mean reconstruction error defined as:

1 T

T t

f (t) -

n

l

m n,l,mZn,l,m(t) ,

where T is the total number of points and t  S3. Fig. 6 shows the results. In both approaches, the

mean reconstruction error decreases as n increases. However, our approach shows a significantly

low mean reconstruction error of 0.0467% at n = 5 compared to the conventional approach, which

has a mean reconstruction error of 0.56% at same n. This result also justifies the utility of Zernike

moments for modeling complex 3D shapes.

8 CONCLUSION
In this work, we derive a novel `volumetric convolution' using 3D Zernike polynomials, which can learn feature representations in B3. We develop the underlying theoretical foundations for volumetric convolution and demonstrate how it can be efficiently computed and implemented using low-cost matrix multiplications. Furthermore, we propose a novel, fully differentiable method to measure the axial symmetry of a function in B3 around an arbitrary axis, using 3D Zernike polynomials. Finally, using these operations as building tools, we propose an experimental architecture, that gives competitive results to state-of-the-art with a relatively shallow network, in 3D object recognition task. An immediate extension to this work would be to explore weight sharing along the radius of the sphere.
10

Under review as a conference paper at ICLR 2019
REFERENCES
Song Bai, Xiang Bai, Zhichao Zhou, Zhaoxiang Zhang, and Longin Jan Latecki. Gift: A real-time and scalable 3d shape search engine. In Computer Vision and Pattern Recognition (CVPR), 2016 IEEE Conference on, pp. 5023­5032. IEEE, 2016.
Wouter Boomsma and Jes Frellsen. Spherical convolutions and their application in molecular modelling. In Advances in Neural Information Processing Systems, pp. 3436­3446, 2017.
Andrew Brock, Theodore Lim, James M Ritchie, and Nick Weston. Generative and discriminative voxel modeling with convolutional neural networks. arXiv preprint arXiv:1608.04236, 2016.
N Canterakis. 3d zernike moments and zernike affine invariants for 3d image analysis and recognition. In In 11th Scandinavian Conf. on Image Analysis. Citeseer, 1999.
Taco S Cohen, Mario Geiger, Jonas Koehler, and Max Welling. Spherical cnns. International Conference on Learning representations (ICLR), 2018.
Yang Gao, Oscar Beijbom, Ning Zhang, and Trevor Darrell. Compact bilinear pooling. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 317­326, 2016.
Alberto Garcia-Garcia, Francisco Gomez-Donoso, Jose Garcia-Rodriguez, Sergio Orts-Escolano, Miguel Cazorla, and J Azorin-Lopez. Pointnet: A 3d convolutional neural network for real-time object class recognition. In Neural Networks (IJCNN), 2016 International Joint Conference on, pp. 1578­1584. IEEE, 2016.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770­778, 2016.
Edward Johns, Stefan Leutenegger, and Andrew J Davison. Pairwise decomposition of image sequences for active multi-view recognition. In Computer Vision and Pattern Recognition (CVPR), 2016 IEEE Conference on, pp. 3813­3822. IEEE, 2016.
Asako Kanezaki, Yasuyuki Matsushita, and Yoshifumi Nishida. Rotationnet: Joint object categorization and pose estimation using multiviews from unsupervised viewpoints. arXiv preprint arXiv:1603.06208, 2016.
Roman Klokov and Victor Lempitsky. Escape from cells: Deep kd-networks for the recognition of 3d point cloud models. In 2017 IEEE International Conference on Computer Vision (ICCV), pp. 863­872. IEEE, 2017.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. In Advances in neural information processing systems, pp. 1097­1105, 2012.
Hou-Biao Li, Ting-Zhu Huang, Yong Zhang, Xing-Ping Liu, and Tong-Xiang Gu. Chebyshev-type methods and preconditioning techniques. Applied Mathematics and Computation, 218(2):260­270, 2011.
Jiaxin Li, Ben M Chen, and Gim Hee Lee. So-net: Self-organizing network for point cloud analysis. arXiv preprint arXiv:1803.04249, 2018.
Daniel Maturana and Sebastian Scherer. Voxnet: A 3d convolutional neural network for real-time object recognition. In Intelligent Robots and Systems (IROS), 2015 IEEE/RSJ International Conference on, pp. 922­928. IEEE, 2015.
Charles R Qi, Hao Su, Matthias Nießner, Angela Dai, Mengyuan Yan, and Leonidas J Guibas. Volumetric and multi-view cnns for object classification on 3d data. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 5648­5656, 2016.
Charles R Qi, Hao Su, Kaichun Mo, and Leonidas J Guibas. Pointnet: Deep learning on point sets for 3d classification and segmentation. Proc. Computer Vision and Pattern Recognition (CVPR), IEEE, 1(2):4, 2017.
11

Under review as a conference paper at ICLR 2019
C Ronchi, R Iacono, and Pier S Paolucci. The "cubed sphere": a new method for the solution of partial differential equations in spherical geometry. Journal of Computational Physics, 124(1): 93­114, 1996.
Sara Sabour, Nicholas Frosst, and Geoffrey E Hinton. Dynamic routing between capsules. In Advances in Neural Information Processing Systems, pp. 3859­3869, 2017.
Nima Sedaghat, Mohammadreza Zolfaghari, Ehsan Amiri, and Thomas Brox. Orientation-boosted voxel nets for 3d object recognition. arXiv preprint arXiv:1604.03351, 2016.
Baoguang Shi, Song Bai, Zhichao Zhou, and Xiang Bai. Deeppano: Deep panoramic representation for 3-d shape recognition. IEEE Signal Processing Letters, 22(12):2339­2343, 2015.
Martin Simonovsky and Nikos Komodakis. Dynamic edge-conditioned filters in convolutional neural networks on graphs. In Proc. CVPR, 2017.
Hang Su, Subhransu Maji, Evangelos Kalogerakis, and Erik Learned-Miller. Multi-view convolutional neural networks for 3d shape recognition. In Proceedings of the IEEE international conference on computer vision, pp. 945­953, 2015.
Jiajun Wu, Chengkai Zhang, Tianfan Xue, Bill Freeman, and Josh Tenenbaum. Learning a probabilistic latent space of object shapes via 3d generative-adversarial modeling. In Advances in Neural Information Processing Systems, pp. 82­90, 2016.
Zhirong Wu, Shuran Song, Aditya Khosla, Fisher Yu, Linguang Zhang, Xiaoou Tang, and Jianxiong Xiao. 3d shapenets: A deep representation for volumetric shapes. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 1912­1920, 2015.
12

Under review as a conference paper at ICLR 2019

Supplementary Material
Volumetric Convolution: Automatic Representation Learning in Unit Ball

A CONVOLUTION WITHIN UNIT SPHERE USING 3D ZERNIKE POLYNOMIALS

Theorem 1: Suppose f, g : X - R3 are square integrable complex functions defined in B3 so that f, f <  and g, g < . Further, suppose g is symmetric around north pole and
 (, ) = Ry()Rz() where R  SO(3). Then,

1 2 

4  n l

00

0

f (, , r), (,)(g(, , r)) sin dddr 

3

n,l,m(f )n,l,0(g)Yl,m(, )
n=0 l=0 m=-l

(19)

where n,l,m(f ), n,l,0(g) and Yl,m(, ) are (n, l, m)th 3D Zernike moment of f , (n, l, 0)th 3D

Zernike moment of g, and spherical harmonics function respectively.

Proof: Since 3D Zernike polynomials are orthogonal and complete in B3, an arbitrary function f (r, , ) in B3 can be approximated using Zernike polynomials as follows.

n l

f (, , r) =

n,l,m(f )Zn,l,m(, , r)

n=0 l=0 m=-l

where n,l,m(f ) could be obtained using,

1 2 

n,l,m(f ) =

f (, , r)Zn,l,mr2sindrdd

00 0

where  denotes the complex conjugate.

Leveraging this property (Eq. 20) of 3D Zernike polynomials Eq. 4 can be rewritten as,

(20) (21)

n l

n

l

f  g(, ) =

n,l,m(f )Zn,l,m, (,)(

n ,l ,m (g)Zn ,l ,m )

n=0 l=0 m=-l

n =0 l =0 m =-l

(22)

But since g(, , r) is symmetric around y, the rotation around y should not change the function. Which ensures,

and hence,

g(r, , ) = g(r,  - , )

(23)

n

l n
n ,l ,m (g)Rn ,l (r)Yl ,m =

l
n ,l ,m (g)Rn ,l (r)Yl ,m e-im 

n =0 l =0 m =-l

n =0 l =0 m =-l

(24)

This is true, if and only if m = 0. Therefore, a symmetric function around y, defined inside the unit sphere can be rewritten as,

which simplifies Eq. 22 to,

n
n ,l ,0(g)Zn ,l ,0
n =0 l =0

(25)

13

Under review as a conference paper at ICLR 2019

n l

n

f  g(, ) =

n,l,m(f )Zn,l,m, (,)(

n ,l ,0(g)Zn ,l ,0)

n=0 l=0 m=-l

n =0 l =0

Using the properties of inner product, Eq. 26 can be rearranged as,

n n l

f  g(, ) =

n,l,m(f )n ,l ,0(g) Zn,l,m, (,)(Zn ,l ,0)

n=0 l=0 n =0 l =0 m=-l

Consider the term (,)(Zn ,l ,0). Then,

(26) (27)

l

(,)(Zn ,l ,0) = (,)(Rn ,l Yl ,0) = Rn ,l (,)(Yl ,0) = Rn ,l

Yl ,m Dml ,0(, )

m =-l

(28)

where Dml ,m is the Wigner-D matrix. But we know that Dml ,0(, ) = Yl ,m (, ). Then Eq. 27 becomes,

n n l

l

f  g(, ) =

n,l,m(f )n ,l ,0(g)

Yl ,m (, ) Zn,l,m, Zn ,l ,m

n=0 l=0 n =0 l =0 m=-l

m =-l

(29)

f  g(, ) = 4  n 3

l
n,l,m(f )n,l,0(g)Yl,m(, )

n=0 l=0 m=-l

(30)

B EQUIVARIANCE OF VOLUMETRIC CONVOLUTION TO 3D ROTATION GROUP
Theorem 1: Suppose f, g : X - R3 are square integrable complex functions defined in B3 so that f, f <  and g, g < . Also, let ,, be a 3D rotation operator that can be decomposed into three Eular rotations Ry()Rz()Ry() and , another rotation operator that can be decomposed into Ry()Rz(). Suppose ,,(g) = ,(g). Then,

(,,)(f )  g(, ) = (,)(f  g)(, )

(31)

where  is the volumetric convolution operator.
Proof: Since (,,)  SO(3), we know that (,,)(f (x)) = f ((-1,,)(x)). Also we know that (,,) : R3  R3 is an isometry.
Define,

(,,)f, (,,)g =

f ((-1,,)(x))g((-1,,)(x))dx

S3

(32)

Consider the Lebesgue measure (S3) = S3 dx. It can be proven that a lebesgue measure invariant under the isometries, which gives us dx = d(,,)(x) = d(-1,,)(x), x  S3. Therefore,

< (,,)f, (,,)g >=

f ((-1,,)(x))g((-1,,)(x))d((-1,,)x) =< f, g >

S3

14

(33)

Under review as a conference paper at ICLR 2019

Let f (, , r) and g(, , r) be the object function and kernel function (symmetric around north pole) respectively. Then volumetric convolution is defined as,

f  g(, ) =< f, (,)g > Applying the rotation (,,) to f , we get,

(34)

(,,)(f )  g(, ) =< (,,)(f ), (,)g > By the result 33, we have,

(35)

(,,)(f )  g(, ) =< f, (-1,,)((,)g) > However, since ,,(g) = ,(g) we get,

(36)

We know that,

(,,)(f )  g(, ) =< f, (-,-,)g >

(37)

Then,

n l

f  g(, ) =< f, (,)g >=

n,l,m(f )n,l,0(g)Yl,m(, )

n=0 l=0 m=-l

(38)

n l

(,,)(f )  g(, ) =< f, (-,-)g >=

n,l,m(f )n,l,0(g)Yl,m( - ,  - )

n=0 l=0 m=-l

(39)

(,,)(f )  g(, ) = (f  g)( - ,  - )

(40)

(,,)(f )  g(, ) = (,)(f  g) Hence, we achieve equivariance over 3D rotations.

(41)

C AXIAL SYMMETRY MEASURE OF A FUNCTION IN B3 AROUND AN ARBITRARY AXIS.

Proposition: Suppose g : X - R3 is a square integrable complex function defined in B3 such that g, g < . Then, the power of projection of g in to S = {Zi} where S is the set of Zernike basis functions that are symmetric around an axis towards (, ) direction is given by,

||sym(,)||=

nl
|| n,l,mYm,l(, )||2

n l=0 m=-l

(42)

where  and  are azimuth and polar angles respectively.

Proof: The subset of complex functions which are symmetric around north pole is S = {Zn,l,0}. Therefore, projection of a function into S gives,

n

symy(, ) =

f, Zn,l,0 zn,l,0(, )

n l=0

(43)

15

Under review as a conference paper at ICLR 2019

To obtain the symmetry function around any axis which is defined by (, ), we rotate the function by (-, -), project into S, and final compute the power of the projection.

sym(,)(, ) =

(-,-)(f ), Zn,l,0 zn,l,0(, )

n,l

(44)

However, rotation is a unitary operator in complex Hilbert space. Any unitary operator defined in a Hilbert space preserves the inner product. Which means for any unitary operator U , and for any two points defined on a complex Hilbert space, x and y,

U (x), U (y) H = x, y H Applying this property to 44 gives,

(45)

sym(,)(, ) =

f, (,)(Zn,l,0) zn,l,0(, )

n,l

Using Eq. 20 we get,

n nl

sym(,)(, ) =

n l m Zn ,l ,m , (,)(Zn,l,0) zn,l,0(, )

n l=0 n l =0 m =-l

Using properties of inner product Eq. 47 further simplifies to,

(46) (47)

n nl

sym(,)(, ) =

n l m Zn ,l ,m , (,)(Zn,l,0) zn,l,0(, )

n l=0 n l =0 m =-l

Using the same derivation as in 28,

(48)

n nl

l

sym(,)(, ) =

n l m

Yl,m (, ) < Zn ,l ,m , Zn,l,m > zn,l,0(, )

n l=0 n l =0 m =-l

m =-l

(49)

Since 3D Zernike Polynomials are orthogonal we get,

4 n l

sym(,)(, ) = 3

n,l,mYm,l(, )zn,l,0(, )

n l=0 m=-l

(50)

In signal theory the power of a function is taken as the integral of the squared function divided by the size of its domain. Following this we get,

nl

nl

||sym(,)||= (

n,l,mYm,l(, ))zn,l,0(, ), (

n ,l ,m Ym ,l (, )zn ,l ,0(, ))

n l=0 m=-l

n l =0 m =-l

(51)

We drop the constants here since they do not depend on the frequency. Simplifying Eq. 51 gives,

nl

l

||sym(,)||=

n,l,mYm,l(, )n,l,m Ym ,l(, )

n l=0 m=-l m =-l

which leads to,

(52)

16

Under review as a conference paper at ICLR 2019

nl

||sym(,)||=

|| n,l,mYm,l(, )||2

n l=0 m=-l

D FUNCTION DEFINITIONS

D.1 SPHERICAL HARMONICS Spherical harmonics are a set of complete orthogonal functions, which are defined on S2.

(53)

Yl,m(, ) = (-1)m

2l + 4

1

(l (l

- +

m)! m)!

Plm(cos)eim

(54)

where l is an integer, m is an integer, |m|< l, and Plm(·) is the associated Legendre function (see appendix D.2).

D.2 ASSOCIATED LEGENDRE FUNCTION Associated Legendre function Plm(x) is defined as,

Plm(x)

=

(-1)m

(1

- x2)m/2 2ll!

dl+m dxl+m

(x2

-

1)l

where l is an integer, m is an integer, |m|< l, and x is a real number.

(55)

D.3 ZERNIKE RADIAL POLYNOMIAL

Rn,m(r)

=

(n-m)/2

k!

((n

+

(-1)k(n - k)! m)/2 - k)! ((n -

m)/2

-

k)!

rn-2k

k=0

(56)

E SPHERICAL CONVOLUTION ON S2

Let the f and g be the shape functions of the object and kernel respectively. Then f and g can be expressed as,

f (, ) =

l
f^(l, m)Yl,m(, ) and, g(, ) =

l
g^(l, m)Yl,m(, )

l m=-l

l m=-l

(57)

where Yl,m is the (l, m)th spherical harmonics function and f^(l, m) and g^(l, m) are (l, m)th frequency components of f and g respectively. Then the frequency components of convolution f  g
can be easily calculated as,

f  g(l, m) = where  denotes the complex conjugate.

4 f^(l, m)g^(l, 0) 2l + 1

(58)

17


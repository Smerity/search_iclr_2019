Under review as a conference paper at ICLR 2019
A MODEL CORTICAL NETWORK FOR SPATIOTEMPORAL
SEQUENCE LEARNING AND PREDICTION
Anonymous authors Paper under double-blind review
ABSTRACT
In this paper, we developed a hierarchical network model, neurally inspired and constrained, to understand how spatiotemporal memories might be learned and encoded in the visual hierarchy that can be used for predicting future episodic events. Our model assumes recurrent connections within each layer and between different layers learn to encode spatial and temporal memories of patterns and context for prediction. Its feedforward analysis path uses a DCNN to develop hierarchical feature representation, while its internal models and feedback synthesis path is modeled by a stack of LSTM. The network learns by minimizing its internal model's prediction error against the incoming visual sequences. It operates on spatiotemporal blocks, rather than individual frames. This allows learning and prediction to be made at the sequence or movement level, yielding superior performance in predicting or extrapolating video sequences. Its computation is efficient because its convolution is operated on residual signals between successive blocks, and between bottom-up representation and synthesized representation generated by feedback. This model is effective in producing state-of-the-art performance in predicting visual sequences in benchmark datasets. Furthermore, we showed that this neurally inspired and constrained model exhibit video familiarity or prediction suppression effects observed along the ventral stream of the primate visual system, suggesting that it is a viable candidate model for hierarchical learning and computation in the visual cortex.
1 INTRODUCTION
We often envision future in terms of a sequence or an episode of events, rather than just an image. How do we generate such prediction of the future in our mind? While the hippocampus is known to play a key role in encoding episodic memories, the actual memories might be encoded in the neocortex (McClelland & McNaughton (1999)). There is indeed evidence that neural activities in the mammalian hierarchical visual cortex, even at the level of the primary visual cortex, reflect the encoding of memories of spatiotemporal patterns at various time scales (Yao et al. (2007); Han et al. (2008); Xu et al. (2012); Huang et al. (2018); Zhang et al. (2018)). Such memories might give our visual system the ability to predict future visual events to facilitate learning and perceptual inference. Prediction has been proposed to be a powerful driving force to the formation of memories and has been used as principle for self-supervised learning of neural networks (Mathieu et al. (2015); Villegas et al. (2017); Srivastava et al. (2015); O'Reilly et al. (2014)) where the error of the prediction of an internal model against the incoming image frame is back-propagated to learn the models.
A model for video prediction based on predictive coding principle (Mumford (1991); Huang & Rao (2011); Dijkstra et al. (2017); Friston (2018)), called PredNet, has been advanced recently as a neurally plausible cortical model (Lotter et al. (2016)). This model learns a hierarchical model of prediction errors, i.e. with a higher layer learning to predict the errors of a lower layer, and has been demonstrated to be effective for video prediction. However, other computer vision models that learn successively higher order representations can make more accurate long-term prediction, i.e, predicting farther into the future more accurately (Wang et al. (2018; 2017); Villegas et al. (2017); Xu et al. (2018)). having a hierarchical features representations with increased abstraction, robustness and invariance is important for object recognition, as well as accurate prediction of video sequences at greater spatial and temporal scales. Thus, although hierarchy of residuals might be a more efficient coding scheme, having a hierarchical of feature representations with increased abstraction and
1

Under review as a conference paper at ICLR 2019
invariance offers more computational advantages not only for long-term video prediction, but for object recognition and other tasks. Also, according to psychological and computationally literature, it is equally, and probably more, neurally plausible (Grossberg (1987); McClelland & Rumelhart (1985); Hinton (2007)).
Here, we propose a cortical network model that combines the advantages for coding and computational efficiency of having a sparse or residual code, and the advantages of having a hierarchy of feature representations to allow higher order and lower order features interact during inference and learning. We test this model in the context of long-range video prediction as a proof of concept. In this model, we combined good features from several state-of-the-art models. To achieve coding efficiency and fast computation, we compute prediction errors at each level, and perform convolution only on these residual signals to greatly speed up computation in the feedforward path, in the style of predictive-corrective residual module framework (Liu et al. (2017); Dave et al. (2017); Pan et al. (2018)). It also satisfies one of the predictive coding criteria of making communication code between cortical areas sparse and efficient. To achieve a hierarchy of feature representation, we integrate the prediction errors with the the representation in the last time step at every layer to reconstruct the current representation. Thus our model contain both hierarchical feature representations across layers as well as prediction error representations in every layer. To realize the complete paradigm of analysis by synthesis (Mumford (1992)), we introduce a LSTM in every layer to encode hierarchical spatiotemporal memories for generating prediction at every layer, as well as for providing feedback to constrain the synthesis at the lower layers. As neurons have spatiotemporal receptive fields, our model uses spatiotemporal block as input and output units, rather than operating on a frame by frame basis as in most state-of-the-art video prediction models (e.g. PredNet, PredRNN++). This allows the model to analyze and make prediction at the level of spatiotemporal blocks that better capture information at the movement level, and thus make better long-range prediction. Thus, the proposed model is efficient both in communication and computation, while enjoying the advantages of having an explicit hierarchical feature representations, reconciling the predictive coding theory and the interactive activation theory in neural computation. We illustrate this model and demonstrates its effectiveness in the context of long range video prediction, and use it to account for some recent neurophysiological observations related to learning of sequence memories in the primate hierarchical visual systems.
2 RELATED WORKS
Our primary goal is to develop a hierarchical cortical model for spatiotemporal memories that can be used for prediction, as well as for vision tasks in general. In this regard, our model is similar conceptually to the hierarchical spatiotemporal memory model (HTM) (Hawkins & George (2006)) but leverages deep learning tools and incorporates more general neural principles such as sparse coding, predictive coding and associative coding. The paradigm of using prediction to learn internal representation for unsupervised or self-supervised learning has been deployed actively in the last few years (Softky (1995); Palm (2012); O'Reilly et al. (2014); Goroshin et al. (2015); Srivastava et al. (2015); Patraucean et al. (2015); Vondrick et al. (2016)), particularly in the area of video prediction research(Mathieu et al. (2015); Kalchbrenner et al. (2017); Tulyakov et al. (2017); Xu et al. (2018); Oh et al. (2015); Villegas et al. (2017); Lee et al. (2018); Wichers et al. (2018)). There are a variety of models, each with its own principle and nice features. They can be roughly grouped into three categories: autoencoders, CNN, and stack of LSTMs. In many case, they involve feedforward and feedback paths, where the feedback path can be done by deconvolution, autoencoder networks, LSTM or adversary networks (Finn et al. (2016); Lotter et al. (2016); Wang et al. (2017; 2018)). Some more advanced models, such as variational autoencoders, allowed multiple hypotheses to be sampled (Babaeizadeh et al. (2017); Denton & Fergus (2018)).
PredNet and PredRNN (or PredRNN++) are the two state-of-the-art models that are most neurally plausible. Both, as ours, used CNN for feedforwad analysis path and LSTM for internal memories to provide feedback. But PredNet used the feedforward convolution to construct a hierarchy of errors, while PredRNN, as most computer vision models, use feedforward convolution to construct a hierarchy of successively more complex and abstract features. PredNet might be more efficient in coding and computation, but we believe that having a hierarchical of feature representations is important for predicting future events in broader spatial and temporal scales. To resolve this tension, we adopt a modified version of the predictive-corrective "residual module" scheme proposed by (Pan
2

Under review as a conference paper at ICLR 2019

et al. (2018)) for video coding that performs sparse convolution on predictive residuals to speed up computation and then reconstruct representation at each level by combining the residuals with past representation. This make it possible for long-range projecting neurons to communicate residual errors only, realizing communication coding efficiency, while maintaining a hierarchy of representation for visual reasoning. Both PredNet and PredRNN use top-down information to help image synthesis in the prediction, but PredRNN channels only the top-layer information to the bottom image representation layer. Our model, like PredNet, use loops between successive to allow tighter interaction between representations of more gradual changes to have more precise alignment for integrating global and local information that can produce higher resolution prediction. Both PredNet and PredRNN use frame-to-frame prediction. We argue that it is important to use spatiotemporal block as a unit for input and output. Biologically, neurons have spatiotemporal receptive fields, and working on spatiotemporal units allow movement to movement or chunk to chunk prediction, which would be more effective for long-range prediction.

3 OUR MODEL

Our model is composed of a stack of Sparse Predictive (SP) Modules. Each SP module can be considered as a visual area along the ventral stream of the primate visual system, such as V1, V2, V4 and IT. Each module contains a feedforward path involving sparse convolution, a LSTM internal model for generating prediction, and a feedback path linking the LSTM in the different modules along the hierarchy. The input and output of both the sparse convolution feedforward network and the LSTMs are spatiotemporal block or chunk, which is a sequence of image frames. Hence, the convolution kernel of a neuron is typically 3D (x,y, time). The LSTM in each layer processes these spatiotemporal sequences or chunks to form chunk level prediction.

Prediction

Input

Rl+1 Rl+1 I l+1

El+1 El+1

Prediction Prediction

(a) (b)

Input Input

I l+1 I l+1 Pl+1

(c)

Rl

Rl El

I l El

(d)

Il Il

Pl

Figure 2: (a) Frame-by-frame prediction model. (b)

Chunk-by-frame prediction model. (c) Chunk-by-chunk

prediction model. (d) In 3DconvLSTM, Hadamard

Figure 1: Two SP modules in two layers. product is replaced by 3D convolution.

3.1 SPARSE PREDICTIVE MODULE In the Sparse Predictive (SP) Module, which is shown in Figure 1, there are two types of errors. The first one, I is the temporal difference Ilk between the current spatiotemporal chunk Ilk and the earlier chunk Ilk-d, where d is the time interval between the two chunks, k is the number of frame
3

Under review as a conference paper at ICLR 2019

in one chunk, and l is the number of layer. The second one is the prediction error Elk, which is the difference between the current bottom-up representation Ilk and the predicted chunk I^lk generated by the internal LSTM model. Because of these errors are sparse, sparse convolution (Liu et al. (2017)) on them to generate the high-level errors, Rlk and Elk respectively, is very fast. The higher level representation Rlk can be recovered by adding Rlk to the earlier representation Rlk-d, to form a hierarchy of bottom-up feature representations. We perform a non-linear transform or pooling to obtain Rlk to obtain Ilk+1, halving its size in each spatial dimension. The Il=0 layer of the bottom layer is the image sequence input. The bottom layer of any higher module can be Rlk or Rlk. If sparse code communication between modules is desired, the projecting output of the module can be the higher order error signals Rlk rather than reconstructed higher order representation Rlk as currently depicted in Figure 1.
The internal model for making prediction is a 3D convolutional LSTM, which is a standard convolutional LSTM, operating on 3D spatiotemporal chunks. The prediction I^lk is derived from Plk, the output of 3D convolutional LSTM (indicated by P in Figure 1). The LSTM maintains its hidden memory Plk-d and receives prediction error Elk-1 of the same module and the lower module, the reconstructed bottom-up representation Rlk-1 and activation error Elk-1, and the top-down feedback from the higher layer LSTM. The entire network is trained by minimizing a loss function which is
the sum of all the prediction errors. The algorithm is shown below:

Ilk =

M axP ool(ReLU (Rlk-1)) xt

l>1 l=1

I^lk =

ReLU (conv(Plk))

l>1

SAT LU (ReLU (conv(Plk))) l = 1

(1)

Ilk = Ilk - Ilk-d, Elk = Ilk - I^lk

(2)

Rlk = spconv(Ilk), Elk = spconv(Elk), Rlk = Rlk-d + Rlk

(3)

Plk = 3DconvLST M (Plk+-1d, Elk-d, M axP ool(ReLU (Rlk-1, Elk-1)), upsample(Plk+1)) (4)

Ltrain =

k
k

l

l nl

nl

Elk

(5)

Algorithm 1 The algorithm of our model

Input: I1k  xt 1: for t = 1 to T do

2: for l = L to 1 do

Top-down processure

3: if l = L then 4: Plk = 3DconvLST M (Plk+-1d, Elk-d, M axP ool(ReLU (Rlk-1, Elk-1))) 5: else 6: Plk = 3DconvLST M (Plk+-1d, Elk-d, M axP ool(ReLU (Rlk-1, Elk-1)), upsample(Plk+1)) 7: end if

8: end for

9: for l = 1 to L do

Bottom-up processure

10: if l = 1 then 11: Ilk = xt, I^lk = SAT LU (ReLU (conv(Plk))) 12: else 13: Rlk = Rlk-d + Rlk, Ilk = M axP ool(ReLU (Rlk-1)), I^lk = ReLU (conv(Plk) 14: end if 15: Rlk = Rlk-d - Rlk, Elk = Rlk - R^lk, Ilk = spconv(Rlk), Elk = spconv(Elk) 16: end for

17: end for

where spconv is sparse convolution, and SATLU is a non-linearity set at the maximum pixel value: SatLU(x; pmax):= min(pmax, x).

3.2 3D CONVOLUTIONAL LSTM
Our model utilizes a 3D convolutional LSTM, operating on the 3D spatiotemporal chunk for each feature channel. This design is inspired by convLSTM and 3D convolution networks (Tran et al.

4

Under review as a conference paper at ICLR 2019

(2015); Shi et al. (2015)). LSTM is proposed by (Hochreiter & Schmidhuber (1997)) with its remarkable performance in sequence learning tasks by integrating gating mechanisms with temporal memory. By keeping a hidden memory in the cell, LSTM has the ability to control information flow and trap the gradient in the cell to prevent it from vanishing too quickly. The FC-LSTM is a multivariate version of LSTM where the input, the cell output and the states are all 1D vectors (Graves (2013)). The major drawback of FC-LSTM in handling spatiotemporal data is its usage of full connections in input-to-state and state-to-state transitions in which no spatial information is encoded. Therefore, in convLSTM (Shi et al. (2015)), a convolution operator is used to replace the Hadamard product for calculating inputs and past states of its local neighbors.
ConvLSTM has greatly improved performance of LSTM in many real-world image-based tasks. However, analyzing one image at one time is time-consuming and expensive, especially for sequence based video tasks. Tran et al. (2015) proposed 3D convolution networks as an effective approach by enlarging two dimensional data into three dimension for spatiotemporal feature learning, which speeds up the training time. Inspired by these works, we proposed 3D convolutional LSTM, a fast and effective model that can perform well in video sequence based tasks by learning horizontal relationship among consecutive temporal chunks. In Figure. 2 (d), in 3DconvLSTM, we define the input video clips with a size of c × d × h × w, where c is the number of channels, d is the number of adjacent frames, h and w are the height and width of the frame, respectively. We also define 3D convolution kernel size by m × k × k, where m is kernel temporal depth and k is kernel spatial size. To ensure that the states have the same number of rows and same number of columns as the inputs, padding is needed before applying the convolution operation. We set the stride to 1. The output size with n kernels is n × d × h × w. We define the inputs as X1, ..., Xt, the cell outputs as C1, ..., Ct, the hidden states as H1, ..., Ht, and the gates as it, ft, ot, and our algorithm of 3D convolutional LSTM is shown below, where the function of 3D convolution is represented as and  is the Hadamard product.

it = (Wxi Xt + Whi Ht-1 + Wci  Ct-1 + bi) ft = (Wxf Xt + Whf Ht-1 + Wcf  Ct-1 + bf ) Ct = ft  Ct-1 + it  tanh(Wxc Xt + Whc Ht-1 + bc) ot = (Wxo Xt + Who Ht-1 + Wco  Ct + bo) Ht = ot  tanh(Ct)

(6)

The existing future video prediction models processed sequences frame-by-frame, as in 2 (a), using one input frame to produce one future frame. We can learn temporal relationship to predict frame based on chunk, as 2 (b), where convLSTM takes in a sequence of consecutive frames and generate one future frame prediction one at a time. However, this chunk-by-frame approach is quite timeconsuming, and we decide to use the chunk-by-chunk model 2 (c), where 3DconvLSTM can learn temporal relationship between chunks and is in fact more effective in long range video prediction.

4 EXPRIMENT
To evaluate the performance of our model, we performed computer vision experiments to predict (1) synthetic sequences, and (2) real-world sequence prediction. We also evaluate whether the model is capable of reproducing some of the video or sequence learning effects observed in the visual cortex.
4.1 SYNTHETIC SEQUENCE PREDICTION ON MOVING-MNIST DATASET
For synthetic video prediction experiment, we compare our results with two state-of-the-art neurally plausible models, PredNet (Lotter et al. (2016)) and PredRNN++ Wang et al. (2018) in a long range video prediction task. In our experiment, we randomly chose subsets of digits in the MNIST dataset1 with each frame containing two handwritten digits bouncing inside a 64 × 64 patch. Each sequence is 40 frames long and its starting position, hence the speed and direction of the movements, are chosen uniformly at random as in [3,4). This extraction process is repeated 15000 times, resulting in a dataset with 10000 training sequences, 2000 validation sequences, and 3000 testing sequences as in Srivastava et al. (2015).
1 http://yann.lecun.com/exdb/mnist/

5

Under review as a conference paper at ICLR 2019
For PredNet, we used the same framework as described in (Lotter et al. (2016)), setting a fourlayer model with kernel size of 3×3 and layer feature channel sizes of (1,32,64,128). To generate multiple future frame prediction using PredNet model, the training process was the same as Lotter et al. (2016), predicting one frame based on a sequence including the last frame. To generate 20 frames into the future, the predicted frame was assumed to be the ground truth and fed into the network as input frame, and the prediction error was set to be the mean absolute error computed earlier when there were real observations. For PredRNN++, we used the same structure as Wang et al. (2018), but trained the network with 20 input frames. Based on (Tran et al. (2015)), we set the kernel size as 3×3×3 in a four-layer framework with kernel number (16,32,64,128).
To test the effectiveness of our scheme of having reconstructed representation and predicted errors in each layer in the hierarchy against PredNet's schemes of hierarchical errors, we replaced the 3DconvLSTM by 2DconvLSTM to generate one prediction frame at one time with chunk set to 1 frame for a more fair comparison (named F-F). In addition, to properly compare the performance of chunk-by-chunk prediction (named C-C) and frame-by-frame prediction (named C-F), we used a sliding chunk window to generate one frame as output at one time. Finally, we also evaluated the effectiveness of the temporal memory frame-by-chunk (C-F) by comparing it against the frame-byframe (F-F) prediction.
For long-range prediction, we trained the network with 40 frames during training. During testing, we tested the first 20 frames in the same way as in training, activating the internal model for generating the prediction. Then for the last 20 frames, we first used the pervious 5-frame prediction as the input, and just let the model run forward. The training and testing processes ran on four GeForce GTX TITAN X GPUs. We evaluate the performance on prediction error by mean-squared error (MSE) and the Structural Similarity Index Measure (SSIM) Wang et al. (2004) on the last 20 frames for the long time prediction task for test datasets. The value of SSIM ranges between -1 and 1, with a larger score indicating greater similarity between two images.
k=1 k=3 k=5 k=7 k=9 k=11 k=13 k=15 k=17 k=19 k=21 k=23 k=25 k=27 k=29 k=31 k=33 k=35 k=37 k=39

PredRNN++ PredNet F-F C-F C-C GT

Figure 3: Experiment results on moving-MNIST dataset, where the first row to last row are ground truth (GT), chunk-by-chunk (C-C) results, chunk-by-frame (C-F) results, frame-by-frame (F-F) results, PredNet results, and PredRNN++ results, respectively.

Table 1: Comparison Results of different meth- Table 2: Comparison Results of different meth-

ods on Moving-MNIST datatset for long time ods on KTH datatset for long time prediction ex-

prediction experiment.

periment.

Method Ours(C-C) Ours(C-F) SPB+ConvLSTM (F-F) PredNet Lotter et al. (2016) PredRNN++ Wang et al. (2018)

SSIM 0.915 0.793 0.692 0.658 0.872

MSE 65.2 73.2 89.5 101.2 69.4

Method Ours(C-C) Ours(C-F) SPB+ConvLSTM (F-F) PredNet Lotter et al. (2016) PredRNN++ Wang et al. (2018)

SSIM 0.882 0.784 0.701 0.656 0.865

MSE 80.3 93.1 103.4 108.9 86.7

Figure 3 and Table 1 compare the results of different models on Moving-MNIST dataset. There are 40 frames in total and we show the results every two frames. The yellow line in the middle represents the border of the first 20 frames and the second 20 frames. The second 20 frames are the predictions from the various models. We can see the C-F method achieves better performance

6

Under review as a conference paper at ICLR 2019

SSIM SSIM SSIM/Time (/d)

Prediction Frame
(a)

Prediction Frame
(b)

Layer Number
(c)

Figure 4: (a) Comparison of different models of SSIM on prediction frames on Moving-MNIST datset; (b) Comparison of different models of SSIM on prediction frames on KTH datset; and (c) Comparison of C-C and C-F methods on effectiveness on SSIM over time.

than C-C method in short term prediction task when actual input frames are still available, but the C-C method outperforms C-F in the long range prediction task, showing the associative memories at the movement levels were being learned for long-range prediction. Less surprisingly, C-F performs better than F-F. confirming that 3D chunk provided additional information to the LSTM for to have a better understanding of the movement tendency. Finally, even the worse performing F-F achievedbetter prediction results than PredNet, indicating that hierarchy of representations is working better than a hierarchy of errors, everything being equal. Lastly, our C-C method outperformed than (Lotter et al. (2016); Wang et al. (2018)).
4.2 REAL-WORLD SEQUNENCE PREDICTION ON KTH DATASET
Schu¨ldt et al. (2004) introduced the KTH2 video database which contains 2391 sequences of six human actions: walking, jogging, running, boxing, hand waving, and hand clapping, performed by 25 subjects in four different scenarios. We divided video clips across all 6 action categories into a training set of 108717 sequences (persons 1-16) and a test set of 4086 sequences (persons 17-25) as (Wang et al. (2018),but doubled the length of each sequence to 40 frames relative to their works. We center-cropped each frame to a 120×120 square and then resized to a spatial resolution of 64 ×64. We used a four-layer network of our model with kernel size as 3×3×3 kernel number (24,48,96,192) in each layer. For PredNet, we used a four-layer model with kernel size of 3×3 and layer channel sizes of (1,48, 96,192). For PredRNN++, we used the same setting as in (Wang et al. (2018)). The training and testing processes are the same as in the Moving-MNIST experiment. We also evaluated the three versions of our network (C-C, C-F, F-F) for comparison. The training and testing processes were run on four GeForce GTX TITAN X GPUs. We again evaluate the performance on prediction error by mean-squared error (MSE) and the Structural Similarity Index Measure (SSIM).
k=1 k=3 k=5 k=7 k=9 k=11 k=13 k=15 k=17 k=19 k=21 k=23 k=25 k=27 k=29 k=31 k=33 k=35 k=37 k=39

PredRNN++ PredNet F-F C-F C-C GT

Figure 5: Experiment results on KTH dataset, where the first row to last row are ground truth (GT), chunk-by-chunk (C-C) results, chunk-by-frame (C-F) results, frame-by-frame (F-F) results, PredNet results, and PredRNN++ results, respectively.
2 http://www.nada.kth.se/cvap/actions/
7

Under review as a conference paper at ICLR 2019
Figure 5 and Table 2 compared the results of the different models on KTH dataset. The results are similar to the observations based on 3, where C-C method outperformed all tested models in long range prediction tasks. We also compared the effectiveness of C-C and C-F methods as a function of time, shown in 4 (c). The SSIM index over time shows that the C-C method is more effective than C-F method, for C-F method performs better than C-C method in the short term perdiction when ground truth images are provided, but setting sliding window is too time-consuming, much more than the performance increase. To evaluate the contribution of representation in hierarchical layers, we train our model with one, two, three, and four layers each time, and use t-SNE (van der Maaten & Hinton (2008)) to visualize the representation of the 20th future frame of 600 testing sequences in six movement of KTH dataset. The results are shown in Figure 6. We can see that with more layers, even the lower layers form better clusters of different movements (e.g. comparing (a) and (c), and better clusters at high level can provide top-down guidance for more accurate prediction (Kheradpisheh et al. (2018)).

(a) (b)

(c) (d)

(e) (f)

(g) (h)

Figure 6: (a)-(d) are the top layers' representation of different number layers' network, from one to four; (e)-(h) are the representation of each layers in a four layers network, from one to four. All layers' representation is shown in Appendix A.

4.3 PREDICTING VISUAL SEQUENCE LEARNING EFFECTS IN THE VISUAL CORTEX
Next, we evaluated whether our model can predict some neurophysiological observations on sequence learning in the primate visual system. A series of experiments in IT in recent years have showed that neurons as a population responded significantly less to familiar sequences than to novel sequences, particularly after the initial responses to the beginning of the sequences. For example, (Meyer & Olson (2011)) trained monkeys to image pairs in a fixed order for over 800 trials for each 8 pair images, and then compared the responses of the neurons to these images in the trained order against the responses of the neurons to the same images but in novel pairings. Figure 7 shows the mean responses of 81 IT neurons during testing stage for predicted pairs and unpredicted pairs, with all the stimuli are presented in both pairs. They found that neural responses to the expected second images in a familiar sequence is much weaker than the neural responses to the image in an unfamiliar order (unexpected). To evaluate whether our model can produce the same effect, we performed exactly the same experiments with 2000 epochs of training on the image pairs, with a gap of 2 frames, and our model produced exactly the same results, with lower responses for the predicted second frames relative to the unpredicted second frames. Each stimulus sequence was presented first with 5 gray frames, followed by 10 frames of the first image in the pair, then 2 gray frames as gap, then 10 frames of the second image in the pair. The responses of the units to the trained set and the untrained set are the same prior to training. After training, the images when arranged in the trained order responded much less after the initial responses than the same images but arranged in unpredicted pairs. The result shown in 8 duplicated the observations in (Meyer & Olson (2011)), the average neural response of E unit is lower than the unpredicted pairs. (Lotter et al. (2018)) also tested the predictive suppression effect, but their model couldn't allow any gap between the stimuli
8

Under review as a conference paper at ICLR 2019
as in the experiment. Our model can handle gap because of our model is processing information in spatiotemporal chunks, as a further testimony that neurons are operating on spatiotemporal chunks.

Figure 7: Predictive suppression in IT neuron Figure 8: Predictive suppression result on E

(Meyer & Olson (2011)).

unit by our model.

Our model showed that both the lower layer and higher layer will show prediction suppression effect in sequence learning, even though the lower layer's neurons have much smaller and localized receptive fields. To evaluate this prediction, we performed a neurophysiological experiments on two macaque monkeys with semi-chronic Gray-Matter multi-electrode arrays (32 channels in one monkey, resulting in 15-20 V1/V2 neurons per session and 96 channels in another monkey, resulting in 40-60 V1/V2 neurons per session) implanted in V1 and V2. Each day, the monkeys were exposed to the same 20 movies designated as "familiar" ("trained"), and 20 novel movies (movies that the monkeys have not seen before). The novel movies were different every day. The familiar and novel-movies-of-the-day were presented 20 times. This process was repeated for 7 days. By testing familiar and novel movies together every session, we could track the development of the familiar sequence learning effect over time. We did not require the neurons remain the same across days, as we assumed with sufficient number of neurons and sufficient number of movies, the low level features of the movies and the tunings of the neurons would be averaged out across the familiar and novel sets per session. We monitored neuronal visual responses in these visual areas across days, comparing the responses of the neurons to familiar movies against novel movies each day. We found that after 3 or 4 days of training, the neurons responded much less to the predicted movies than the novel movies as shown in Figure 9.
Figure 9 (a) shows the progression of the prediction suppression effect in V1/V2 (cells combined) in one monkey, each dot represents a neuron's prediction suppression or familiarity suppression index, which is the ratio of (U-P)/(U+P) where U is the responses of the neurons to unpredicted movies after the initial responses, and P is the responses of the neurons to the predicted movies after the initial responses. Neurons are categorized into 3 groups: neural averaged response to predicted movies is significantly higher than response to unpredicted movies (green); neural averaged response to predicted movies is significantly lower than response to unpredicted movies (red); and neural averaged response to predicted movies is not significantly different from response to unpredicted movies (blue). All of these three cases have p-value of 0.05 and are conducted using t-test. The mean of all neural responses across training days is shown as the magenta curve. The prediction suppression effect gradually ramped up and get saturated.
We performed a similar experiment to our networks. We randomly extracted 20 sequences from the BAIR dataset (Ebert et al. (2017)), resized sequence length to 40 frames and each frame size 64×64, and separated them into two sets, A and B. Then we used a pre-trained model with KTH dataset to test the average neuron response of the center 8×8 neurons of each channels. of each layer's E, P , and R units. The original normalized responses for both set A and set B are the same (not shown). Then, we trained the network with set A with each sequence being trained for 2000 epoch and tested the same response of these three types of units in each layer. Figure 9 showed that the responses to the trained set A was significantly suppressed relative to the responses in set B after 1000 epochs of training. Interestingly, we found the prediction suppression effects can be observed in all three types of neurons ­ the representation neurons R, the prediction neurons P and the prediction error neurons E. Obviously, E would necessarily show prediction suppression as the internal model for the movie got better, but the fact that R and E also got suppressed are very interesting, suggesting the representation might become more efficient for the trained movies.
9

Under review as a conference paper at ICLR 2019

(c) E1

(d) P1

(e) R1

(a) Experiment development.

(b) Monkey neuron response.

(f) E4

(g) P4

(h) R4

Figure 9: (a) The progression of familiarity effect development.; (b) Averaged PSTH of monkey neuron response of the 1-2 days and 5-12 days, which the latter one shows the familiarity effect ; (c)(e) Bottom layer's normalized average neuron response of three units; (f)-(h) Top layers normalized average neuron response of three units.

5 CONCLUSION
In this paper, we proposed a new hierarchical model for long time memory learning, which contains predictive coding mechanism and sparse convolution added to speed up the time. A new chunkby-chunk movement level prediction way is proposed with 3DconvLSTM learning temporalspatial relationship. The prediction is generated by top-down, bottom-up, and horizontal connection, and representation reconstruction in each layer perserves information to deal with vanishing gradient. Our model outperform than existing methods in our experiment results and shows biologically similar function as primates, like familiarity effect in monkey's viusal cortex.
ACKNOWLEDGMENTS
This work is supported by the Intelligence Advanced Research Projects Activity (IARPA) via Department of Interior/ Interior Business Center (DoI/IBC) contract number D16PC00007. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright annotation thereon. Disclaimer: The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of IARPA, DoI/IBC, or the U.S. Government.
REFERENCES
Mohammad Babaeizadeh, Chelsea Finn, Dumitru Erhan, Roy H. Campbell, and Sergey Levine. Stochastic variational video prediction. CoRR, abs/1710.11252, 2017.
Achal Dave, Olga Russakovsky, and Deva Ramanan. Predictive-corrective networks for action detection. 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 2067­ 2076, 2017.
Emily L. Denton and Rob Fergus. Stochastic video generation with a learned prior. In ICML, 2018.
Nadine Dijkstra, Peter Zeidman, Sasha Ondobaka, Marcel A J van Gerven, and Karl J. Friston. Distinct top-down and bottom-up brain connectivity during visual perception and imagery. In Scientific Reports, 2017.
Frederik Ebert, Chelsea Finn, Alex Lee, and Sergey Levine. Self-supervised visual planning with temporal skip connections. In Conference on Robot Learning (CoRL), 2017.
Chelsea Finn, Ian J. Goodfellow, and Sergey Levine. Unsupervised learning for physical interaction through video prediction. In NIPS, 2016.
Karl J. Friston. Does predictive coding have a future? Nature Neuroscience, 21:1019­1021, 2018.
10

Under review as a conference paper at ICLR 2019
Ross Goroshin, Michae¨l Mathieu, and Yann LeCun. Learning to linearize under uncertainty. In NIPS, 2015.
Alex Graves. Generating sequences with recurrent neural networks. CoRR, abs/1308.0850, 2013.
Stephen Grossberg. Competitive learning: From interactive activation to adaptive resonance. Cognitive Science, 11:23­63, 1987.
Feng Han, Natalia Caporale, and Yang Dan. Reverberation of recent visual experience in spontaneous cortical waves. Neuron, 60:321­327, 2008.
Jeff Hawkins and Dileep George. Hierarchical temporal memory concepts , theory , and terminology. 2006.
Geoffrey E. Hinton. Learning multiple layers of representation. Trends in cognitive sciences, 11 10: 428­34, 2007.
Sepp Hochreiter and Ju¨rgen Schmidhuber. Long short-term memory. Neural Computation, 9:1735­ 1780, 1997.
Ge Huang, Suchitra Ramachandran, Tai Sing Lee, and Carl R. Olson. Neural correlate of visual familiarity in macaque area v2. The Journal of neuroscience : the official journal of the Society for Neuroscience, 2018.
Yanping Huang and Rajesh P.N Rao. Predictive coding. In Cogn Sci, 2011.
Nal Kalchbrenner, Aa¨ron van den Oord, Karen Simonyan, Ivo Danihelka, Oriol Vinyals, Alex Graves, and Koray Kavukcuoglu. Video pixel networks. In ICML, 2017.
Saeed Reza Kheradpisheh, Mohammad Ganjtabesh, Simon J. Thorpe, and Timothe´e Masquelier. Stdp-based spiking deep convolutional neural networks for object recognition. Neural networks : the official journal of the International Neural Network Society, 99:56­67, 2018.
Alex X. Lee, Richard Zhang, Frederik Ebert, Pieter Abbeel, Chelsea Finn, and Sergey Levine. Stochastic adversarial video prediction. CoRR, abs/1804.01523, 2018.
Xingyu Liu, Jeff Pool, Song Han, and William J. Dally. Efficient sparse-winograd convolutional neural networks. CoRR, abs/1802.06367, 2017.
William Lotter, Gabriel Kreiman, and David D. Cox. Deep predictive coding networks for video prediction and unsupervised learning. CoRR, abs/1605.08104, 2016.
William Lotter, Gabriel Kreiman, and David Cox. A neural network trained to predict future video frames mimics critical properties of biological neuronal responses and perception. CoRR, abs/1805.10734, 2018.
Michae¨l Mathieu, Camille Couprie, and Yann LeCun. Deep multi-scale video prediction beyond mean square error. CoRR, abs/1511.05440, 2015.
James L. McClelland and Bruce L. McNaughton. Complementary learning systems 1 why there are complementary learning systems in the hippocampus and neocortex : Insights from the successes and failures of connectionist models of learning and memory. 1999.
James L. McClelland and David E. Rumelhart. Distributed memory and the representation of general and specific information. Journal of experimental psychology. General, 114 2:159­97, 1985.
Travis Meyer and Carl R. Olson. Statistical learning of visual transitions in monkey inferotemporal cortex. Proceedings of the National Academy of Sciences of the United States of America, 108 48:19401­6, 2011.
David Mumford. On the computational architecture of the neocortex. Biological Cybernetics, 65: 135­145, 1991.
David Mumford. On the computational architecture of the neocortex. ii. the role of cortico-cortical loops. Biological cybernetics, 66 3:241­51, 1992.
11

Under review as a conference paper at ICLR 2019
Junhyuk Oh, Xiaoxiao Guo, Honglak Lee, Richard L. Lewis, and Satinder P. Singh. Actionconditional video prediction using deep networks in atari games. In NIPS, 2015.
Randall C. O'Reilly, Dean Wyatte, and John Rohrlich. Learning through time in the thalamocortical loops. 2014.
Rasmus Berg Palm. Prediction as a candidate for learning deep hierarchical models of data. 2012.
Bowen Pan, Wuwei Lin, Xiaolin Fang, Chaoqin Huang, Bolei Zhou, and Cewu Lu. Recurrent residual module for fast inference in videos. CoRR, abs/1802.09723, 2018.
Viorica Patraucean, Ankur Handa, and Roberto Cipolla. Spatio-temporal video autoencoder with differentiable memory. CoRR, abs/1511.06309, 2015.
Christian Schu¨ldt, Ivan Laptev, and Barbara Caputo. Recognizing human actions: a local svm approach. Proceedings of the 17th International Conference on Pattern Recognition, 2004. ICPR 2004., 3:32­36 Vol.3, 2004.
Xingjian Shi, Zhourong Chen, Hao Wang, Dit-Yan Yeung, Wai-Kin Wong, and Wang chun Woo. Convolutional lstm network: A machine learning approach for precipitation nowcasting. In NIPS, 2015.
William R. Softky. Unsupervised pixel-prediction. In NIPS, 1995.
Nitish Srivastava, Elman Mansimov, and Ruslan Salakhutdinov. Unsupervised learning of video representations using lstms. In ICML, 2015.
Du Tran, Lubomir D. Bourdev, Rob Fergus, Lorenzo Torresani, and Manohar Paluri. Learning spatiotemporal features with 3d convolutional networks. 2015 IEEE International Conference on Computer Vision (ICCV), pp. 4489­4497, 2015.
Sergey Tulyakov, Ming-Yu Liu, Xiaodong Yang, and Jan Kautz. Mocogan: Decomposing motion and content for video generation. CoRR, abs/1707.04993, 2017.
Laurens van der Maaten and Geoffrey E. Hinton. Visualizing data using t-sne. 2008.
Ruben Villegas, Jimei Yang, Yuliang Zou, Sungryull Sohn, Xunyu Lin, and Honglak Lee. Learning to generate long-term future via hierarchical prediction. In ICML, 2017.
Carl Vondrick, Hamed Pirsiavash, and Antonio Torralba. Generating videos with scene dynamics. In NIPS, 2016.
Yunbo Wang, Mingsheng Long, Jianmin Wang, Zhifeng Gao, and Philip S. Yu. Predrnn: Recurrent neural networks for predictive learning using spatiotemporal lstms. In NIPS, 2017.
Yunbo Wang, Zhifeng Gao, Mingsheng Long, Jianmin Wang, and Philip S. Yu. Predrnn++: Towards a resolution of the deep-in-time dilemma in spatiotemporal predictive learning. In ICML, 2018.
Zhou Wang, Alan C. Bovik, Hamid R. Sheikh, and Eero P. Simoncelli. Image quality assessment: from error visibility to structural similarity. IEEE Transactions on Image Processing, 13:600­612, 2004.
Nevan Wichers, Ruben Villegas, Dumitru Erhan, and Honglak Lee. Hierarchical long-term video prediction without supervision. In ICML, 2018.
Shengjin Xu, Wanchen Jiang, Mu-Ming Poo, and Yang Dan. Activity recall in visual cortical ensemble. In Nature Neuroscience, 2012.
Ziru Xu, Yunbo Wang, Mingsheng Long, and Jianmin Wang. Predcnn: Predictive learning with cascade convolutions. In IJCAI, 2018.
Haishan Yao, Lei Shi, Feng Han, Hongfeng Gao, and Yang Dan. Rapid learning in cortical coding of visual scenes. Nature Neuroscience, 10:772­778, 2007.
Yimeng Zhang, Tai Sing Lee, Ming Fu Li, Fang Liu, and Shiming Tang. Convolutional neural network models of v1 responses to complex patterns. Journal of Computational Neuroscience, pp. 1­22, 2018.
12

Under review as a conference paper at ICLR 2019
APPENDIX A RECONSTRUCTED REPRESENTATION IN HIERARCHY

(a) Layer 1_1

(b) Layer 2_1

(c) Layer 2_2

(d) Layer 3_1

(e) Layer 3_2

(f) Layer 3_3

(g) Layer 4_1

(h) Layer 4_2

(i) Layer 4_3

(j) Layer 4_4

Figure 10: Visualization of (a) one-layer network; (b)-(c) two-layer network; (d)-(f) three-layer network; and (g)-(j) four-layer network.

Figure 10 shows comparison results of representation in the networks with different number of layers. The results are generated by t-SNE van der Maaten & Hinton (2008) on KTH dataset during testing process.
13

Under review as a conference paper at ICLR 2019
B NEURON EXPERIMENT RESULTS

(a) E1

(b) E2

(c) E3

(d) E4

(e) P1

(f) P2

(g) P3

(h) P4

(i) R1

(j) R2

(k) R3

(l) R4

Figure 11: Prediction suppression simulation results on each layers E, P , and R units, where the abscissa is number of epoch and the ordinate is normalized average neuron response.

14


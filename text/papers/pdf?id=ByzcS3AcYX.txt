Under review as a conference paper at ICLR 2019
TTS-GAN: A GENERATIVE ADVERSARIAL NETWORK FOR STYLE MODELING IN A TEXT-TO-SPEECH SYSTEM
Anonymous authors Paper under double-blind review
ABSTRACT
The modeling of style when synthesizing natural human speech from text has been the focus of significant attention. Some state-of-the-art approaches train an encoder-decoder network on paired text and audio samples xtxt, xaud by encouraging its output to reconstruct xaud. The synthesized audio waveform is expected to contain the verbal content of xtxt and the auditory style of xaud. Unfortunately, modeling style in TTS is somewhat under-determined and training models with a reconstruction loss alone is insufficient to disentangle content and style from other factors of variation. In this work, we introduce TTS-GAN, an end-to-end TTS model that offers enhanced content-style disentanglement ability and controllability. We achieve this by combining a pairwise training procedure, an adversarial game, and a collaborative game into one training scheme. The adversarial game concentrates the true data distribution, and the collaborative game minimizes the distance between real samples and generated samples in both the original space and the latent space. As a result, TTS-GAN delivers a highly controllable generator, and a disentangled representation. Benefiting from the separate modeling of style and content, TTS-GAN can generate human fidelity speech that satisfies the desired style conditions. TTS-GAN achieves start-of-the-art results across multiple tasks, including style transfer (content and style swapping), emotion modeling, and identity transfer (fitting a new speaker's voice).
1 INTRODUCTION
In the past few years, we have seen exciting developments in Text-To-Speech (TTS) systems with the use of deep neural networks. These models typically aim to synthesize human-like speech from text in an end-to-end fashion. Ideally, synthesized speech should convey the given text content in an appropriate auditory style which we refer to as style modeling. Modeling style is of particular importance for many practical applications such as intelligent conversational agents and assistants. Yet, this is an incredibly challenging task because the same text can map to different speaking styles, making the problem somewhat under-determined. To this end, the recently proposed Tacotron-based approaches (Wang et al., 2018; Skerry-Ryan et al., 2018a) use a piece of reference speech audio to specify the expected style. Given a text, audio input, they assume two independent latent variables: c that encodes content from text, and s that encodes style from the reference audio, where c and s are produced by a text encoder and a style encoder, respectively. A new audio waveform can be consequently generated by a decoder conditioned on c and s, i.e. p(x|c, s). Thus, it is straight forward to train the model that minimizes the log-likelihood by a reconstruction loss.
However, this method makes it challenging for s to exclusively encode style as no constraints are placed on the disentanglement of the style components from the content components within the reference audio. The result being that the style encoder is impeded from producing an informative style representation. In other words, there are no constraints that prevent the content components within reference audio from flowing through the model, making it easy for the model to just copy all the information within the reference audio to the output during training.
To help address some of the limitations of the prior work, we build TTS-GAN, which provides enhanced controllability and disentanglement ability. Rather than only training on single paired text-audio sample (the text and audio are aligned with each other), i.e. (xtxt, xaud)  x~, we adopt a pairwise training procedure to enforce TTS-GAN to correctly map from one text to two
1

Under review as a conference paper at ICLR 2019

audio references (xtxt, x+aud, x-aud), i.e. (xtxt, xa+ud)  x~+; (xtxt, xa-ud)  x~-. Where x+aud is paired with xtxt, and xa+ud is unpaired (randomly sampled). Training TTS-GAN involves solving an adversarial game and a collaborative game. The adversarial game concentrates the true joint data distribution p(x, c) by using a conditional GAN loss. The collaborative game is built to minimize the distance of generated samples from the real samples in both original space and latent space. Specifically, we introduce two additional losses, the reconstruction loss and the style loss. The style loss is produced by drawing inspiration from image style transfer, which can be used to give explicit style constraints. During training, the the generator and discriminator combat each other to match a joint distribution. While at the same time, they also collaborate with each other in order to minimize the distance of the expected sample and the synthesized sample in both original space and hidden space. As a result, TTS-GAN delivers a highly controllable generator, and a disentangled representation. 1

2 BACKGROUND

TTS can be formulated as a cross-domain mapping problem, i.e. given the source domain S (text) and target domain T (audio), we want to learn a mapping F : S  T such that the distribution of F (S) matches the distribution T . When modeling style in TTS, F shall be conditioned on a style
variable (where the style could specified in many forms, such as a reference audio waveform or a label). Given a text and audio pair (xtxt, xaud), the goal under this formulation is to synthesize a new audio waveform that contains the textual content specified by xtxt and the auditory style specified by xaud. This is solved by training on paired data (xtxt and xaud are aligned with each other) with a reconstruction loss in some previous works, e.g. Tacotron-based systems (Wang et al., 2018; Skerry-
Ryan et al., 2018a). Here, We describe this solution via a conditional probabilistic model admitting two independent sources of variation: a content variable c1:T with T words specified by text xtxt, and a style variable s given by the reference audio xaud. Given (xtxt, xaud), we can sample:

content : c1:T  q(c1:T |xtxt), style : s  q(s|xaud), and output x~  p(x|c1:T , s), (1)

p(x|c1:T , s) is a likelihood function described by a decoder network, Dec. We define deterministic
encoders Encc that maps xtxt to their corresponding content components, and Encs that parameterizes the approximate posterior q(s|xaud). It is natural to consider the conditional likelihood to be written as x  p(x|Encc(xtxt), Encs(xaud)), and the training objective could be maximizing
the log-likelihood:

Ec1:T q(c1:T |xtxt), sq(s|xaud) log p(x|c, s)

(2)

Ideally, the learned mapping F should be bijective, i.e. there should be a one-to-one correspondence
between input conditions and the output audio waveform. However, we argue that training only on
paired data with maximum likelihood objective is insufficient to learn a bijective mapping. Unlike xtxt that purely contains content components, xaud consists of both style components s and other factors z, such as verbal content that matches with xtxt. Therefore, the model needs to be able to disentangle s from z. Otherwise, in the case of training on paired data by maximum likelihood objective, the model could simply learn to copy the waveform information from xaud to the output and ignore s. When given the same xtxt but different xaud to such a model, it may still map sample to the same x~, i.e., F is not bijective any more. In the following sections, we introduce a way to
prevent this degenerate issue.

3 TTS-GAN
Our proposed approach makes use of pair-wise training and GAN architecture. The training pipeline is illustrated in Figure 1. This training procedure could also be considered as the swapping of style components. After swapping, the content components of both observations will remain the same, while the sources of style will change, and be aligned with xa+ud and xa-ud, respectively. The two games involved in training TTS-GAN are described in detail in Sec. 3.1 and Sec. 3.2.
1Speech synthesis results are available at https://researchdemopage.wixsite.com/tts-gan

2

Under review as a conference paper at ICLR 2019

Figure 1: Training and testing architectures of TTS-GAN. (a) A content encoder Encc encodes

xtxt into the text embedding c1:T , and a style encoder Encs encodes both paired and unpaired

audio takes

esaacmhpolefs,thxe+autwd oancdonxda-iutido,nisnt(oc,stysl+e )emanbded(dci,ngss-s)+,

and and

s-, respectively. generates x~+ 

The decoder Dec p(x|c, s+), and

x~-  p(x|c, s-). All the losses involved in solving the adversarial game (D) and collaborative

game (R and C) are labeled with dashed lines. (b) Given xtxt and xaud, we produce synthesized

speech with Encc, Encs and Dec. Note that xaud does not have to be paired with xtxt.

3.1 ADVERSARIAL GAME

Because x~- need not be aligned with the content factors of xa-ud, we cannot enforce the reconstruction of x-aud. Instead, we enforce that both x~+ and x~- are assigned high probabilities of belonging to the target domain by means of generative adversarial networks Goodfellow et al. (2014). Specifically, we use a conditional GAN (Mirza & Osindero, 2014) to model a joint distribution of audio and content (i.e. D(x, c)), which provides a stronger constraint by enforcing the decision to be always conditioned on the content variable c. We define the min-max adversarial game:

Ladv = min max LG + LD
GD

(3)

LG = -Ec,ss+ log D(G(c, s), c)3 - Ec,ss- log D(G(c, s), c)3

(4)

LD = -Ec,ss+ log D(G(c, s), c)1 - Ec,ss- log D(G(c, s), c)2 - Ec log D(xa+ud, c)3 (5)

Unlike the traditional binary setting of D, we make D a ternary classifier with D(·)i representing the probability of x being either "fake from paired input" (D(·)1), "fake from unpaired input" D(·)2, or "real audio sample" D(·)3. This ternary setting makes the discriminator more powerful because it
has to distinguish subtle differences between samples generated from paired and unpaired input. A

similar setting has also been used in cross-domain image generation by (Taigman et al., 2016). Our

generator consists of both the encoders Encc and Encs and the decoder Dec. The discriminator network is used only during training.

3.2 COLLABORATIVE GAME

Although the adversarial game theoretically drives p(x, c, s) toward the true data distribution, we

find it to be insufficient to find the desired distribution, as there is little supervision from the ob-

servation what difficult to find

s should represent. Especially for the correct correspondence. As a

xa-ud, result,

the lack of a pairwise relationship makes it G might generate high-fidelity samples x~-

with incorrect s, and D will still accept it as long as its style is different from x~+. Therefore, we

impose explicit constraints on the generated samples with a style loss and a reconstruction loss.

Style Loss. In the computer vision literature, Gatys et al. (2016) have shown that the artistic style of an image can be effectively represented by the gram matrix of features maps produced by a convolutional neural network. The gram matrix captures patch-level appearance statistics, such as texture, in a location-invariant manner. It is thus natural to expect that a gram matrix of feature maps computed from a mel-spectrogram captures local statistics of an audio signal in the frequency-time

3

Under review as a conference paper at ICLR 2019

domain, representing low-level characteristics of sound such as prosody. This has motivated some initial attempts in the auditory style modeling (Dmitry Ulyanov, 2016; Barry & Kim, 2018).
Let X and X~ be the feature maps of the mel-spectrograms from the reference and the synthesized audio samples, respectively. We compute the gram matrices W and G as the inner-product of vectorized feature maps X and X~ , respectively:

Gi,j = X~ikX~jk, and Wi,j = XikXjk
kk

(6)

The style loss Lsty is the L2 distance between G and W over all pairs of filters i and j:

1 Lsty(G, W ) = Nf2

i,j

(Gij - Wij )2

(7)

where Nf is the number of filters. To produce the features maps, most existing approaches in image style transfer use the VGG-19 (Simonyan & Zisserman, 2014) pretrained on ImageNet (Russakovsky et al., 2015). However, mel-spectrograms look quite different from the natural images of the ImageNet, making the VGG-19 unsuitable for our work. We found that a simple four-layer CNN with random weights, denoted by R, perform just well for our purpose; similar findings have been reported recently by Ulyanov et al. (2017), showing that the structure of a CNN is sufficient to capture a great deal of low-level image statistics.
Reconstruction Loss. As x~+ is expected to be the same as x+aud, we include Eq. 2 and encourage the reconstruction in the original mel-spectrogram space:

Lori = -Ecq(c|xtxt),sq(s+|x+aud)(log p(x|c, s+))

(8)

We also encourage reconstruction in the latent space by introducing an inference network C : xaud  l which approximates the posterior p(l|xaud) as l  pc(l|xaud) = C(xaud). C reduces to an N-way classifier if l is categorical. In our model, C and Encs share all convolution layers and there is one final fully-connected layer to output parameters for the conditional distribution pc(l|xaud). To train pc(l|xaud), we define a collaboration game in the latent space:

Llat = j={+,-} -Exxaj ud [log pc(l|x)] - Exx~j [log pc(l|x)]

(9)

Minimizing the first term w.r.t. C guides C toward the true posterior p(l|xaud), while minimizing the second term w.r.t. G enhances G with extra controllability, i.e. it minimizes the chance that G
could generate samples that would otherwise be falsely predicted by C. Note that we also minimize
the second term w.r.t. C, which proves effective during training that uses synthetic samples to augment the predictive power of C. In summary, minimizing both Lsty and Lrec can be seen as a collaborative game between players C, R and G that drives p(x|c, s) to match p(x|c, s), and pc(l|x) to match the posterior p(l|x), the reconstruction loss is thus given by:

Lrec = Lori + Llat

(10)

3.3 IMPLEMENTATION DETAILS

We train our TTS-GAN with a combination of the GAN loss, style loss, and reconstruction loss, the overall loss function is given by:

Lall = Ladv + Lsty + Lrec

(11)

where  = 0.1,  = 10. TTS-GAN is built upon Tacotron (Wang et al., 2017b) that predicts melspectrograms directly from character sequences. The predicted mel-spectrogram can be synthesized directly to speech using either the WaveNet vocoder (van den Oord et al., 2016) or the GriffinLim method (Griffin & Lim, 1984). In our experiments, we use the Griffin-Lim for fast waveform generation. For Encc, we use the same text encoder architecture of Skerry-Ryan et al. (2018b). The style encoder Encs is a combination of reference encoder and style token layers proposed in Wang et al. (2018). We combine the encoded s and c as in Tacotron, i.e. for a content sequence c of length L, we concatenate the style embedding with each state of the text embeddings. The inference network C takes as input a style embedding and processes it through a fully-connected

4

Under review as a conference paper at ICLR 2019
layers followed by batch normalization and Relu is added on top of each convolution layer. The output is mapped to an N-way classification layer. R is a 2-D fully-convolution neural network with four layers, with filter dimensions of 32, 32, 64, 64, respectively. The discriminator D has the similar architecture with the reference encoder Encs, except that the final state from the RNN is first concatenated with text embedding, and then connected to a fully-connected layer followed by ReLU. Finally it is mapped to a 3-way classification layer. We train TTS-GAN for at least 200k steps with a minibatch size of 32 using the Adam optimizer. During training, R is fixed weights. For testing, C, R and D are not needed, and we just need to send a text, audio pairs into the model (unpaired audios are not needed in the testing stage), which is shown in Figure 1.
4 RELATED WORK
Text-To-Speech (TTS): Recently, rapid progress has been achieved in TTS with end-to-end trained neural networks, e.g., WaveNet (van den Oord et al., 2016), DeepVoice (Arik et al., 2017), VoiceLoop (Taigman et al., 2018), Char2Wav (Jose Sotelo, 2017), and Tacotron (Skerry-Ryan et al., 2018b). Consequently, modeling style in TTS has become a subject of extensive study. DeepVoice2 (Gibiansky et al., 2017) and DeepVoice3 Ping et al. (2018) learn one or more lookup tables that store information about different speaker identities. However, they are limited to synthesizing voices of speaker identities seen during training. Unlike DeepVoice2 and DeepVoice3, Nachmani et al. (2018), which is based on VoiceLoop, can fit unseen speakers' voice at testing time. There is also a collection of approaches that are based on Tacotron, e.g., Tacotron-prosody (Wang et al., 2017a), Tacotron2 (Skerry-Ryan et al., 2018a) and GST (Wang et al., 2018). Tacotron2 uses an encoder to compute a style embedding from a reference audio waveform, where the embedding provides style information that is not provided by the text. The Global-Style-Token (GST) extends Tacotron2 by adding a new attention layer that captures a wide range of acoustic styles.
Domain mapping by GANs: Recently, GANs have shown promising results in various domain mapping problems. Cycle-GAN (Zhu et al., 2017a) and UNIT (Liu et al., 2017) perform imageto-image translation by adding a cycle-consistency loss to the learning objective of GANs. Further research has extended this to cross-domain translation. StackGAN (Zhang et al., 2016) generates images from text, and DA-GAN (Ma et al., 2018) operates across different domains, e.g., object transfiguration, human face to cartoon face, skeleton to natural object. Another line of work performs one-sided domain mapping without using the cycle consistency loss, e.g.,(Taigman et al., 2016). Moving beyond one-to-one domain mapping, Bicycle GAN Zhu et al. (2017b) maps samples from one domain to multiple target domains. Our work can also be considered as a one-sided crossdomain mapping that does not require cycle consistency, which makes the training more practical. We also follow the concept of Bicycle GAN that promotes a one-to-many mapping. To the best of our knowledge, ours is the first to formulate TTS as a cross domain mapping problem using GANs.
Style transfer: The recent success in image style transfer Gatys et al. (2016) has motivated approaches that model the acoustic style of sound using spectrogram. For example, Dmitry Ulyanov (2016) uses a simple 1-D convolutional layer with a ReLU to compute feature maps and then obtain style features by computing the gram matrix. Barry & Kim (2018) followed the same concept and adopted two different audio representations, the mel-spectrogram and the constant Q transform spectrogram. Inspired by this, in this work, we adopt the image style transfer concept to impose explicit style constraints on audio mel-spectrogram.
5 EXPERIMENTS
We evaluate TTS-GAN from three perspectives: content vs. style disentanglement ability (Sec. 5.1), effectiveness of style modeling (Sec. 5.2), and controllability (Sec. 5.3). We use two datasets in our experiments: EMT-4: An in-house dataset of 22,377 American English audio-text samples, with a total of 24 hours. All the audio samples are read by a single speaker, in four emotion categories: happy, sad, angry and neutral. For each text sample, there is only one audio sample labeled with one of the four emotion styles. VCTK: A publicly available, multi-speaker dataset containing recordings of clean speech from 109 speakers, with a total of 44 hours. As the raw audio clips have different specifications, we preprocess them by downsampling the audio to 24kHz and trimming leading and trailing silence, reducing the median duration from 3.3 seconds to 1.8 seconds.
5

Under review as a conference paper at ICLR 2019

Table 1: Performance comparisons with the-state-of-arts, and ablation study on different datasets.

Comparisons
Ablation study TTS-GAN

Tacotron2
GST
DeepVoice2
Ladv Ladv +Lsty GST+Lsty +Lrec Ladv +Lsty +Lrec

EMT-4

Recon. error 0.42 0.61 ­ 0.73 0.75 0.58 0.76

WER 10.6 10.2
­ 11.1 10.9 22.3 10.2

VCTK

Recon. error 0.70 0.77 0.81 0.81 0.83 0.77 0.83

WER 19.4 18.1 18.5 19.2 18.9 28.9 18.2

We compare our method with three state-of-the-art approaches: Tacotron2 (Skerry-Ryan et al., 2018a) is similar to our model but trained on the reconstruction loss only. The style embeddings are obtained from the reference encoder directly. GST (Wang et al., 2018) incorporates the Global Style Tokens to Tacotron2. DeepVoice2 (Gibiansky et al., 2017) learns a look-up table capturing embeddings for different speaker identity. As DeepVoice2 is particularly designed for multi-speaker modeling, so comparisons with DeepVoice2 is only performed on VCTK. We also conduct extensive ablation study to investigate the importance of different loss terms.
5.1 CONTENT VS. STYLE DISENTANGLEMENT ABILITY
Reconstruction error of style embeddings: If the style encoder Encs has successfully disentangled style from other factors of variation in the audio input, we expect the style embedding s to contain very little information about the content of the audio input. Therefore, we should expect poor performance when we try to reconstruct the audio sample purely from s. This motivates us to evaluate our model with the task of reconstructing audio samples from style embeddings. To this end, we train an autoencoder, where the encoder has the same architecture as Encs and the decoder has six deconvolutional layers, with each layer having batch normalization and ReLU activation. To set the baseline, we first train the autoencoder from scratch using only the L2 reconstruction loss; this results in the reconstruction error of 0.12. Next, we use precomputed style embeddings from different approaches and train only the decoder network using the reconstruction loss.
We report the results under the columns "Recon. error" in Table 1. Tacotron2 achieves the lowest reconstruction error, suggesting that the approach has the weakest ability to disentangle style from other factors in audio input. GST shows improvement over Tacotron2, which demonstrates the effectiveness of the style token layer that acts as an information bottleneck from audio input to style embeddings. DeepVoice2 performs much better than both Tacotron2 and GST on the VCTK dataset. This shows the model particularly works well on modeling speaker identities. Compared to the three state-of-the-art approaches, our TTS-GAN performs the best on both datasets. We also evaluate the importance of different loss terms in our model. We can see that the adversarial loss Ladv provides a significant improvement over the baseline models, which suggests the effectiveness of our adversarial loss and pairwise training. When we add the style loss we get further improvements. We also remove the adversarial loss and add the style and reconstruction losses to the baseline GST; this produces even worse results. It is because, when training only on paired data with GST's encoder-decoder network, the reconstruction loss already imposes very strong supervision. In this case, additional constraints might on the contrary impaired the performance due to the risk of over fitting. While as TTS-GAN is adversarially trained, the GAN loss regularizes the model, thus under this case, the style loss and reconstruction loss can help optimizing in a better way.
Content and style swapping: We follow the qualitative evaluation method from Mathieu et al. (2016) and generate audio samples by combining content and style embeddings from different ground-truth pairs of text and audio. Specifically, we randomly sample four (text, audio) pairs from the EMT-4 dataset, one for each emotion category, and create 16 permutations of text (content) and audio (style). The samples located on the diagonal line can be considered as results of parallel style transfer, i.e. the given reference audio sample is aligned with the text. While for the other ones, they are results by unparalleled style transfer. Each column has the same content and varies in style, each row shares the same style, but with different content. By hearing the samples, we can see
6

Under review as a conference paper at ICLR 2019
Figure 2: t-SNE visualization of the learned latent spaces for (a) EMT-4 and (b) VCTK. that the results are comparable for both parallel transfer and unparalleled transfer, which means the content and style components are disentangled. Even when transferring on two samples which are separated by a large distance in the latent space, e.g. sad to happy (row 2, line 4), the style can also be correctly mapped (compare this to sad to sad (row 4, line 4)). We have uploaded the synthesized audio clips to our demo page; the readers are strongly encouraged to listen to the samples.
5.2 EFFECTIVENESS OF STYLE MODELING
Speaker style modeling: We further evaluate the effectiveness of our approach on modeling styles by means of speaker verification. Specifically, we compare our style embeddings with the i-vector representation used in modern speaker verification systems (Kinnunen et al., 2017), on the speaker classification task. We report the results under the columns "Embeddings" in Table 2. We can see that, despite the fact that the i-vectors are specifically designed for speaker classification, TTS-GAN can still achieve comparable results, which suggests that TTS-GAN can produce generic feature representation for various auditory styles including speaker identity. Visualization of style embedding: Figure 2 shows the t-SNE projection (van der Maaten & Hinton, 2008) of style embeddings from (a) the EMT-4 dataset and (b) the VCTK dataset. To create the plots, we randomly sampled 1,000 instances from each dataset: (a) 250 instances from each of the four emotion categories, and (b) 125 instances from 9 speakers (3 male and 5 female). We can see that the projections show clear boundaries between different style (emotion and speaker) categories. Interestingly, "sad" is far from the other three emotion categories; we believe that this is because sad speech usually have much lower pitch compared to other emotion categories. "neutral" is projected to the middle, which has roughly the same distance with other emotion categories. Also, we can see that there is a clear boundary between male samples and female samples.
5.3 CONTROLLABILITY OF TTS-GAN
A good TTS system should allow users to control both content and style of the output. We consider two factors that affect the controllability of a TTS model: the fidelity, i.e. the synthesized speech should contain the desired content in a clearly audible form, and the naturalness, i.e. the synthesized speech should contain the desired style. WER of synthesized samples: To validate the fidelity, we assess the performance of synthesized samples in a speech recognition task. We use a pre-trained ASR model based on WaveNet (van den Oord et al., 2016) to compute Word Error Rate (WER) for the samples synthesized by each model. Results are shown in Table 1. TTS-GAN performs comparably with, and sometimes even better than, the state-of-the-art approaches. Note that WER measures only the correctness of verbal content, not its auditory style. The results suggests that all the methods we have compared perform reasonably well in controlling the verbal content in TTS. When trained with more constraints on GST, i.e. GST+Lsty+ Lrec, the performance gets worse. We suspect that this is because the autoencoder training procedure used in GST already gives strong supervision to the decoder. Thus, when added with more constraints, the model has overfitted to the training data. Our TTS-GAN does not have such problem because of the unpaired samples used during training, which act as strong regularizer.
7

Under review as a conference paper at ICLR 2019

Table 2: Classification accuracy for synthesized samples and learned style embeddings.

EMT-4

VCTK

seen unseen

Tacotron2
68% 55% 51%

Synthesized Audio GST DeepVoice2
77% ­ 65% 74% 62% ­

TTS-GAN
80% 72% 70%

Embeddings i-vector TTS-GAN
­­ ­­ 75% 71%

Classification accuracy on synthesized samples: As the style we are modeling are all categorical, we evaluate the synthesized samples by a classification task. We first train two classifiers on each dataset, which have 98% and 83% accuracy for EMT-4 and VCTK, respectively. We select 1000 samples synthesized from test set on EMT-4. To assess on VCTK, we test samples from both seen and unseen speakers, where `seen speakers' mean the speakers are part of the training set, while the reference audio are selected from the test set. `unseen speaker' means the speakers have never be seen during training, which means the model is asked to fit a new speaker's voice on testing stage. The results are shown in Table 2. As we can see, on the EMT-4 dataset, TTS-GAN performs the better than Tacotron2 and GST. When tested on the seen data on VCTK, DeepVoice2 performs the best, but it fails to generalize to unseen speakers. TTS-GAN performs well in both cases.
Style transfer: To qualitatively evaluate TTS-GAN, we conduct style transfer. In this experiment, we want to compare TTS-GAN against GST in how well they model varied styles in EMT-4. We randomly selected 15 sentences, where 10 of the sentences are from the test set, and 5 of them are picked on web (out of the dataset). To perform style transfer, we select four different reference audios from `happy', `angry', `sad' and `neutral', all of the reference audio samples are unseen during training. Each sentence is paired with these four reference audio samples for synthesizing, which will produce 60 new audio samples in total. The results can be found in our demo page. We also compare TTS-GAN against GST on the task of unparalleled transfer at scale. Specifically, we follow the same setting in Wang et al. (2018) to ran side-by-side subjective study on 7-point ratings (-3 to 3) from "model A is the closest to the reference style" to "model B is the closest to the reference style", where model B was ours.
We recruited seven participants. Each listened to all 15 permutation of content and rated each set of audio style (emotions) comparing the result of our model versus the Tachotron2 model. They rated each pair of audio outputs on the 7-point scale. We performed a single-sample T-Test on the resulting ratings averaged across all participants. µ>0 means our model was judged as closer to the reference. Overall the emotion samples the participants rated our model as significantly closer to the reference (µ=0.872, p 0.001). For each of the styles individually our model was consistently rated as significantly closer to the reference (neutral: µ=0.295, p=0.01, happy: µ=0.905, p 0.001, sad: µ=1.646, p 0.001, angry: µ=0.641, p 0.001). These results provide further evidence that our model can synthesize speech with the correct content and distinct auditory styles that are closer to the reference than the state-of-the-art comparison.
6 CONCLUSION
We propose TTS-GAN, an end-to-end conditional generative model for TTS style modeling. TTSGAN is built upon Tacotron, with an enhanced content-style disentanglement ability and controllability. The proposed pairwise training approach that involves a adversarial game and a collaborative game together, result in a highly controllable generator with disentangled representations. Benefiting from the separate modeling of content c and style s, TTS-GAN can synthesize high fidelity speech signals with the correct content and realistic style, resulting in natural human-like speech. We demonstrated our approach on two TTS datasets with different auditory styles (emotion and speaker identity), and show that our approach establishes state-of-the-art quantitative and qualitative performance on a variety of tasks. For future research, an important direction can be training on unpaired data under an unsupervised setting. In this way, the requirements for a lot of work on aligning text and audios can be much released.
8

Under review as a conference paper at ICLR 2019

REFERENCES
Sercan O¨ mer Arik, Mike Chrzanowski, Adam Coates, Greg Diamos, Andrew Gibiansky, Yongguo Kang, Xian Li, John Miller, Jonathan Raiman, Shubho Sengupta, and Mohammad Shoeybi. Deep voice: Real-time neural text-to-speech. CoRR, abs/1702.07825, 2017.

Shaun Barry and Youngmoo Kim. style transfer for musical audio using multiple time-frequency representations, 2018. URL https://openreview.net/forum?id=BybQ7zWCb.

Vadim Lebedev Dmitry Ulyanov.

Audio texture synthesis and style

transfer.

2016.

URL https://dmitryulyanov.github.io/

audio-texture-synthesis-and-style-transfer/.

L. A. Gatys, A. S. Ecker, and M. Bethge. Image style transfer using convolutional neural networks. In 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 2414­2423, June 2016. doi: 10.1109/CVPR.2016.265.

Andrew Gibiansky, Sercan Arik, Gregory Diamos, John Miller, Kainan Peng, Wei Ping, Jonathan Raiman, and Yanqi Zhou. Deep voice 2: Multi-speaker neural text-to-speech. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Advances in Neural Information Processing Systems 30, pp. 2962­2970. Curran Associates, Inc., 2017.

Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Z. Ghahramani, M. Welling, C. Cortes, N. D. Lawrence, and K. Q. Weinberger (eds.), Advances in Neural Information Processing Systems 27, pp. 2672­2680. Curran Associates, Inc., 2014. URL http://papers. nips.cc/paper/5423-generative-adversarial-nets.pdf.

D. Griffin and Jae Lim. Signal estimation from modified short-time fourier transform. IEEE Transactions on Acoustics, Speech, and Signal Processing, 32(2):236­243, April 1984. ISSN 00963518. doi: 10.1109/TASSP.1984.1164317.

Kundan Kumar Joao Felipe Santos Kyle Kastner Aaron Courville Yoshua Bengio Jose Sotelo, Soroush Mehri. Char2wav: End-to-end speech synthesis. In International Conference on Learning Representations, workshop, 2017.

T. Kinnunen, L. Juvela, P. Alku, and J. Yamagishi. Non-parallel voice conversion using i-vector plda: towards unifying speaker verification and transformation. In 2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 5535­5539, March 2017. doi: 10. 1109/ICASSP.2017.7953215.

Ming-Yu Liu, Thomas Breuel, and Jan Kautz. Unsupervised image-to-image translation networks. CoRR, abs/1703.00848, 2017. URL http://arxiv.org/abs/1703.00848.

Shuang Ma, Jianlong Fu, Chang Wen Chen, and Tao Mei. Da-gan: Instance-level image translation by deep attention generative adversarial networks. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018.

Michael F Mathieu, Junbo Jake Zhao, Junbo Zhao, Aditya Ramesh, Pablo Sprechmann, and Yann LeCun. Disentangling factors of variation in deep representation using adversarial training. In D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett (eds.), Advances in Neural Information Processing Systems 29, pp. 5040­5048. Curran Associates, Inc., 2016.

Mehdi Mirza and Simon Osindero. Conditional generative adversarial nets. arXiv preprint arXiv:1411.1784, 2014.

Eliya Nachmani, Adam Polyak, Yaniv Taigman, and Lior Wolf. Fitting new speakers based on a short untranscribed sample. CoRR, abs/1802.06984, 2018. URL http://arxiv.org/abs/ 1802.06984.

Wei Ping, Kainan Peng, Andrew Gibiansky, Sercan O. Arik, Ajay Kannan, Sharan Narang, Jonathan Raiman, and John Miller. Deep voice 3: 2000-speaker neural text-to-speech. In International Conference on Learning Representations, 2018.

9

Under review as a conference paper at ICLR 2019
Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei. ImageNet Large Scale Visual Recognition Challenge. International Journal of Computer Vision (IJCV), 115(3):211­252, 2015. doi: 10.1007/s11263-015-0816-y.
Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. CoRR, abs/1409.1556, 2014. URL http://arxiv.org/abs/1409.1556.
R. J. Skerry-Ryan, Eric Battenberg, Ying Xiao, Yuxuan Wang, Daisy Stanton, Joel Shor, Ron J. Weiss, Rob Clark, and Rif A. Saurous. Towards end-to-end prosody transfer for expressive speech synthesis with tacotron. In Proceedings of the 35th International Conference on Machine Learning, ICML 2018, Stockholmsma¨ssan, Stockholm, Sweden, July 10-15, 2018, pp. 4700­4709, 2018a.
R. J. Skerry-Ryan, Eric Battenberg, Ying Xiao, Yuxuan Wang, Daisy Stanton, Joel Shor, Ron J. Weiss, Rob Clark, and Rif A. Saurous. Towards end-to-end prosody transfer for expressive speech synthesis with tacotron. CoRR, abs/1803.09047, 2018b.
Yaniv Taigman, Adam Polyak, and Lior Wolf. Unsupervised cross-domain image generation. CoRR, abs/1611.02200, 2016. URL http://arxiv.org/abs/1611.02200.
Yaniv Taigman, Lior Wolf, Adam Polyak, and Eliya Nachmani. Voiceloop: Voice fitting and synthesis via a phonological loop. In International Conference on Learning Representations, 2018.
Dmitry Ulyanov, Andrea Vedaldi, and Victor S. Lempitsky. Deep image prior. CoRR, abs/1711.10925, 2017.
Aron van den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alexander Graves, Nal Kalchbrenner, Andrew Senior, and Koray Kavukcuoglu. Wavenet: A generative model for raw audio. In Arxiv, 2016.
L.J.P. van der Maaten and G.E. Hinton. Visualizing high-dimensional data using t-sne. 2008.
Yuxuan Wang, R. J. Skerry-Ryan, Ying Xiao, Daisy Stanton, Joel Shor, Eric Battenberg, Rob Clark, and Rif A. Saurous. Uncovering latent style factors for expressive speech synthesis. CoRR, abs/1711.00520, 2017a. URL http://arxiv.org/abs/1711.00520.
Yuxuan Wang, RJ Skerry-Ryan, Daisy Stanton, Yonghui Wu, Ron J Weiss, Navdeep Jaitly, Zongheng Yang, Ying Xiao, Zhifeng Chen, Samy Bengio, et al. Tacotron: Towards end-to-end speech synthesis. arXiv preprint arXiv:1703.10135, 2017b.
Yuxuan Wang, Daisy Stanton, Yu Zhang, RJ-Skerry Ryan, Eric Battenberg, Joel Shor, Ying Xiao, Ye Jia, Fei Ren, and Rif A. Saurous. Style tokens: Unsupervised style modeling, control and transfer in end-to-end speech synthesis. In Proceedings of the 35th International Conference on Machine Learning, ICML 2018, Stockholmsma¨ssan, Stockholm, Sweden, July 10-15, 2018, pp. 5167­5176, 2018.
Han Zhang, Tao Xu, Hongsheng Li, Shaoting Zhang, Xiaolei Huang, Xiaogang Wang, and Dimitris N. Metaxas. Stackgan: Text to photo-realistic image synthesis with stacked generative adversarial networks. CoRR, abs/1612.03242, 2016. URL http://arxiv.org/abs/1612. 03242.
Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A. Efros. Unpaired image-to-image translation using cycle-consistent adversarial networks. CoRR, abs/1703.10593, 2017a. URL http:// arxiv.org/abs/1703.10593.
Jun-Yan Zhu, Richard Zhang, Deepak Pathak, Trevor Darrell, Alexei A. Efros, Oliver Wang, and Eli Shechtman. Toward multimodal image-to-image translation. CoRR, abs/1711.11586, 2017b. URL http://arxiv.org/abs/1711.11586.
10


Under review as a conference paper at ICLR 2019
PROBABILISTIC FEDERATED NEURAL MATCHING
Anonymous authors Paper under double-blind review
ABSTRACT
In federated learning problems, data is scattered across different servers and exchanging or pooling it is often impractical or prohibited. We develop a Bayesian nonparametric framework for federated learning with neural networks. Each data server is assumed to train local neural network weights, which are modeled through our framework. We then develop an inference approach that allows us to synthesize a more expressive global network without additional supervision or data pooling. We then demonstrate the efficacy of our approach on federated learning problems simulated from two popular image classification datasets.
1 INTRODUCTION
The standard machine learning paradigm involves algorithms that learn from centralized data, possibly pooled together from multiple data sources. The computations involved may be done on a single machine or farmed out to a cluster of machines. However, in the real world, data often lives in silos and amalgamating them may be rendered prohibitively expensive by communication costs, time sensitivity, or privacy concerns. Consider, for instance, data recorded from sensors embedded in wearable devices. Such data is inherently private, can be voluminous depending on the sampling rate of the sensing modality, and may be time sensitive depending on the analysis of interest. Pooling data from many users is technically challenging owing to the severe computational burden of moving large amounts of data, and fraught with privacy concerns stemming from potential data breaches that may expose the user's protected health information (PHI). Federated learning avoids these pitfalls by obviating the need for centralized data and instead designs algorithms that learn from sequestered data sources with different data distributions. To be effective, such algorithms must be able to extract and distill important statistical patterns from various independent local learners coherently into an effective global model without centralizing data. This will allow us to avoid the prohibitively expensive cost of data communication. To achieve this, we develop and investigate a probabilistic federated learning framework with a particular emphasis on training and aggregating neural network models on siloed data. We proceed by training local models for each data source, in parallel. We then match the estimated local model parameters (groups of weight vectors in the case of neural networks) across data sources to construct a global network. The matching, to be formally defined later, is governed by the posterior of a Beta-Bernoulli process (BBP) (Thibaux & Jordan, 2007; Yurochkin et al., 2018), a Bayesian nonparametric model that allows the local parameters to either match existing global ones or create a new global parameter if existing ones are poor matches. Our construction allows the size of the global network to flexibly grow or shrink as needed to best explain the observed data. Crucially, we make no assumptions about how the data is distributed between the different sources or even about the local learning algorithms. These may be adapted as necessary, for instance to account for non-identically distributed data. Further, we only require communication after the local algorithms have converged. This is in contrast with popular distributed training algorithms (Dean et al., 2012) that rely on frequent communication between the local machines. Our construction also leads to compressed global models with fewer parameters than the set of all local parameters. Unlike naive ensembles of local models, this allows us to store fewer parameters and leads to more efficient inference at test time, requiring only a single forward pass through the compressed model as opposed to J forward passes, once for each local model. While techniques such as distillation (Hinton et al., 2015) allow for the cost of multiple forward passes to be amortized, training the distilled model itself requires access to data pooled across all sources, a luxury unavailable in our federated learning scenario. In summary, the key question we seek to answer in this paper is the following: given
1

Under review as a conference paper at ICLR 2019

pre-trained neural networks trained locally on non-centralized data, can we learn a compressed federated model without accessing the original data, while improving on the performance of the local networks? The remainder of the paper is organized as follows. We briefly introduce the Beta-Bernoulli process in Section 2 before describing our model for federated learning in Section 3. We thoroughly vet the proposed models and demonstrate the utility of the proposed approach in Section 4. Finally, Section 5 discusses limitations and open questions.

2 BACKGROUND AND RELATED WORK

Our approach builds on tools from Bayesian nonparametrics, in particular the Beta-Bernoulli Pro-

cess (BBP) (Thibaux & Jordan, 2007) and the closely related Indian Buffet Process (IBP) (Griffiths

& Ghahramani, 2011). We briefly review these ideas before describing our approach. Consider

a random measure Q drawn from a Beta Process with mass Q| 0, H  BP(1, 0H). It follows that Q is a discrete (not formed by pairs (qi, i) 2 [0, 1]   of weights and atoms. breaking construction (Teh et al., 2007): ci  Beta( 0, 1), qi

pT=prhaoerQbaawmjib=eeili1tigetchryjt)sam0n{dqeaaint}sdhui1e=rbe1aatsQofeomlm=lsoewaaPresauidrsqretaiicwHkni-,

i.i.d from the (scaled) base measure i  H/H() with domain . In this paper,  is simply RD for some D. Subsets of atoms in the random measure Q are then selected using a Bernoulli process with

aaaopmranbciahroJssincei(gsabmlTsjuciej,opa,nspiiu.sr)eetr.res2usTQce{Jtdi,0|oTTi,nn11j,|}dtQh.e.es.cn,roiTBb,tJaeTetsPijo1t(:nhQ=ef)oBPBfroeestiriPambj-jBp=iHelirc1niJi0,,oty.wu+).lhl.aiPe,nrpJderi.oibmsEcJjesiias|ocqsmh.ii eTM,tjiwamBirshgeeaesirnlnrcseaooalumlilazleliiiddn(ig=qtshicQ)erPe8IitnineJj.d=dmTiu1a1oecnbgaejBsseiutudhr(feeedfpreef,etpotnerPhdmnirseodencehdceniiesbcersyys-.

The IBP can be equivalently described by the following culinary metaphor. J customers arrive se-

quentially at a buffet and choose dishes to sample as follows, the first customer tries Poisson( 0) dishes. Every subsequent j-th customer tries each of the previously selected dishes according to

their popularity, i.e. dish i with probability mi/j, and then tries Poisson( 0/j) new dishes.

The IBP, which specifies a distribution over sparse binary matrices with infinitely many columns, was originally demonstrated for latent factor analysis (Ghahramani & Griffiths, 2005). Several extensions to the IBP (and the equivalent BBP) have been developed, see Griffiths & Ghahramani (2011) for a review. Our work is related to a recent application of these ideas to distributed topic modeling (Yurochkin et al., 2018), where the authors use the BBP for modeling topics learned from multiple collections of document, and provide an inference scheme based on the Hungarian algorithm (Kuhn, 1955). Extending these ideas to federated learning of neural networks requires significant innovations and is the primary focus of our paper.

Federated learning has recently garnered attention from the machine learning community. Smith et al. (2017) pose federated learning as a multi-task learning problem, which exploits the convexity and decomposability of the cost function of the underlying support vector machine (SVM) model for distributed learning. This approach however does not extend to the neural network structure considered in our work. Others (McMahan et al., 2017) use strategies based on simple averaging of the local learner weights to learn the federated model. However, as pointed out by the authors, such naive averaging of model parameters can be disastrous for non-convex cost functions. To cope, they have to use a heuristic scheme where the local learners are forced to share the same random initialization. In contrast, our proposed framework is naturally immune to such issues since its development assumes nothing specific about how the local models were trained. Moreover, unlike the previous work of McMahan et al. (2017), our framework is non-parametric in nature and it therefore allows the federated model to flexibly grow or shrink its complexity (i.e., its sizes) to account for the varying data complexity.

There is also significant work on distributed deep learning Lian et al. (2015; 2017); Moritz et al. (2015); Li et al. (2014); Dean et al. (2012). However, the emphasis of these works is on scalable training from large data and they typically require frequent communication between the distributed nodes to be effective. Yet others explore distributed optimization with a specific emphasis on communication efficiency (Zhang et al., 2013; Shamir et al., 2014; Yang, 2013; Ma et al., 2015; Zhang & Lin, 2015). However, as pointed out by McMahan et al. (2017), these works primarily focus on

2

Under review as a conference paper at ICLR 2019

settings with convex cost functions and often assume that each distributed data source contains an equal number of data instances. These assumptions, in general, do not hold in our scenario.

3 PROBABILISTIC FEDERATED NEURAL MATCHING

We now apply this Bayesian nonparametric machinery to the problem of federated learning with neural networks. Our goal will be to identify subsets of neurons in each of the J local models that match to neurons in other local models, and then use these to form an aggregate model where the matched parts of each of the local models are fused together.

Our approach to federated learning builds upon the following basic problem. Suppose we have

trained J Multilayer Perceptrons (MLPs) with one hidden layer each. For the jth MLP j = 1, . . . , J,

let Vj(0) 2 RDLj and v~j(0) 2 RLj be weights and biases of the hidden layer; Vj(1) 2 RLjK and

vo~jf(1n) e2uroRnKs

be on

weights and biases of the softmax layer; D be the hidden layer; and K the number of classes.

the data dimension, Lj the number We consider a simple architecture:

fj(x) = softmax( (xVj(0) + v~j(0))Vj(1) + v~j(1)) where (·) is some nonlinearity (sigmoid, ReLU,

etc.). Given the collection of weights and biases {Vj(0), v~j(0), Vj(1), v~j(1)}jJ=1 we want to learn a global

LneuralPneJjt=w1oLrkj

with weights and biases is an unknown number

(0) 2 RDL, of hidden units

~(0) 2 of the

RL, (1) 2 RLK , ~(1) 2 RK global network to be inferred.

,

where

Our first observation is that ordering of neurons of the hidden layer of an MLP is permutation

invariant. Consider any permutation  (1, . . . , Lj) of the j-th MLP ­ reordering columns of Vj(0),

bvaialuseesov~fj(x0).

aTnhderreofworse,ofinVstj(e1a)daocfcotrredaitnigngtowei(g1h, t.s.

. , Lj) will not affect the as matrices and biases as

outputs vectors

fj(x) for any we view them

as unordered collections of vectors Vj(0) = {vj(0l ) 2 RD}lL=j1, Vj(1) = {vj(1l ) 2 RLj }lK=1 and scalars v~j(0) = {v~j(0l ) 2 R}lL=j1 correspondingly.

Hidden layers in neural networks are commonly viewed as feature extractors. This perspective can

be justified by the fact that last layer of a neural networks is simply a softmax regression. Since

neural networks greatly outperform basic softmax regression in a majority of applications, neu-

ral networks must be supplying high quality features constructed from the input features. Math-

ematically, in our problem setup, every hidden neuron of j-th MLP represents a new feature

x~l(vj(0l ), v~j(0l )) = (hx, vj(0l )i + v~j(0l )). Our second observation is that each of the (vj(0l ), v~j(0l )) acts as a parameterization of the corresponding neuron's feature extractor. Since each of the given MLPs

was trained on the same general type of data (not necessarily homogeneous), we assume that they

should share at least some feature extractors that serve the same purpose. However, due to the

permutation invariance described previously, a feature extractor indexed by l from the j-th MLP is unlikely to correspond to a feature extractor with the same index from a different MLP. In order to

construct a set of global process of grouping and

fceoamtubrienienxgtrfaecatoturrse(enxeturraocntos)rs{ofi(0c)o2lleRctDio,n~oi(0f)M2LRP}s.iL=1

we

must

model

the

3.1 SINGLE LAYER NEURAL MATCHING

We now present the key building block of our modeling framework, our Hierarchical BBP (Thibaux

& Jordan, 2007) based model of the neurons and weights of multiple MLPs. Our generative model

is as prior

follows. First, draw with a base measure

a collection H and mass

opfargalmobeatlerato0m, Qs (h=idPdeinqliayei .r

neurons) from a Beta process In our experiments we choose

H = N (µ0, 0) as the base measure with µ0 2 RD+1+K and diagonal 0. Each i 2 RD+1+K

is a concatenated weight-bias pairs

wveitchtothr eocfo[rri(e0s)po2ndRinDg,

w~i(e0i)gh2tsRo,f

thi(1e)

2 RK ] softmax

formed from regression.

the

feature

extractor

Next, for each batch (server) j = 1, . . . , J, generate a batch specific distribution over global atoms

(neurons):

X Qj|Q  BP(1, jQ), then Qj := pji i ,

(1)

i

3

Under review as a conference paper at ICLR 2019

Twhhiebraeutxhe&pJjoisrdvaanry(2a0r0o7u)n.dNcoowrr,efsoproenadcihngj

qi. =

The 1, . . .

distributional properties , J select a subset of the

ogflopbjail

are described in atoms for batch

j via the Bernoulli process:

X Tj := bji i , where bji|pji  Bern(pji) 8i.

(2)

i

Tj is supported by atoms {i : bji = 1, i = 1, 2, . . .}, which represent the identities of the atoms (neurons) used by batch (server) j. Finally, assume that observed local atoms are noisy measurements of the corresponding global atoms:

vjl|Tj  N (Tjl, j) for l = 1, . . . , Lj, where Lj := card(Tj),

(3)

where vjl = [vj(0l ), v~j(0l ), vj(1l )] are the weights, biases, and softmax regression weights corresponding to the l-th neuron of the j-th MLP trained with Lj neurons on the data of batch j. Under this model, the key quantity to be inferred is the collection of random variables that match observed atoms (neurons) at any batch to the global atoms. We denote the collection of these random variables as {Bj}Jj=1, where Bij,l = 1 implies that Tjl = i (there is a one-to-one correspondence between {bji}i1=1 and Bj ).

Maximum a posteriori estimation. We now derive an algorithm for MAP estimation of global atoms for the model presented above. The objective function to be maximized is the posterior of {i}1i=1 and {Bj }jJ=1:

arg max P ({i}, {Bj}|{vjl}) / P ({vjl}|{i}, {Bj})P ({Bj})P ({i}).
{i},{Bj }

(4)

Note that the next proposition easily follows from Gaussian-Gaussian conjugacy (Supplement 1):

Proposition 1. Given {Bj}, the MAP estimate of {i} is given by

^i =

µ0/ 1/

0202++PPjj,l,lBBij,ijl,vl/jl/j2

2 j

for i = 1, . . . , L,

(5)

where for simplicity we assume 0 = I

2 0

and

j

=

I

2 j

.

Using this fact we can cast optimization natural logarithm we obtain:

corresponding

to

(4)

with

respect

to

only

{Bj }jJ=1.

Taking

arg max
{Bj }

1 2

X
i

kµ0

/

2 0

1/

02++PPj,jl,Bl Bij,lij,vl/jl

/
2 j

j2k2

+ log(P ({Bj}).

(6)

Detailed derivation of this and subsequent results are given in Supplement 1. We consider an iterative

optimization approach: fixing all but one Bj we find corresponding optimal assignment, then pick

a new j at random and proceed until convergence. In the following we will use notation j to

say "all but j". Let L j = max{i : Bi,lj = 1} denote number of active global weights outside of group j. We now rearrange the first term of (6) by partitioning it into i = 1, . . . , L j and

sfiuo=nmcetLionnejub+royn1s,fur.bo.tm.ra,cLbtaintcjgh+tjerLimsjsm. WiantdceheaperdeentidonetgnelrtoeobsftaelBdneijnuarsonondlvininaogntidnfog0rtoBhtahjte,rPwheilnsBec:eij,lw2e

can {0,

modify 1}, i.e.

objective it is 1 if

L Xj +Lj XLj Bij,l
i=1 l=1

kµ0/

2 0

+

vjl/

1/

2 0

+

1/

2 j 2 j

+ +

P P

j,l Bij,lvjl/

j,l Bij,l/

2 j

j2k2

Now we consider the second term of (6):

kµ0/

2 0

1/

P

+

2 0

+

P

j,l Bij,lvjl/

j,l Bij,l/

2 j

!

2 j

k2

.

(7)

log P ({Bj}) = log P (Bj|B j) + log P (B j).

First, because we are optimizing for Bj, we can ignore log P (B j). Second, due to exchangeability of batches (i.e. customers of the IBP), we can always consider Bj to be the last batch (i.e. last

4

Under review as a conference paper at ICLR 2019

customer of the IBP). Let mi j = P j,l Bij,l denote number of times batch weights were assigned to global weight i outside of group j. We now obtain the following:

LXj XLj Bij,l log
i=1 l=1

J

mi j mi

j

L Xj +Lj XLj



+ Bij,l log

i=L j +1 l=1

0
J

 log(i L j) .

(8)

Combining (7) and (8) we obtain the assignment cost objective, which we solve with the Hungarian algorithm.

CPrij,olp=ositi:8>><onkk2µµ.001//T/1h/002202++e+02vva+1jjs/ll1s///ij2gjj22j2n+k+m2PPentjjk,,cllµoBB10s/ij/ijt,,llsv02/02pjkle2j2/cij2fikc22altoiognk(µifo0/1r/Lfi02n+02d+Pji)Pn+gj,Bjl2,Bl ljBijo,iiljg,svl:/jJl0/j2,

2 j

k2

+

log

mi j J mi

j

,

iL j

L j < i  L j + Lj.

(9)

PWe
i

PthelnBiaj,lpCpiljy,l

the Hungarian algorithm described in Supplement and obtain the neuron matching assignments.

1

to

find

the

minimizer

of

We summarize the overall single layer inference procedure in Figure 1 below.

Server 1

Server 2

Server 3

Outputs

Hidden layers
Input Match and merge neurons to form aggregate layer
Outputs
Global hidden layer
Input

Algorithm 1 Single Layer Neural Matching 1: Collect hidden layers from the J servers and
form vjl. 2: Form assignment cost matrix per (9). 3: Compute matching assignments Bj using the
Hungarian algorithm (Supplement 1). 4: Enumerate all resulting unique global neu-
rons and use (5) to infer the associated global weight vectors from all instances of the global neurons across the J servers. 5: Concatenate the global neurons and the inferred weights and biases to form the new global hidden layer.

Figure 1: Single layer Probabilistic Neural Matching algorithm showing matching of three MLPs. Nodes in the graphs indicate neurons, neurons of the same color have been matched. Our approach consists of using the corresponding neurons in the output layer to convert the neurons in each of the J servers to weight vectors referencing the output layer. These weight vectors are then used to form a cost matrix, which the Hungarian algorithm then uses to do the matching. Finally, the matched neurons are then aggregated and averaged to form the new layer of the global model.

3.2 MULTILAYER NEURAL MATCHING
The model we have presented thus far can handle any arbitrary width single layer neural network, which is known to be theoretically sufficient for approximating any function of interest (Hornik et al., 1989). However, deep neural networks with moderate layer widths are known to be beneficial both practically (LeCun et al., 2015) and theoretically (Poggio et al., 2017). We extend our neural matching approach to these deep architectures by defining a generative model of deep neural network weights from outputs back to inputs (top-down). Let C denote the number of hidden layers and Lc the number of neurons on the cth layer. Then LC+1 = K is the number of labels and L0 = D is the input dimension. In the top down approach, we consider the global atoms to be vectors of outgoing weights from a neuron instead of weights forming a neuron as it was in the single hidden layer model. This change is needed to avoid base measures with unbounded dimensions. Starting with the top hidden layer c = C, we generate each layer following a model similar to that used in the single layer case. For each layer we generate a collection of global atoms and select a subset of them for each batch using Hierarchical Beta-Bernoulli process construction. Lc+1 is the number of neurons on the layer c + 1, which controls the dimension of the atoms in layer c.

5

Under review as a conference paper at ICLR 2019

Definition 1 (Multilayer generative process). Starting with layer c = C, generate (as in the single layer process)

Qc|

c 0

,

H

c,

Lc+1

 BP(1,

c 0

H

c),

X then Qc = qic

ic ,

ic  N (µc0, c0),

µc0

2 RLc+1

Xi

Qcj |

jc, Qc  BP(1,

c j

Qc),

Tjc

:=

bjci ic , where bcji|pjci  Bern(pcji).

(10)

i

This Tjc is the set of global atoms (neurons) used by batch j in layer c, it is contains atoms {ic : bjci = 1, i = 1, 2, . . .}. Finally, generate the observed local atoms:

vjcl|Tjc,  N (Tjcl, cj) for l = 1, . . . , Lcj, where Ljc := card(Tjc).

(11)

Next, ative

compute the generated number of global neurons Lc = process for the next layer c 1. Repeat until all layers

acraerdge{n[eJjr=a1teTdjc(}c

and repeat this = C, . . . , 1).

gener-

An important difference from the single layer model is that we should now set to 0 some of the

dimensions of vjcl 2 RLc+1 since they correspond to weights outgoing to neurons of the layer c + 1 not present on the batch j, i.e. vjcli := 0 if bjc+i 1 = 0 for i = 1, . . . , Lc+1. The resulting model can be understood as follows. There is a global fully connected neural network with Lc neurons on

layer c and there are J weights corresponding

partially connected neural to the remaining Lc Ljc

nneeutwroonrsksarweiztherLoecjsaacntidvehanveeurnoonsefofnecltalyoecracll,yw. hile

Remark 1. Our model can conceptually handle permuted ordering of the input dimensions across

batches, however in most practical cases the ordering of input dimensions is consistent across

batches, making the weights connecting the first hidden layer to the input only permutation invariant

on the side of the first hidden layer. Similarly to how all weights were concatenated in the single

hidden layer added to the

model, model,

we we

consider omitted it

µto0c

2 RD+Lc+1 for c simplify notation.

=

1.

We

also

note

that

the

bias

term

can

be

Inference Following the top-down generative model, we adopt a greedy inference procedure that first infers the matching of the top layer and then proceeds down the layers of the network. This is possible because the generative process for each layer depends only on the identity and number of the global neurons in the layer above it, hence once we infer the c + 1th layer of the global model we can apply the single layer inference algorithm (Algorithm 1) to the cth layer. This greedy setup is illustrated in Figure 1 in Supplement 2.

The per-layer inference derivation is a straightforward copy of the single layer case, yielding the following propositions.

Proposition 3. The assignment cost specification for finding Bj,c is:

Cij,,lc =

8> k kµ0c/(

c 0

)2

+vjcl

/(

< 1/(

c 0

)2

+1/(

jcjc))22++PP

j,l Bij,,lcvjcl/(

c j

)2

j,l Bij,,lc/(

c j

)2

2

k k + log ,µ0c

/(

c 0

1/(

)2

P +

c 0

)2

P +

j,l Bij,,lcvjcl/(

j,l Bij,,lc/(

c j

c j

)2

)2

2

mi j,c J mi j,c

i  Lc j

:> kµc0/(

c 0

)2

+vjcl

/(

c j

)2

k2

1/(

c 0

)2

+1/(

c j

)2

kµc0 /(

c 0

)2

k2

1/(

c 0

)2

2 log(i

Lc j) + 2 log

0
J

,

Lc j < i  Lc j + Lcj ,

where for algorithm

simplicity to find the

mweinaimssiuzmereof Pc0 i=PIl(B0icj,,)lc2Caij,n,lcdandjc

= I( obtain

c j

)2.

We then apply

the neuron matching

the Hungarian assignments.

Proposition 4. Given the assignment {Bj,c}, the MAP estimate of {ic} is given by

^ic

=

µc0/( 1/(

0c0c))22++PPjj,l,lBBij,ij,l,c,lcv/jc(l/j(c)2jc)2

for i

=

1, . . . , L.

(12)

We combine these propositions and summarize the overall multilayer inference procedure in Algorithm 1 in Supplement 2.

3.3 STREAMING NEURAL MATCHING In this section we propose an extension of our modeling framework to handle streaming data. Such data naturally arises in many federated learning settings. Consider, again the example of wearable

6

Under review as a conference paper at ICLR 2019

devices. Data recorded by sensors on these devices is naturally temporal and memory constraints typically require streaming processing of the data.

Bayesian paradigm naturally fits into the streaming scenario - posterior of step s becomes prior for step s + 1. We generalize our single hidden layer model to streaming setting (our approach naturally extends to multilayer scenario).

The differences in the generative model effect (2) and (3), which become:

Tjs

:=

P
i

bsji

i , where bsji|pji

 Bern(pji).

vjsl|Tjs  N (Tjsl, j) for l = 1, . . . , Ljs, s = 1, . . . , S.

We derive cost expression for the streaming extension in the Supplementary.

(13) (14)

4 EXPERIMENTS

To verify our methodology we simulate federated learning scenario using two standard datasets: MNIST and CIFAR-10. We randomly partition each of these datasets into J batches. Two partition strategies are of interest: (a) homogeneous partition when each batch has approximately equal proportion of each of the K classes; and (b) heterogeneous when batch sizes and class proportions are unbalanced. We achieve the latter by simulating pk  DirJ (0.2) and allocating pk,j proportion of instances of class k to batch j. Note that due to the small concentration parameter (0.2) of the Dirichlet distribution, some sampled batches may not have any examples of certain classes of data. For each pair of partition strategy and dataset we run 10 trials to obtain mean accuracies and standard deviations. In our empirical studies below, we will show that our framework can aggregate multiple local neural networks (NNs) trained independently on different batches of data into an efficient, modest-size global neural network that performs competitively against ensemble methods and outperforms distributed optimization.

Baselines. We consider four baselines, however each of them violates at least one of the three

constraints of our federated learning problem, i.e. no data pooling, infrequent communication,

modest-size global model. Uniform ensemble (U-Ens) (Dietterich, 2000) is a classic technique for

aggregating multiple learners. For a test case, each batch neural network outputs class probabilities

which are averaged across batches to produce the prediction of class probabilities. The disadvantage

of this approach is networks resulting

hinigahmcoasmtepructalatisosnifiael rcwositthatPthje,cteLscjtihnigdtdiemneuansitist.eWsseeingthiatleldy

stacks all ensemble

batch neural (W-Ens) is a

heuristic extension for heterogeneous partitioning - class k probability of batch j is weighted by the

proportion of instances of class k on batch j when taking the average across batch network outputs.

Knowledge distillation (KD) of Hinton et al. (2015) is an extension of ensemble, where new, modest

size, neural network is trained to mimic the behavior of an ensemble, however this requires pooling

training examples on the master node. Our last baseline is one of the distributed optimization ap-

proaches -- downpour SGD (D-SGD) by Dean et al. (2012). The limitation of this method is that

it requires frequent communication between batch servers and master node to exchange gradient

information and update local copies of weights. In our experiments downpour SGD was allowed to

communicate once every training epoch (total of 10 rounds of communications), while our method

and other baselines only communicated once, i.e. after batch neural networks have been trained.

Results. In the first set of experiments we compare Probabilistic Federated Neural Matching (PFNM) against four baselines for varying number of batches J (Fig. 2). When number of batches grows, average size of a single batch decreases and corresponding neural networks do not converge to a good solution (or result in a bad gradient after an epoch). This significantly degrades performance of the downpour SGD and also affects PFNM in the case of heterogeneous CIFAR-10 (Fig. 2d). We observe that D-SGD at first improves with increasing number of batches and then drops down in performance abruptly -- at first increasing number of batches essentially increases number of communications, since each batch sends gradients to the server, without hurting the quality of gradients, however when size of batches decreases gradients become worse and D-SGD behaves poorly. On the other hand, ensemble approaches only require a collection of weak classifiers to perform well, hence their performance does not noticeably degrade as the quality of batch neural networks deteriorates. This advantage comes at a price of high computational burden when making a prediction, since we need to do a forward pass for an input observation through each of the

7

Under review as a conference paper at ICLR 2019
(a) MNIST homogeneous (b) MNIST heterogeneous (c) CIFAR homogeneous (d) CIFAR heterogeneous Figure 2: Test accuracy comparison for varying number of batches J
(a) MNIST homogeneous (b) MNIST heterogeneous (c) CIFAR homogeneous (d) CIFAR heterogeneous Figure 3: Test accuracy comparison for varying number of layers C with J = 10
batch networks. Interestingly, weighted ensemble performs worse than uniform ensemble on heterogeneous CIFAR-10 case - this again could be due to the low quality of batch networks which hurts our method and makes uniform ensemble more robust than weighted. In the second experiment we fix J = 10 and consider multilayer batch neural networks with number of layers C from 1 to 6. We see (Fig. 3 that our multilayer PFNM can handle deep networks as it continues to be comparable to ensemble techniques and outperform D-SGD. In the Supplementary we analyze sizes of the master neural network learned by PFNM, parameter sensitivity, streaming extension and explore performance of downpour SGD with more frequent communications. We conclude that for federated learning applications when prediction time is limited (hence ensemble approaches are not suitable) and communication is expensive, PFNM is a strong solution candidate.
5 DISCUSSION
In this work we have developed models for matching fully connected networks, and experimentally demonstrated the capabilities of our methodology, particularly when prediction time is limited and communication is expensive. We also observed the importance of convergent local neural networks that serve as inputs to our matching algorithms. Poor quality local neural network weights will affect the quality of the master network. In future work we plan to explore more sophisticated ways to account for uncertainty in the weights of small batches. Additionally, our matching approach is completely unsupervised ­ incorporating some form of supervised signal may help to improve the performance of the global network when local networks are low quality. Finally, it is of interest to extend our modeling framework to other architectures such as Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs). The permutation invariance necessitating matching inference arises in CNNs too -- any permutation of the filters results in same output, however additional bookkeeping is needed due to pooling operations.
8

Under review as a conference paper at ICLR 2019
REFERENCES
Jeffrey Dean, Greg Corrado, Rajat Monga, Kai Chen, Matthieu Devin, Mark Mao, Andrew Senior, Paul Tucker, Ke Yang, Quoc V Le, et al. Large scale distributed deep networks. In Advances in neural information processing systems, pp. 1223­1231, 2012.
Thomas G Dietterich. Ensemble methods in machine learning. In International workshop on multiple classifier systems, pp. 1­15. Springer, 2000.
Zoubin Ghahramani and Thomas L Griffiths. Infinite latent feature models and the Indian buffet process. In Advances in Neural Information Processing Systems, pp. 475­482, 2005.
Thomas L Griffiths and Zoubin Ghahramani. The Indian buffet process: An introduction and review. Journal of Machine Learning Research, 12:1185­1224, 2011.
Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531, 2015.
Kurt Hornik, Maxwell Stinchcombe, and Halbert White. Multilayer feedforward networks are universal approximators. Neural networks, 2(5):359­366, 1989.
Harold W Kuhn. The Hungarian method for the assignment problem. Naval Research Logistics (NRL), 2(1-2):83­97, 1955.
Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. nature, 521(7553):436, 2015. Mu Li, David G Andersen, Jun Woo Park, Alexander J Smola, Amr Ahmed, Vanja Josifovski,
James Long, Eugene J Shekita, and Bor-Yiing Su. Scaling distributed machine learning with the parameter server. In OSDI, volume 14, pp. 583­598, 2014. Xiangru Lian, Yijun Huang, Yuncheng Li, and Ji Liu. Asynchronous parallel stochastic gradient for nonconvex optimization. In Advances in Neural Information Processing Systems, pp. 2737­2745, 2015. Xiangru Lian, Ce Zhang, Huan Zhang, Cho-Jui Hsieh, Wei Zhang, and Ji Liu. Can decentralized algorithms outperform centralized algorithms? a case study for decentralized parallel stochastic gradient descent. In Advances in Neural Information Processing Systems 30, pp. 5330­5340. 2017. Chenxin Ma, Virginia Smith, Martin Jaggi, Michael Jordan, Peter Richtarik, and Martin Takac. Adding vs. averaging in distributed primal-dual optimization. In Proceedings of the 32nd International Conference on Machine Learning, pp. 1973­1982, 2015. Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas. Communication-efficient learning of deep networks from decentralized data. In Artificial Intelligence and Statistics, pp. 1273­1282, 2017. Philipp Moritz, Robert Nishihara, Ion Stoica, and Michael I Jordan. Sparknet: Training deep networks in spark. arXiv preprint arXiv:1511.06051, 2015. Tomaso Poggio, Hrushikesh Mhaskar, Lorenzo Rosasco, Brando Miranda, and Qianli Liao. Why and when can deep-but not shallow-networks avoid the curse of dimensionality: a review. International Journal of Automation and Computing, 14(5):503­519, 2017. Ohad Shamir, Nati Srebro, and Tong Zhang. Communication-efficient distributed optimization using an approximate newton-type method. In International conference on machine learning, pp. 1000­ 1008, 2014. Virginia Smith, Chao-Kai Chiang, Maziar Sanjabi, and Ameet S Talwalkar. Federated multi-task learning. In Advances in Neural Information Processing Systems, pp. 4424­4434, 2017. Yee Whye Teh, Dilan Gru¨r, and Zoubin Ghahramani. Stick-breaking construction for the Indian buffet process. In Artificial Intelligence and Statistics, pp. 556­563, 2007.
9

Under review as a conference paper at ICLR 2019 Romain Thibaux and Michael I Jordan. Hierarchical Beta processes and the Indian buffet process.
In Artificial Intelligence and Statistics, pp. 564­571, 2007. Tianbao Yang. Trading computation for communication: Distributed stochastic dual coordinate
ascent. In Advances in Neural Information Processing Systems, pp. 629­637, 2013. M. Yurochkin, Z. Fan, A. Guha, P. Koutris, and X. Nguyen. Streaming dynamic and distributed
inference of latent geometric structures. arXiv preprint arXiv:1809.08738, 2018. Yuchen Zhang and Xiao Lin. Disco: Distributed optimization for self-concordant empirical loss. In
International conference on machine learning, pp. 362­370, 2015. Yuchen Zhang, John Duchi, Michael I Jordan, and Martin J Wainwright. Information-theoretic
lower bounds for distributed statistical estimation with communication constraints. In Advances in Neural Information Processing Systems, pp. 2328­2336, 2013.
10

Under review as a conference paper at ICLR 2019

SUPPLEMENTARY MATERIAL FOR PROBABILISTIC FEDERATED NEURAL MATCHING
Anonymous authors Paper under double-blind review

1 SINGLE HIDDEN LAYER INFERENCE

The goal of maximum a posteriori (MAP) estimation is to maximize posterior probability of the latent variables: global atoms {i}i1=1 and assignments of observed neural network weight estimates to global atoms {Bj}Jj=1, given estimates of the batch weights {vjl for l = 1, . . . , Lj}Jj=1:

arg max P ({i}, {Bj}|{vjl}) / P ({vjl}|{i}, {Bj})P ({Bj})P ({i}).
{i},{Bj }

(1)

MAP estimates given matching (Proposition 1 in the main text) First we note that given {Bj}

it is straightforward to find MAP estimates of {i} based on Gaussian-Gaussian conjugacy:

^i =

1/P02j+,l BPij,jl,vl jBl/ij,l/j2

2 j

for i = 1, . . . , L,

(2)

where L = max{i : Bij,l = 1 for l = 1, . . . , Lj, j = 1, . . . , J} is the number of active global

atoms, which is an (unknown) latent random variable identified by {Bj}. For simplicity we assume

0 = I

2 0

,

j

=

I

2 j

and

µ0

=

0.

Inference of atom assignment. We can now cast optimization corresponding to (1) with respect

to only {Bj}Jj=1. Taking natural logarithm we obtain:

01

1 2

X @ k^ik2
2 i0

+

D

log(2

2 0

)

+

X

Bij,l

kvjl

j,l

2 ^ik2 A + log(P ({Bj}).
j

(3)

Let us first simplify the first term of (3):

01

1 2

X @ k^ik2
2 i0

+

D log(2

2 0

)

+

X

Bij,l

kvjl

j,l

^ik2 A
2 j

0

1

=

1 2

X
i

@ h^i, ^ii
2 0

+

D

log(2

02) + X Bij,l hvjl, vjli
j,l

2hvjl, ^ii + h^i, ^ii A
2 jl

00

1

1

=

1 2

X @h^i, ^ii @
i

1
2 0

X +
j,l

Bij,l A
2 j

+ D log(2

02)

2h^i

,

X

Bij,l

vjl
2

iA

j,l j

00

1

1

=

1 2

X @h^i, ^ii @
i

1
2 0

X +
j,l

Bij,l A
2 j

D log(2 02)A

=

1 2

X

i

1k/P02j,+l BPij,ljv,ljBl/ij,lj2/k2j2

!

D log(2

2 0

)

.

(4)

We consider an iterative optimization approach: fixing all but one Bj we find corresponding optimal assignment, then pick a new j at random and proceed until convergence. In the following we will

1

Under review as a conference paper at ICLR 2019

use notation j to say "all but j". Let L j = max{i : Bi,lj = 1} denote number of active global weights outside of group j. We now rearrange (4) by partitioning it into i = 1, . . . , L j and i = L j + 1, . . . , L j + Lj. We are interested in solving for Bj, hence we can modify objective function by subtracting terms independent of Bj:

X
i

1k/P02j,+l BPij,ljv,ljBl/ij,lj2/k2j2

!

D log(2

2 0

)

= +

iLXiL==LX1jj +jL+kj11P/ l102kB/+Pij,l02Pvl +Bjlli/jPB,lvijj2l,jlB+/l/ij,Pj2lj2/+k2j2jP,!l B. jij,,llvBjilj,/l/

2 j

k2
2 j

kP

1/

2 0

+j,lPBij,ljv,ljBl/ij,lj2/k2j2

!

(5)

Now observe that

P
l

Bij,l

2

{0, 1}, i.e.

it

is 1 if some neuron from batch j

is matched to global

neuron i and 0 otherwise. Due to this we can rewrite (5) as a linear sum assignment problem:

+

LXj XLj Bij,l
i=1 l=1
L Xj +Lj XLj
i=L j +1 l=1

kvj 1/
Bij,l

l/
2 0

2 j
+

+ 1/

P
2 j

+j,Pl Bij,jl,vl Bjl/ij,l

!

kvjl/

2 j

k2

1/

2 0

+

1/

2 j

.

2 j
/

k2
2 j

P k

1/

2 0

+j,lPBij,ljv,ljBl/ij,lj2/k2j2

!

(6)

Now we consider second term of (3):

log P ({Bj}) = log P (Bj|B j) + log P (B j).

First, because we are optimizing for Bj, we can ignore log P (B j). Second, due to exchangeability

of batches (i.e. customer of the

customers of IBP). Let mi

the j=

IPBP)j,,lwBeij,cladnenaolwteaynsumcobnesridoef rtiBmejstboabtcehthweeilgahsttsbwatecrhe

(i.e. last assigned

to global atom i outside of group j. We now obtain the following:

00 1 0 1

1

log P (Bj|B

j)

=

LXj

XLj @@

Bij,lA

log

mi J

j

+ @1

i=1 l=1

0 10

XLj Bij,lA log J
l=1
1

mi j A J

(7)

L Xj +Lj log @

XLj

L Xj +Lj

Bij,lA! + @

XLj Bij,lA log

0
J

.

i=L j +1 l=1

i=L j +1 l=1

We now rearrange (7) as linear sum assignment problem:

LXj XLj Bij,l log
i=1 l=1

J

mi j mi

j

L Xj +Lj XLj



+ Bij,l log

i=L j +1 l=1

0
J

 log(i L j) .

(8)

CPoimPbilnBinijg,l

(6) Cij,l,

and (8) where:

we

arrive

at

the

cost

specification

for

finding

Bj

as

minimizer

of

Cij,l =

8> kvjl < 1/

/
2 0

2 j

P +

+1/

2 j

jP,l Bij,l + j,l

vj l / Bij,l

2 j
/

k2
2 j

+ log ,k P

1/

2 0

+j,Pl Bij,jl,vl Bjlij/,l

2 j
/

k2
2 j

mi j J mi j

iL j

:> kvjl/

2 j

k2

1/

2 0

+1/

2 j

2 log(i

L

j) + 2 log

0
J

,

L j < i  L j + Lj.

(9)

This completes the proof of Proposition 2 in the main text.

2 MULTILAYER INFERENCE DETAILS
Figure 1 illustrates the overall multilayer inference procedure visually, and Algorithm 1 provides the details.

2

Under review as a conference paper at ICLR 2019

Algorithm 1 Multilayer Probabilistic Neural Matching

1: LC+1 number of outputs

2: # Top down iteration through layers

3: for layers c = C, C 1, . . . , 2 do

4: Collect hidden layer c from the J servers and form vjcl.

5: Call Single Layer Neural Matching algorithm with output dimension Lc+1 and input dimen-

sion 0 (since we do not use the weights connecting to lower layers here).

6: Form global neuron layer c from output of the single layer matcher.

7: 8:

Lc end for

card([jJ=1Tjc) (greedy approach).

9: # Match bottom layer using weights connecting to both the input and the layer above.

10: Call Single Layer Neural Matching algorithm with output dimension L2 and input dimension

equal to the number of inputs.

11: Return global assignments and form global mutltilayer model.

Figure 1: Probabilistic Neural Matching algorithm showing matching of three multilayer MLPs. Nodes in the graphs indicate neurons, neurons of the same color have been matched. On the left, the individual layer matching approach is shown, consisting of using the matching assignments of the next highest layer to convert the neurons in each of the J servers to weight vectors referencing the global previous layer. These weight vectors are then used to form a cost matrix, which the Hungarian algorithm then uses to do the matching. Finally, the matched neurons are then aggregated and averaged to form the new layer of the global model. As shown on the right, in the multilayer setting the resulting global layer is then used to match the next lower layer, etc. until the bottom hidden layer is reached (Steps 1, 2, 3,... in order).

3 STREAMING NEURAL MATCHING

In this section we present inference for the streaming extension of our model described in Section

3.3 of the main text. Bayesian paradigm naturally fits into the streaming scenario - posterior of step

s becomes prior for step s + 1:

2,s+1 0,i

=

1/

1

2,s 0,i

+

P
j,l

Bij,,ls/

,
2 j

µs0+,i 1

=

µ0s / 1/

0202,,is,,is++PPjj,l,lBBij,ij,l,s,lsv/jsl

/
2 j

2
j for i = 1, . . . , Ls.

(10)

The cost for finding Bj,s becomes:

Cji,,ls =

>8 kµs0,i/

2,s 0,i

+vjsl

/

< 1/

2,s 0,i

+1/

j2j2++PP

j,l Bij,,lsvjsl/

j,l Bij,,ls/

2 j

2 j

k2

+ log ,kµs0,i/
1/

0202,,,i,sis++PP

j,l Bij,,lsvjsl/

j,l Bij,,ls/

2 j

2 j

k2

1+mjs,i s mjs,i

>: kµ0s,i/

2,s 0,i

+vjsl

/

2 j

k2

1/

2,s 0,i

+1/

2 j

kµs0,i /

2,s 0,i

k2

1/

2,s 0,i

2 log(i

Ls

j)

+

2

log

0
sJ

,

Ls j

< i  Ls j

+ Lsj ,

(11)

where

first

case

is

for

i



Ls

j

and

mjs,i

=

Ps 1
a=1

P
l

Bij,,la

is

the

popularity

of

global

atom

i

in

group

j up to step closed form

s. in

thWe eBnayoetesitahnant olongpaPra(mBejt,rsi|cBlitejr,sa,tu{rBe,jt,o1}tJjh=e1b,e.s.t.o,f{oBujr,skn1o}wJjl=e1d)geis,

not and

available in we replaced

corresponding terms in the cost with a heuristic.

3

Under review as a conference paper at ICLR 2019

4 EXPERIMENTAL DETAILS AND ADDITIONAL RESULTS
Code to reproduce our results will be released after the review period. Below are the details of the experiments. Data partitioning. In the federated learning setup, we analyze data from multiple sources, which we call batches. Data on the batches in general does not overlap and may have different distributions. To simulate federated learning scenario we consider two partition strategies of MNIST and CIFAR10. For each pair of partition strategy and dataset we run 10 trials to obtain mean accuracies and standard deviations. The easier case is homogeneous partitioning, i.e. when class distributions on batches are approximately equal as well as batch sizes. To generate homogeneous partitioning with J batches we split examples for each of the classes into J approximately equal parts to form J batches. In the heterogeneous case batches are allowed to have highly imbalanced class distributions as well as highly variable sizes. To simulate heterogeneous partition, for each class k, we sample pk  DirJ (0.2) and allocate pk,j proportion of instances of class k of the complete dataset to batch j. Note that due to small concentration parameter, 0.2, of the Dirichlet distribution, some batches may entirely miss examples of a subset of classes. Batch networks training. Our modeling framework and ensemble related methods operate on collection of weights of neural networks from all batches. Any optimization procedure and software can be used locally on batches for training neural networks. We used PyTorch (Paszke et al., 2017) as software framework and Adam optimizer (Kingma & Ba, 2014) with default parameters unless otherwise specified. For reproducibility we summarize all parameter settings in Table 1.
Table 1: Parameter settings for batch neural networks training

MNIST

CIFAR-10

Neurons per layer Learning rate L2 regularization Minibatch size Epochs Weights initialization Bias initialization

50 0.01 10 6 32 10 N (0, 0.01) 0.1

50 0.001 10 5 32 10 N (0, 0.01) 0.1

4.1 PARAMETER SETTINGS FOR THE BASELINES

We first formally define the ensemble procedure. Let y^j 2 K 1 denote probability distribu-

tion over K classes output x. Then uniform ensemble

by neural prediction

insetawrgormk atxraJi1nPed jJo=n1

data y^j,k .

from batch j for some test input To define weighted ensemble, let

nj,k

k
denote number of examples of class k on batch j

and nk

=

PJ
j=1

nj,k

denote total num-

bhareegrtekmorofagexexnnae1mkouPpslejJps=aor1tfintcijol,kansy^isnj,gkk.waicTtrhhoiesssnsiaselmlabbhaleetc.uhriesst.icParpepdricotaicohn

of the weighted ensemble we defined to potentially

is as follows better handle

Knowledge distillation approach (Hinton et al., 2015) trains a new master neural network to minimize cross entropy between output of the master neural network and outputs of the batch neural networks. The architecture of the master neural network has to be set manually - we use 500 neurons per layer. Note that PFNM infers the number of neurons per layer of the master network from the batch weights. For the knowledge distillation approach it is required to pool input data from all of the batches to the master server. For training master neural network we used PyTorch, Adam optimizer and parameter settings as in Table 1.

For the downpour SGD (Dean et al., 2012) we used PyTorch, Adam optimizer and parameter settings as in Table 1 for the local learners. Master neural network was also optimized with Adam and same

4

Under review as a conference paper at ICLR 2019
learning rate as in the Table 1. Weights of the master neural network were updated in the end of every epoch (total of 10 rounds of communication) and then sent to each of the local learners to proceed with the next epoch. Note that with this approach global network and networks for each of the batches are bounded to have identical number of neurons per layer, which is 50 in our experiments. We tried increasing number of neurons per layer, however did not observe any performance improvements. 4.2 ADDITIONAL EXPERIMENTAL RESULTS Master network size of PFNM. Our model for matching neural networks is nonparametric and hence can infer appropriate size of the master network from the batch weights. The "discovery" of new neurons is controlled by the second case of our cost term expression in (9), i.e. when L j < i  L j + Lj. In practice however we want to avoid impractically large master networks, hence we truncate the largest possible value of i in the cost computation to min(L j +Lj, max(Lj, 700)+1). This means that when global network has 700 or more neurons, we only allow for it to grow by 1 in a single multibatch Hungarian algorithm iteration. In Figure 2 we summarize network sizes learned by PFNM in experiments corresponding to increasing number of batches (Figure 2 of the main text) and increasing number of hidden layers (Figure 3 of the main text). The maximum possible size is 50JC (because of 50 neurons per batch per layer), which is practically the size of the master model of the ensemble approaches. We see that size of the master network of PFNM is noticeably more compact than simply stacking batch neural networks. The saturation around 700 neurons in Figure 2a is due to the truncation procedure described previously.

(a) Varying J, C = 1

(b) Varying C, J = 10

Figure 2: Network sizes for varying number of batches and layers

Downpour SGD with frequent communication In the main text we considered downpour SGD with total of 10 rounds of communication -- one after each training epoch. This implies 20J communications, 10J for batches to send their gradients to the master server and 10J for the master server to send copy of the global neural network to each of the batch neural networks. In our federated learning problem setup frequent communication is discouraged, however it is interesting to study the minimum number of communications needed for D-SGD to produce competitive result. To empirically quantify this we show test accuracy of D-SGD with increasing number of communication rounds on MNIST with heterogeneous partitioning and J = 25 (Fig. 3a). PFNM and ensemble based methods are shown for comparison - they communicate only once (post batch networks training) in all of our experiments. We see that in this case D-SGD requires more than 4000 communication rounds (8000J communications) to produce good result. Such large amount of communication is impossible in practice for the majority of federate learning scenarios.
5

Under review as a conference paper at ICLR 2019

(a) D-SGD with varying number of communication (b) Streaming experiment on heterogeneous MNIST

rounds

with J = 3, S = 15

Figure 3: Additional experiments

Streaming federated learning experiment To simulate streaming federated learning setup we

partition data into J groups using both homogeneous and heterogeneous strategies as before. Then

each group is randomly split into S parts. At step s = 1, . . . , S the part of data indexed by s from

each of the groups is revealed and used for updating the models. For this experiment we consider

heterogeneous partitioning of MNIST with J = 3 batches and S = 15 steps. On every step we

evaluate accuracy on the test dataset and summarize performance of all methods in Figure 3b. For

our method, PFNM-Streaming, we perform matching based on the cost computations from Section

3 to update the global neural network. We initialize weights of the j-th batch neural network for the

next step s + 1 according to the model posterior after s steps (which is the prior for step s + 1):

subsample Lsj+1 0 neurons from the global network according to popularity counts {msj,i}i and

concatenate with set to 0), then add

0smnaelulroamnsoiunnittiaolfizGeaduwssiitahnµn00oi(sper.ioFromr eDa-nSbGeDfowreeaunpyddaatetagilsoboablsenrevuerda,l

which is network

weights after each step and then use these weights as initialization for batch neural networks on

the next step. To extend ensemble based methods to streaming setting we simply update local

neural networks sequentially as the new data becomes available and use them to evaluate ensemble

performance at each step.

4.3 PARAMETER SENSITIVITY ANALYSIS FOR PFNM

O.o.nu.dr=pmaoraJ2dm.elTestheperr,efisre0snt, tpceoadrnaitmnroeSlsteedcr,tiisoc02no,v3iesrotyhf eothfperniemowravinnaertuiearxnotcnehsaoavfnewdtehciorgerheretspspaoorfantmhdeientggelrloysbianl02cn,reeu0arsaainnl gdnet0w2 io=nrckr.eSa12see=csthe size of the learned master network. The third parameter, 2, is the variance of the local neural

network weights around corresponding master network weights. We analyze empirically effect of

these parameters on the accuracy for single hidden layer model with J = 25 batches in Figure 4.

The heatmap indicates the accuracy on the training data - we see that for all parameter values con-

sidered and 0,

performance doesn't not which we set to 10 and

fluctuate significantly. 1 respectively in all of

PFNM appears to our experiments.

be robust to Parameter

c2hhoaiscessliogfhtly02

higher impact on the performance and we set it using training data during experiments. To quantify

importance of 2 in Figure 5.

2 for fixed We see that

f02or=ho1m0 oagnedne0ou=s

1 we plot average train data accuracies for varying partitioning and one hidden layer 2 has almost no

effect on the performance (Fig. 5a and Fig. 5c). In the case of heterogeneous partitioning (Fig. 5b

and Fig. 5d), effect of 2 is more noticeable, however all considered values result in competitive

performance.

6

Under review as a conference paper at ICLR 2019

(a) MNIST homogeneous

(b) MNIST heterogeneous

(c) CIFAR homogeneous

(d) CIFAR heterogeneous

Figure 4: Parameter sensitivity analysis for J = 25

7

Under review as a conference paper at ICLR 2019

(a) MNIST homogeneous

(b) MNIST heterogeneous

(c) CIFAR homogeneous

(d) CIFAR heterogeneous

Figure 5: Sensitivity analysis of

2 for fixed

2 0

=

10

and

0 = 1 for varying J

8

Under review as a conference paper at ICLR 2019
REFERENCES
Jeffrey Dean, Greg Corrado, Rajat Monga, Kai Chen, Matthieu Devin, Mark Mao, Andrew Senior, Paul Tucker, Ke Yang, Quoc V Le, et al. Large scale distributed deep networks. In Advances in neural information processing systems, pp. 1223­1231, 2012.
Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531, 2015.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.
Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in pytorch. In NIPS-W, 2017.
9


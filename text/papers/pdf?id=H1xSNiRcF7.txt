Under review as a conference paper at ICLR 2019
SMOOTHING THE GEOMETRY OF PROBABILISTIC BOX EMBEDDINGS
Anonymous authors Paper under double-blind review
ABSTRACT
There is growing interest in geometrically-inspired embeddings for learning hierarchies, partial orders, and lattice structures, with natural applications to transitive relational data such as entailment graphs. Recent work has extended these ideas beyond deterministic hierarchies to probabilistically calibrated models, which enable learning from uncertain supervision and inferring soft-inclusions among concepts, while maintaining the geometric inductive bias of hierarchical embedding models. We build on the Box Lattice model of Vilnis et al. (2018), which showed promising results in modeling soft-inclusions through an overlapping hierarchy of sets, parameterized as high-dimensional hyperrectangles (boxes). However, the hard edges of the boxes present difficulties for standard gradient based optimization; that work employed a special surrogate function for the disjoint case, but we find this method to be fragile. In this work, we present a novel hierarchical embedding model, inspired by a relaxation of box embeddings into parameterized density functions using Gaussian convolutions over the boxes. Our approach provides an alternative surrogate to the original lattice measure that improves the robustness of optimization in the disjoint case, while also preserving the desirable properties with respect to the original lattice. We demonstrate increased or matching performance on WordNet hypernymy prediction, Flickr caption entailment and a MovieLens-based market basket dataset. We show especially marked improvements in the case of sparse data, where many conditional probabilities should be low, and thus boxes should be nearly disjoint.
1 INTRODUCTION
Embedding methods have long been a key technique in machine learning, providing a natural way to convert semantic problems into geometric problems. Early examples include the vector space (Salton et al., 1975) and latent semantic indexing (Deerwester et al., 1990) models for information retrieval. Embeddings experienced a renaissance after the publication of Word2Vec (Mikolov et al., 2013), a neural word embedding method (Bengio et al., 2003; Mnih & Hinton, 2009) that could run at massive scale.
Recent years have seen an interest in structured or geometric representations. Instead of representing e.g. images, words, sentences, or knowledge base concepts with points, these methods instead associate them with more complex geometric structures. These objects can be density functions, as in Gaussian embeddings (Vilnis & McCallum, 2015; Athiwaratkun & Wilson, 2017; 2018), convex cones, as in order embeddings (Vendrov et al., 2016; Lai & Hockenmaier, 2017), or axis-aligned hyperrectangles, as in box embeddings (Vilnis et al., 2018; Subramanian & Chakrabarti, 2018). These geometric objects more naturally express ideas of asymmetry, entailment, ordering, and transitive relations than simple points in a vector space, and provide a strong inductive bias for these tasks.
In this work, we focus on the probabilistic Box Lattice model of Vilnis et al. (2018), because of its strong empirical performance in modeling transitive relations, probabilistic interpretation (edges in a relational DAG are replaced with conditional probabilities), and ability to model complex joint probability distributions including negative correlations. Box embeddings (BE) are a generalization of order embeddings (OE) (Vendrov et al., 2016) and probabilistic order embeddings (POE) (Lai & Hockenmaier, 2017) that replace the vector lattice ordering (notions of overlapping and enclos-
1

Under review as a conference paper at ICLR 2019
ing convex cones) in OE and POE with a more general notion of overlapping boxes (products of intervals).
While intuitively appealing, the "hard edges" of boxes and their ability to become easily disjoint, present difficulties for gradient-based optimization: when two boxes are disjoint in the model, but have overlap in the ground truth, no gradient can flow to the model to correct the problem. This is of special concern for (pseudo-)sparse data, where many boxes should have nearly zero overlap, while others should have very high overlap. This is especially pronounced in the case of e.g. market basket models for recommendation, where most items should not be recommended, and entailment tasks, most of which are currently artificially resampled into a 1:1 ratio of positive to negative examples. To address the disjoint case, Vilnis et al. (2018) introduce an ad-hoc surrogate function. In contrast, we look at this problem as inspiration for a new model, based on the intuition of relaxing the hard edges of the boxes into smoothed density functions, using a Gaussian convolution with the original boxes.
We demonstrate the superiority of our approach to modeling transitive relations on WordNet, Flickr caption entailment, and a MovieLens-based market basket dataset. We match or beat existing state of the art results, while showing substantial improvements in the pseudosparse regime.
2 RELATED WORK
As mentioned in the introduction, there is much related work on structured or geometric embeddings. Most relevant to this work are the order embeddings of Vendrov et al. (2016), which embed a non-probabilistic DAG or lattice in a vector space with order given by inclusion of embeddings' forward cones, the probabilistic extension of that model due to Lai & Hockenmaier (2017), and the box lattice (BL) of Vilnis et al. (2018), which we extend. Concurrently to Vilnis et al. (2018), another hyperrectangle-based generalization of order embeddings was proposed by Subramanian & Chakrabarti (2018), also called box embeddings (BE). The difference between the two models is that BL is a probabilistic model that assigns edges conditional probabilities according to degrees of overlap, while BE is a deterministic model in the style of order embeddings -- an edge is considered present only if one box entirely encloses another.
Our approach to smoothing the energy landscape of the model using Gaussian convolution is common in mollified optimization and continuation methods, and is increasingly making its way into machine learning models such as Mollifying Networks (Gulcehre et al., 2016b), diffusion-trained networks (Mobahi, 2016), and noisy activation functions (Gulcehre et al., 2016a).
Our focus on embedding orderings and transitive relations is a subset of knowledge graph embedding. While this field is very large, the main difference of our probabilistic approach is that we seek to learn an embedding model which maps concepts to subsets of event space, giving our model an inductive bias especially suited for transitive relations as well as fuzzy concepts of inclusion and entailment.
3 BACKGROUND
3.1 PARTIAL ORDERS AND LATTICES
A non-strict partially ordered set (poset) is a set P equipped with a binary relation such that for all a, b, c  P ,
Reflexivity: a a Antisymmetry: a b a implies a = b Transitivity: a b c implies a c
This generalizes the standard concept of a totally ordered set to allow some elements to be incomparable. Posets provide a good formalism for the kind of acyclic directed graph data found in many knowledge bases with transitive relations.
A lattice is a poset where any subset has a a unique least upper and greatest lower bound, which will be true of all posets (lattices) considered in this paper. A bounded lattice is a lattice with two
2

Under review as a conference paper at ICLR 2019
extra elements, called top, denoted and bottom, denoted , which are respectively the least upper bound and greatest lower bound of the entire space. The least upper bound of two elements a, b  P is called the join, denoted a  b, and the greatest lower bound is called the meet, denoted a  b. A bounded lattice must satisfy these properties:
Idempotency: a  a = a  a = a Commutativity: a  b = b  a and a  b = b  a Associativity: a  b  c = a  (b  c) and (a  b  c) = a  (b  c) Absorption: a  (a  b) = a and a  (a  b) = a Bounded:  a
Note that the extended real numbers, R  {-, }, form a bounded lattice (and in fact, a totally ordered set) under the min and max operations as the meet () and join () operations. So do sets partially ordered by inclusion, with  and  as  and . Thinking of these special cases gives the intuition for the fourth property, absorption. The  and  operations can be swapped, along with reversing the poset relation , to give a valid lattice, called the dual lattice. In the real numbers this just corresponds to a sign change. A semilattice has only a meet or join, but not both. Note. In the rest of the paper, when the context is clear, we will also use  and  to denote min and max of real numbers, in order to clarify the intuition behind our model.
3.2 VECTOR LATTICE
A vector lattice, also known as a Riesz space (Zaanen, 1997), or Hilbert lattice when the accompanying vector space has an inner product, is a vector space endowed with a lattice structure. A standard choice of partial order for the vector lattice Rn is to use the product order from the underlying real numbers, which specifies for all x, y  Rn
x y  i  {1..n}, xi  yi Under this order, meet and join operations are pointwise min and max, which gives a lattice structure. In this formalism, the Order Embeddings of Vendrov et al. (2016) embed partial orders as vectors using the reverse product order, corresponding to the dual lattice, and restrict the vectors to be positive. The vector of all zeroes represents , and embedded objects become "more specific" as they get farther away from the origin.
3.3 BOX LATTICE
Vilnis et al. (2018) introduced a box lattice, wherein each concept in a knowledge graph is associated with two vectors, the minimum and maximum coordinates of an axis-aligned hyperrectangle, or box (product of intervals).
We can define a partial ordering by inclusion of boxes, letting (xm,i, xM,i) represent the maximum and minimum at each coordinate i, with  and  denoting max and min when applied to the scalar coordinates, and a box lattice structure as
x  y =  if x and y disjoint, else
x  y = [xm,i  ym,i, xM,i  yM,i]
i
x  y = [xm,i  ym,i, xM,i  yM,i]
i
where the meet is the intersecting box, or bottom (the empty set) where no intersection exists, and join is the smallest enclosing box.
To associate a measure, marginal probabilities of (collections of) events are given by the volume of the (intersection) box under a suitable probability measure. For event x with associated box (xm, xM ), probability is simply p(a) = ni (xM,i - xm,i) under the uniform measure (this is a
3

Under review as a conference paper at ICLR 2019

probability measure because the boxes are constrained to lie in the unit hypercube). p() is zero since no probability mass is assigned to the empty set. Since boxes are simply special cases of sets, it is intuitive that this is a valid probability measure, but it can also be shown to be compatible with the meet semilattice structure in a precise sense (Leader, 1971).
4 METHOD
4.1 MOTIVATION: OPTIMIZATION AND SPARSE DATA
When using gradient-based optimization to learn box embeddings, an immediate problem identified in the original work is that when two concepts are incorrectly given as disjoint by the model, no gradient signal can flow since the meet (intersection) is exactly zero, with zero derivative. To see this, note that for a pair of 1-dimensional boxes (intervals), the volume of the meet under the uniform measure p as given in Section 3.3 is

p(x  y) = mh(xm,i  ym,i - xM,i  yM,i)

(1)

where mh is the standard hinge function, mh(x) = 0  x = max(0, x).
The hinge function has a large flat plateau at 0 when intervals are disjoint. This issue is especially problematic when the lattice to be embedded is (pseudo-)sparse, that is, most boxes should have very little or no intersection, since if training accidentally makes two boxes disjoint there is no way to recover with the naive measure. The authors propose a surrogate function to optimize in this case, but we will use a more principled framework to develop alternate measures that avoid this pathology, improving both optimization and final model quality.

4.2 RELAXED GEOMETRY

The intuition behind our approach is that the "hard edges" of the standard box embeddings lead to unwanted gradient sparsity, and we seek a relaxation of this assumption that maintains the desirable properties of the base lattice model while enabling better optimization and preserving a geometric intuition. For ease of exposition, we will refer to 1-dimensional intervals in this section, but the results carry through from the representation of boxes as products of intervals and their volumes under the associated product measures.
The first observation is that, considering boxes as indicator functions of intervals, we can rewrite the measure of the joint probability p(x  y) between intervals x = [a, b] and y = [c, d] as an integral of the product of those indicators:

p(x  y) = 1[a,b](x)1[c,d](x)dx
R
since the product has support (and is equal to 1) only in the areas where the two intervals overlap.

A solution suggests itself in replacing these indicator functions with functions of infinite support. We elect for kernel smoothing, specifically convolution with a normalized Gaussian kernel, equivalent to an application of the diffusion equation to the original functional form of the embeddings (indicator functions) and a common approach to mollified optimization and energy smoothing (Neelakantan et al., 2015; Gulcehre et al., 2016b; Mobahi, 2016). Specifically, given x = [a, b], we associate the smoothed indicator function
b
f (x; a, b, 2) = 1[a,b](x)  (x; 2) = 1[a,b](z)(x - z; 2)dz = (x - z; 2)dz
Ra

We then wish to evaluate, for two lattice elements x and y with associated smoothed indicators f and g,

p(x  y) = f (x; a, b, 12)g(x; c, d, 22)dx
R
This integral admits a closed form solution.

(2)

4

Under review as a conference paper at ICLR 2019

Proposition 1. Let m(x) = (x)dx be an antiderivative of the standard normal CDF. Then the solution to equation 2 is given by,

p(x  y) = 

m(

b-c 

)

+

m(

a-d 

)

-

m(

b-d 

)

-

m(

a-c 

)





soft(

b-c 

)

+



soft(

a-d 

)

-



soft(

b-d 

)

+



soft(

a-c 

)

(3) (4)

where  = 12 logistic sigmoid,

+ 22, and 

soft(x)

=

 1.702

= .

log(1

+

exp(x))

is

the

softplus

function,

the

antiderivative

of

the

Proof. The first line is proved in Appendix A, the second approximation follows from the approximation of  by a logistic sigmoid given in Bowling et al. (2009).

Note that, in the zero-temperature limit, as  goes to zero, we recover the formula

p(x



y)

=

lim
0



soft(

b-c 

)

+



soft(

a-d 

)

-



soft(

b-d 

)

+



soft(

a-c 

)

= mh(b - c) + mh(a - d) - mh(b - d) + mh(a - c)

= mh(b  d - a  c)
which we would expect from convolution with a zero-bandwidth kernel (a Dirac delta function, the identity element under convolution). This is true for the exact formula using (x)dx, and the softplus approximation.

Unfortunately, for any  > 0, multiplication of Gaussian-smoothed indicators does not give a valid meet operation on a function lattice, for the simple reason that f 2 = f , except in the case of indicator
functions, violating the idempotency requirement of Section 3.1.

More importantly, for practical considerations, if we are to treat the outputs of p as probabilities, the consequence is

p(x|x)

=

p(x, x) p(x)

=

p(x  x) p(x)

=

1

(5)

which complicates our applications that train on conditional probabilities. However, by a modification of equation 3, we can obtain a function p such that p(x  x) = p(x), while retaining the smooth

optimization properties of the Gaussian model.

Recall that for the hinge function mh, but not for the (non-zero temperature) softplus, mh(b - c) + mh(a - d) - mh(b - d) + mh(a - c) = mh(b  d - a  c)
where the left hand side is the zero-temperature limit of the Gaussian model. Inspired by the functional form of the Gaussian model in equation 3, we make the following modification. By the commutativity of min and max with monotonic functions, we have

soft(b - c)  soft(a - d)  soft(b - d)  soft(a - c) = soft(b  d - a  c)

Because softplus upper-bounds the hinge function it is capable of outputting values that are greater

than 1, and therefore must be normalized. In our experiments, we use two different approaches to

normalization. For experiments with a relatively small number of entities (all besides Flickr), we

allow the boxes to learn unconstrained, and divide each dimension by the measured size of the global

minimum and maximum (Gm, GM ) at that dimension

msoft(i) (x)

=

msoft(x) msoft(Gm -Gm )

For data where computing these values repeatedly is infeasible, we project onto the unit hypercube

and normalize by msoft(1).

Note that, while equivalent in the zero temperature limit to the standard uniform probability measure of the box model, this function, like the Gaussian model, is not a valid probability measure on the entire joint space of events (the lattice). However, neither is factorization of a conditional probability table using a logistic sigmoid link function, which is commonly used for the similar tasks. Our approach retains the inductive bias of the original box model, is equivalent in the limit, and satisfies the necessary condition that psoft (x,x) = psoft (x). A comparison of the 3 different functions is given in Figure 1, with the softplus overlap showing much better behavior for highly disjoint boxes than the Gaussian model, while also preserving the meet property.

5

Under review as a conference paper at ICLR 2019

(a) Standard (hinge) overlap (b) Gaussian overlap,   {2, 6}

(c) Softplus overlap

Table 1: Comparison of different overlap functions for two boxes of width 0.3 as a function of their centers. Note that in order to achieve high overlap, the Gaussian model must drastically lower its temperature, causing vanishing gradients in the tails.

5 EXPERIMENTS

5.1 WORDNET

Method transitive word2gauss OE Li et al. (2017) POE Box Smoothed Box

Test Accuracy % 88.2 86.6 90.6 91.3 91.6 92.2 92.0

Table 2: Classification accuracy on WordNet test set.

We perform experiments on the WordNet hypernym prediction task in order to evaluate the performance of these improvements in practice. The WordNet hypernym hierarchy contains 837,888edges after performing the transitive closure on the direct edges in WordNet. We used the same train/dev/test split as in Vendrov et al. (2016). Positive examples are randomly chosen from the 837k edges, while negative examples are generated by swapping one of the terms to a random word in the dictionary.
The smoothed box model performs nearly as well as the original box lattice in terms of test accuracy1. While our model requires less hyper-parameter tuning than the original, we suspect that our performance would be increased on a task with a higher degree of sparsity than the 50/50 positive/negative split of the standard WordNet data, which we explore in the next section.

5.2 IMBALANCED WORDNET
In order to confirm our intuition that the smoothed box model performs better in the sparse regime, we perform further experiments using different numbers of positive and negative examples from the WordNet mammal subset, comparing the box lattice, our smoothed approach, and order embeddings (OE) as a baseline. The training data is the transitive reduction of this subset of the mammal WordNet, while the dev/test is the transitive closure of the training data. The training data contains 1,176 positive examples, and the dev and test sets contain 209 positive examples. Negative examples are generated randomly using the ratio stated in the table.
As we can see from the table, with balanced data, both Box and Smoothed Box models outperform our OE baseline, and nearly match the full transitive closure. As the number of negative examples increases, the performance drops for the original box model, but Smoothed Box still outperforms OE and Box in all setting. This superior performance on imbalanced data is important for e.g. real-world entailment graph learning, where the number of negatives greatly outweigh the positives.
1Accuracy is calculated by applying same threshold which maximized the accuracy in dev set.

6

Under review as a conference paper at ICLR 2019

Positive:Negative 1:1 1:2 1:6 1:10

Box 0.9436 0.7439 0.5829 0.4886

OE 0.9395 0.8476 0.6429 0.5859

Smoothed Box 0.9976 0.9173 0.7702 0.7155

Table 3: F1 scores of the box lattice, order embeddings, and our smoothed model, for different levels of label imbalance on the WordNet mammal subset.

5.3 FLICKR

We conduct experiments on the Flickr entailment dataset. Flickr is a large-scale caption entailment dataset containing of 45 million image caption pairs. In order to perform an apple-to-apple comparison with existing results we use the exact same dataset from Vilnis et al. (2018). In this case, we do constrain the boxes to the unit cube, using the same experiment setting as Vilnis et al. (2018), except we apply the softplus function before calculating the volume of the boxes.
We report KL divergence and Pearson correlation on the full test data, unseen pairs (caption pairs which are never occur in training data) and unseen captions (captions which are never occur in training data). As shown in Table 4 the following table, we see a slight performance gain compared to the original model, with improvements most concentrated on unseen captions.

Full test data POE POE* Box Smoothed Box Unseen pairs POE POE* Box Smoothed Box Unseen captions POE POE* Box Smoothed Box

P (x|y) KL 0.031 0.031 0.020 0.018
0.048 0.046 0.025 0.024
0.127 0.084 0.050 0.036

Pearson R 0.949 0.949 0.967 0.969
0.920 0.925 0.957 0.957
0.696 0.854 0.900 0.917

Table 4: KL and Pearson correlation between model and gold probability.

5.4 MOVIELENS

We apply our method to a market-basket task constructed using the MovieLens dataset. Here, the

task is to predict users' preference for movie A given that they liked movie B. We first collect

all pairs of user-movie ratings higher than 4 points (strong preference) from the MovieLens-20M

dataset. From this we further prune to just a subset of movies which have more than 100 user

ratings to make sure that counting statistics are significant enough. This leads to 8545 movies in our

dataset.

We calculated the conditional probability P (A|B)

=

P (A,B) P (B)

=

#rating(A,B )>4 /#users #rating(B )>4 /#users

.

We randomly picked 100K conditional probabilities for training data and 10k probabilities for dev

and test data 2.

We compare with several baselines: low-rank matrix factorization, complex bilinear factorization (Trouillon et al., 2016), and two hierarchical embedding methods, POE (Lai & Hockenmaier, 2017) and the Box Lattice (Vilnis et al., 2018). Since the training matrix is asymmetric, we used separate embeddings for target and conditioned movies. For the complex bilinear model, we added

2In the dev and test data, we also removed all the P (A|B) where P (B|A) appeared in the training data.

7

Under review as a conference paper at ICLR 2019

one additional vector of parameters to capture the "imply" relation. We evaluate on the test set using KL divergence, Pearson correlation, and Spearman correlation with the ground truth probabilities.
From the results in Table 5, we can see that our smoothed box embedding method outperforms the original box lattice as well as all other baselines' performances, especially in Spearman correlation, the most relevant metric for recommendation, a ranking task.

Matrix Factorization Complex Bilinear Factorization POE Box Smoothed Box

KL 0.0173 0.0141 0.0168 0.0144 0.0139

Pearson R 0.8549 0.8771 0.8630 0.8791 0.8889

Spearman R 0.8374 0.8636 0.8478 0.8566 0.8858

Table 5: Performance comparison between different methods over the MovieLens Dataset.

6 CONCLUSION AND FUTURE WORK
We presented an approach to smoothing the energy and optimization landscape of probabilistic box embeddings and provided a theoretical justification for the smoothing. Due to a decreased number of hyper-parameters this model is easier to train, and, furthermore, met or surpassed current state-ofthe-art results on several interesting datasets. We further demonstrated that this model is particularly effective in the case of sparse data and more robust to poor initialization.
Tackling the learning problems presented by rich, geometrically-inspired embedding models is an open and challenging area of research, which this work is far from the last word on. This task will become even more pressing as the embedding structures become more complex, such as unions of boxes or other non-convex objects. To this end, we will continue to explore both function lattices, and constraint-based approaches to learning.
REFERENCES
Ben Athiwaratkun and Andrew Gordon Wilson. Multimodal word distributions. In ACL, 2017.
Ben Athiwaratkun and Andrew Gordon Wilson. Hierarchical density order embeddings. In International Conference on Learning Representations, 2018. URL https://openreview.net/ forum?id=HJCXZQbAZ.
Yoshua Bengio, Re´jean Ducharme, Pascal Vincent, and Christian Jauvin. A neural probabilistic language model. Journal of machine learning research, 3(Feb):1137­1155, 2003.
Shannon R Bowling, Mohammad T Khasawneh, Sittichai Kaewkuekool, and Byung R Cho. A logistic approximation to the cumulative normal distribution. Journal of Industrial Engineering and Management, 2(1), 2009.
Scott Deerwester, Susan T Dumais, George W Furnas, Thomas K Landauer, and Richard Harshman. Indexing by latent semantic analysis. Journal of the American society for information science, 41 (6):391­407, 1990.
Caglar Gulcehre, Marcin Moczulski, Misha Denil, and Yoshua Bengio. Noisy activation functions. In International Conference on Machine Learning, pp. 3059­3068, 2016a.
Caglar Gulcehre, Marcin Moczulski, Francesco Visin, and Yoshua Bengio. Mollifying networks. arXiv preprint arXiv:1608.04980, 2016b.
Tony Jebara, Risi Kondor, and Andrew Howard. Probability product kernels. Journal of Machine Learning Research, 5(Jul):819­844, 2004.
Alice Lai and Julia Hockenmaier. Learning to predict denotational probabilities for modeling entailment. In EACL, 2017.
8

Under review as a conference paper at ICLR 2019
Solomon Leader. Measures on semilattices. Pacific Journal of Mathematics, 39(2):407­423, 1971. Xiang Li, Luke Vilnis, and Andrew McCallum. Improved representation learning for predicting
commonsense ontologies. NIPS Workshop on Structured Prediction, 2017. Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. Distributed representa-
tions of words and phrases and their compositionality. In NIPS, 2013. Andriy Mnih and Geoffrey E Hinton. A scalable hierarchical distributed language model. In Ad-
vances in neural information processing systems, pp. 1081­1088, 2009. Hossein Mobahi. Training recurrent neural networks by diffusion. arXiv preprint arXiv:1601.04114,
2016. Arvind Neelakantan, Luke Vilnis, Quoc V Le, Ilya Sutskever, Lukasz Kaiser, Karol Kurach, and
James Martens. Adding gradient noise improves learning for very deep networks. arXiv preprint arXiv:1511.06807, 2015. Gerard Salton, Anita Wong, and Chung-Shu Yang. A vector space model for automatic indexing. Communications of the ACM, 18(11):613­620, 1975. Sandeep Subramanian and Soumen Chakrabarti. New embedded representations and evaluation protocols for inferring transitive relations. SIGIR 2018, 2018. The´o Trouillon, Johannes Welbl, Sebastian Riedel, E´ ric Gaussier, and Guillaume Bouchard. Complex embeddings for simple link prediction. In International Conference on Machine Learning, pp. 2071­2080, 2016. Ivan Vendrov, Ryan Kiros, Sanja Fidler, and Raquel Urtasun. Order-embeddings of images and language. In ICLR, 2016. Luke Vilnis and Andrew McCallum. Word representations via gaussian embedding. In ICLR, 2015. Luke Vilnis, Xiang Li, Shikhar Murty, and Andrew McCallum. Probabilistic embedding of knowledge graphs with box lattice measures. In ACL. Association for Computational Linguistics, 2018. Adriaan C. Zaanen. Introduction to Operator Theory in Riesz Spaces. Springer Berlin Heidelberg, 1997. ISBN 9783642644870.
9

Under review as a conference paper at ICLR 2019

Supplementary Material

A PROOF OF GAUSSIAN OVERLAP FORMULA

We wish to evaluate, for two lattice elements x and y, with associated smoothed indicators f and g,

b
f (x; a, b, 2) = 1[a,b](x)  (x; 2) = 1[a,b](z)(x - z; 2)dz = (x - z; 2)dz
Ra

p(x  y) = f (x; a, b, 12)g(x; c, d, 22)dx
R

(6)

Since the Gaussian kernel is normalized to have total integral equal to 1, so as not to change the overall areas of the boxes, the concrete formula is

(z; 2) =

1

-z2
e 22

 2

Since the antiderivative of  is the normal CDF, this may be recognized as the difference (x; a, 2) - (x; b, 2), but this does not allow us to easily evaluate the integral of interest, which
is the integral of the product of two such functions.

To evaluate equation 6, recall the identity (Jebara et al., 2004; Vilnis & McCallum, 2015)

(x - µ1; 12)(x - µ2; 22)dx = (µ1 - µ2; 12 + 22)
R

(7)

For convenience, let  :=  1 . Applying Fubini's theorem and using equation 7, we have
12 +22

bd

p(x  y) =

(x - y; 12) dy (x - z; 22) dz dx

Ra

c

db
= (y - z;  -2) dy dz

ca

db

=  ( (y - z)) dy dz

ca

d

= ( (b - z)) - ( (a - z)) dz

c

=

-1  (m( (b

-

d))

-

m( (a

-

d))

-

m( (b

-

c))

+

m( (a

-

c)))

and therefore, with  =  -1,

p(x  y) = 

m(

b-c 

)

+

m(

a-d 

)

-

m(

b-d 

)

-

m(

a-c 

)

as desired.

10


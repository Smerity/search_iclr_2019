Under review as a conference paper at ICLR 2019
MODULATED VARIATIONAL AUTO-ENCODERS FOR MANY-TO-MANY MUSICAL TIMBRE TRANSFER
Anonymous authors Paper under double-blind review
ABSTRACT
Generative models have been successfully applied to image style transfer and domain translation. However, there is still a wide gap in the quality of results when learning such tasks on musical audio. Furthermore, most translation models only enable one-to-one or one-to-many transfer by relying on separate encoders or decoders and complex, computationally-heavy models. In this paper, we introduce the Modulated Variational auto-Encoders (MoVE) to perform musical timbre transfer. We define timbre transfer as applying parts of the auditory properties of a musical instrument onto another. First, we show that we can achieve this task by conditioning existing domain translation techniques with Feature-wise Linear Modulation (FiLM). Then, by replacing the usual adversarial translation criterion by a Maximum Mean Discrepancy (MMD) objective, we alleviate the need for an adversarial objective. This allows a faster and more stable training along with a controllable latent space encoder. By further conditioning our system on several different instruments, we can generalize to many-to-many transfer within a single variational architecture able to perform multi-domain transfers. Our models map inputs to 3-dimensional representations, successfully translating timbre from one instrument to another and supporting sound synthesis from a reduced set of control parameters. We evaluate our method in reconstruction and generation tasks while analyzing the auditory descriptor distributions across transferred domains. We show that this architecture allows for generative controls in multi-domain transfer, yet remaining light, fast to train and effective on small datasets1.
1 INTRODUCTION
Music information can be analyzed in many forms, each of which conveys different specificities over musical qualities. Among these, timbre is the set of properties that distinguishes one instrument from another playing at the same pitch and loudness. Timbre has become a core concept in music composition since the 19th century (McAdams (2013)). It has been studied using human dissimilarity ratings to construct timbre spaces, which exhibit the perceptual relationships between instruments (Grey (1977)). However, these spaces are not invertible to the signal domain and do not generalize to new examples (McAdams et al. (2006)). The heavy reliance on hand-crafted audio descriptors to analyze timbre perception altogether leads to a lack of established models to understand and generate timbres (McAdams (2013)). Moreover, the specific nature of music tasks requires tailored evaluation principles that are yet to be ascertained (Jaffe (1995)).
Recent advances in generative models open alternative avenues to analyze highly dimensional data and tackle complex subsequent tasks. Amongst these, the idea of style transfer (Gatys et al. (2015)) has recently gained a flourishing interest. This approach allows to modify the stylistic features of an image while preserving its overall content and led to the more generic question of domain translation. In the recent UNsupervised Image-to-image Translation (UNIT) model, Liu et al. (2017) proposed to learn a shared latent space with a Variational Auto-Encoder (VAE) and translate between different data domains with an adversarial criterion. However, specific properties of the generation cannot be controlled and that discriminative objective might lead to an unstable and longer training. Here, we first extend this approach to musical transfer while improving it by introducing Modulated Variational auto-Encoders (MoVE) that offer control over the generation through conditioning.
1 Audio examples, source code and animations available at https://github.com/anonymous124/iclr2019MoVE
1

Under review as a conference paper at ICLR 2019
Furthermore, by replacing the discriminative networks by a Maximum Mean Discrepancy (MMD) objective, we alleviate the need for an additional adversarial training specific to each domain.
Although UNIT provides a powerful framework, it only applies to one-to-one transfer. This implies that a different model has to be trained for each pair of domains. To mitigate this issue, Choi et al. (2017) proposed StarGAN which performs many-to-many transfer between several domains. However, it relies solely on Generative Adversarial Networks (GANs) and do not learns an implicit task representation to interact with. In the music realm, Mor et al. (2018) proposed Universal Music Translation (UMT), which does not use GANs. Although it enables translation across multiple complex audio domains, this method requires to learn a separate decoder for each domain, which leads to a prohibitive training time. In contrast to these methods, we show that MoVE can be further conditioned on domain information and generalizes to many-to-many transfer with a single encoder and decoder architecture able to perform multi-domain transfer. The resulting models are rather lightweight and fast to train while effective on a moderate amount of examples.
Here, we define timbre transfer as applying a variable part of the auditory properties of a musical instrument onto another. We circumvent the lack of definition for timbre by considering each instrument as a separate domain that maps onto a common latent representation. We further address the crucial need for interactivity and control in creative applications such as audio synthesis. Accordingly, our method yields 3-dimensional latent spaces that can be explored and controlled through high-level explicit variables such as pitch and octave values. This supports sound generation with smoothly evolving timbre qualities and complex domain transfers from a reduced set of parameters. Finally, we analyze traditional audio descriptor distributions when transferring between multiple domains or decoding across latent dimensions to demonstrate the generative capacities of our model.
2 RELATED WORKS
Style transfer and image translation. In computer vision, style transfer (Gatys et al. (2015)) has been proposed to generate images that preserve the content from a source image but feature stylistic qualities belonging to another target image. Although this technique provides compelling results, it operates on local textural information and fails to capture higher-level semantic properties of the style. Further research has been carried to address this question of domain translation, first proposed by Isola et al. (2017). In the fully supervised setting, this translation would require paired samples. However, such datasets are scarce and the concept of existing samples exactly matching the translation task is restrictive from a generative perspective. In the UNIT approach (Liu et al. (2017)), the underlying assumption is that two hypothetically matching samples should map onto the same point in a shared latent space. Hence, translation is achieved by partially weight-shared VAEs in order to map the two separate domains to a common latent representation. Learning is performed with an auxiliary pair of adversarial discriminators which push translated samples to match the distributions of their respective domains. An additional cycle-consistency objective (Zhu et al. (2017)) reinforces the shared learning by ensuring that translated samples can be retrieved back to their original domains. However, this architecture can only operate on single domain pairs.
In order to provide many-to-many translations, Choi et al. (2017) proposed to replace weight-sharing by conditioning a single GAN. This allows to train on multiple domains simultaneously, while enabling control over the generative process. However, the authors evaluate only on highly similar domains (eg. face attributes). Furthermore, this approach relies solely on GANs, which are notoriously difficult to train, prone to lack full support over the data (Grover et al. (2017)) and do not provide a latent space encoder.
Recently, Feature-wise Linear Modulation (FiLM) has been proposed to improve conditioning by learning conditional bias and scaling throughout a network (Perez et al. (2017)). This method was successfully used in image stylization (Ghiasi et al. (2017)) where adaptive modulation conditioned on a style image is applied after each intermediate instance normalization. Here, we show that by relying on FiLM layers for domain conditioning, we can perform many-to-many domain translation with a single VAE architecture, as depicted in Fig. 1(c). Moreover, by using a MMD criterion, we alleviate the need for GANs or specific adversarial discriminators. Hence, we obtain an unsupervised, lightweight and easy to train model with a general and controllable latent space.
2

Under review as a conference paper at ICLR 2019

X1 <latexit sha1_base64="SeadpDWXhuEGM3q8JvmENjkz7Cs=">AAAC1HicjVHLSsNAFD2Nr1ofrbp0EyyCq5KIqMuCG5cV7APaUibTaRuaF8lEKLUrcesPuNVvEv9A/8I7YwpqEZ2Q5My559yZe68TeW4iLes1Zywtr6yu5dcLG5tb28XSzm4jCdOYizoPvTBuOSwRnhuIunSlJ1pRLJjveKLpjC9UvHkj4sQNg2s5iUTXZ8PAHbicSaJ6pWLHZ3LEmTdtzXpTe9Yrla2KpZe5COwMlJGtWlh6QQd9hOBI4UMggCTsgSGhpw0bFiLiupgSFxNydVxghgJ5U1IJUjBix/Qd0q6dsQHtVc5Euzmd4tEbk9PEIXlC0sWE1Wmmjqc6s2J/yz3VOdXdJvR3slw+sRIjYv/yzZX/9alaJAY41zW4VFOkGVUdz7Kkuivq5uaXqiRliIhTuE/xmDDXznmfTe1JdO2qt0zH37RSsWrPM22Kd3VLGrD9c5yLoHFcsa2KfXVSrp5mo85jHwc4onmeoYpL1FDXM3/EE56NhnFr3Bn3n1Ijl3n28G0ZDx96sZXE</latexit>

E1 <latexit sha1_base64="L7I14Fl5z+VCH94aIEzWRQranjU=">AAACyHicjVHLSsNAFD2Nr1pfVZdugkVwVRIp6LIggriqYFqhlpJMp3VomoTJRCmlG3/ArX6Z+Af6F94ZU1CL6IQkZ86958zce4MkFKlynNeCtbC4tLxSXC2trW9sbpW3d5ppnEnGPRaHsbwO/JSHIuKeEirk14nk/igIeSsYnup4647LVMTRlRonvDPyB5HoC+Yroryz7sSddssVp+qYZc8DNwcV5KsRl19wgx5iMGQYgSOCIhzCR0pPGy4cJMR1MCFOEhImzjFFibQZZXHK8Ikd0ndAu3bORrTXnqlRMzolpFeS0sYBaWLKk4T1abaJZ8ZZs795T4ynvtuY/kHuNSJW4ZbYv3SzzP/qdC0KfZyYGgTVlBhGV8dyl8x0Rd/c/lKVIoeEOI17FJeEmVHO+mwbTWpq1731TfzNZGpW71mem+Fd35IG7P4c5zxoHlVdp+pe1ir1Wj7qIvawj0Oa5zHqOEcDHnkLPOIJz9aFlVj31vgz1Srkml18W9bDBytmkOw=</latexit>

D1
<latexit sha1_base64="1AUbF3IWKP1stROBlfrPYVeRteo=">AAAC1HicjVHLSsNAFD2Nr1ofrbp0EyyCq5JIQZcFXbisYB9QS5lMp21oXiQTodSuxK0/4Fa/SfwD/QvvjCmoRXRCkjPnnnNn7r1O5LmJtKzXnLG0vLK6ll8vbGxubRdLO7vNJExjLho89MK47bBEeG4gGtKVnmhHsWC+44mWMz5T8daNiBM3DK7kJBJdnw0Dd+ByJonqlYrXPpMjzrzp+aw3tWe9UtmqWHqZi8DOQBnZqoelF1yjjxAcKXwIBJCEPTAk9HRgw0JEXBdT4mJCro4LzFAgb0oqQQpG7Ji+Q9p1MjagvcqZaDenUzx6Y3KaOCRPSLqYsDrN1PFUZ1bsb7mnOqe624T+TpbLJ1ZiROxfvrnyvz5Vi8QAp7oGl2qKNKOq41mWVHdF3dz8UpWkDBFxCvcpHhPm2jnvs6k9ia5d9Zbp+JtWKlbteaZN8a5uSQO2f45zETSPK7ZVsS+r5Vo1G3Ue+zjAEc3zBDVcoI6GnvkjnvBsNI1b4864/5Qaucyzh2/LePgASjOVrg==</latexit>

X1!,21 <latexit sha1_base64="aL9eTlsWzklOM/E1R5ukq+9mDCc=">AAAC5nicjVHLSsQwFD3W93vUpZvqILiQoRVBl4IblwqODjg6pJk4E+yLNFWkzNqdO3HrD7jVTxH/QP/Cm1hBHURT2p6ce89J7r1BGspMe97LgDM4NDwyOjY+MTk1PTNbmZs/zJJccVHnSZioRsAyEcpY1LXUoWikSrAoCMVRcL5j4kcXQmUyiQ/0VSpOItaJ5ZnkTBPVqiw1I6a7nIVFo3daNJXsdDVTKrl0/V6r8NfWe61K1at5drn9wC9BFeXaSyrPaKKNBBw5IgjE0IRDMGT0HMOHh5S4ExTEKULSxgV6mCBtTlmCMhix5/Tt0O64ZGPaG8/MqjmdEtKrSOlihTQJ5SnC5jTXxnPrbNjfvAvrae52Rf+g9IqI1egS+5fuM/O/OlOLxhm2bA2SakotY6rjpUtuu2Ju7n6pSpNDSpzBbYorwtwqP/vsWk1maze9ZTb+ajMNa/a8zM3xZm5JA/Z/jrMfHK7XfK/m729UtzfKUY9hEctYpXluYhu72EOdvK/xgEc8OV3nxrl17j5SnYFSs4Bvy7l/B6bQnSA=</latexit>

X2 <latexit sha1_base64="n0JOMKx5ZhVYJ3GEJIGAM0Sj32I=">AAAC1HicjVHLSsNAFD2Nr1ofrbp0EyyCq5IUUZcFNy4r2AfUUibTaRuaF8lEKLUrcesPuNVvEv9A/8I7YwpqEZ2Q5My559yZe68TeW4iLes1Zywtr6yu5dcLG5tb28XSzm4zCdOYiwYPvTBuOywRnhuIhnSlJ9pRLJjveKLljM9VvHUj4sQNgys5iUTXZ8PAHbicSaJ6peK1z+SIM2/anvWm1VmvVLYqll7mIrAzUEa26mHpBdfoIwRHCh8CASRhDwwJPR3YsBAR18WUuJiQq+MCMxTIm5JKkIIRO6bvkHadjA1or3Im2s3pFI/emJwmDskTki4mrE4zdTzVmRX7W+6pzqnuNqG/k+XyiZUYEfuXb678r0/VIjHAma7BpZoizajqeJYl1V1RNze/VCUpQ0Scwn2Kx4S5ds77bGpPomtXvWU6/qaVilV7nmlTvKtb0oDtn+NcBM1qxbYq9uVxuXaSjTqPfRzgiOZ5ihouUEdDz/wRT3g2msatcWfcf0qNXObZw7dlPHwAfRKVxQ==</latexit>

E2 <latexit sha1_base64="vw7FIO9r76h8qn6br+pMQHJiy1Y=">AAACyHicjVHLSsNAFD2Nr1pfVZdugkVwVZJS0GVBBHFVwbSFWkoyndaheZFMlFK68Qfc6peJf6B/4Z0xBbWITkhy5tx7zsy914t9kUrLei0YS8srq2vF9dLG5tb2Tnl3r5VGWcK4wyI/Sjqem3JfhNyRQvq8EyfcDTyft73xmYq373iSiii8lpOY9wJ3FIqhYK4kyjnvT2uzfrliVS29zEVg56CCfDWj8gtuMEAEhgwBOEJIwj5cpPR0YcNCTFwPU+ISQkLHOWYokTajLE4ZLrFj+o5o183ZkPbKM9VqRqf49CakNHFEmojyEsLqNFPHM+2s2N+8p9pT3W1Cfy/3CoiVuCX2L9088786VYvEEKe6BkE1xZpR1bHcJdNdUTc3v1QlySEmTuEBxRPCTCvnfTa1JtW1q966Ov6mMxWr9izPzfCubkkDtn+OcxG0alXbqtpX9Uqjno+6iAMc4pjmeYIGLtCEQ94Cj3jCs3FpxMa9MflMNQq5Zh/flvHwAS3HkO0=</latexit>

zs <latexit sha1_base64="lEIbb37npPOooBbM7q+xdtIDfGE=">AAACxnicjVHLSsNAFD2Nr/quunQTLIKrkkhBlwU3XVa0D6ilJOm0Ds2LzESpRfAH3OqniX+gf+GdcQpqEZ2Q5My595yZe6+fhlxIx3ktWAuLS8srxdW19Y3Nre3Szm5LJHkWsGaQhEnW8T3BQh6zpuQyZJ00Y17kh6ztj89UvH3DMsGT+FJOUtaLvFHMhzzwJFEXd33RL5WdiqOXPQ9cA8owq5GUXnCFARIEyBGBIYYkHMKDoKcLFw5S4nqYEpcR4jrOcI810uaUxSjDI3ZM3xHtuoaNaa88hVYHdEpIb0ZKG4ekSSgvI6xOs3U8186K/c17qj3V3Sb0941XRKzENbF/6WaZ/9WpWiSGONU1cKop1YyqLjAuue6Kurn9pSpJDilxCg8onhEOtHLWZ1trhK5d9dbT8TedqVi1D0xujnd1Sxqw+3Oc86B1XHGdinteLdeqZtRF7OMARzTPE9RQRwNN8h7hEU94tupWbOXW7WeqVTCaPXxb1sMHq1mQVw==</latexit>
(a)

D2
<latexit sha1_base64="HhKHPaVbmtqywlN86XMLNoyQHKo=">AAAC1HicjVHLSsNAFD2Nr1ofrbp0EyyCq5KIoMuCLlxWsA9oS5lMp21oXiQTodSuxK0/4Fa/SfwD/QvvjCmoRXRCkjPnnnNn7r1O5LmJtKzXnLG0vLK6ll8vbGxubRdLO7uNJExjLuo89MK45bBEeG4g6tKVnmhFsWC+44mmMz5X8eaNiBM3DK7lJBJdnw0Dd+ByJonqlYodn8kRZ970YtabHs96pbJVsfQyF4GdgTKyVQtLL+igjxAcKXwIBJCEPTAk9LRhw0JEXBdT4mJCro4LzFAgb0oqQQpG7Ji+Q9q1MzagvcqZaDenUzx6Y3KaOCRPSLqYsDrN1PFUZ1bsb7mnOqe624T+TpbLJ1ZiROxfvrnyvz5Vi8QAZ7oGl2qKNKOq41mWVHdF3dz8UpWkDBFxCvcpHhPm2jnvs6k9ia5d9Zbp+JtWKlbteaZN8a5uSQO2f45zETSOK7ZVsa9OytWTbNR57OMARzTPU1RxiRrqeuaPeMKz0TBujTvj/lNq5DLPHr4t4+EDTJSVrw==</latexit>

X1!,22 <latexit sha1_base64="Qfr5UDiqRGUmRNNary28WuS2NqM=">AAAC5nicjVHLSsQwFD3W1/gedemmOgguZGgHQZcDblwqODrgqKQxzgT7Ik0VKbN2507c+gNu9VPEP9C/8CZW8IFoSpOTc+85yc0N0lBm2vOeB5zBoeGR0crY+MTk1PRMdXZuL0tyxUWLJ2Gi2gHLRChj0dJSh6KdKsGiIBT7wdmmie+fC5XJJN7Vl6k4jFg3lqeSM03UcXWxEzHd4yws2v2joqNkt6eZUsmF2+gfF/4qzdWaV/fscH8CvwQ1lGM7qT6hgxMk4MgRQSCGJhyCIaPvAD48pMQdoiBOEZI2LtDHOGlzyhKUwYg9o7lLu4OSjWlvPDOr5nRKSL8ipYtl0iSUpwib01wbz62zYX/zLqynudslrUHpFRGr0SP2L91H5n91phaNU2zYGiTVlFrGVMdLl9y+irm5+6kqTQ4pcQafUFwR5lb58c6u1WS2dvO2zMZfbKZhzZ6XuTlezS2pwf73dv4Ee42679X9nbVac61sdQULWMIK9XMdTWxhGy3yvsI9HvDo9Jxr58a5fU91BkrNPL4M5+4NqTedIQ==</latexit>

X1 <latexit sha1_base64="SeadpDWXhuEGM3q8JvmENjkz7Cs=">AAAC1HicjVHLSsNAFD2Nr1ofrbp0EyyCq5KIqMuCG5cV7APaUibTaRuaF8lEKLUrcesPuNVvEv9A/8I7YwpqEZ2Q5My559yZe68TeW4iLes1Zywtr6yu5dcLG5tb28XSzm4jCdOYizoPvTBuOSwRnhuIunSlJ1pRLJjveKLpjC9UvHkj4sQNg2s5iUTXZ8PAHbicSaJ6pWLHZ3LEmTdtzXpTe9Yrla2KpZe5COwMlJGtWlh6QQd9hOBI4UMggCTsgSGhpw0bFiLiupgSFxNydVxghgJ5U1IJUjBix/Qd0q6dsQHtVc5Euzmd4tEbk9PEIXlC0sWE1Wmmjqc6s2J/yz3VOdXdJvR3slw+sRIjYv/yzZX/9alaJAY41zW4VFOkGVUdz7Kkuivq5uaXqiRliIhTuE/xmDDXznmfTe1JdO2qt0zH37RSsWrPM22Kd3VLGrD9c5yLoHFcsa2KfXVSrp5mo85jHwc4onmeoYpL1FDXM3/EE56NhnFr3Bn3n1Ijl3n28G0ZDx96sZXE</latexit>
X2 <latexit sha1_base64="n0JOMKx5ZhVYJ3GEJIGAM0Sj32I=">AAAC1HicjVHLSsNAFD2Nr1ofrbp0EyyCq5IUUZcFNy4r2AfUUibTaRuaF8lEKLUrcesPuNVvEv9A/8I7YwpqEZ2Q5My559yZe68TeW4iLes1Zywtr6yu5dcLG5tb28XSzm4zCdOYiwYPvTBuOywRnhuIhnSlJ9pRLJjveKLljM9VvHUj4sQNgys5iUTXZ8PAHbicSaJ6peK1z+SIM2/anvWm1VmvVLYqll7mIrAzUEa26mHpBdfoIwRHCh8CASRhDwwJPR3YsBAR18WUuJiQq+MCMxTIm5JKkIIRO6bvkHadjA1or3Im2s3pFI/emJwmDskTki4mrE4zdTzVmRX7W+6pzqnuNqG/k+XyiZUYEfuXb678r0/VIjHAma7BpZoizajqeJYl1V1RNze/VCUpQ0Scwn2Kx4S5ds77bGpPomtXvWU6/qaVilV7nmlTvKtb0oDtn+NcBM1qxbYq9uVxuXaSjTqPfRzgiOZ5ihouUEdDz/wRT3g2msatcWfcf0qNXObZw7dlPHwAfRKVxQ==</latexit>

Es <latexit sha1_base64="SqQR7zCkVpHqVx0Fg+D8/SFnezc=">AAACyHicjVHLSsNAFD2Nr1pfVZdugkVwVRIp6LIggriqYFqhlpJMp3VomoTMRCmlG3/ArX6Z+Af6F94ZU1CL6IQkZ86958zce4MkFFI5zmvBWlhcWl4prpbW1jc2t8rbO00ZZynjHovDOL0OfMlDEXFPCRXy6yTl/igIeSsYnup4646nUsTRlRonvDPyB5HoC+Yroryz7kROu+WKU3XMsueBm4MK8tWIyy+4QQ8xGDKMwBFBEQ7hQ9LThgsHCXEdTIhLCQkT55iiRNqMsjhl+MQO6TugXTtnI9prT2nUjE4J6U1JaeOANDHlpYT1abaJZ8ZZs795T4ynvtuY/kHuNSJW4ZbYv3SzzP/qdC0KfZyYGgTVlBhGV8dyl8x0Rd/c/lKVIoeEOI17FE8JM6Oc9dk2Gmlq1731TfzNZGpW71mem+Fd35IG7P4c5zxoHlVdp+pe1ir1Wj7qIvawj0Oa5zHqOEcDHnkLPOIJz9aFlVj31vgz1Srkml18W9bDB8hokS4=</latexit>

Xn <latexit sha1_base64="NQPR2moZQqrbVLEzbzJyrkcaJIY=">AAAC1HicjVHLSsNAFD2Nr1ofrbp0EyyCq5KIqMuCG5cV7APaUibTaRuaF8lEKLUrcesPuNVvEv9A/8I7YwpqEZ2Q5My559yZe68TeW4iLes1Zywtr6yu5dcLG5tb28XSzm4jCdOYizoPvTBuOSwRnhuIunSlJ1pRLJjveKLpjC9UvHkj4sQNg2s5iUTXZ8PAHbicSaJ6pWLHZ3LEmTdtzXrTYNYrla2KpZe5COwMlJGtWlh6QQd9hOBI4UMggCTsgSGhpw0bFiLiupgSFxNydVxghgJ5U1IJUjBix/Qd0q6dsQHtVc5Euzmd4tEbk9PEIXlC0sWE1Wmmjqc6s2J/yz3VOdXdJvR3slw+sRIjYv/yzZX/9alaJAY41zW4VFOkGVUdz7Kkuivq5uaXqiRliIhTuE/xmDDXznmfTe1JdO2qt0zH37RSsWrPM22Kd3VLGrD9c5yLoHFcsa2KfXVSrp5mo85jHwc4onmeoYpL1FDXM3/EE56NhnFr3Bn3n1Ijl3n28G0ZDx8L3ZYB</latexit>

zs <latexit sha1_base64="lEIbb37npPOooBbM7q+xdtIDfGE=">AAACxnicjVHLSsNAFD2Nr/quunQTLIKrkkhBlwU3XVa0D6ilJOm0Ds2LzESpRfAH3OqniX+gf+GdcQpqEZ2Q5My595yZe6+fhlxIx3ktWAuLS8srxdW19Y3Nre3Szm5LJHkWsGaQhEnW8T3BQh6zpuQyZJ00Y17kh6ztj89UvH3DMsGT+FJOUtaLvFHMhzzwJFEXd33RL5WdiqOXPQ9cA8owq5GUXnCFARIEyBGBIYYkHMKDoKcLFw5S4nqYEpcR4jrOcI810uaUxSjDI3ZM3xHtuoaNaa88hVYHdEpIb0ZKG4ekSSgvI6xOs3U8186K/c17qj3V3Sb0941XRKzENbF/6WaZ/9WpWiSGONU1cKop1YyqLjAuue6Kurn9pSpJDilxCg8onhEOtHLWZ1trhK5d9dbT8TedqVi1D0xujnd1Sxqw+3Oc86B1XHGdinteLdeqZtRF7OMARzTPE9RQRwNN8h7hEU94tupWbOXW7WeqVTCaPXxb1sMHq1mQVw==</latexit>
(b)

...
... ... ...

D1
<latexit sha1_base64="1AUbF3IWKP1stROBlfrPYVeRteo=">AAAC1HicjVHLSsNAFD2Nr1ofrbp0EyyCq5JIQZcFXbisYB9QS5lMp21oXiQTodSuxK0/4Fa/SfwD/QvvjCmoRXRCkjPnnnNn7r1O5LmJtKzXnLG0vLK6ll8vbGxubRdLO7vNJExjLho89MK47bBEeG4gGtKVnmhHsWC+44mWMz5T8daNiBM3DK7kJBJdnw0Dd+ByJonqlYrXPpMjzrzp+aw3tWe9UtmqWHqZi8DOQBnZqoelF1yjjxAcKXwIBJCEPTAk9HRgw0JEXBdT4mJCro4LzFAgb0oqQQpG7Ji+Q9p1MjagvcqZaDenUzx6Y3KaOCRPSLqYsDrN1PFUZ1bsb7mnOqe624T+TpbLJ1ZiROxfvrnyvz5Vi8QAp7oGl2qKNKOq41mWVHdF3dz8UpWkDBFxCvcpHhPm2jnvs6k9ia5d9Zbp+JtWKlbteaZN8a5uSQO2f45zETSPK7ZVsS+r5Vo1G3Ue+zjAEc3zBDVcoI6GnvkjnvBsNI1b4864/5Qaucyzh2/LePgASjOVrg==</latexit>

Xd!1 <latexit sha1_base64="fI+M5kqkUDTOxaEILjfNZ7c8V/Y=">AAAC5HicjVHLSsNAFD3G97vq0oXBIrgqiRR0KbhxqWC1YLVMptN2MC8mE0VCl+7ciVt/wK1+i/gH+hfeGSP4QHRCkjPn3nNm7r1BGspMe97zkDM8Mjo2PjE5NT0zOzdfWVg8zJJccdHgSZioZsAyEcpYNLTUoWimSrAoCMVRcLZj4kfnQmUyiQ/0ZSpOItaLZVdypolqV1ZaEdN9zsKiOTgtWkr2+poplVy4/qBddAbtStWreXa5P4FfgirKtZdUntBCBwk4ckQQiKEJh2DI6DmGDw8pcScoiFOEpI0LDDBF2pyyBGUwYs/o26PdccnGtDeemVVzOiWkV5HSxRppEspThM1pro3n1tmwv3kX1tPc7ZL+QekVEavRJ/Yv3Ufmf3WmFo0utmwNkmpKLWOq46VLbrtibu5+qkqTQ0qcwR2KK8LcKj/67FpNZms3vWU2/mIzDWv2vMzN8WpuSQP2v4/zJzjcqPlezd+vV7fr5agnsIxVrNM8N7GNXeyhQd5XuMcDHp2uc+3cOLfvqc5QqVnCl+XcvQHcWJzh</latexit>

D2
<latexit sha1_base64="HhKHPaVbmtqywlN86XMLNoyQHKo=">AAAC1HicjVHLSsNAFD2Nr1ofrbp0EyyCq5KIoMuCLlxWsA9oS5lMp21oXiQTodSuxK0/4Fa/SfwD/QvvjCmoRXRCkjPnnnNn7r1O5LmJtKzXnLG0vLK6ll8vbGxubRdLO7uNJExjLuo89MK45bBEeG4g6tKVnmhFsWC+44mmMz5X8eaNiBM3DK7lJBJdnw0Dd+ByJonqlYodn8kRZ970YtabHs96pbJVsfQyF4GdgTKyVQtLL+igjxAcKXwIBJCEPTAk9LRhw0JEXBdT4mJCro4LzFAgb0oqQQpG7Ji+Q9q1MzagvcqZaDenUzx6Y3KaOCRPSLqYsDrN1PFUZ1bsb7mnOqe624T+TpbLJ1ZiROxfvrnyvz5Vi8QAZ7oGl2qKNKOq41mWVHdF3dz8UpWkDBFxCvcpHhPm2jnvs6k9ia5d9Zbp+JtWKlbteaZN8a5uSQO2f45zETSOK7ZVsa9OytWTbNR57OMARzTPU1RxiRrqeuaPeMKz0TBujTvj/lNq5DLPHr4t4+EDTJSVrw==</latexit>

Xd!2 <latexit sha1_base64="YkAMR43i3H2udZwLYXinUCgqJUA=">AAAC5HicjVHLSsNAFD3G97vq0oXBIrgqiRR0KbhxqWC1YLVMptN2MC8mE0VCl+7ciVt/wK1+i/gH+hfeGSP4QHRCkjPn3nNm7r1BGspMe97zkDM8Mjo2PjE5NT0zOzdfWVg8zJJccdHgSZioZsAyEcpYNLTUoWimSrAoCMVRcLZj4kfnQmUyiQ/0ZSpOItaLZVdypolqV1ZaEdN9zsKiOTgtWkr2+poplVy4G4N20Rm0K1Wv5tnl/gR+Caoo115SeUILHSTgyBFBIIYmHIIho+cYPjykxJ2gIE4RkjYuMMAUaXPKEpTBiD2jb492xyUb0954ZlbN6ZSQXkVKF2ukSShPETanuTaeW2fD/uZdWE9zt0v6B6VXRKxGn9i/dB+Z/9WZWjS62LI1SKoptYypjpcuue2Kubn7qSpNDilxBncorghzq/zos2s1ma3d9JbZ+IvNNKzZ8zI3x6u5JQ3Y/z7On+Bwo+Z7NX+/Xt2ul6OewDJWsU7z3MQ2drGHBnlf4R4PeHS6zrVz49y+pzpDpWYJX5Zz9wbevZzi</latexit>

Dn
<latexit sha1_base64="EKRUKZd1prx3s25mPr+M0ehGGrU=">AAAC1HicjVHLSsNAFD2Nr1ofrbp0EyyCq5JIQZcFXbisYB9QS5lMp21oXiQTodSuxK0/4Fa/SfwD/QvvjCmoRXRCkjPnnnNn7r1O5LmJtKzXnLG0vLK6ll8vbGxubRdLO7vNJExjLho89MK47bBEeG4gGtKVnmhHsWC+44mWMz5T8daNiBM3DK7kJBJdnw0Dd+ByJonqlYrXPpMjzrzp+aw3DWa9UtmqWHqZi8DOQBnZqoelF1yjjxAcKXwIBJCEPTAk9HRgw0JEXBdT4mJCro4LzFAgb0oqQQpG7Ji+Q9p1MjagvcqZaDenUzx6Y3KaOCRPSLqYsDrN1PFUZ1bsb7mnOqe624T+TpbLJ1ZiROxfvrnyvz5Vi8QAp7oGl2qKNKOq41mWVHdF3dz8UpWkDBFxCvcpHhPm2jnvs6k9ia5d9Zbp+JtWKlbteaZN8a5uSQO2f45zETSPK7ZVsS+r5Vo1G3Ue+zjAEc3zBDVcoI6GnvkjnvBsNI1b4864/5Qaucyzh2/LePgA21CV6w==</latexit>

Xd!n <latexit sha1_base64="Ttb1fMZFZ6zdeQMZdSI6ROMRoUM=">AAAC5HicjVHLSsNAFD3G97vq0oXBIrgqiRR0KbhxqWC1YLVMptN2MC8mE0VCl+7ciVt/wK1+i/gH+hfeGSP4QHRCkjPn3nNm7r1BGspMe97zkDM8Mjo2PjE5NT0zOzdfWVg8zJJccdHgSZioZsAyEcpYNLTUoWimSrAoCMVRcLZj4kfnQmUyiQ/0ZSpOItaLZVdypolqV1ZaEdN9zsKiOTgtWkr2+poplVy48aBddAbtStWreXa5P4FfgirKtZdUntBCBwk4ckQQiKEJh2DI6DmGDw8pcScoiFOEpI0LDDBF2pyyBGUwYs/o26PdccnGtDeemVVzOiWkV5HSxRppEspThM1pro3n1tmwv3kX1tPc7ZL+QekVEavRJ/Yv3Ufmf3WmFo0utmwNkmpKLWOq46VLbrtibu5+qkqTQ0qcwR2KK8LcKj/67FpNZms3vWU2/mIzDWv2vMzN8WpuSQP2v4/zJzjcqPlezd+vV7fr5agnsIxVrNM8N7GNXeyhQd5XuMcDHp2uc+3cOLfvqc5QqVnCl+XcvQFueJ0e</latexit>

d1 <latexit sha1_base64="6fJs7wlJQOL6WVPfW9obwGNWGCM=">AAACxnicjVHLSsNAFD2Nr1pfVZdugkVwVRIp6LLgpsuK9gG1lCSd1qF5MZkopQj+gFv9NPEP9C+8M05BLaITkpw5954zc+/105Bn0nFeC9bS8srqWnG9tLG5tb1T3t1rZ0kuAtYKkjARXd/LWMhj1pJchqybCuZFfsg6/uRcxTu3TGQ8ia/kNGX9yBvHfMQDTxJ1ORy4g3LFqTp62YvANaACs5pJ+QXXGCJBgBwRGGJIwiE8ZPT04MJBSlwfM+IEIa7jDPcokTanLEYZHrET+o5p1zNsTHvlmWl1QKeE9ApS2jgiTUJ5grA6zdbxXDsr9jfvmfZUd5vS3zdeEbESN8T+pZtn/lenapEY4UzXwKmmVDOqusC45Lor6ub2l6okOaTEKTykuCAcaOW8z7bWZLp21VtPx990pmLVPjC5Od7VLWnA7s9xLoL2SdV1qu5FrVKvmVEXcYBDHNM8T1FHA020yHuMRzzh2WpYsZVbd5+pVsFo9vFtWQ8f2h6P/w==</latexit>
X1 <latexit sha1_base64="SeadpDWXhuEGM3q8JvmENjkz7Cs=">AAAC1HicjVHLSsNAFD2Nr1ofrbp0EyyCq5KIqMuCG5cV7APaUibTaRuaF8lEKLUrcesPuNVvEv9A/8I7YwpqEZ2Q5My559yZe68TeW4iLes1Zywtr6yu5dcLG5tb28XSzm4jCdOYizoPvTBuOSwRnhuIunSlJ1pRLJjveKLpjC9UvHkj4sQNg2s5iUTXZ8PAHbicSaJ6pWLHZ3LEmTdtzXpTe9Yrla2KpZe5COwMlJGtWlh6QQd9hOBI4UMggCTsgSGhpw0bFiLiupgSFxNydVxghgJ5U1IJUjBix/Qd0q6dsQHtVc5Euzmd4tEbk9PEIXlC0sWE1Wmmjqc6s2J/yz3VOdXdJvR3slw+sRIjYv/yzZX/9alaJAY41zW4VFOkGVUdz7Kkuivq5uaXqiRliIhTuE/xmDDXznmfTe1JdO2qt0zH37RSsWrPM22Kd3VLGrD9c5yLoHFcsa2KfXVSrp5mo85jHwc4onmeoYpL1FDXM3/EE56NhnFr3Bn3n1Ijl3n28G0ZDx96sZXE</latexit>
X2 <latexit sha1_base64="n0JOMKx5ZhVYJ3GEJIGAM0Sj32I=">AAAC1HicjVHLSsNAFD2Nr1ofrbp0EyyCq5IUUZcFNy4r2AfUUibTaRuaF8lEKLUrcesPuNVvEv9A/8I7YwpqEZ2Q5My559yZe68TeW4iLes1Zywtr6yu5dcLG5tb28XSzm4zCdOYiwYPvTBuOywRnhuIhnSlJ9pRLJjveKLljM9VvHUj4sQNgys5iUTXZ8PAHbicSaJ6peK1z+SIM2/anvWm1VmvVLYqll7mIrAzUEa26mHpBdfoIwRHCh8CASRhDwwJPR3YsBAR18WUuJiQq+MCMxTIm5JKkIIRO6bvkHadjA1or3Im2s3pFI/emJwmDskTki4mrE4zdTzVmRX7W+6pzqnuNqG/k+XyiZUYEfuXb678r0/VIjHAma7BpZoizajqeJYl1V1RNze/VCUpQ0Scwn2Kx4S5ds77bGpPomtXvWU6/qaVilV7nmlTvKtb0oDtn+NcBM1qxbYq9uVxuXaSjTqPfRzgiOZ5ihouUEdDz/wRT3g2msatcWfcf0qNXObZw7dlPHwAfRKVxQ==</latexit>

Es <latexit sha1_base64="SqQR7zCkVpHqVx0Fg+D8/SFnezc=">AAACyHicjVHLSsNAFD2Nr1pfVZdugkVwVRIp6LIggriqYFqhlpJMp3VomoTMRCmlG3/ArX6Z+Af6F94ZU1CL6IQkZ86958zce4MkFFI5zmvBWlhcWl4prpbW1jc2t8rbO00ZZynjHovDOL0OfMlDEXFPCRXy6yTl/igIeSsYnup4646nUsTRlRonvDPyB5HoC+Yroryz7kROu+WKU3XMsueBm4MK8tWIyy+4QQ8xGDKMwBFBEQ7hQ9LThgsHCXEdTIhLCQkT55iiRNqMsjhl+MQO6TugXTtnI9prT2nUjE4J6U1JaeOANDHlpYT1abaJZ8ZZs795T4ynvtuY/kHuNSJW4ZbYv3SzzP/qdC0KfZyYGgTVlBhGV8dyl8x0Rd/c/lKVIoeEOI17FE8JM6Oc9dk2Gmlq1731TfzNZGpW71mem+Fd35IG7P4c5zxoHlVdp+pe1ir1Wj7qIvawj0Oa5zHqOEcDHnkLPOIJz9aFlVj31vgz1Srkml18W9bDB8hokS4=</latexit>

Xn <latexit sha1_base64="NQPR2moZQqrbVLEzbzJyrkcaJIY=">AAAC1HicjVHLSsNAFD2Nr1ofrbp0EyyCq5KIqMuCG5cV7APaUibTaRuaF8lEKLUrcesPuNVvEv9A/8I7YwpqEZ2Q5My559yZe68TeW4iLes1Zywtr6yu5dcLG5tb28XSzm4jCdOYizoPvTBuOSwRnhuIunSlJ1pRLJjveKLpjC9UvHkj4sQNg2s5iUTXZ8PAHbicSaJ6pWLHZ3LEmTdtzXrTYNYrla2KpZe5COwMlJGtWlh6QQd9hOBI4UMggCTsgSGhpw0bFiLiupgSFxNydVxghgJ5U1IJUjBix/Qd0q6dsQHtVc5Euzmd4tEbk9PEIXlC0sWE1Wmmjqc6s2J/yz3VOdXdJvR3slw+sRIjYv/yzZX/9alaJAY41zW4VFOkGVUdz7Kkuivq5uaXqiRliIhTuE/xmDDXznmfTe1JdO2qt0zH37RSsWrPM22Kd3VLGrD9c5yLoHFcsa2KfXVSrp5mo85jHwc4onmeoYpL1FDXM3/EE56NhnFr3Bn3n1Ijl3n28G0ZDx8L3ZYB</latexit>

{Ci} <latexit sha1_base64="YR1fUkKAW6RaUueeLZLXbauQ6JI=">AAAC1nicjVHLSsNAFD2Nr1pfUZdugkVwVRIp6LLgxmUFq4W2lMl01MG8SCZKKXUnbv0Bt/pJ4h/oX3hnTEEtohOSnDn3njNz7/WTQGbKdV9L1szs3PxCebGytLyyumavb5xmcZ5y0eJxEKdtn2UikJFoKakC0U5SwUI/EGf+1aGOn12LNJNxdKKGieiF7CKS55IzRVTfXu+OuiFTl5wFo8NxX3bHfbvq1lyznGngFaCKYjVj+wVdDBCDI0cIgQiKcACGjJ4OPLhIiOthRFxKSJq4wBgV0uaUJSiDEXtF3wvadQo2or32zIya0ykBvSkpHeyQJqa8lLA+zTHx3Dhr9jfvkfHUdxvS3y+8QmIVLon9SzfJ/K9O16JwjgNTg6SaEsPo6njhkpuu6Js7X6pS5JAQp/GA4ilhbpSTPjtGk5nadW+Zib+ZTM3qPS9yc7zrW9KAvZ/jnAanezXPrXnH9WqjXoy6jC1sY5fmuY8GjtBEi7xv8IgnPFtt69a6s+4/U61SodnEt2U9fADa4Jax</latexit>
zs <latexit sha1_base64="lEIbb37npPOooBbM7q+xdtIDfGE=">AAACxnicjVHLSsNAFD2Nr/quunQTLIKrkkhBlwU3XVa0D6ilJOm0Ds2LzESpRfAH3OqniX+gf+GdcQpqEZ2Q5My595yZe6+fhlxIx3ktWAuLS8srxdW19Y3Nre3Szm5LJHkWsGaQhEnW8T3BQh6zpuQyZJ00Y17kh6ztj89UvH3DMsGT+FJOUtaLvFHMhzzwJFEXd33RL5WdiqOXPQ9cA8owq5GUXnCFARIEyBGBIYYkHMKDoKcLFw5S4nqYEpcR4jrOcI810uaUxSjDI3ZM3xHtuoaNaa88hVYHdEpIb0ZKG4ekSSgvI6xOs3U8186K/c17qj3V3Sb0941XRKzENbF/6WaZ/9WpWiSGONU1cKop1YyqLjAuue6Kurn9pSpJDilxCg8onhEOtHLWZ1trhK5d9dbT8TedqVi1D0xujnd1Sxqw+3Oc86B1XHGdinteLdeqZtRF7OMARzTPE9RQRwNN8h7hEU94tupWbOXW7WeqVTCaPXxb1sMHq1mQVw==</latexit>
(c)

Ds
<latexit sha1_base64="g4BQoykbeqVe8I7FM/iRLaennzQ=">AAAC0HicjVHLSsNAFD2Nr1pfVZdugkVwVRIRdFnQhcsq9gG2lMl02obmZTIRSyni1h9wq18l/oH+hXfGFNQiOiHJmXPvOTP3Xify3ERa1mvOmJtfWFzKLxdWVtfWN4qbW/UkTGMuajz0wrjpsER4biBq0pWeaEaxYL7jiYYzPFHxxo2IEzcMLuUoEm2f9QO353ImiWq3fCYHnHnj00kn6RRLVtnSy5wFdgZKyFY1LL6ghS5CcKTwIRBAEvbAkNBzBRsWIuLaGBMXE3J1XGCCAmlTyhKUwYgd0rdPu6uMDWivPBOt5nSKR29MShN7pAkpLyasTjN1PNXOiv3Ne6w91d1G9HcyL59YiQGxf+mmmf/VqVokejjWNbhUU6QZVR3PXFLdFXVz80tVkhwi4hTuUjwmzLVy2mdTaxJdu+ot0/E3nalYtedZbop3dUsasP1znLOgflC2rbJ9fliqHGajzmMHu9ineR6hgjNUUSPvazziCc/GhXFr3Bn3n6lGLtNs49syHj4AsPKUsw==</latexit>

d2 <latexit sha1_base64="neWnGPPZLyr8FCq41KbGmfODoKU=">AAACxnicjVHLSsNAFD2Nr1pfVZdugkVwVZJS0GXBTZcV7QO0lCSd1qF5MZkopQj+gFv9NPEP9C+8M05BLaITkpw5954zc+/105Bn0nFeC9bS8srqWnG9tLG5tb1T3t3rZEkuAtYOkjARPd/LWMhj1pZchqyXCuZFfsi6/uRMxbu3TGQ8iS/lNGX9yBvHfMQDTxJ1MRzUBuWKU3X0sheBa0AFZrWS8guuMUSCADkiMMSQhEN4yOi5ggsHKXF9zIgThLiOM9yjRNqcshhleMRO6Dum3ZVhY9orz0yrAzolpFeQ0sYRaRLKE4TVabaO59pZsb95z7SnutuU/r7xioiVuCH2L9088786VYvECKe6Bk41pZpR1QXGJdddUTe3v1QlySElTuEhxQXhQCvnfba1JtO1q956Ov6mMxWr9oHJzfGubkkDdn+OcxF0alXXqbrn9UqjbkZdxAEOcUzzPEEDTbTQJu8xHvGEZ6tpxVZu3X2mWgWj2ce3ZT18ANx+kAA=</latexit>
Xd!1 1 <latexit sha1_base64="g4RWoZ74jvXRUYPh+bKb9fevndc=">AAAC5nicjVHLSsNAFD3GV31XXbqJFsFVSUTQpeDGpYKtBatlMh3bwbyYTBQJXbtzJ279Abf6KeIf6F94Z4zgA9EJSc6ce8+ZufcGaSgz7XnPQ87wyOjYeGVicmp6ZnauOr/QzJJccdHgSZioVsAyEcpYNLTUoWilSrAoCMVhcLZj4ofnQmUyiQ/0ZSqOI9aL5ankTBPVqS63I6b7nIVFa9Apuh1/cFK0lez1NVMquXD9Qada8+qeXe5P4JeghnLtJdUntNFFAo4cEQRiaMIhGDJ6juDDQ0rcMQriFCFp4wIDTJI2pyxBGYzYM/r2aHdUsjHtjWdm1ZxOCelVpHSxSpqE8hRhc5pr47l1Nuxv3oX1NHe7pH9QekXEavSJ/Uv3kflfnalF4xRbtgZJNaWWMdXx0iW3XTE3dz9VpckhJc7gLsUVYW6VH312rSaztZveMht/sZmGNXte5uZ4NbekAfvfx/kTNNfrvlf39zdq2xvlqCtYwgrWaJ6b2MYu9tAg7yvc4wGPTt+5dm6c2/dUZ6jULOLLcu7eAJalnYU=</latexit>
Xd!1 2 <latexit sha1_base64="fwUFlUSlBLHWwofMXBKBO0wmbno=">AAAC5nicjVHLSsNAFD3GV31XXbqJFsFVSUTQpeDGpYKtBatlMh3bwbyYTBQJXbtzJ279Abf6KeIf6F94Z4zgA9EJSc6ce8+ZufcGaSgz7XnPQ87wyOjYeGVicmp6ZnauOr/QzJJccdHgSZioVsAyEcpYNLTUoWilSrAoCMVhcLZj4ofnQmUyiQ/0ZSqOI9aL5ankTBPVqS63I6b7nIVFa9Apuh1/cFK0lez1NVMquXDXB51qzat7drk/gV+CGsq1l1Sf0EYXCThyRBCIoQmHYMjoOYIPDylxxyiIU4SkjQsMMEnanLIEZTBiz+jbo91Ryca0N56ZVXM6JaRXkdLFKmkSylOEzWmujefW2bC/eRfW09ztkv5B6RURq9En9i/dR+Z/daYWjVNs2Rok1ZRaxlTHS5fcdsXc3P1UlSaHlDiDuxRXhLlVfvTZtZrM1m56y2z8xWYa1ux5mZvj1dySBux/H+dP0Fyv+17d39+obW+Uo65gCStYo3luYhu72EODvK9wjwc8On3n2rlxbt9TnaFSs4gvy7l7A5kGnYY=</latexit>

Xd!1 n <latexit sha1_base64="/VTRcaQJD4amTsXI1ymGhR1H9Ow=">AAAC5nicjVHLSsNAFD3GV31XXbqJFsFVSUTQpeDGpYKtBatlMh3bwbyYTBQJXbtzJ279Abf6KeIf6F94Z4zgA9EJSc6ce8+ZufcGaSgz7XnPQ87wyOjYeGVicmp6ZnauOr/QzJJccdHgSZioVsAyEcpYNLTUoWilSrAoCMVhcLZj4ofnQmUyiQ/0ZSqOI9aL5ankTBPVqS63I6b7nIVFa9Apuh1/cFK0lez1NVMquXDjQada8+qeXe5P4JeghnLtJdUntNFFAo4cEQRiaMIhGDJ6juDDQ0rcMQriFCFp4wIDTJI2pyxBGYzYM/r2aHdUsjHtjWdm1ZxOCelVpHSxSpqE8hRhc5pr47l1Nuxv3oX1NHe7pH9QekXEavSJ/Uv3kflfnalF4xRbtgZJNaWWMdXx0iW3XTE3dz9VpckhJc7gLsUVYW6VH312rSaztZveMht/sZmGNXte5uZ4NbekAfvfx/kTNNfrvlf39zdq2xvlqCtYwgrWaJ6b2MYu9tAg7yvc4wGPTt+5dm6c2/dUZ6jULOLLcu7eACfRncI=</latexit>

Figure 1: Different approaches to domain translation. (a) One-to-one transfer models such as UNIT are restricted to domain pairs. (b) One-to-many transfer models such as UMT require to train a different decoder for each domain. (c) Our proposed many-to-many transfer model (MoVE) allows to perform multi-domain transfer with a single encoder and decoder, while providing control over the generation with external high-level conditioning variables.

Audio translations. Recent applications of generative models to audio have shown promising results, notably supported by solutions to efficiently generate waveforms such as Wavenet (Van Den Oord et al. (2016)) or SampleRNN (Mehri et al. (2017)). Most of these proposals target voice signals and there is still a large gap when addressing musical data. Some approaches have tackled musical style transfer (Verma & Smith (2018)). However, as pointed by Dai et al. (2018), musical style is a multimodal and multi-scale notion, which implies a variety of underlying factors. Specifically for domain translation, Mor et al. (2018) proposed a Universal Music Translation (UMT) network that globally translates musical recordings between different genres and instrument domains. Using a single Wavenet encoder and separate decoders for each domain, this approach is able to transform a given melody so that it is played by different instruments. By design, this method requires to train a specific Wavenet decoder for each of the target domain. It does not provide control over audio synthesis and the learned representation does not allow direct visualization nor transfer of only specific parts of timbre attributes. Hence, it does not enable informed generative processes, musical interaction and creativity. Our proposal targets 3-dimensional latent spaces supporting timbre transfer and continuous synthesis paths with explicit control over musical attributes.
3 MUSICAL TIMBRE TRANSFER
Musical timbre can be defined as the set of auditory qualities that distinguishes two instruments playing the same note at the same loudness. Seminal studies relying on human dissimilarity ratings provided an interesting step towards understanding music perception (McAdams et al. (1995)). However, the ordination techniques used yield non-invertible and fixed timbre spaces. Hence, they do not support audio synthesis nor do they provide a way to manipulate timbre structures. Signal processing techniques have also been developed to process and alter timbre. However, these rely on complex analysis schemes that decompose sounds into large sets of parameters (Serra (1997)), precluding intuitive control over the audio synthesis process.
Here, we propose to use generative models in order to perform musical timbre transfer. In order to circumvent the complexity of defining timbre, our underlying hypothesis is that each instrument defines a timbral domain, which contains all style qualities that shape its identity. Musical timbre transfer can be achieved by transforming a certain amount of the auditory features of a musical instrument according to another (eg. like playing a saxophone with a bow). Transferring all timbre properties of an instrument leads to domain translation, while partial modification of these amounts to style transfer. Furthermore, our goal is to obtain a controllable model that can be used for creative purposes. Hence, we aim to obtain 3-dimensional latent spaces along with high-level musical parameters that enable human interaction and control over the generation.
This type of transfer can be performed in several ways, as depicted in Fig. 1. First, one-to-one transfer models such as UNIT map samples from a given pair of domains to a shared latent space. By learning separate layers and weight-shared layers in both the decoder and encoder, domain translation can be assessed through adversarial discriminators. We first adapt this model to timbre transfer and show that we can alleviate the need for GAN training by using an alternative MMD objective.
3

Under review as a conference paper at ICLR 2019

It leads to a faster and more stable learning that we further enhance by modulating shared layers with FiLM layers (Perez et al. (2017)) on pitch and octave. This provides an explicit control over generation, altering or not the pitch regardless of timbre. The one-to-many transfer models (UMT) allow to work with multiple domains but require to learn a different decoder for each. This leads to a more complex and longer training and reduces the generalization ability of the model gained through multi-task learning. Here, we show that our proposed Modulated Variational auto-Encoder (MoVE) allows to perform many-to-many transfers with a single VAE simultaneously processing all domains. The success of our solution relies on an efficient domain conditioning, together with external control variables, performed through FiLM layers acting on the whole network. This solution offers a greater generalization power by jointly learning all transfer tasks within a single architecture. The resulting latent space successfully models joint and conditional distributions over several instrument domains. This also enables control with semantic labels, while providing interactive 3-dimensional spaces to synthesize novel tones from a reduced set of control parameters.

3.1 ONE-TO-ONE TRANSFER

Our one-to-one transfer model is based on an architecture similar to UNIT (Liu et al. (2017)) where
the core idea is to learn a latent space that is shared between two domains X1 and X2. Based on samples x1  X1 and x2  X2, we aim to model the joint distribution pX1,X2 (x1, x2) over the two domains. By learning domain-specific encoders E1 and E2, matching samples drawn from each marginal distribution pX1 (x1) and pX2 (x2) should map onto the same latent code z = E1(x1) = E2(x2). Equally, any latent code can be decoded back to any of the two domains d  {1, 2} by learning appropriate decoders xd = Dd(z). A paired VAE implements this assumption through separate domain-specific layers {ed ; dd} alternated with weight-shared ones {ews ; dws}. The full encoders and decoders are defined by the composition of both parts Ed = ews ed and Dd = dd dws.

Each VAE is trained with a reconstruction loss on its own domain, by approximating the intractable
latent conditional p(z|x) with a parametric encoding network q(z|x) with   . In comparison to UNIT, we both use a Gaussian encoder q and decoder p so that z  q(z|x) = N (µ(x), (x)) and x  p(x|z) = N (µ(z), (z)). Training the model amounts to optimize { ; } on the Evidence Lower Bound Objective (ELBO), defined as a Negative Log-Likelihood (NLL) term on
the output prediction error and a -weighted Kullback-Leibler Divergence (KLD) term that assesses
the error from the approximate latent density against the intractable true posterior distribution.

Lre,c. = Eq(z)[log p(x|z)] -   DKL[q(z|x) p(z)]

(1)

This inference objective allows to learn structured low-dimensional and invertible representations of the data, while disentangling generative factors in the encoded variables (Higgins et al. (2016)).

Translation is performed by switching domains between the encoding and decoding stages eg. x12 = D2  E1(x1). However, there is usually no matching sample x2 that could allow to perform the optimization of the reconstruction error L12,21 = err(x12 x2). To circumvent this challenge, UNIT relies on GANs to discriminate the generated translations against the target data distributions they model. However, this GAN criterion leads to a more complex and possibly unstable training process. Here, we show that we can efficiently replace the adversarial criterion by a differentiable distance measure on the probability distribution spaces. We minimize the Maximum Mean Discrepancy (MMD), a non-parametric kernel method (Gretton et al. (2012)), between the set of transferred samples x12  p2 (x|z, 1) and a randomly sampled set from the target domain x¯2  pX2

L12,21 = MMD[x12 x¯2] = Ex,x [k(x, x )] - 2  Ex,¯x[k(x, x¯)] + E¯x,¯x [k(x¯, ¯x )] {x, x }  x12 and {x¯, x¯ }  ¯x2

(2)

n

where k is a Radial Basis Functions (RBF) kernel k(x, x ) =

exp-i x-x

2
.

i=1

Reconstruction and translation objectives are jointly optimized with an extra circle-consistency (CC)
criterion. It consists in encoding a translated sample back to the latent space and decoding it to its source domain so that xcc1 = D1  E2(x12). Hence, this double translation should retrieve the initial sample and the reconstruction error can be optimized with a NLL loss.

Lcc11,2 = Eq2 (z|x12)[log p1 (x|z)]

(3)

4

Under review as a conference paper at ICLR 2019

Finally, the complete optimization objective is defined as

Ltr,ain = Lr1e,c11 + Lr2e,c22 + MMD(L12,21 + L21,12 ) + CC(Lcc11,2 + Lcc22,1 )

(4)

where MMD and CC allow to weigh the relative influence of different objectives. For the purpose of controllable musical timbre transfer, we further apply conditioning at the input of the weight-

shared networks by concatenating one-hot encoded pitch classes and octaves. This pushes the shared

encoder to structure note-agnostic features, while providing control over the generation.

3.2 MANY-TO-MANY TRANSFER

In order to alleviate the one-to-one limitation that requires a different training for each domain pair, we propose the single MoVE architecture as depicted in Figure 2. All layers are shared over the multiple domains processed, by learning a single modulated encoder Es and decoder Ds. Transfer is performed by switching the categorical condition between different instruments. Hence, the practical success of this method highly depends on the conditioning strategy, which must also retain the pitch and octave control. To do so, we use an input embedding that jointly maps these categorical conditions to dense vectors fed into FiLM generators. We replace each intermediate batch normalization with instance normalization and activation is followed by a FiLM modulation layer (conditional instance normalization). Biasing and scaling are either applied feature-wise for 1-dimensional activations or channel-wise after 2-dimensional feature maps. A different generator output is used for modulating each instance normalization layer depending on its shape. The MoVE model trains in reconstruction with the ELBO and in transfer with the MMD, which is separately computed for each instrument against each of the others.

00010

Domain

Input

FiLM generators

1 0 0 0 00 0 0 0 0 0 0 0 0 0 0 10 0 0 0
Generation controls

FiLM generators

Output

0100 0
Domain

µx
<latexit sha1_base64="j5tWunesIsBMaRLiIHTwonnxLH8=">AAAC3HicjVG5TsNAEH0xVwiXgYKCxiJCCk1kJ4RAh0RDCRI5JBIi22zCKr5krxEoSkeHaPkBWvgexB/AXzC7OBIUCNayPfPmvdk5nMjjiTDNt5w2NT0zO5efLywsLi2v6KtrzSRMY5c13NAL47ZjJ8zjAWsILjzWjmJm+47HWs7wSMZb1yxOeBiciduIdX17EPA+d21BUE/f6Phpx+GDUse3xZXTH92MpbvT04tm2VTHMMu16p5Vr5FRP6hWajXDykJFZOck1F/RwSVCuEjhgyGAINuDjYSec1gwERHWxYiwmCyu4gxjFEibEosRwyZ0SN8BeecZGpAvcyZK7dItHr0xKQ1skyYkXky2vM1Q8VRlluhvuUcqp6ztlv5OlssnVOCK0L90E+Z/dbIXgT72VQ+ceooUIrtzsyypmoqs3PjWlaAMEWHSvqR4TLarlJM5G0qTqN7lbG0Vf1dMiUrfzbgpPmSVtODJFo3fjWalbJll63S3eLifrTqPTWyhRPus4xDHOEFD1f+EZ7xoF9qddq89fFG1XKZZx4+jPX4CMPiZKw==</latexit>

MMD Criterion
FiLM Transpose conv.
FiLM FC layer
FiLM FC layer
FiLM Convolution layer

x <latexit sha1_base64="TDFz8zB+naaXmUIeMNEL8SN/TuA=">AAAC33icjVHLSsNAFD2Nr/quutNNsAi6KUlptd0V3LisaFWwIpM4bYfmRTIRSym4cydu/QG3+jfiH+hfeGdMQReiE5Kce+49Z+bOdSJPJNKy3nLGxOTU9Ex+dm5+YXFpubCyepKEaezylht6YXzmsIR7IuAtKaTHz6KYM9/x+KnT31f502seJyIMjuUg4hc+6waiI1wmibosrLePRNdnbUd0t9s+kz2nM7wZqXDnslC0SrWKXa3WTatkV3br9T0FyvVdu2baJUuvIrLVDAuvaOMKIVyk8MERQBL2wJDQcw4bFiLiLjAkLiYkdJ5jhDnSplTFqYIR26dvl6LzjA0oVp6JVru0i0dvTEoTW6QJqS4mrHYzdT7Vzor9zXuoPdXZBvR3Mi+fWIkesX/pxpX/1aleJDqo6R4E9RRpRnXnZi6pvhV1cvNbV5IcIuIUvqJ8TNjVyvE9m1qT6N7V3TKdf9eVilWxm9Wm+FCnpAGPp2j+Dk7KJZumflgpNmrZqPPYwCa2aZ57aOAATbTI+xZPeMaLwYw74954+Co1cplmDT+W8fgJlumafQ==</latexit>

x <latexit sha1_base64="rQfg3lTmF5JrkDIpa8LGUIBzY+M=">AAACzXicjVHLSsNAFD2Nr1pfVZdugkVwVRIRdFlw484K9oFVJInTdmheTCZiqXXrD7jV3xL/QP/CO+MU1CI6IcmZc+85M/dePw15Jh3ntWDNzM7NLxQXS0vLK6tr5fWNZpbkImCNIAkT0fa9jIU8Zg3JZcjaqWBe5Ies5Q+OVLx1w0TGk/hMDlN2GXm9mHd54Emizi8iT/b97uh2fFWuOFVHL3sauAZUYFY9Kb/gAtdIECBHBIYYknAIDxk9HbhwkBJ3iRFxghDXcYYxSqTNKYtRhkfsgL492nUMG9NeeWZaHdApIb2ClDZ2SJNQniCsTrN1PNfOiv3Ne6Q91d2G9PeNV0SsRJ/Yv3STzP/qVC0SXRzqGjjVlGpGVRcYl1x3Rd3c/lKVJIeUOIWvKS4IB1o56bOtNZmuXfXW0/E3nalYtQ9Mbo53dUsasPtznNOguVd1nap7ul+p1cyoi9jCNnZpngeo4Rh1NMg7xiOe8GydWLl1Z91/ploFo9nEt2U9fADHk5Ob</latexit>

2x

3x

q
<latexit sha1_base64="Zuxkv1bxiVjfk3oVqFQpldXwY6o=">AAAC6XicjVHLSsNAFD2N7/qKunQTrELdlEQEuyy4calgVTBSknTaDs3LZCLW0B9w507c+gNu9UfEP9C/8M6Ygg9EJyQ599x7zsyd68Y+T4VpvpS0sfGJyanpmfLs3PzCor60fJRGWeKxphf5UXLiOinzeciaggufncQJcwLXZ8duf1fmjy9YkvIoPBSDmJ0FTjfkHe45gqiWvn7eyu24x4dVO3BEz+3kV0PDDnjbGMWXw82WXjFrplrGT2AVoIJi7Uf6M2y0EcFDhgAMIQRhHw5Sek5hwURM3Bly4hJCXOUZhiiTNqMqRhUOsX36dik6LdiQYumZKrVHu/j0JqQ0sEGaiOoSwnI3Q+Uz5SzZ37xz5SnPNqC/W3gFxAr0iP1LN6r8r072ItBBXfXAqadYMbI7r3DJ1K3IkxufuhLkEBMncZvyCWFPKUf3bChNqnqXd+uo/KuqlKyMvaI2w5s8JQ3Y+j7On+Boq2aZNetgu9KoF6OexirWUKV57qCBPeyjSd7XeMAjnrS+dqPdancfpVqp0Kzgy9Lu3wFgh56U</latexit>

(z | x)

4x

p (x
<latexit sha1_base64="5ZBg6ocXQzPHkFd2fsFRuhCGUTY=">AAAC6XicjVHLSsNAFD2Nr/quunQTrELdlEQEXRbcuKxgH9BKmaRTOzQvkomopT/gzp249Qfc6o+If6B/4Z0xBbWITkhy7rn3nJk714k8kUjLes0ZU9Mzs3P5+YXFpeWV1cLaej0J09jlNTf0wrjpsIR7IuA1KaTHm1HMme94vOEMjlS+ccHjRITBqbyK+JnPzgPREy6TRHUK21GnLftcslLbZ7Lv9IaXI7Pti645jq9Hu51C0SpbepmTwM5AEdmqhoUXtNFFCBcpfHAEkIQ9MCT0tGDDQkTcGYbExYSEznOMsEDalKo4VTBiB/Q9p6iVsQHFyjPRapd28eiNSWlihzQh1cWE1W6mzqfaWbG/eQ+1pzrbFf2dzMsnVqJP7F+6ceV/daoXiR4OdQ+Ceoo0o7pzM5dU34o6ufmlK0kOEXEKdykfE3a1cnzPptYkund1t0zn33SlYlXsZrUp3tUpacD2z3FOgvpe2bbK9sl+sXKYjTqPTWyhRPM8QAXHqKJG3jd4xBOejYFxa9wZ95+lRi7TbODbMh4+AAainnA=</latexit>

|

z)

3x

x~ <latexit sha1_base64="MznLeNBWvHLdK0cw0Hl8OthuwJg=">AAAC13icjVHLSsNAFD2Nr1pftS7dBIvgqiQi6LLgxmUF+5C2lCSdtkPzIplISyjuxK0/4Fb/SPwD/QvvjCmoRXRCkjPn3nNm7r126PJYGMZrTltaXlldy68XNja3tneKu6VGHCSRw+pO4AZRy7Zi5nKf1QUXLmuFEbM822VNe3wu480bFsU88K/ENGRdzxr6fMAdSxDVK5Y6grt9lnY8S4zsQTqZzXrFslEx1NIXgZmBMrJVC4ov6KCPAA4SeGDwIQi7sBDT04YJAyFxXaTERYS4ijPMUCBtQlmMMixix/Qd0q6dsT7tpWes1A6d4tIbkVLHIWkCyosIy9N0FU+Us2R/806Vp7zblP525uURKzAi9i/dPPO/OlmLwABnqgZONYWKkdU5mUuiuiJvrn+pSpBDSJzEfYpHhB2lnPdZV5pY1S57a6n4m8qUrNw7WW6Cd3lLGrD5c5yLoHFcMY2KeXlSrlazUeexjwMc0TxPUcUFaqiT9wSPeMKzdq3danfa/Weqlss0e/i2tIcPyKmXgg==</latexit>

Figure 2: The Modulated Variational auto-Encoder (MoVE) provides a single architecture able to perform many-to-many transfer while controlling the generation with external parameters. Both the domain and control information are processed to modulate different layers of the architecture.

4 EXPERIMENTS
Dataset. In order to learn our timbre transfer models, we rely on the Studio-On-Line (SOL) database of orchestral instrument note recordings (Ballet et al. (1999)). We selected 12 instruments across the 4 families of wind (Alto-Saxophone, Bassoon, Clarinet, Flute, Oboe), brass (EnglishHorn, French-Horn, Tenor-Trombone, Trumpet), string (Cello, Violin) and keyboard (Piano). We consider each instrumental subset as a timbral domain Xi, which contains the full tessitura of each instrument at different velocities (amounting to around 100 to 200 samples per domain). We split these subsets into 90% training notes and 10% test set. The audio waveforms are down-sampled to 22050Hz before computing the Non-Stationary Gabor Transform (NSGT) (Balazs et al. (2011)). This spectral transform allows to map to a perceptual pitch scale, while remaining iteratively invertible to the signal domain (Perraudin et al. (2013)). NSGTs are computed on a scale of 500 Mel bins ranging from 10Hz to 11000Hz. The resulting matrix data is sliced into chunks of 16 temporal frames, amounting for a context of about 120ms. This yields a final input size of 16x500 dimensions. We keep only the magnitude information and lowest values are floored to 6e-5 before applying a logarithmic transform. Finally, we normalize the entire dataset by computing a zero-mean unit-range normalization on all training samples.
5

Under review as a conference paper at ICLR 2019

Implementation details. The one-to-one transfer network is implemented as follows. The first encoding stacks ed are domain-specific, each composed of two 2-dimensional strided convolutions, an intermediate flattening step and a fully-connected (FC) layer. The intermediate representation is either concatenated (denoted as CAT-po models) with the conditioning vector of size 21 (12 pitch and 9 octave classes) or modulated by FiLM (denoted as FiLM-po models). Follows a weight-shared set ews of two FC layers and two Gaussian encoder output layers mapping the input to a shared 3dimensional latent space. All intermediate layers are followed by batch normalization and a LeakyReLU non-linearity. This structure is mirrored in the decoders, where the latent code is conditioned and fed into a weight-shared block dws of 3 FC layers. Follow separate decoding stacks dd each with a FC layer, two transpose convolutions that up-sample the representation and two Gaussian decoder outputs. The final output activation is a Tanh applied to the decoded means, according to the initial data scaling. Full details of the architectures are given in appendix A.
The many-to-many transfer model relies on the same architecture, but without domain-specific encoders and decoders. Hence, all layers are similar but a single network jointly processes all domains thanks to FiLM layers (denoted as FiLM-poi models). For these, an embedding layer maps our categorical vocabulary of pitch, octave and instrument classes to dense vectors which are processed by two FiLM generators, one for the encoder and one for the decoder. Each has 3 FC layers followed by scaling and biasing output pairs that each map to the size of the modulated layer. We replace all batch normalization by instance normalization and apply FiLM generator outputs as linear transform modulating normalized hidden activations. Conditional instance normalization is performed feature-wise for 1-dimensional vectors and channel-wise for 2-dimensional feature maps.
Regarding optimization, all training objectives are simultaneously back-propagated throughout all networks. We use a Xavier weight initialization and the ADAM optimizer with an initial learning rate of 1e-4. Following the -warmup procedure (Sønderby et al. (2016)), only the NLL reconstruction objective is optimized in the first epochs and the KLD strength is gradually increased from 0 to 1 during half the total number of training epochs. Similarly, we introduce the translation objective after 40 epochs and the optional circle-consistency objective after 60 epochs. We train on mini-batches of size 128 and the MMD is evaluated against batches of size 2048 sampled from the target distributions and computed with three Gaussian kernel parameter values {0.05, 0.1, 1}. We found the magnitude of MMD gradients to be much smaller than that of the ELBO. Hence, we set MMD to 1e5. Given that our models are light, the training over instrument pairs or triplets can be done in less than 24 hours on a single mid-range GPU (eg. NVIDIA TITAN Xp 12Gb).

4.1 ONE-TO-ONE TRANSFER
First, we compare our MoVE proposal to UNIT on the one-to-one transfer task. To do so, we learn a different model for each pair of instruments. We perform incremental comparisons by ablating certain aspects of our proposal to assess their importance. First, we add concatenative conditioning of pitch and octave to UNIT (noted UNIT(GAN;C-po)). Then we add our proposed alternative MMD criterion replacing the GAN objective (UNIT(MMD;C-po)). Then, we introduce the FiLM layers leading to our MoVE proposal. The first version still features separate domain-specific encoders and decoders, so it is noted MoVE* (MMD; F-po). By further introducing domain conditioning and relying on a single VAE (as in Figure 2), we obtain our proposal MoVE (MMD; F-pod).

Evaluation scores. input samples x and

rTeocoenvsatlruuactteiornescox~n.stTruhcetiRonoopte-rMfoeramna-Snqceusa,reweErcroomr pRuMteSsEe=veral

criteria between (x-~x)2 and Log-

Spectral Distortion LSD=

(10log10 (

x2 ~x2

))2

provide

different

assessments

of

how

various

models

are able to reconstruct samples from the test set. Therefore, they only assess reconstruction abilities

without domain transfer. To evaluate the quality of domain transfers, we compute the Maximum

Mean Discrepancy (MMD) and the non-differentiable k-Nearest Neighbour (k-NN) test (Friedman

& Rafsky (1983)). Both are dissimilarity measures computed between the target data distribution

and transferred samples. Hence, we evaluate test set transfers between different target domains.

Reconstruction and domain transfer. The averaged reconstruction and transfer results are presented in Table 3, while separate evaluations for different pairs are in Annex B. As we can see, the UNIT-MMD model obtains the highest within-domain reconstruction score, while the MoVE model achieves better domain translation. Hence, it appears that the MMD increases reconstruction

6

Under review as a conference paper at ICLR 2019

performance, and that the FiLM conditioning ameliorates the transfer. It also seems that relying on a single encoder and decoder for domain transfer might provide better generalization, as can be verified by looking at the relative MMD and kNN scores on the transfer task. Indeed, it seems that the modulated but separate layers approach perform worse, while the single architecture performs better on most evaluations.

Table 1: Evaluations of various models on the test sets

UNIT (GAN) UNIT (GAN; C-po) UNIT (MMD; C-po)
MoVE* (MMD; F-po) MoVE (MMD; F-pod)

RMSE
0.3412 0.3011 0.3036 0.3134 0.3339

reconstructions

LSD

MMD ( = 0.05)

718.47 2.117e-2

693.22 1.989 e-2

692.41 2.125 e-2

762.51 9.632 e-3 781.11 2.587 e-3

k-NN (k = 10) 57269 57806 57102
57273 57509

transfers

MMD

k-NN

( = 0.05) (k = 10)

2.038 e-2 43180

9.112 e-2 43414

2.304 e-2 43878

3.153 e-2 43443 1.747 e-2 43173

Audio descriptors topology. Audio descriptors are features used to compare the qualities of different sounds (Peeters et al. (2011)). Hence, we rely on these to assess the effect of transfer, while providing a deeper understanding of its behavior. We compute the spectral flatness, centroid, roll-off and loudness on test samples reconstructed on their own domain or transferred to the other domain. Distribution and sample-specific plots for the spectral centroid are presented in Figure 3.

Spectral centroid distribution
4e3 3e3 4e3

3e3 3e3
2e3 2e3 1e3 1e3

Transfer to Violin

Transfer to Saxophone

2e3 3e3
2e3 1e3
1e3

Original Saxophone Violin transfer to samples Violin

2e3 3e3
Original Saxophone

1e3 2e3
Original violin

Original Violin Saxophone transfer to
samples Saxophone

Figure 3: Understanding the effect of musical timbre transfer through audio descriptor distributions.
As we can see, the transfer produces an almost exact match of the descriptor distribution to the target domain. This shows the success in transferring multimodal distributions of auditory properties, as all the modes of the descriptors' distributions are preserved. The scatter plot also suggests that the centroid transfer is highly influenced by the loudness of the sample. This correlates to perception studies, as playing an instrument louder usually leads to a higher centroid McAdams et al. (1995).
In order to further understand how the latent space is organized with respect to audio descriptors, we provide their spatial topology in Figure 4. To compute this, we define a sampling grid over the latent space and decode the audio at each point to compute their descriptors. As we can see, the audio descriptors are locally very smooth. Furthermore, one key observation is that the latent space of both conditioned target domains follow the same overall topology. Animations showing the complete latent descriptor topology are available on the supporting webpage.
Latent space synthesis and performance. As the latent space provides continuous audio synthesis and that our method introduce high-level conditioned controls, we can use our proposal as a full musical synthesizer. Furthermore, as we map to 3-dimensional spaces, the user can directly interact with the space while performing timbre transfer. Furthermore, although these models are trained to

7

Under review as a conference paper at ICLR 2019

Centroid

d1 d2

X -1 1

Flatness

d1 d2

Figure 4: Topology of the latent space with respect to audio descriptors.

transfer single instrumental notes, they still can be used to transfer a full melody recording between timbre domains. To do so, the recording is split and iteratively reconstructed by transfering each signal window to the target domain. Audio examples of applying this strategy to transfer a complete instrumental solo are also available in the supporting webpage.
4.2 MANY-TO-MANY TRANSFER
Here, we evaluate the application of MoVE to perform many-to-many transfer. Given our new architecture, this simply consists in training on multiple domains at once by modulating with the appropriate domain information. This architecture allows us to train a single model for different domains and thus to perform multi-domain translation. The conditioning vector is then composed of the pitch, the octave and here the instrument of the corresponding example. This conditioning vector is then processed by an embedding to ease the FILM conditioning. Results are presented in Table 2 : we can see that the MoVE architecture is able to reconstruct and transfer multiple domains at the same time at the cost of a slight decrease in performance, even in the case of diverse domains (here Alto-Sax, Flute, Violin and French-Horn).

Table 2: Many-to-Many MoVE reconstruction & transfer scores

Alto-Saxophone Flute Violin
French-Horn

averaged reconstructions

RMSE

LSD

MMD

k-NN

( = 0.05) (k = 10)

0.5327 835.67 2.117 e-2 42299

0.4593 761.46 2.119 e-2 49719

0.3271 773.65 5.659 e-3 58013

0.6239 869.69 3.404 e-3 70946

averaged transfers

MMD

k-NN

( = 0.05) (k = 10)

2.386 e-2 59157

1.975 e-2 57277

1.452 e-2 55379

2.086 e-2 51317

5 CONCLUSION
We introduced the Modulated Variational auto-Encoders (MoVE), which perform many-to-many domain transfer within a single architecture and without adversarial training while providing highlevel control over the generation. We effectively adapted this technique to musical timbre transfer and showed the successes of our method for audio synthesis. As our technique is generic, it could be applied to other types of data such as image or video. The architecture itself opens up a range of potential sonic applications such as playing style conditioning, transfers between acoustical and electronic instruments, and even with non-musical sound domains. Another avenue of research to be investigated is controlling the amount of transfer performed by the model.
REFERENCES
Peter Balazs, Monika Do¨rfler, Florent Jaillet, Nicki Holighaus, and G Velasco. Theory, implementation and applications of nonstationary gabor frames. Journal of computational and applied

8

Under review as a conference paper at ICLR 2019
mathematics, 236(6), 2011.
Guillaume Ballet, Riccardo Borghesi, Peter Hoffmann, and Fabien Levy. Studio online 3.0: An internet "killer application" for remote access to ircam sounds and processing tools. Journee Informatique Musicale (JIM), 1999.
Yunjey Choi, Minje Choi, Munyoung Kim, Jung-Woo Ha, Sunghun Kim, and Jaegul Choo. Stargan: Unified generative adversarial networks for multi-domain image-to-image translation. arXiv preprint, 1711, 2017.
Shuqi Dai, Zheng Zhang, and Gus Xia. Music style transfer issues: A position paper. arXiv preprint arXiv:1803.06841, 2018.
Jerome H. Friedman and Lawrence C. Rafsky. Graph-theoretic measures of multivariate association and prediction. Ann. Statist., 11(2):377­391, 06 1983.
LA Gatys, AS Ecker, and M Bethge. A neural algorithm of artistic style. Nature Communications, 2015.
Golnaz Ghiasi, Honglak Lee, Manjunath Kudlur, Vincent Dumoulin, and Jonathon Shlens. Exploring the structure of a real-time, arbitrary neural artistic stylization network. arXiv preprint arXiv:1705.06830, 2017.
Arthur Gretton, Karsten M. Borgwardt, Malte J. Rasch, Bernhard Scho¨lkopf, and Alexander Smola. A kernel two-sample test. J. Mach. Learn. Res., 13:723­773, March 2012.
John M Grey. Multidimensional perceptual scaling of musical timbres. the Journal of the Acoustical Society of America, 61(5):1270­1277, 1977.
Aditya Grover, Manik Dhar, and Stefano Ermon. Flow-gan: Combining maximum likelihood and adversarial learning in generative models. arXiv preprint arXiv:1705.08868, 2017.
Irina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick, Shakir Mohamed, and Alexander Lerchner. beta-vae: Learning basic visual concepts with a constrained variational framework. ICLR Conference, 2016.
Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A Efros. Image-to-image translation with conditional adversarial networks. In 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 5967­5976. IEEE, 2017.
David A Jaffe. Ten criteria for evaluating synthesis techniques. Computer Music Journal, 19(1): 76­87, 1995.
Ming-Yu Liu, Thomas Breuel, and Jan Kautz. Unsupervised image-to-image translation networks. In Advances in Neural Information Processing Systems, pp. 700­708, 2017.
Stephen McAdams. Timbre as a structuring force in music. In Proceedings of Meetings on Acoustics ICA2013, volume 19, pp. 035050. ASA, 2013.
Stephen McAdams, Suzanne Winsberg, Sophie Donnadieu, Geert De Soete, and Jochen Krimphoff. Perceptual scaling of synthesized musical timbres: Common dimensions, specificities, and latent subject classes. Psychological research, 58(3):177­192, 1995.
Stephen McAdams, Bruno L. Giordano, Patrick Susini, Geoffroy Peeters, and Vincent Rioux. A meta-analysis of acoustic correlates of timbre dimensions. Journal of the Acoustical Society of America, 120(5), 2006.
Soroush Mehri, Kundan Kumar, Ishaan Gulrajani, Rithesh Kumar, Shubham Jain, Jose Sotelo, Aaron Courville, and Yoshua Bengio. Samplernn: An unconditional end-to-end neural audio generation model. ICLR Conference, 2017.
Noam Mor, Lior Wolf, Adam Polyak, and Yaniv Taigman. A universal music translation network. arXiv preprint arXiv:1805.07848, 2018.
9

Under review as a conference paper at ICLR 2019
Geoffroy Peeters, Bruno L. Giordano, Patrick Susini, Nicolas Misdariis, and Stephen McAdams. The timbre toolbox: Extracting audio descriptors from musical signals. The Journal of the Acoustical Society of America, 130(5):2902­2916, 2011.
Ethan Perez, Florian Strub, Harm De Vries, Vincent Dumoulin, and Aaron Courville. Film: Visual reasoning with a general conditioning layer. arXiv preprint arXiv:1709.07871, 2017.
Nathanae¨l Perraudin, Peter Balazs, and Peter L Søndergaard. A fast griffin-lim algorithm. In Applications of Signal Processing to Audio and Acoustics (WASPAA), 2013 IEEE Workshop on, pp. 1­4. IEEE, 2013.
Xavier Serra. Musical Sound Modeling with Sinusoids plus Noise, pp. 91­122. Studies on New Music Research. Swets & Zeitlinger, 1997.
Casper Kaae Sønderby, Tapani Raiko, Lars Maaløe, Søren Kaae Sønderby, and Ole Winther. How to train deep variational autoencoders and probabilistic ladder networks. arXiv preprint arXiv:1602.02282, 2016.
Aaron Van Den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves, Nal Kalchbrenner, Andrew Senior, and Koray Kavukcuoglu. Wavenet: A generative model for raw audio. arXiv preprint arXiv:1609.03499, 2016.
Prateek Verma and Julius O Smith. Neural style transfer for audio spectograms. arXiv preprint arXiv:1801.01589, 2018.
Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A Efros. Unpaired image-to-image translation using cycle-consistent adversarial networks. In Computer Vision (ICCV), 2017 IEEE International Conference on, pp. 2242­2251. IEEE, 2017.
10

Under review as a conference paper at ICLR 2019

APPENDIX A - ARCHITECTURE DETAILS

Detailed layer definitions are given according to the following nomenclature, by default all learnable output biases are trained.

2-dimensional strided convolutions = conv [out channels, (2D kernel), (2D stride), (2D padding)] 2-dimensional strided transpose convolutions = convT [out channels, (2D kernel), (2D stride), (2D padding), (2D output padding)] fully-connected layer = FC [out features] batch normalization = BN-1D or BN-2D for 1 or 2-dimensional instance normalization = IN-1D or IN-2D for 1 or 2-dimensional hidden activation = act for Leaky-ReLU embedding = embed [output size] FiLM conditioning = FiLM-1D or FiLM-2D for feature-wise or channel wise linear transform layers from the FiLM generator outputs, size according to the [hidden shape] concatenative conditioning = CAT [condition size]

(2d) indicates two domain-specific instances of the same layer (2g) indicates two Gaussian output instances of the same layer

encoders Enc.0
Norm.E0 Enc.1
Norm.E1
Enc.2 Norm.E2
Enc.3 Norm.E3
Enc.4 Norm.E4
µz; z
decoders Dec.0
Norm.D0 Dec.1
Norm.D1 Dec.2
Norm.D2 Dec.3
Norm.D3
Dec.4 Norm.D4
Dec.5 Norm.D5
µx; x
FiLM generators
FiLM encoder outputs FiLM decoder outputs

UNIT (GAN; C-po)

MoVE* (MMD; F-po)

(2d) conv [32,(9,21),(3,3),(4,10)]

BN-2D + act

IN-2D + act

(2d) conv [64,(6,15),(1,3),(0,7)]

BN-2D + act

IN-2D + act

MoVE (MMD; F-pod) conv [32,(9,21),(3,3),(4,10)] IN-2D + act + FiLM-2D[E0] conv [64,(6,15),(1,3),(0,7)] IN-2D + act + FiLM-2D[E1]

intermediate flattening

(2d) FC [4096] BN-1D + act + CAT [21]
BN-1D + act
BN-1D + act

FC [4096] IN-1D + act FC [2048] IN-1D + act + FiLM-1D[E3] FC [1024] IN-1D + act + FiLM-1D[E4] (2g) FC [3]

sampling z  N (µz, z)

CAT [21] + FC [1024] BN-1D + act

BN-1D + act

BN-1D + act BN-1D + act

(2d) FC [64*56]

FC [1024] IN-1D + act + FiLM-1D[D0] FC [2048] IN-1D + act + FiLM-1D[D1] FC [4096]
IN-1D + act FC [64*56]
IN-1D + act

intermediate unflattening to 64 channels

(2d) convT [64,(6,15),(1,3),(0,7),(0,1)]

BN-2D + act

IN-2D + act

(2d) convT [32,(9,15),(3,3),(4,7),(0,1)]

BN-2D + act

IN-2D + act

(2d*2g) convT [1,(5,15),(1,1),(2,7),(0,0)]

convT [64,(6,15),(1,3),(0,7),(0,1)] IN-2D + act + FiLM-2D[D4]
convT [32,(9,15),(3,3),(4,7),(0,1)] IN-2D + act + FiLM-2D[D5]
(2g) convT [1,(5,15),(1,1),(2,7),(0,0)]

training: x  N (TanH(µx), x) generation: x  TanH(µx)

embed [21] FC [128] + IN-1D + act FC [512] + IN-1D + act FC [2048] + IN-1D + act

FC [FiLM-1D[E3]+FiLM-1D[E4]] FC [FiLM-1D[E0]+FiLM-1D[E1]+FiLM-1D[E3]+FiLM-1D[E4]] FC [FiLM-1D[D0]+FiLM-1D[D1]] FC [FiLM-1D[D0]+FiLM-1D[D1]+FiLM-1D[D4]+FiLM-1D[D5]]

11

Under review as a conference paper at ICLR 2019

APPENDIX B - DETAILED TRANSFER EVALUATIONS
Following scores of the table 3 are averaged on the pairs Alto-Saxophone+Violin ; Flute+FrenchHorn ; Oboe+Cello and Clarinet+Piano.

Table 3: Averaged scores for reconstruction and transfer over the test set samples

UNIT (GAN; C-po) MoVE* (MMD; F-po) MoVE (MMD; F-pod)

averaged reconstructions

RMSE

LSD

MMD

k-NN

( = 0.05) (k = 10)

0.4083 701.06 1.7223 e-2 59499

0.4405 788.09 1.244 e-2 59300

0.4300 779.25 1.007 e-2 59282

averaged transfers

MMD

k-NN

( = 0.05) (k = 10)

1.955 e-2 60269

1.650 e-2 59975

1.487 e-2 60455

Table 4: Pair-wise evaluations on Flute and French-Horn test sets

UNIT (GAN; C-po) MoVE* (MMD; F-po) MoVE (MMD; F-pod)
UNIT (GAN; C-po) MoVE* (MMD; F-po) MoVE (MMD; F-pod)

reconstructions Flute

RMSE.

LSD.

MMD. k-NN. ( = 0.05) (k = 10)

0.2773 680.49 3.425 e-3 50344

0.3724 792.67 1.967 e-2 49840

0.2697 698.68 2.226 e-2 49945

reconstructions French-Horn

0.5442 736.96 1.942 e-4 71672

0.5749 800.64 3.458 e-3 72029

0.6026 820.71 2.584 e-3 71650

transfers to French-Horn

MMD.

k-NN.

( = 0.05) (k = 10)

2.991 e-4

71596

6.375 e-3

71582

2.364 e-2

71848

transfers to Flute

8.888 e-3

52304

4.156 e-2

51250

2.049 e-2

52348

Table 5: Pair-wise evaluations on Oboe and Cello test sets

UNIT (GAN; C-po) MoVE* (MMD; F-po) MoVE (MMD; F-pod)
UNIT (GAN; C-po) MoVE* (MMD; F-po) MoVE (MMD; F-pod)

RMSE.
0.4928 0.5641 0.5509
0.2784 0.3226 0.3487

reconstructions Oboe

LSD.

MMD. k-NN. ( = 0.05) (k = 10)

686.52 8.196 e-3 45660

808.06 2.093 e-3 45705

776.80 3.440 e-3 45378

reconstructions Cello

711.37 9.579 e-3 61775

777.27 2.093 e-3 61736

818.90 1.769 e-3 61952

transfers to Cello MMD. k-NN. ( = 0.05) (k = 10) 2.238 e-2 61267 3.290 e-3 62272 7.486 e-3 62036 transfers to Oboe 6.394 e-3 47548 2.428 e-3 47250 8.352 e-3 47787

12

Under review as a conference paper at ICLR 2019

Table 6: Pair-wise evaluations on Clarinet and Piano test sets

UNIT (GAN; C-po) MoVE* (MMD; F-po) MoVE (MMD; F-pod)
UNIT (GAN; C-po) MoVE* (MMD; F-po) MoVE (MMD; F-pod)

reconstructions Clarinet

RMSE.

LSD.

MMD. k-NN. ( = 0.05) (k = 10)

0.6915 778.33 3.918 e-2 76035

0.7351 916.88 1.144 e-2 75440

0.6878 884.63 1.614 e-2 75604

reconstructions Piano

6.682 e-2 543.09 2.610 e-2 70925

6.522 e-2 582.98 1.276 e-2 70055

5.727 e-2 578.66 1.547 e-2 70044

transfers to Piano MMD. k-NN. ( = 0.05) (k = 10) 2.827 e-2 74273 1.287 e-2 73753 1.462e-2 74349 transfers to Clarinet 4.071 e-2 74268 6.790 e-3 72761 1.884 e-2 74801

Table 7: many-to-many MoVE evaluation on Clarinet Cello, Tenor-Trombone (T-Trombone) and Piano test sets (350 epochs only)

input
Clarinet Cello
T-Trombone Piano

RMSE. 0.6832 0.3043 0.7362 4.965 e-2

reconstructions

LSD.

MMD. ( = 0.05)

873.17 9.776 e-3

755.58 2.115 e-3

1047.1 1.281 e-2

542.72 9.773 e-3

k-NN. (k = 10) 76093
61756
25629
69573

targets
Cello T-Trombone
Piano Clarinet T-Trombone Piano Clarinet
Cello Piano Clarinet Cello T-Trombone

transfers MMD. k-NN. ( = 0.05) (k = 10) 1.205 e-2 64117 7.450 e-3 29896 0.1233 75278 1.345 e-3 74569 1.326 e-3 28632 0.1595 74420 2.089 e-2 73320 1.652 e-2 61006 0.1649 71939 0.1482 75589 0.1155 63947 0.2038 28675

13


Under review as a conference paper at ICLR 2019
LEARNING WITH LITTLE DATA: EVALUATION OF DEEP LEARNING ALGORITHMS
Anonymous authors Paper under double-blind review
ABSTRACT
Deep learning has become a widely used tool in many computational and classification problems. Nevertheless obtaining and labeling data, which is needed for strong results, is often expensive or even not possible. In this paper three different algorithmic approaches to deal with limited access to data are evaluated and compared to each other. We show the drawbacks and benefits of each method. One successful approach, especially in one-shot learning tasks, is the use of external data even after training during the classification task. Another successful approach, which achieves state of the art results in semi-supervised learning (SSL) benchmarks, is consistency regularization. Especially virtual adversarial training (VAT) has shown strong results and will be investigated in this paper. The aim of consistency regularization is to find a smooth manifold in which the data lies and therefore to increase the generalization capability. Generative adversarial networks (GANs) have also shown strong empirical results. Mainly the GAN architecture is used to create additional data and therefor to increase the generalization capability of the classification network. Furthermore we consider the use of unlabeled data for further performance improvement. The use of unlabeled data is investigated both for GANs and VAT.
1 INTRODUCTION
Deep neural networks have shown great performance in a variety of tasks, like speech or image recognition. However often extremely large datasets are necessary for achieving this. In real world applications collecting data is often very expensive in terms of cost or time. Furthermore collected data is often unbalanced or even incorrect labeled. Hence performance achieved in academic papers is hard to match.
Recently different approaches tackled these problems and tried to achieve good performance, when otherwise fully supervised baselines failed to do so. One approach to learn from very few examples, the so called few-shot learning task, consists of giving a collection of inputs and their corresponding similarities instead of input-label pairs. This approach was thoroughly investigated in Koch et al. (2015), Vinyals et al. (2016), Snell et al. (2017) and gave impressive results tested on the Omniglot dataset (Lake et al. (2011)). In essence a task specific similarity measure is learned, that first embeds the inputs before comparison.
Furthermore semi-supervised learning (SSL) achieved strong results in image classification tasks. In SSL a labeled set of input-target pairs (x, y)  DL and additionally an unlabeled set of inputs x  DUL is given. Generally spoken the use of DUL shall provide additional information about the structure of the data. Using generative models it is possible to create additional labeled or unlabeled samples and leverage information from these samples. The approach of using generative models for SSL was studied in Kingma et al. (2014), Odena (2016),Dai et al. (2017), and Salimans et al. (2016) with significant improvements over fully supervised baselines.
Consistency regularization bears a similar approach to using generative models. It relies on leveraging information from non realistic samples. But instead of generative models, which create additional data, consistency regularization is using existing unlabeled data, which is perturbed in a realistic way, in order to find a smooth manifold on which the data lies. Different approaches to consistency regularization can be found in Miyato et al. (2018), Sajjadi et al. (2016), Samuli & Timo (2017), and Tarvainen & Valpola (2017).
1

Under review as a conference paper at ICLR 2019
The aim of this paper is to investigate how different approaches behave compared to each other. Therefore specific image recognition tasks are created with varying amount of labeled data. Beyond that it is further explored how different amounts of unlabeled data support the task, whilst also varying the size of labeled data. The possible accuracy improvement by labeled and unlabeled examples is compared to each other. Since there is a correlation between category mismatch of unlabeled data and labeled data (Oliver et al. (2018)) we investigate how this correlation behaves for different approaches.
2 ALGORITHMIC APPROACHES
When dealing with little data, transfer learning (Yosinski et al. (2014), Bengio (2011)) offers for many use cases a good method. Transfer learning relies on transferring knowledge from a base model, which was trained on a similar problem, to another problem. The weights from the base model, which was trained on a seperate big dataset, are then used as initializing parameters for the target model. The weights of the target model are afterwards fine-tuned. Whilst often yielding good results, nevertheless a similar dataset for the training of the base model is necessary. Many problems are too specific and similar datasets are not available. Therefore the generalization of this approach is somehow limited. In order to increase the generalization of this work transfer learning is not investigated.
Instead this paper focuses on generative models, consistency regularization, and the usage of external data during the classification of new unlabeled samples. Since there exist several algorithms for each of these approaches, only one representative algorithm for each of the three approaches is picked and compared against each other.
2.1 USAGE OF EXTERNAL DATA DURING CLASSIFICATION
The usage of external data after training during the classification task is a common technique used in few shot learning problems. Instead of input-label pairs, the network is trained with a collection of inputs and their similarities.
Due to its simplicity and good performance the approach by Koch et al. (2015), which is inspired by Bromley et al. (2014), is used in this paper. Koch et al. (2015) uses a convolutional siamese neural network, which basically learns an embedding of the inputs. The same convolutional part of the network is used for two inputs x1 and x2. After the convolution each input is flattened into a vector. Afterwards the L1 distance between the two embeddings is computed and fed into a fully-connected layer, which outputs a similarity between [0, 1]. In order to classify a test image x into one of K categories, a support set {xk}Kk=1 with examples for each category is used. The input x is compared to each element in the support set and the category corresponding to the maximum similarity is returned. When there are examples per class the query can be repeated several times, such that the network returns the class with the highest average similarity.
Using this approach is advantageous, when the number of categories is high or not known at all. On the downside the prediction of the category depends on a support set and furthermore the computational effort of predicting a category increases with O(K), since a comparison has to be made for each category.
2.2 CONSISTENCY REGULARIZATION
Consistency regularization relies on increasing the robustness of a network against tiny perturbations of the input. In essence d (f (x; ), f (x^; )) shall be minimized, whereas d is a distance measurement like euclidean distance or Kullback-Leibler divergence and x^ is the perturbed input. It is possible to sample x from both DL and DUL.
An empirical investigation Oliver et al. (2018) has shown, that virtual adversarial training (VAT) Miyato et al. (2018) achieved the best results compared to other consistency regularization methods, like mean teacher (Tarvainen & Valpola (2017)) and -model (Sajjadi et al. (2016), Samuli & Timo
2

Under review as a conference paper at ICLR 2019

(2017)). VAT is a training method, which is greatly inspired by adversarial training (Goodfellow et al. (2015)). The perturbation radv of the input x can be computed as

r  N 0,  I dim(x)

(1)

g = rd(f (x, ), f (x + r, ))

(2)

g radv = ||g||

(3)

,where  and are hyperparameters, which have to be tuned for each task. After the perturbation was added to x consistency regularization is applied. The distance between the clean (not perturbed) prediction and perturbed prediction d(f (x, ), f (x + radv, )) shall be minimized. In order to reduce the distance the gradients are just backpropagated through f (x + radv). Combining VAT with entropy minimization Yves & Yoshua (2005) it is possible to further increase the performance
Miyato et al. (2018). For entropy minimization an additional loss term is computed as:

- f (x; )log[f (x; )]

(4)

and added to the overall loss. This way the network is forced to make more confident predictions regardless of the input.

2.3 GENERATIVE MODELS
Generative models are commonly used for increasing the accuracy or robustness of models in a semior unsupervised manner (Kingma et al. (2014), Zhao et al. (2016), Springenberg (2015), Odena (2016), Radford et al. (2016)).
A popular approach is the use of generative adversarial neural networks (GANs), introduced by Ian et al. (2014). The goal of a GAN is to train a generator network G, wich produces realistic samples by transforming a noise vector z as xfake = G(z, ), and a discriminator network D, which has to distinguish between real samples xreal  pData and fake samples xfake  G.
In this paper the training method defined in Salimans et al. (2016) is used. Using this approach the output of D consists of K + 1 categories, whereas K is the number of categories the classifier shall be actually trained on. One additional extra category is added for samples generated by D. Since the output of D is over-parameterized the logit output lK+1, which represents the fake category, is permanently fixed to 0 after training. The loss function consists of two parts Lsupervised and Lunsupervised, which can be computed as:

Lsupervised = -Ex,ypdata log[p(y|x, y < K + 1)]

(5)

Lunsupervised = -{Expdata log[1 - p(y = K + 1|x)] + ExG log[p(y = K + 1|x)]}. (6)
Lsupervised represents the standard classification loss, i.e. negative log probability. Lunsupervised itself again consists of two parts, the first part forces the network to output a low probability of fake category for inputs x  pdata and corresponding a high probability for inputs x  G. Since the the category y is not used in Lunsupervised, the input x can be sampled from both DL and DUL. In order to further improve the performance feature matching is used, as described in Salimans et al. (2016).
3

Under review as a conference paper at ICLR 2019

3 EXPERIMENTS
Three different experiments are conducted in this paper using the MNIST dataset. In the first experiment no external unlabeled data is used. Instead, the amount of labeled data in each category is varied and the three methods are compared to each other. In the second experiment the amount of labeled and unlabeled data is varied, in order to explore how unlabeled data can compensate labeled data. The last experiment considers class distribution mismatch while the amount of labeled and unlabeled data is fixed. In the second and third experiment only two methods are compared, since only generative models and consistency regularization allow the use of external unlabeled data.
All methods are compared to a standard model, which consists of three convolutional layers, followed by two fully-connected layers. ReLU nonlinearities were used in all hidden layers. For training the Adam optimizer (Diederik & Jimmy (2014)) was used. Furthermore batch normalization (offe & Szegedy (2015)), dropout (Srivastava et al. (2014)), and max-pooling was used. The models, representing the three different approaches, had the same computational power as the standard model, i.e. three convolutional layers and two fully connected layers. The test accuracy was calculated in all experiments with a separate test dataset, which contains 500 samples per category. Train and test set have no overlap. All experiments were conducted using the PyTorch framework (Paszke et al. (2017)).
3.1 VARYING AMOUNT OF LABELED DATA AND NO UNLABELED DATA
In this experiment the amount of labeled data is varied. Furthermore there is not used any unlabeled external data, except for the GAN method, where the creation of fake samples by the generator G is part of the training procedure. The amount of labeled data is varied on a logarithmic scale in the range between [0, 200]. For each amount of labeled data and training approach (i.e. baseline, VAT Miyato et al. (2018), GAN Salimans et al. (2016), and siamese neural network Koch et al. (2015)) the training procedure was repeated eight times. Each network was trained for 20.000 weight updates. Afterwards for each method the best results was picked. This way the possible accuracy improvement is evaluated. Another approach would be to calculate the means and standard deviations in order to investigate the robustness.

Accuracy in % Accuracy improvement in %

Baseline
VAT 100

GAN Siamese

90

80

70

60

50

40 100 101 102 Number of training elements per category

VAT 20

GAN Siamese

15

10

5

0

-5

-10

-15 100 101 102 Number of training elements per category

Figure 1: Comparison of different methods with varying amount of labeled data on MNIST. Left: Total accuracy achieved with each method. Right: Accuracy improvements compared to baseline method.
Figure 1 shows the results obtained in this experiment. Using 200 labeled samples per category the baseline network is able to reach near 100 % accuracy. With just one labeled sample per class the baseline networks reaches already around 40 %, which is a already good compared to 10 %, when random guessing. Generally all three methods are consistent with the literature, such that all methods are superior over the baseline accuracy in the low data regime 1-10 samples per category. Using a siamese neural network the accuracy can be even significantly improved in the low data
4

Under review as a conference paper at ICLR 2019

regime. With just one labeled sample the siamese architecture already reaches around 58 %. When using a dataset with more categories, like Omniglot, the advantage of using siamese networks should be even higher in the low data regime. The performance of this approach becomes worse compared to the baseline model, when using more than 10 labeled examples per class. Contrary the GAN and VAT approach are able to improve the accuracy regardless of the number of labeled examples per category. When there are less than 10 labeled examples VAT is superior over GAN. With more than 50 examples the GAN method performes about 1­2 % better than VAT.

3.2 VARYING AMOUNT OF LABELED DATA AND UNLABELED DATA
In this experiment the amount of labeled and unlabeled data was varied. Since only the generative models and consistency regularization allow the of use unlabeled data, siamese neural networks have not been investigated in this experiment. Each of the two different approaches (VAT and GAN) has been trained eight times for every point (number of labeled examples and unlabeled examples per category), afterwards the run with the highest result was picked as the final result. Each network was trained for 50.000 weight updates in order to ensure convergence. Baseline results have been computed a similar way. For each amount of labeled data eight networks have been trained and the best result has been picked. Afterwards the difference between VAT/ GAN and baseline has been calculated.

GAN
102

VAT
102

0.15

0.10

101 101

0.05

Number of labeled elements per category Number of unlabeled elements per category
Accuracy improvement

100100 101 102 Number of unlabeled elements per category

100100 101 102 Number of unlabeled elements per category

0.00

Figure 2: Comparison of VAT and GAN training with varying amount of labeled and additional unlabeled data to baseline method for MNIST dataset.
Figure 2 shows the results of this experiment. On the left side the results achieved with the GAN method and on the right side the results achieved with VAT are visualized. Both methods show a significant increase in terms of accuracy when the amount of labeled data is low and corresponding the amount of unlabeled data is high. When the amount of labeled data increases the amount of necessary unlabeled data also increases in order to achieve the same accuracy improvements. VAT achieves better results with less unlabeled data, when there is little labeled data ( 2-10 examples per category). On contrast GANs achieve better results when there is a moderate amount of labeled examples ( 10-50 examples per category) and also many unlabeled examples. When the amount labeled examples is high both methods behave approximately equal.
The average accuracy improvement by adding additional unlabeled or labeled examples is shown in figure 3 for GAN and VAT. A logarithmic scale is used for both, the x- and y-axis. In order to calculate the improvement per additional extra sample a map like in figure 2 was calculated, but instead of taking the maximum achieved accuracy, the mean accuracy was computed. Afterwards the average improvement per column/ row, which correspond to labeled/ unlabeled examples, was determined and divided by the number of added samples. This averages have been afterwards smoothed in order to better visualize global trends.
5

Under review as a conference paper at ICLR 2019

Acc. improvement/ element in % Acc. improvement/ element in %

101 VAT GAN
100
10 1 100 101 102 Number of additional labeled elements per category

VAT 100 GAN
10 1
100 101 102 Number of additional unlabeled elements per category

Figure 3: Average accuracy improvement per element by adding additional unlabeled/ labeled samples for GAN and VAT on MNIST. Left: Accuracy improvement per labeled example. Right: Accuracy improvement per unlabeled example.

Overall the accuracy improvement decreases for higher numbers of samples for both labeled and unlabeled examples. The improvement per additional labeled example is for both training methods approximately equal. In contrast unlabeled examples behave for GAN and VAT different. Adding unlabeled examples performs better for VAT, when the number of unlabeled examples is low. When there is a high number of unlabeled examples, GAN performs better. Furthermore any labeled example is about 10 times more valuable than unlabeled examples.
3.3 CLASS DISTRIBUTION MISSMATCH
In this experiment the possibility of adding additional unlabeled examples, which do not correspond to the target labels (mismatched samples), is investigated. This experiment was done for VAT in Oliver et al. (2018). In this work the investigation is extended in such a way that the results for VAT are compared to GAN. Furthermore not only the extend of mismatch, but also the influence of the amount of additional unlabeled examples is investigated. MNIST consists of numbers with labels [0, 9] and the aim is to train a neural network, which is able to classify numbers with the categories [0, 6], hence the network has six outputs. Mismatched examples belong to the categories [7, 9]. The number of labeled examples per category is fixed to be five. For each amount of mismatch and method eight neural networks have been trained for 20.000 weight updates. Afterwards their averages have been calculated. For baseline results eight neural networks have been trained and their average accuracies ± standard deviations computed.

Accuracy in % Accuracy in %

Baseline 5 unl. Samples 95
90

20 unl. Samples 100 unl. Samples
VAT

95 90

Baseline 5 unl. Samples

20 unl. Samples 100 unl. Samples
GAN

85 85

80 80

75 75

70 70

0 20 40 60 80 100

0 20 40 60 80 100

Class mismatch in %

Class mismatch in %

Figure 4: Comparison of different methods with varying amount of mismatch in the unlabeled data on MNIST.
6

Under review as a conference paper at ICLR 2019
Figure 4 shows the results of this experiment. Overall the accuracy decreases for both methods when the class mismatch increases. As in the experiments before the GAN method shows little to no accuracy improvement, when the additional amount of unlabeled data is low. For 20 and respectively 100 additional unlabeled elements per class both methods show an approximate equal accuracy improvement, when there is no class mismatch. When the class mismatch is very high (80100 %) using VAT results in worse performance than baseline results. Using GAN the performance is in the worst case at the same level as baseline performance. GAN shows a linear correlation between accuracy and class mismatch. On contrast VAT shows a parabolic trend. All in all both methods show an accuracy improvement up to quite high (> 50 %) amounts of class mismatch.
Furthermore the seem to be no correlation between mismatch and amount of additional unlabeled samples for the GAN method. Using VAT the extend of class mismatch can be higher, when there are more unlabeled samples.
4 CONCLUSION
In this paper three methods for dealing with little data have been compared to each other. When the amount of labeled data is very little and no unlabeled data is available, siamese neural networks offer the best alternative in order to achieve good results in terms of accuracy. Furthermore when there is additional unlabeled data available using GANs or VAT offer a good option. VAT outperforms GAN when the amount of data is low and on contrast GANs should be preferred for moderate or high amounts of data. Nevertheless both methods must be tested for any individual use case, since the behavior of these methods may change for different datasets. When collecting data it is regardless of the chosen training method necessary to avoid class mismatch.
In further work a combined training with VAT and GANs should be examined, for example consistency regularization could be applied to the discriminator in the GAN framework. Also a combination of siamese neural networks and consistency regularization could be bear a useful approach.
REFERENCES
Yoshua Bengio. Deep learning of representations for unsupervised and transfer learning. In Proceedings of the 2011 International Conference on Unsupervised and Transfer Learning Workshop, volume 2 of UTLW'11, pp. 17­37, 2011.
Jane Bromley, Isabelle Guyon, Yann LeCun, Eduard Sa¨ckinger, and Roopak Shah. Generative adversarial nets. In Advances in Neural Information Processing Systems 27, pp. 2672­2680. 2014.
Zihang Dai, Zhilin Yang, Fan Yang, William W Cohen, and Ruslan R Salakhutdinov. Good semisupervised learning that requires a bad gan. In Advances in Neural Information Processing Systems, pp. 6510­6520. 2017.
Kingma Diederik and Ba Jimmy. Adam: A method for stochastic optimization. In International Conference on Learning Representations. 2014.
Ian Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples. In International Conference on Learning Representations. 2015.
Goodfellow Ian, Pouget Abadie Jean, Mirza Mehdi, Xu Bing, Warde-Farley David, Ozair Sherjil, Courville Aaron, and Bengio Yoshua. Generative adversarial nets. In Advances in Neural Information Processing Systems 27, pp. 2672­2680. 2014.
Diederik Kingma, Danilo Rezende, Shakir Mohamed, and Max Welling. Semi-supervised learning with deep generative models. In Advances in Neural Information Processing Systems, pp. 3581­ 3589. 2014.
Gregory Koch, Richard Zemel, and Ruslan Salakhutdinov. Siamese neural networks for one-shot image recognition. In ICML Deep Learning Workshop, volume 2, 2015.
Brenden Lake, Ruslan Salakhutdinov, Jason Gross, and Joshua Tenenbaum. One shot learning of simple visual concepts. In CogSci. 2011.
7

Under review as a conference paper at ICLR 2019
Takeru Miyato, Shin ichi Maeda, Masanori Koyama, and Shin Ishii. Virtual adversarial training: a regularization method for supervised and semi-supervised learning. IEEE transactions on pattern analysis and machine intelligence, 2018.
Augustus Odena. Semi-supervised learning with generative adversarial networks, 2016. arxiv preprint arXiv:1410.5093.
Sergey offe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In International Conference on Machine Learning. 2015.
Avital Oliver, Augustus Odena, Colin Raffel, Ekin D. Cubuk, and Ian J. Goodfellow. Realistic evaluation of semi-supervised learning algorithms. In International Conference on Learning Representations Workshop. 2018.
Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in pytorch. In NIPS-W, 2017.
Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep convolutional generative adversarial networks. In International Conference on Learning Representations. 2016.
Mehdi Sajjadi, Mehran Javanmardi, and Tolga Tasdizen. Regularization with stochastic transformations and perturbations for deep semi-supervised learning. In Advances in Neural Information Processing Systems 29, pp. 1163­1171. 2016.
Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, Xi Chen, and Xi Chen. Improved techniques for training gans. In Advances in Neural Information Processing Systems, pp. 2234­2242. 2016.
Laine Samuli and Aila Timo. Temporal ensembling for semi-supervised learning. In International Conference on Learning Representations. 2017.
Jake Snell, Kevin Swersky, and Richard Zemel. Prototypical networks for few-shot learning. In Advances in Neural Information Processing Systems, pp. 4077­4087. 2017.
Jost Tobias Springenberg. Unsupervised and semi-supervised learning with categorical generative adversarial networks. In International Conference on Learning Representations. 2015.
Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learning Research, 15:1929­1958, 2014.
Antti Tarvainen and Harri Valpola. Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results. In Advances in Neural Information Processing Systems, pp. 1195­1204. 2017.
Oriol Vinyals, Charles Blundell, Timothy Lillicrap, Koray Kavukcuoglu, and Daan Wierstra. Matching networks for one shot learning. In Advances in Neural Information Processing Systems, pp. 3630­3638, 2016.
Jason Yosinski, Jeff Clune, Yoshua Bengio, and Hod Lipson. How transferable are features in deep neural networks? In Advances in Neural Information Processing Systems, pp. 3320­3328. 2014.
Grandvalet Yves and Bengio Yoshua. Semi-supervised learning by entropy minimization. In Advances in Neural Information Processing Systems 17, pp. 529­536. 2005.
Junbo Zhao, Michael Mathieu, Ross Goroshin, and Yann Lecun. Stacked what-where auto-encoders. In International Conference on Learning Representations Workshop. 2016.
8


Under review as a conference paper at ICLR 2019
GENERALIZED ADAPTIVE MOMENT ESTIMATION
Anonymous authors Paper under double-blind review
ABSTRACT
Adaptive gradient methods have experienced great success in training deep neural networks (DNNs). The basic idea of the methods is to track and properly make use of the first and/or second moments of the gradient for model-parameter updates over iterations for the purpose of removing the need for manual interference. In this work, we propose a new adaptive gradient method, referred to as generalized adaptive moment estimation (Game). From a high level perspective, the new method introduces two more parameters w.r.t. AMSGrad (S. J. Reddi & Kumar (2018)) and one more parameter w.r.t. PAdam (Chen & Gu (2018)) to enlarge the parameterselection space for performance enhancement while reducing the memory cost per iteration compared to AMSGrad and PAdam. The saved memory space amounts to the number of model parameters, which is significant for large-scale DNNs. Our motivation for introducing additional parameters in Game is to provide algorithmic flexibility to facilitate a reduction of the performance gap between training and validation datasets when training a DNN. Convergence analysis is provided for applying Game to solve both convex optimization and smooth nonconvex optimization. Empirical studies for training four convolutional neural networks over MNIST and CIFAR10 show that under proper parameter selection, Game produces promising validation performance as compared to AMSGrad and PAdam.
1 INTRODUCTION
Stochastic gradient descent (SGD) and its variants have become the mainstream training methods in machine learning (ML) due to its simplicity and effectiveness. In general, SGD is known to work reasonably well regardless of their problem structure if the learning rate is set properly in a dynamical manner over training iterations. Intuitively speaking, if optimization problems admit certain structural properties (e.g., gradients magnitudes not balanced across the parameter set), advanced gradient descent methods exploiting the structural properties would be likely to boost optimization performance. In 2011, Duchi et al. firstly proposed to track the second moment of gradients and then scale each gradient coordinate using the tracked information before updating the model parameters, which is referred to as AdaGrad (Duchi et al. (2011)). From a conceptual point of view, AdaGrad adaptively adjusts "individual learning rates" for all the model parameters, allowing for the parameters to be updated on a roughly equal scale. It is found that AdaGrad converges significantly faster than SGD when the gradients are sparse. Its performance deteriorates when the gradients are dense due to a rapid decay of the learning rates.
Since the pioneering work of AdaGrad, various adaptive gradient methods have been proposed on tracking and using the first and/or second moment of the gradients for effective parameter updates. The methods include, for example, RMSProp (Tieleman & Hinton (2012)), Adam (Kingma & Ba (2017)), and NAdam (Dozat (2016)). The main difference of the above methods from AdaGrad is that the first and/or second moment of the gradients are tracked via exponential moving averaging to enhance the importance of the most recent gradients.
Despite the wide usage of Adam in ML community, Reddit et al. have recently shown that the method does not even converge for a class of specially constructed convex functions (S. J. Reddi & Kumar (2018)) due to the non-monotonicity of the "individual learning rates" being multiplied to the gradients. To fix the convergence issue of Adam, the authors proposed a so-called AMSGrad method by additionally tracking the maximum value of the second moment of gradients over iterations (i.e., vector v^ of Alg. 1 in Table 1). Later on, Chen and Guo generalized AMSGrad by introducing one free parameter (i.e., p of Alg. 2 in Table 1), referred to as PAdam (Chen & Gu (2018)). The authors'
1

Under review as a conference paper at ICLR 2019

Table 1: Comparison of three adaptive gradient methods

Alg. 1: AMSGrad

Alg. 2: PAdam

Alg. 3: Game

Input: x1, {t}, {1t}, 2 Init.: m0  0, v^0  0, v0  0 for t = 1 to T do

Input: x1, {t}, {1t}, 2 Init.: m0  0, v^0  0, v0  0 for t = 1 to T do

Input: x1, {t}, {1t}, 2 Init.: m0  0, v^0  0 for t = 1 to T do

gt = xf (xt; t)
mt = 1tmt-1 + (1 - 1t)gt vt = 2vt-1 + (1 - 2)gt2
v^t = max(vt, v^t-1) xt+1  xt - tV^t-1/2mt end for

gt = xf (xt; t)
mt = 1tmt-1 + (1 - 1t)gt vt = 2vt-1 + (1 - 2)gt2
v^t = max(vt, v^t-1) xt+1  xt - tV^t-pmt end for

gt = xf (xt; t)
mt = 1tmt-1 + (1 - 1t)gt vt = 2v^t-1 + (1 - 2)|gt|q
v^t = max(vt, v^t-1) xt+1  xt - tV^t-pmt end for

Comparison of analysis results for smooth nonconvex optimization

(Zhou et al. (2018)) :

O

d T

+

GT T

1

; 1

<

22 , p

=

1 4

new results :

new results :

O

d T

+

GT T

when p = 1/4

O

d T

+

GT T

when pq = 1/2

Notations: V^t = diag(v^t); GT = E

d i=1

g1:T ,i

2

; d : dimension of x

primary motivation is to improve the generalization performance by tuning the parameter. Both AMSGrad and PAdam require additional memory in comparison to Adam for storing v^.
While machine learning has seen rapid advances in algorithm development, theoretical convergence analysis has also made remarkable progress recently. The work of (Zhou et al. (2018)) extends the original analysis for PAdam in (Chen & Gu (2018)) from convex optimization to smooth nonconvex optimization. In another recent article (Chen et al. (2018)), the authors also considered smooth nonconvex optimization and provided convergence analysis for a class of Adam-related algorithms including AMSGrad and Adam. The convergence bounds developed in the above two articles differ in their analysis approach and step-size selection. From a high level point of view, analysis on nonconvex optimization is highly valuable in practice as training deep neural networks (DNNs) is well known to be a nonconvex optimization problem. An improved convergence analysis on existing algorithms would provide insights on designing more advanced adaptive gradient methods.
In this paper, we make two main contributions. Firstly, we propose a new adaptive gradient method named generalized adaptive moment estimation (Game). As shown in Table 1, Game tracks only two variables (m, v^) 1 over iterations as compared to AMSGrad and PAdam, which track three variables (m, v, v^). We emphasize that since the dimension of v is the same as the number of model parameters, the memory saved by Game can be remarkable for modern large-scale neural networks, which usually have millions of parameters. Furthermore, Game introduces two parameters (i.e., (p, q) of Alg. 3 in Table 1) in comparison to PAdam which introduces one parameter (i.e., p of Alg. 2 in Table 1) to further enhance its flexibility. The parameter q enables Game to track information of the qth moment of gradient magnitude rather than only the second moment of the gradients. By doing so, the parameter provides one more degree of freedom for the method to balance the tradeoff between convergence speed on training data and generalization ability on validation data.
Secondly, in our theoretical convergence analysis for Game, we manage to remove the condition on the relationship between 1 and 2 while 1  2 and 1  2p are required in (S. J. Reddi & Kumar (2018)) for AMSGrad and (Zhou et al. (2018)) for PAdam, respectively. We provide convergence analysis for both convex optimization and smooth nonconvex optimization. The results for nonconvex case are briefly summarized in Table 1 with the analysis results from (Zhou et al. (2018)) as a reference. Finally, our experimental results on training four convolutional neural networks (CNNs) for MNIST and CIFAR10 suggest that with a proper setup of (p, q), Game produces promising validation performance in comparison to AMSGrad and PAdam.
2 PROBLEM SETUP
Before introducing the problem, we describe the notation used in the paper, which are basically in line with that of (S. J. Reddi & Kumar (2018)) and (Chen & Gu (2018)) for consistency. We denote
1v is not tracked but just computed and used at each iteration.

2

Under review as a conference paper at ICLR 2019

scalars by lower-case letters, vectors by bold lower-case letters, and matrices by bold upper-case

letters. Following convention, we denote the lp (p  1) norm of a vector x  Rd by x p =

(

d i=1

|xi|p)1/p

.

As p



, the l

norm takes a special form

x  = maxdi=1 |xi|. We let

g1:t,i = [g1,i, g2,i, · · · , gt,i], where gt,i is the ith element of a vector gt  Rd. We use E[·] to denote

the expectation operation. Given two sequences {at} and {bt}, the notation at = O(bt) indicates

that the magnitude of at is proportional to that of bt for t  0.

Formally, we reconsider solving the stochastic smooth nonconvex optimization studied in (Ghadimi & Lan (2013; 2016); Zhou et al. (2018))

min f (x) = E [f (x; )] ,
xRd

(1)

where x represents the parameters of a model in a vector form and  is a random variable. Due to randomness of , one can only obtain an unbiased noisy gradient f (x; ), satisfying f (x) = E[f (x; )]. In practice, the variable  often represents the random mini-batch state. For the above situation, we denote the function realization at iteration t as ft(x; t), where t is a realization of . Solving (1) is asymptotically equivalent to addressing the following optimization

T
lim min ft(x; t).
T  xRd t=1
We will consider both the two formulations (1) and (2) in the reminder of the paper.

(2)

3 GENERALIZED ADAPTIVE MOMENT ESTIMATION (GAME)

In this section, we present our new adaptive gradient method for solving (1). Our algorithm Game is motivated by making two observations about AMSGrad and PAdam in Table 1. Firstly, both the two methods have to track the parameter v^ in addition to v. In particular, the parameter v^t at iteration t is computed as

vt = 2vt-1 + (1 - 2)gt2 v^t = max(vt, v^t-1).

(3) (4)

It is seen from (3)-(4) that vt is a function of the squared gradients {gj2}jt=1 while v^t tracks the maximum values of {vj}tj=1 up to iteration t. From a high level perspective, it feels redundant to keep both vt and v^t in AMSGrad and PAdam as both parameters are related to the second moment of the gradients. One natural question is if it is sufficient to keep only one parameter about the
second moment of the gradients without sacrificing convergence speed. Less memory usage is always
desirable in designing an algorithm for simplicity and applicability.

Secondly,

it

is

clear

from

Table

1

that

PAdam

is

designed

by

replacing

the

diagonal

matrix

V^t-

1 2

with

V^t-p

at

iteration

t

in

AMSGrad,

where

p



[0,

1 2

].

One

can

easily

show

that

p

=

0

corresponds

to

SGD

while

p

=

1 2

leads

to

AMSGrad.

The

parameter

p

establishes

a

smooth

connection

between

SGD and AMSGrad, allowing the resulting method PAdam to carry the advantages of both methods.

The empirical study in (Chen & Gu (2018)) suggests that when p is properly chosen in the range

[0,

1 2

],

PAdam

shows

better

generalization

performance

than

AMSGrad,

which

is

as

expected

due

to

the fact SGD usually produces good generalization performance. To summarize, the above technique

of introducing an additional parameter to an existing algorithm adds one more degree of freedom

to enhance its applicability. One can find many examples in applied mathematics that use similar

techniques for knowledge expansion. For instance, the extension from l2 norm to lp norm and

generalization from Cauchy-Schwarz inequality to Hölder's inequality.

Based on the above two observations, we propose a new adaptive gradient method named Game as summarized in Table 1. Basically, the new method only tracks two parameters (m, v^), where v^t at iteration t is computed as

vt = 2v^t-1 + (1 - 2)|gt|q v^t = max(vt, v^t-1),

(5) (6)

where q > 0 is our newly introduced parameter and |gt|q denotes element-wise qth power of |gt|. It is clear from (5)-(6) that vt is only a temporary parameter at iteration t and is not required to be stored

3

Under review as a conference paper at ICLR 2019

in the memory for next iteration. Therefore, it is safe to say that Game saves roughly 33 percent memory per iteration as compared to AMSGrad and PAdam by removing the need for tracking {vt}.

We now study the impact of saved memory by Game when training large-scale neural networks. Note that the dimension of v is actually the number of model parameters. That is, Game manages to save a memory space which is equivalent to the size of the neural network to be trained. It is known that state-of-the-art neural networks for challenging tasks (e.g., ImageNet competition (Russakovsky et al. (2015))) tend to be extremely deep and consist of millions of parameters (Simonyan & Zisserman (2016)). When applying Game to train those large-scale networks, the memory saved by the new training method becomes remarkable with AMSGrad as a reference.

Next

we

let

q

=

2

and

p

=

1 2

for

Game

to

make

a

fair

comparison

w.r.t.

AMSGrad

.

In

this

case,

one can easily show that v^t in (6) is either greater than or equal to the one in (4). As a result, Game

would

always

have

either

equal

or

smaller

effective

learning

rate

t

V^t-

1 2

than

AMSGrad.

The

above

property implies that Game is more conservative than AMSGrad and PAdam due to the update

reformulation (5)-(6). If needed, one can adjust the parameters {t} to larger values to make Game

more aggressive.

Finally we consider the new scalar parameter q introduced in (5). From an algebraic point of view, introduction of parameter q allows v^ in (5)-(6) to track information of the qth moment of the gradient magnitude over iterations. As q decreases, small gradients would be amplified while large gradients would be suppressed, leading to a decreasing dynamic range of v^. When q  0, it is not difficult to show that Game approaches SGD. Therefore, the parameter q has a similar effect as the parameter p of PAdam. We point out that when 2 > 0, we cannot merge p and q into one parameter via certain reformulation of the updating expressions of Game. That is, p and q play different roles in Game. Our purpose of introducing q in addition to p is to enlarge the parameter-selection space of Game and improve generalization performance with proper parameter setup.

4 CONVERGENCE ANALYSIS FOR GAME

In this section, we analyze the convergence of Game for both convex optimization and smooth nonconvex optimization. The cost regret will be studied for the convex case while gradient expectation will be considered for the nonconvex case, which represents two different analysis approaches.

4.1 ANALYSIS FOR CONVEX OPTIMIZATION

Formally, we define the cost regret up to iteration T as

T
RT = [ft(xt; t) - ft(xT ; t)] ,
t=1

(7)

where xT = arg min xt is a causal estimate

ofTt=x1Tfta(txit)edraetnioontets

the optimal as obtained

solution within the iteration by following Game in Table

range [1, T ], and 1. Our objective

is to derive an upper bound of RT and then quantify its convergence behaviour as T increases.

Next we present our convergence analysis: 
Lemma 1. Let (1, 2)  [0, 1), 1t  1, pq < 2 and p, q > 0, and t = / t. Then the quantity

T t=1

t

V^t-p/2mt

2 2

of Game is upper bounded by

T
t
t=1

V^t-p/2mt

2 2



  1 + log T (1 - 1)2(1 - 2)p

 
dT

1/2

 



|gj,i|2(2-pq)

. 

i=1 j=1

(8)

Proof. See Appendix A for proof.

Remark 1. We emphasize that there is a major difference between the upper bound expression in (8)
and those derived in (S. J. Reddi & Kumar (2018)) and (Chen & Gu (2018)) for analyzing AMSGrad
and PAdam. That is the new upper bound does not put any restriction on the relationship between 1 and 2 for (8) to hold while in the above two articles, it is required that 1  22p, where p is the parameter of PAdam.

4

Under review as a conference paper at ICLR 2019

Theorem 1 (convex). Suppose {ft} are close, proper and convex functions (Sawaragi et al. (1985)).

Let 1, 2  [0, 1), 1t  1, pq < 2 and p, q > 0, and t = / t. Assume ft(x; t)  

G for all t  T and distance between any xt generated by Game xm - xT   D. Then we have the following bound on the regret

and

xT

is

bounded,

i.e.,

RT

  D2 T
(1 - 1)

d
v^Tp ,i
i=1

+

D2 2(1 - 1)

T t=1

d i=1

1tv^tp,i t

+

(1

  1 + log T - 1)3(1 - 2)p


d
 
i=1


T

j=1

1/2

|gj,i|2(2-pq)

 

(9)

Based on Lemma 1, one can easily derive the upper bound expression (9) by following the derivation

steps for Theorem 4 in (S. J. Reddi & Kumar (2018)). When pq = 1 and p = 1/2, (9) can be

simplified as

RT



 D2 T (1 - 1)

d i=1

v^T1/,i2

+

D2 2(1 - 1

)

T t=1

d i=1

1tv^tp,i t

 +  1 + log T
(1 - 1)3( 1 - 2

d
g1:T,i 2 .(10)
i=1

One can see that no constraint is imposed between the relationship of 1 and 2 for the upper bound to hold. We have conducted empirical studies and found that both AMSGrad and Game converge even when 2 < 1.

4.2 ANALYSIS FOR SMOOTH NONCONVEX OPTIMIZATION

Differently from the analysis for convex optimization, we will consider gradient expectation for

nonconvex case. To simplify study later on, we choose the output xout from {xt}tT=2 with probability

t-1/(

T -1 j=1

j ),

which

is

in

line

with

the

definition

in

(Zhou

et

al.

(2018)).

In

practice,

it

is

natural

to take the most recent estimate xT at last iteration T as output.

We now introduce the L-smooth assumption needed for analysis: Assumption 1 (L-smooth). f (x) = Ef (x; ) is L-smooth: for any x, y  Rd, we have

|f (x) - f (y) +

f (y), x - y

| L 2

x-y

22.

(11)

Furthermore, f (x) is lower bounded, i.e., infx f (x) > -.

The above smooth assumption is standard for nonconvex optimization. It essentially requires the object function f changes smoothly in the parameter space. See (Zhou et al. (2018); Chen et al. (2018)) for employing the assumption in their analysis.

We now present our convergence analysis in two steps. Firstly, we provide upper bounds for the two

quantities

T t=1

t2E

V^t-pmt

2 2

and

T t=1

t2

E

V^t-pgt

2 2

in a lemma below. We then show

the main result in a theorem, which are derived based on the lemma.

Lemma 2. Let 1, 2  [0, 1), pq  1 and p, q > 0, the step sizes j  j-1 for all j > 1, and r  [2pq - 1, 1]. Assuming ft(x; t)   G for all t  T , we then have

T
t2E
t=1
T
t2E
t=1

V^t-pmt

2 2

V^t-pgt

2 2



12G(1+r-2pq)T (1+r)/2

(1 - 2)2p

E

d
( g1:T,i 2)(1-r)
i=1



12T pq (1 - 2)2p

E

d
( |g1:T,i 2)2(1-pq)
i=1

.

(12) (13)

Proof. See Appendix B for proof.

5

Under review as a conference paper at ICLR 2019

We note that a scalar parameter r  [2pq - 1, 1] is introduced in the upper bound expression of Lemma 2. As will be discussed in Corollary 1 later on, the parameter r provides a freedom to tune the expression (12) and merge with other expressions for simplicity.

Remark 2. Again the two upper bounds (12)-(13) do not require any constraint on the relationship of 1 and 2, which is consistent with the results in Lemma 1.
Theorem 2 (nonconvex). Let 1, 2  [0, 1), pq < 1 and p, q > 0, the step sizes t = 1 for all t > 1, and r  [2pq - 1, 1]. Assume ft(x; t)   G for all t  T and Assumption 1 holds. The output xout of Game satisfies

E

f (xout)

2 2

 M1 T -1

+

M2d T -1

+

M3T pq T -1

E

d
( g1:T,i 2)2(1-pq)

i=1

+

M4T (1+r)/2 T -1 E

d
( g1:T,i 2)1-r
i=1

T  2,

(14)

where

M1 = Gpqf /1

M2

=

G2+pq v^1-p d(1 - 1)

1

+

G2 (1 - 2)p

M3

=

2LGpq 1 (1 - 2)2p

M4

=

4LG1+r-pq 1 (1 - 2)2p

1 2 1 - 1

(15) (16) (17) (18)

where f = f (x1) - infx f (x).

Proof. See Appendix C for proof.

It is clear from Theorem 2 that the two parameters p and q have to satisfy certain conditions. That is p, q > 0 and pq < 1. The conditions suggest if one parameter is chosen large, the other one should be set small to avoid divergence. In practice, it is found that Game also converges when pq = 1. This leaves us an open question on how to elaborate the convergence analysis to cover the special case of pq = 1 for Game.

We now study the upper bound on the right hand side of (14). The expression includes four quantities, where the first two quantities are independent of the gradients {g1:T,i|i = 1, . . . , d} while the last two are contributed by the inequalities derived in Lemma 2. All the four scalar parameters Mi, i = 1, . . . , 4, are independent of iteration number T . The convergence of Game can be established if
one can show that the upper bound expression approaches to zero as T increases.

In the following, we simplify the upper bound expression in (14) by specifying pq and r with

particular values and then study its convergence behaviour.

Corollary 1.

Let pq

=

1 2

and r

= 2pq - 1 in Theorem 2, the upper bound of E

f (xout)

2 2

then

takes the form of

E



f (xout)

2 2

 M1 + M2d + (M3 + M4)1

(T - 1)1 T - 1

T -1

T E

d
g1:T,i 2
i=1

T  2. (19)

As GT

summarized in Table / T ), where GT =

1, E

It

is immediate

d i=1

g1:T ,i

that 2.

the upper bound in (19) As reflected in Table 1,

is proportional to O(d/T + the upper bound also holds

for PAdam even though the iteration procedure (3)-(4) of PAdam is different from (5)-(6) of Game.

We are now in a position to study under what conditions Game would converge. It is not difficult

to conclude that when GT

is of order GT

=

O((dT )s) where s

<

1 2

,

(19)

tends

to

converge

with

6

Under review as a conference paper at ICLR 2019

training loss

training loss

0.2
0.1
0 0
0.8 0.6 0.4 0.2
0
0.8 0.6 0.4 0.2
0
0.8 0.6 0.4 0.2
0

validation loss

Padam Amsgrad Game

0.1 0.08 0.06 0.04

0.02

20 40 60
epochs

80

0 20 40
epochs

(a) Performance over MNIST using SERLU

Padam Amsgrad Game

1 0.8 0.6

Padam Amsgrad Game
60 80
Padam Amsgrad Game

validation loss

0.4

100
epochs

200 0

100
epochs

(b) Performance over CIFAR10 using SERLU
1

Padam Amsgrad Game

0.8 0.6

200
Padam Amsgrad Game

validation loss

0.4

100
epochs

200 0

100
epochs

(c) Performance over CIFAR10 using ELU

Padam Amsgrad Game

1 0.8 0.6

200
Padam Amsgrad Game

validation loss

0.4

100
epochs

200 0

100
epochs

(d) Performance over CIFAR10 using ReLU

200

training loss

training loss

Figure 1: Performance comparison of PAdam, AMSGrad, and Game for training four CNNs (see Table 2-5 for the detailed CNN architectures in Appendix D).



the speed O(d/T + ds/T 1/2-s). We note that in Duchi et al. (2011)), the assumption GT

dT

was used for analyzing AdaGrad, which was later on employed for convergence analysis of other

adaptive gradient methods (see (Zhou et al. (2018)) and (S. J. Reddi & Kumar (2018)) for example).

The assumption GT = O((dT )s) is slightly stronger than GT

dT as in practice, the dimension

d could be larger than T when training large-scale neural networks.

7

Under review as a conference paper at ICLR 2019
5 EXPERIMENTAL RESULTS
In the experiment, we evaluated the effectiveness of AMSGrad, PAdam and Game for training convolutional neural networks (CNNs). The classification problems on the two datasets MNIST and CIFAR10 were considered. To alleviate overfitting, each of the two datasets was augmented with additional training data (e.g., shifting images vertically and/or horizontally, and image flipping for CIFAR10). In brief, we tested four CNNs: the 1st one consists of four layers for MNIST using the activation function SERLU (Zhang & Li (2018)), the 2nd consists of 8 layers for CIFAR10 using SERLU, and the 3rd and 4th have the same structure as the 2nd one but using ELU (Clevert et al. (2016)) and ReLU (Nair & Hinton (2010)), respectively. More detailed information of the four CNNs can be found in Table 2-5 of Appendix D.
We first consider the CNN training for MNIST using SERLU. The shared parameters among the three methods include (t, 1, 2) = (0.001, 0.9, 0.999), which was recommended in (Kingma & Ba (2017)) for Adam. On the other hand, the special parameters include p = 0.125 for PAdam2 and (p, q) = (0.5, 1) for Game. The setup (p, q) = (0.5, 1) was found empirically to be more effective than other tested values. It is worth noting that q = 1 indicates that Game tracks information of the gradient magnitude rather than the second moment of gradients.
Next we study the CNN training for CIFAR10 using SERLU, ELU and ReLU. As the three neural networks (see Table 3-5) are deeper than that for MNIST, we employed shift-dropout for SERLU and dropout for ELU and ReLU to combat overfitting, respectively. The parameter setup is slightly different from the one for MNIST. In particular, the shared parameters among the three methods include (1, 2) = (0.9, 0.999) and dropout rate of 0.2. Special parameters include t = 0.0001 for AMSGrad, 3 (t, p) = (0.001, 0.125) for PAdam, and (t, p, q) = (0.001, 0.5, 1) for Game.
The convergence results for training the four CNNs are demonstrated in Figure 1. It is seen that Game produces better generalization performance (on validation datasets) than AMSGrad in subplot (a) and (c) - (d) while in the same three subplots, AMSGrad exhibits the fastest convergence speed over training data. The two methods have roughly the same convergence performance in subplot (b), suggesting that selection of activation functions also affects the convergence behaviours. It is observed that SERLU performs slightly better than ELU and ReLU in terms of validation loss. In all the four subplots, PAdam converges slightly slower due to the fact the a small value p = 0.125 is selected as compared to p = 0.5 of AMSGrad.
To briefly summarize, the above convergence results suggest that Game produces promising generalization performance in comparison to AMSGrad and PAdam on validation datasets. The nice convergence behaviour of Game might be because the method tracks the 1st moment of gradient magnitude by setting q = 1. As a result, Game achieves better validation performance by slightly sacrificing convergence speed on training dataset. If one also takes into account of memory usage (see Table 1), it is clear that Game is simpler to implement and requires less memory resource, rendering Game an advantage over AMSGrad and PAdam.
6 CONCLUSIONS AND FUTURE WORKS
In this paper, we have proposed a new adaptive gradient method termed as Game. The new method only needs to track two parameters (m, v^) in comparison to AMSGrad and PAdam, which track three parameters (m, v, v^), thus requiring only two-thirds of memory w.r.t. the two methods. The saved memory scales along with the number of model parameters, which becomes significant when training large-scale neural networks. Furthermore, Game introduces one additional parameter q, allowing the method to track information of the qth moment of the gradient magnitude, while AMSGrad and PAdam only consider tracking the 2nd moment of gradients. The freedom of tuning parameter q makes Game more flexible. Theoretical convergence analysis is provided for applying Game to solve both convex and smooth nonconvex optimization. Experimental results on MNIST and CIFAR10 demonstrate that Game produces promising generalization performance in comparison to AMSGrad and PAdam. Given that Game demands only two-thirds of memory in implementation, we conclude that the new method is a promising candidate for training large-scale neural networks.
2This parameter value p = 0.125 was suggested by the authors of (Chen & Gu (2018)). 3It is found that t = 0.001 is not stable when applying AMSGrad to train the CNNs in this case.
8

Under review as a conference paper at ICLR 2019
REFERENCES
J. Chen and Q. Gu. Closing the Generalization Gap of Adaptive Gradient Methods in Training Deep Neural Netwoks. arXiv preprint arXiv:1806.0676v1, June 2018.
X. Chen, S. Liu, R. Sun, and M. Hong. On the Convergence of A Class of Adam-Type Algorithms for Non-Convex Optimization. arXiv:1808.02941v1 [cs. LG], 2018.
D.-A. Clevert, T. Unterthiner, and S. Hochreiter. Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs). arXiv:1511.07289v5 [cs.LG], 2016.
T. Dozat. Incorporating Nesterov Momentum into Adam. In International conference on Learning Representations (ICLR), 2016.
J. Duchi, E. Hazan, and Y. Singer. Adaptive Subgradient Methods for Online Learning and Stochastic Optimization. Journal of Machine Learning Research, 12:2121­2159, 2011.
S. Ghadimi and G. Lan. Stochastic First-and Zeroth-Order Methods for Nonconvex Stochastic Programming. SIAM Journal on Optimization, 23:2341­2368, 2013.
S. Ghadimi and G. Lan. Accelerated Gradients Methods for Nonxonvex Nonlinear and Stochastic Programming. Mathematical Programming, 156:59­99, 2016.
D. P. Kingma and J. L. Ba. Adam: A Method for Stochastic Optimization. arXiv preprint arXiv:1412.6980v9, 2017.
V. Nair and G. E. Hinton. Rectified Linear Units Improve Restricted Boltzmann Machines. In Proceedings of the 27th International Conference on Machine Learning,, 2010.
O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, A. C. Berg, and F.-F. Li. Unified Convergence Analysis of Stochastic Momentum Methods for Convex and Non-Convex Optimization. arXiv:1409.0575v3, 2015.
S. Kale S. J. Reddi and S. Kumar. On The Convergence of Adam and Beyond. In International conference on Learning Representations (ICLR), 2018.
Y. Sawaragi, H. Nakayama, and T. Tanino. Theory of Multiobjective Optimization. Elsevier Science, 1985. K. Simonyan and A. Zisserman. Very Deep Convolutional Networks for Large-Scale Image Recognition. In
International conference on Learning Representations (ICLR), 2016. T. Tieleman and G. Hinton. Lecture 6.5-RMSProp: Divide The Gradient by a Running Average of Its Recent
Magnitude. COURSERA: Neural networks for machine learning, 2012. T. Yang, Q. Lin, and Z. Li. Unified Convergence Analysis of Stochastic Momentum Methods for Convex and
Non-Convex Optimization. arXiv:1604.03257, 2016. G. Zhang and H. Li. Effectiveness of Scaled Exponentially-Regularized Linear Units (SERLUs).
arXiv:1807.10117 [cs.LG], July 2018. D. Zhou, Y. Tang, Z. Yang, Y. Cao, and Q. Gu. On the Convergence of Adaptive Gradient Methods for Nonconvex
Optimization. arXiv:1808.05671v1 [cs. LG], 2018.
9

Under review as a conference paper at ICLR 2019

APPENDIX

A PROOF OF LEMMA 1

Before formally presenting the proof, we first introduce a lemma below: Lemma 3. Let v^t be defined as in Game and q > 0. Then each component v^t,i is lower bounded by v^t,i  (1 - 2)|gt,i|q, i = 1, . . . , d.
Proof. The proof follows directly from the computation v^t,i = max(v^t-1,i, 2v^t-1,i+(1-2)|gt,i|q) as summarized in Table 1 for Game.

With Lemma 3, we are ready to describe the proof for Lemma 1.

tE

V^t-p/2mt

2 2

can be upper bounded by

t

V^t-p/2mt

2 2

Firstly, the quantity

= t = t

d mt2,i i=1 v^tp,i
d1 i=1 v^tp,i

t

(1
j=1

-

1j )

t-j 2
k=11(t-k+1)gj,i

(a)
 t

d1 i=1 v^tp,i

tj=11t-j gj,i 2

(b)
 t

d1 i=1 v^tp,i

t j

=11t-j

jt=11t-j |gj,i|2

(c)


t

1 - 1

d1 i=1 v^tp,i

t j=1

1t-j

|gj,i|2



(d)


t 1 - 1

d

i=1

t j=1

1 v^jp,i

1t-j

|gj,i|2





(e)


t (1 - 1)(1 - 2)p

d

i=1

t
1t-j |gj,i|(2-pq)
j=1

(20)

where step (a) uses 1t  1 < 1, step (b) uses Cauchy-Schwarz inequality, step (c) uses

t j=1

1t-j



1/(1 - 1),

step

(d)

uses

v^tp,i



v^jp,i

for

all

j



t,

and

step (e)

uses the

results

in Lemma 3.

Next taking the summation of (20) over t = 1 to t = T produces

T

t

V^t-p/2mt

2 2

t=1





1 (1 - 1)(1 - 2)p

T

t=1

d i=1

t
t1t-j |gj,i|(2-pq)
j=1



(a)


 (1 - 1)(1 - 2)p

d

i=1

T
|gj,i|(2-pq)
j=1

T t=j

1t-j  t





 (1 - 1)(1 - 2)p

d

i=1

T j=1

1 j

|gj,i|(2-pq)

T t=j

1t-j 

10

Under review as a conference paper at ICLR 2019



(b)


(1

 - 1)2(1 -

2)p

d

i=1

T j=1

1 j

|gj,i

|(2-pq)

 

1/2 

1/2

(c)


(1 -

 1)2(1 -

2)p

d
 
i=1

T
 |gj,i|2(2-pq)
j=1

T1  j
j=1

 

(d)


(1

  1 + log T - 1)2(1 - 2)p


d
 
i=1


T

1/2



|gj,i|2(2-pq)

 

j=1

(21)

 where step (a) uses t = / t, step (b) uses

T t=j

1t-j

<

1/(1 - 1),

step

(c)

follows

from

Cauchy-Schwarz inequality, and step (d) uses

T t=1

1/t



(1

+

log

T

).

The

proof

is

complete.

B PROOF OF LEMMA 2

We first consider the summation

T t=1

t2E

V^t-pmt

2 2

.

Each quantity t2E

V^t-pmt

2 2

can

be upper bounded by

t2E

V^t-pmt

2 2

= t2E

d mt2,i i=1 v^t2,pi

= t2E

d1 i=1 v^t2,pi

t
(1
j=1

-

1)1t-j

gj,i

2

(a)
 t2(1 - 1)2E

d1 i=1 v^t2,pi

tj=11t-j |gj,i|(1+r-2pq)

jt=11t-j |gj,i|(1-r+2pq)

(b)
 t2(1 - 1)2E

d1 i=1 v^t2,pi

t j

=11t-j

G(1+r-2pq)

tj=11t-j |gj,i|(1-r+2pq)

(c)
 t2(1 - 1)G(1+r-2pq)E

d1 i=1 v^t2,pi

jt=11t-j |gj,i|(1-r+2pq)



(d)


t2(1

-

1)G(1+r-2pq)E

d

i=1

t j=1

1 v^j2,pi

1t-j |gj,i|(1-r+2pq)



(e)


t2(1

- 1)G(1+r-2pq)

d

(1 - 2)2p

E
i=1

t
1t-j |gj,i|(1-r)
j=1

(22)

where step (a) uses Cauchy-Schwarz inequality and the property that 2pq - 1  r  1 and pq  1,

step (b) uses |gj,i|  gj   G, step (c) uses

t j=1

1t-j



1/(1 - 1), step (d) uses v^t2,pi



v^j2,pi

for all j  t, and step (e) uses the results in Lemma 3.

Taking the summation of (22) over t = 1 to t = T produces

T

t2E

V^t-pmt

2 2

t=1





(1 - 1)G(1+r-2pq)

T

(1 - 2)2p

E
t=1

d i=1

t
t21t-j |gj,i|(1-r)
j=1

(23)

11

Under review as a conference paper at ICLR 2019



(a)


12(1 - 1)G(1+r-2pq)

d

(1 - 2)2p

E
i=1

T
|gj,i|(1-r)
j=1

T t=j

1t-j 



(b)


12G(1+r-2pq) (1 - 2)2p

E



d i=1

T
|gj,i|(1-r)
j=1

 

(1-r)/2

(c)


12G(1+r-2pq) (1 - 2)2p

E

 

d i=1

T
 |gj,i|2
j=1

 T (1+r)/2 

=

12G(1+r-2pq)T (1+r)/2 (1 - 2)2p

E

d
( g1:T,i 2)(1-r)
i=1

where step (a) uses the property that aj  aj-1 for all j > 1, step (b) uses

T t=j

1t-j

<

1/(1 - 1),

and step (c) follows from Holder's inequality. The proof is complete for (12).

The quantity

T t=1

t2E

V^t-pgt

2 2

in (13) can be upper bounded as

T
t2E
t=1

V^t-pgt

2 2

T
= t2E
t=1

d gt2,i i=1 v^t2,pi

(a) T
 12E
t=1

d gt2,i i=1 (1 - 2)2p|gt,i|2pq



(=b)

(1

12 - 2)2p

E



d i=1

T
|gj,i |2-2pq 
j=1

 

1-pq 

(c)


(1

12 - 2)2p

E

d
 
i=1

T

j=1

|gj,i|2

 T pq 

=

12T pq (1 - 2)2p E

d
( |g1:T,i 2)2(1-pq)
i=1

where step (a) uses the property that aj  aj-1 for all j > 1 and the results in Lemma 3, step (b) uses pq < 1, p, q > 0, and step (c) - (d) follow from Holder's inequality. The proof is complete for
(13).

C PROOF OF THEOREM 2

In brief, we use the technique of parameter transformation proposed in (Yang et al. (2016)) to study
Game for solving stochastic nonconvex optimization. In particular, we let x0 = x1 and for each t1

zt

=

xt

+

1

1 - 1

(xt

-

xt-1)

=

1

1 - 1 xt

-

1

1 - 1

xt-1

,

(24)

where z1 = x1 for the special case t = 1. We note that the technique has also been employed in
(Zhou et al. (2018)) and (Chen et al. (2018)) for analyzing PAdam and a class of Adam-type methods. Instead of considering {xj}Tj=0 directly, we tackle {zj}jT=1 as in the literature.

In (Zhou et al. (2018)) and (Chen et al. (2018)), the authors provided a general upper bound for f (zt+1) - f (zt), which also holds for Game. We now summarize their result in a lemma below:

12

Under review as a conference paper at ICLR 2019

Lemma 4. Suppose the sequence {zj}Tj=1 is as defined in (24). Then under Assumption 1, the quantity f (zt+1) - f (zt) is upper bounded by

f (z2) - f (z1)  - f (x1)T 1V^1-pg1 + 2L

1V^1-pg1

2 2

(25)

f (zt+1)

-

f (zt)



-

f (xt)T

t-1V^t--p1gt

+

1

1 -

1

G2(

t-1v^t--p1

1-

tv^t-p 1)

+ 2L

tV^t-pgt

2 2

+

4L

1 1 - 1

2

xt - xt-1

2 2

t  2.

(26)

With Lemma 4, we are ready to present the proof for Theorem 2. Firstly, taking expectation on both sides of (25) produces

E[f (z2) - f (z1)]  E

-f (x1)T 1V^1-pg1 + 2L

1V^1-pg1

2 2

E

d1

f (x1)  ·

V^1-pg1

 + 2L

1V^1-pg1

2 2

(a)
E
(b)
E

d1 (1 - 2)p

f (x1)  ·

|g|1-pq

d1G2-pq (1 - 2)p

+ 2L

1V^1-pg1

2 2

,

 + 2L

1V^1-pg1

2 2

(27)

where step (a) uses V^1 = (1 - 2)|g1|q and pq < 1, and step (b) uses ft(x; t)   G.

Next by rearranging terms and taking expectation in (26), we have

E

f (zt+1)

+

G2 1 - 1

tv^t-p

1-

f (zt)

+

G2 1 - 1

t-1v^t--p1

1

E

-f (xt)tt-1V^t--p1gt + 2L

tV^t-pgt

2 2

+

4L

1 1 - 1

2

xt-1 - xt

2 2

(=a) -f (xt)tt-1V^t--p1f (xt) + E

2L

tV^t-pgt

2 2

+

4L

1 1 - 1

2

t-1V^t--p1mt-1

2 2

(b)
 -t-1

f (xt)

2 2

(Gpq

)-1

+

E

2L

tV^t-pgt

2 2

+

4L

1 1 - 1

2

t-1V^t--p1mt-1

2 2

, (28)

where step (a) uses the property that E[gt] = f (xt), and step (b) uses the inequality v^t-1   Gq which can be derived from ft(x; t)   G, t  1. Taking the summation of (27)-(28) over t = 1 to T produces

T

(Gpq )-1

t-1E

f (xt)

2 2

t=2

E

f (z1)

+

G2 1v^1-p 1 - 1

1

+

d1G2-pq (1 - 2)p

-

f (zT +1)

+

G2 1 - 1

T v^T-p

1

T
+ 2L E
t=1

tV^t-pgt

2 2

+ 4L

1 2 T

1 - 1

E
t=2

t-1V^t--p1mt-1

2 2

(a)
E

f + G2 1v^1-p 1 - 1

1

+

d1G2-pq (1 - 2)p

T
+ 2L E
t=1

tV^t-pgt

2 2

+ 4L

1 2 T

1 - 1

E
t=1

tV^t-pmt

2 2

,

(29)

where step (a) uses the inequality f = f (x1) - infx f (x)  f (x1) - f (zT +1) and z1 = x1.

13

Under review as a conference paper at ICLR 2019

Finally, substituting (12)-(13) into (29) produces

E

f (xout)

2 2

=

1

T t=2

t-1

T
t-1E
t=2

f (xt)

2 2



Gpq

T t=2

t-1

E

f

+

G2 1v^1-p 1 - 1

1

+

d1G2-pq (1 - 2)p

+

2LGpq

T t=2

t-1

12T pq (1 - 2)2p

E

d
( g1:T,i 2)2(1-pq)
i=1

+

4LGpq

T t=2

t-1

1 1 - 1

2

12

G(1+r-2pq)T (1+r)/2 (1 - 2)2p

E

(=a)

Gpq f (T - 1)1

+

T

1 -1

G2+pq v^1-p 1 - 1

1

+

dG2 (1 - 2)p

d
( g1:T,i 2)1-r
i=1

+

T pq T -1

·

2LGpq 1 (1 - 2)2p

E

d
( g1:T,i 2)2(1-pq)
i=1

+

T (1+r)/2 T -1

·

4LG1+r-pq 1 (1 - 2)2p

1 2 1 - 1 E

d
( g1:T,i 2)1-r
i=1

=

M1 T -1

+

M2d T -1

+

M3T pq T -1 E

d
( g1:T,i 2)2(1-pq)

i=1

+

M4T (1+r)/2 T -1 E

d
( g1:T,i 2)1-r
i=1

,

where step (a) uses the property t = 1 for all t  2, and {Ml}l4=1 are given by (15)-(18). The proof is complete.

D CNN ARCHITECTURES IN EXPERIMENTS
Remark 3. The full name of SERLU in Table 3 is so called scaled exponentially-regularized linear unit. Furthermore, the shift-dropout in Table 3 is designed specifically for SERLU, which includes dropout as a special case.

Table 2: CNN for MNIST using SERLU

Layer 1
Layer 2
Layer 3 Layer 4

conv.: 3 × 3@32 (SERLU) conv.: 3 × 3@64 (SERLU)
max-pooling dense: 512 neurons (SERLU)
dense + softmax

14

Under review as a conference paper at ICLR 2019

Table 3: CNN for CIFAR10 using SERLU

Layer 1 Layer 2 Layer 3 Layer 4 Layer 5 Layer 6
Layer 7 Layer 8

conv.: 3 × 3@32 (SERLU) conv.: 3 × 3@32 (SERLU)
max-pooling shift-dropout conv.: 3 × 3@64 (SERLU) conv.: 3 × 3@64 (SERLU) max-pooling shift-dropout conv.: 3 × 3@128 (SERLU) conv.: 3 × 3@128 (SERLU) max-pooling shift-dropout
dense: 512 neurons (SERLU) shift-dropout
dense + softmax

Table 4: CNN for CIFAR10 using ELU

Layer 1 Layer 2 Layer 3 Layer 4 Layer 5 Layer 6
Layer 7 Layer 8

conv.: 3 × 3@32 (ELU) conv.: 3 × 3@32 (ELU)
max-pooling dropout
conv.: 3 × 3@64 (ELU) conv.: 3 × 3@64 (ELU)
max-pooling dropout
conv.: 3 × 3@128 (ELU) conv.: 3 × 3@128 (ELU)
max-pooling dropout
dense: 512 neurons (ELU) dropout
dense + softmax

Table 5: CNN for CIFAR10 using ReLU

Layer 1 Layer 2 Layer 3 Layer 4 Layer 5 Layer 6
Layer 7 Layer 8

conv.: 3 × 3@32 (ReLU) conv.: 3 × 3@32 (ReLU)
max-pooling dropout
conv.: 3 × 3@64 (ReLU) conv.: 3 × 3@64 (ReLU)
max-pooling dropout
conv.: 3 × 3@128 (ReLU) conv.: 3 × 3@128 (ReLU)
max-pooling dropout
dense: 512 neurons (ReLU) dropout
dense + softmax

15


Under review as a conference paper at ICLR 2019
POSTERIOR ATTENTION MODELS FOR SEQUENCE TO SEQUENCE LEARNING
Anonymous authors Paper under double-blind review
ABSTRACT
Modern neural architectures critically rely on attention for mapping structured inputs to sequences. In this paper we show that prevalent attention architectures do not adequately model the dependence among the attention and output variables along the length of a predicted sequence. We present an alternative architecture called Posterior Attention Models that relying on a principled factorization of the full joint distribution of the attention and output variables propose two major changes. First, the position where attention is marginalized is changed from the input to the output. Second, the attention propagated to the next decoding stage is a posterior attention distribution conditioned on the output. Empirically on five translation and two morphological inflection tasks the proposed posterior attention models yield better predictions and alignment accuracy than existing attention models.
1 INTRODUCTION
Attention is a critical module of modern neural models for sequence to sequence learning as applied to tasks like translation, grammar error correction, morphological inflection, and speech to text conversion. Attention specifies what part of the input is relevant for each output. Many variants of attention have been proposed including soft (Bahdanau et al., 2014; Luong et al., 2015), sparse (Martins & Astudillo, 2016), local (Luong et al., 2015), hard (Xu et al., 2015; Zaremba & Sutskever, 2015), and monotonic hard attention (Yu et al., 2016; Aharoni & Goldberg, 2017). The most prevalent of these is soft attention that computes attention for each output as a multinomial distribution over the input states. The multinomial probabilities serve as weights, and an attention weighted sum of input states serves as relevant context for the output and subsequent attention. Soft attention is end to end differentiable, easy to implement, and hence widely popular. Hard attention and sparse attentions are difficult to implement and not popularly used.
In this paper we revisit the statistical soundness of the way soft attention and other variants capture the dependence between attention and output variables, and among multiple attention variables along the length of the sequence. Our investigation leads to a more principled model that we call the Posterior Attention Model (PAM). We start with an explicit joint distribution of all output and attention variables in a predicted sequence. We then propose a tractable approximation that retains the advantages of forward dependence and token-level decomposition that leads to efficient training and inference. However, the computations performed at each decode step has two important differences with existing models. First, at each decoding step the probability of the output token is a mixture of output probability for each attention. In contrast, existing models take a mixture of the input, and compute a single output distribution from this diffused mixed input. We show that our direct coupling of output and attention gives the benefit of hard attention without its computational challenges. Second, we introduce the notion of a posterior attention distribution, that is, the attention distribution conditioned on the current output. We show that it is both statistically sounder and more accurate to condition subsequent attention on the output corrected posterior attention, rather than the output independent prior attention as in existing models.
We evaluate the posterior attention model on five translation tasks and two morphological inflection tasks. We show that posterior attention provides improved BLEU score, higher alignment accuracy, and better input coverage. We also empirically analyze the reasons behind the improved performance of the posterior attention model. We discover that the entropy of posterior attention is much lower than
1

Under review as a conference paper at ICLR 2019

entropy of soft attention. This is a significant find that challenges the current practice of computing attention distribution without considering the output token. The running time overhead of posterior attention is only 40% over existing soft-attention.

2 JOINT DISTRIBUTION FOR ATTENTION AND OUTPUT VARIABLES

Our goal is to model the conditional distribution Pr(y|x) of an output sequence y = y1, . . . , yn given an input sequence x = x1, . . . , xm. Each output yt is a discrete token from a typically large vocabulary V . Each xj can be any abstract input. Typically a RNN encodes the input sequence into a sequence of state vectors x1, . . . , xm, which we jointly denote as x1:m. Each yt depends not only on other tokens in the sequence, but on some specific focused part of the input sequence. A
hidden variable at, called the attention variable, denotes which part of x1:m the output yt depends on. We denote the set of all attention as a = a1, . . . , an. During training the input x and output y are observed but the attention a is hidden. Hence, we write Pr(y|x) as

Pr(y|x1:m) =

Pr(y, a|x1:m) =
a

a1,...,an Pr(y1, . . . , yn, a1, . . . , an|x1:m)

(1)

The number of variables involved in this summation is daunting, and we need to approximate. We first review how existing soft attention-based encoder decoder models handle this challenge.

2.1 EXISTING ATTENTION-BASED ENCODER DECODER MODEL

yEab1x,lie.sst.i.an,sgytE-nnt1=co1adsPearr(-dDyete|cxco1od:dmeer,rys(t1Ea,tD.e.)s.tn,,eysttw-o1ot)hr.kasAt fPadrce(tcyoo|rxdiz1ee:rmPR)rN(=Ny|xs1u:ntmm=)m1 bPayrri(zayeptsp|xtlh1y:eimnvg,ascrti)ha.abilTnehrleuelndegisothtnrihbyiusvttoiaorrnyiof each attention variable at is computed as a function of the decoder state and encoder state as: Pr(a|x1:m, st)  eA(xa,st). Here A(., .) is an end-to-end trained function of input state xa and decoder state st. We will use the short form Pt(a) for Pr(at|x1:m, st). Thereafter, an attention
weighted sum of the input states a Pt(a)xa called input context ct is computed. The distribution of yt is computed from ct (capturing attention) and st capturing previous y as:

Pr(y|x1:m) =

n
t=1 Pr(yt|st, ct =

a Pt(a)xa)

(2)

Next, ct is fed to the decoder RNN along with yt for computing the next state: st+1 = RNN(st, ct, yt). Figure 1[left] summarizes the compute equations of the encoder-decoder model. If we view Equation 2 as an approximation of the full joint distribution in Equation 1, we find that the treatment of the attention variables has been rather ad hoc. Attention was introduced as an after-thought of first factorizing on the yt variables, the interaction among multiple ats is not expressed statistically, and the influence of at on yt by diffusing the inputs is less than satisfactory. We next present a statistically sounder model of the interaction of the various attention and output variables, while being more efficient to compute than the full joint method of Equation 1. We call our proposed approach: Posterior Attention Models or PAM.

2.2 POSTERIOR ATTENTION MODELS

Our goal is to express the joint distribution as a product of tractable terms computed at each time step
much like in existing ED model, but via a less ad hoc treatment of the attention variables a1, . . . , an. We use y<t, a<t to denote all output and attention variables before t that is, y1, , . . . yt-1, a1, . . . at-1. Here and in the rest of the paper we will drop x1:m to use the shorter form P (y) for Pr(y|x1:m). We first factorize Eq 1 via chain rule, like in ED but jointly on both a and y.

P (y) =

P (y, a) =
a

a<n,an P (yn|y<n, a<n, an)P (an|y<n, a<n)P (y<n, a<n)

We then make the mild assumption that the output yt at each step is dependent only on at and previous outputs y<t and is independent of all other attention variables. That is, P (yt|y<t, a<n, an) =

2

Under review as a conference paper at ICLR 2019

P (yt|y<t, at). This allows us to factorize the above joint as:

P (y) =

an P (yn|y<n, an) a<n P (an|y<n, a<n)P (y<n, a<n)

= P (y<n)

an P (yn|y<n, an)

a<n

P

(an|a<n,

y<n)

P

(y<n, a<n) P (y<n)

= P (y<n) an P (yn|y<n, an) a<n P (an|a<n, y<n)P (a<n|y<n)
n
= t=1 at P (yt|y<t, at) a<t P (at|a<t, y<t)P (a<t|y<t)
The last equality is after applying the same rewrite recursively on P (y<n). Thus, we have expressed the joint distribution in terms of factors that apply at each decoding step t while conditioned only on previous outputs and attention. We can use the same RNN trick to summarize y<t as a fixed length vector. The main intractable term in the above is a<t P (at|a<t, y<t)P (a<t|y<t) = P (at|y<t). This is attention at step 't' conditioned on all previous outputs. For reasons that will soon become clear we call this the prior attention at t and denote as Priort(a). In existing models, this attention is computed independently at each step using the RNN state (Eq 10) whereas we propose to expand out
the expressions and more carefully capture the dependencies among attention variables.

2.2.1 COMPUTATION OF ATTENTION DISTRIBUTION
We take a step beyond existing RNN-based dependence transfer and explicitly model the dependency among adjacent attention. We fall back on the decoder RNN to approximate the dependence on the attention and outputs before the immediate past. This gives us:
P (at|a<t, y<t)  P (at|st-1, at-1, yt-1) The RNN state is an approximate summary and its update will be discussed shortly. The Prior(at) becomes:
Prior(at) = a<t P (at|a<t, y<t)P (a<t|y<t)  at-1 P (at|st-1, at-1, yt-1)P (at-1|y<t) We call P (at-1|y<t) = P (at-1|y<(t-1), yt-1) as the posterior attention Postr(at-1) since this is the attention distribution after observing the output label at that step, and not just the previous steps as in prior attention. We expect this attention to be more accurate than the prior that is computed without knowledge of the output token at that step. We compute posterior attention at any t using prior attention at t - 1 by applying Bayes rule as follows:

Postrt(at)

=

P (at|y<t, yt)

=

P (yt|y<t, at)P (at|y<t) P (yt|y<t)

=

P (yt|y<t, at)Priort(at) P (yt|y<t)

(3)

Priort(at) =

P (at|st-1, at-1, yt-1)Postrt-1(at-1)
at-1

(4)

The above equation gives us the important insight that the attention at step t should be computed from

the posterior attention of the previous step. Intuitively, also it makes sense because attention reflects

an alignment of the input and output, and its distribution will improve if the output is known. For

an exact method to compute the P (at|at-1, yt-1, st-1) term in the equation of prior attention, we

would need to design an attention logic with four arguments: xat , xat-1 , yt-1, and sn-1. In contrast, existing models compute attention logits A(xa, st) with only two arguments. To avoid the extra

attention parameters, we designed three light-weight methods of capturing these dependencies. In all

these variants we introduce only a handful of extra parameters which can be learned end-to-end.

Postr-Joint The simplest of these uses the same decoder RNN to absorb the posterior attention of the previous step. Essentially, we used deterministic attention technique used by Xu et al. (2015) to efficiently approximate computation of Priort(a) using first order Taylor expansion.
Priort(at) = P (at|st-1, yt-1, a )Postrt-1(a )  P (at|st-1, yt-1, Postrt-1(a )xa )  P (at|st)
aa
(5) The above equation suggests that the decoder RNN state should be updated as st = RNN(st-1, a Postrt-1(a )xa , yt-1). The computation here is thus similar to existing ED model's but the crucial difference is that the context used to update the RNN is computed from posterior

3

Under review as a conference paper at ICLR 2019

attention, and not the prior attention. We will see that this gives rise to significant improvement in accuracy.

Next we experiment with models that explicitly couple adjacent attention. These models utilize a separate index based coupling between attention positions and are of the form

logP (at|st-1, at-1) = k(at, at-1) + A(xat , st-1)

(6)

A(xat , st-1) is the attention logit computed from the previous RNN step and k(at, at-1) is the attention coupling energy.

We experiment with two types of coupling energies:

Proximity biased coupling k(at, at-1) is given by I(|at - at-1| < 3)at-at-1 . This model has a natural bias towards attending on inputs where attention has focused recently. We label this model as
Prox-Postr-Joint in our results below

Monotonicity biased coupling k(at, at-1) is a monotonic energy given by I(at > at-1)at-at-1-1. This model biases attention towards a monotonic attention which keeps moving ahead. As we shall see tasks with natural monotonic attention benefit from this form of bias. This
model is denoted as Mono-Postr-Joint in our experiments.

2.3 PUTTING IT ALL TOGETHER

In Figure 1 we put together the final set of equations that are used to compute the output distribution and contrast with existing attention model. We call this overall architecture as Posterior Attention Model (PAM). First note that in PAM, we explicitly compute a joint distribution of output and attention at each step and marginalize out the attention. Thus, the output is a mixture of multiple output distributions each of which is a function of one focused input (like in hard attention), and not a diffused sum of the input (like in soft attention). This difference in the way attention is marginalized is not only statistically sound, but also leads to higher accuracy. The only downside of the joint model is that we need to compute m softmaxes for each output yt, and this may be impractical when the vocabulary size is large. A simple and effective fix to this is to select the Top-K attentions based on Priort and compute the final output distribution as.

P (yt|st, xa)Priort(a)  Priort(a)P (yt|st, xa)
a aTopK(Priort (a))

(7)

Small values of K (order 6), suffice to provide good performance1. The second difference is that the

attention distribution that is propagated to the next step is posterior to observing the current output.

We derived this from a principled rewrite of the joint distribution, and were pleasantly surprised to see

significant accuracy gains by this subtle difference in the way the decoder state is updated. Computing

the posterior attention does not incur any additional overheads because the joint attention-output

distribution was already materialized in the first equation. However, due to the sparsity induced by

the top-k operation on attention probabilities, the posterior probabilities are unrealistically sparse.

As such we augment the posterior attention using input from standard attention, by using a equally

weighted combination of the two distributions. Third, the prior attention distribution is explicitly

conditioned on the previous attention. This allowed us to incorporate various application-specific

natural biases like proximity and monotonicity of adjacent attentions.

3 RELATED WORK
The de facto standard for sequence to sequence learning via neural networks is the encoder decoder model. Ever since their first introduction in Bahdanau et al. (2014), many different attention models have been proposed. We discuss them here.
Soft Attention is the de-facto mechanism for seq2seq learning et al (2018). It was proposed for translation in Bahdanau et al. (2014) and refined further in Luong et al. (2015). The output derives from an attention averaged context. The advantage is end to end differentiability.
1Extensive empirical justification behind this approximation appears in a related work that we suppress due to double blind restrictions.

4

Under review as a conference paper at ICLR 2019

nm
Pr(y|x1:m) = Pr(yt|st, Pt(a)xa) (8)
t=1 a=1

st+1 = RNN(st, yt, Pt(a)xa)

a

Pt(a) =

eA (xa,st)

m r=1

eA (xr ,st)

(9) (10)

nm

Pr(y|x1:m) =

P (yt|st, xj)Priort(a) (11)

t=1 a=1

st+1 = RNN(st, yt, Postrt(a)xa)

a

Postrt(a) =

P (yt|st, xa)Priort(a) a P (yt|st, xa )Priort(a )

(12) (13)

Priort(at) = P (at|st-1, a )Postrt-1(a ) (14)

a

P (at|st-1, a ) = See Section 2.2.1

(15)

Figure 1: Comparing the Equations for computing Pr(y|x1:m) of existing encoder decoder model based on soft attention (Left) with our Posterior Attention Model (Right)

Hard Attention was proposed in Xu et al. (2015) and attends to exactly one input state for an output. The merit of hard attention is that the output is determined from a single input rather than an average of all inputs. However due to non-differentiability , training Hard-Attention requires the REINFORCE Williams (1992) algorithm and is subject to high variance, requiring careful tricks to train reliably. Yu et al. (2016) keep the encoder and decoder independent to allow for easier marginalization. Aharoni & Goldberg (2017) use a monotonic hard attention and avoid the problem, by supervising hard attention with external alignment information.
Sparse/Local Attention Many attempts have been made to bridge the gap between soft and hard attention. Luong et al. (2015) proposes local attention that averages a window of input. This has been refined later to include syntax (Chen et al., 2017; Sennrich & Haddow, 2016; Chen et al., 2018) and has been explored for image captioning in Gregor et al. (2015). A related idea to harden attention is to make it sparse using sparsity inducing operators (Martins & Astudillo, 2016; Niculae & Blondel, 2017). However, all sparse/local attention methods continue to compute P (y) from an attention weighted sum of inputs like in soft attention.
Recurrent Attention Yang et al. (2016) have previously modeled relationship between the attentions at different time steps by using a recurrent history mechanism. The attention history of an input word and its surrounding words are captured in a summary vector by an RNN, which is provided as further input to the attention mechanism for incorporating dependence on history. While both works model dependence between attention at different steps, our principled rewrite of the joint distribution shows that posterior attention should be the link to the next attention.
Structured Attention Networks Similar to this work, Kim et al. (2017) interpret attention as latent structural variable. The authors then take advantage of easy inference in certain graphical models to implement forms of segmental and syntactic attention. Liu & Lapata (2018) extend the same technique to non-projective dependencies in document modeling. However these work only focus on attention at each step independently whereas our focus is modeling the dependency among adjacent attention. Moreover our posterior attention framework is independent of how the prior attention at each position is modeled. In this paper we assumed a multinomial distribution but the structured distribution of Kim et al. (2017) can also benefit from our posterior coupling.
Variational Posterior Attention Schulz et al. (2018) deploy a stochastic decoder based on chaining multiple latent variables, and use a variational approach to train the model. The motivation behind the model is the existence of several fluent translations for a sentence which can differ in their syntactic forms. Bahuleyan et al. (2017) and Zhou & Neubig (2017) proposes variational attention but attention is still a function of x. Their goal is also to increase diversity. Our goal is to improve performance by relying on the posterior.

4 EXPERIMENTS
We compare our posterior attention model on two sequence to sequence learning tasks: machine translation and morphological inflection.
5

Under review as a conference paper at ICLR 2019

Methods compared

Soft: This is the standard soft attention mechanism with Luong attention.

Sparse: This is the sparse-attention model presented in Niculae & Blondel (2017).

Postr-Joint: This is our default posterior attention network as described in 2.2.1 . We try two more variants of PAMbased on explicit coupling called

Mono-Postr-Joint: which refers to the monotonic biased model in 2.2.1

Prox-Postr-Joint: which refers to the explicitly coupled proximity biased model described in 2.2.1

Prior-Joint: This is our model minus the posterior attention. That is, we use joint attention output distribution as in Eq 11 but prior and RNN updates are as in soft attention.

4.1 MACHINE TRANSLATION

We experiment on five language pairs from three datasets: IWSLT15 EnglishVietnamese, IWSLT14 GermanEnglish Cettolo et al. (2015); and WAT17 JapaneseEnglish Nakazawa et al. (2016). We use a 2 layer bi-directional encoder and 2 layer decoder with 512 LSTM units and 0.2 dropout with vanilla SGD optimizer. Instead of tuning hyper-parameters for our method we used Soft-Attention tuned parameters.

Dataset Attention

IWSLT14 DE-EN

Soft Sparse Prior-Joint Postr-Joint Mono-Postr-Joint Prox-Postr-Joint
Soft Sparse

PPL
9.61 9.85 8.47 8.51 8.23 8.26
10.68 10.89

BLEU B=4 B=10
28.6 28.5 28.4 28.0 29.7 29.6 29.8 29.7
30 29.9 29.8 29.7
24.2 24.2 23.4 23.3

Overall Comparison Our results are in Table 1 where we show perplexity (PPL) and BLEU with beam size 4 and 10. All Postr-Joint

IWSLT14 EN-DE

Prior-Joint Postr-Joint Mono-Postr-Joint Prox-Postr-Joint

8.72 25.4 8.6 25.6 8.45 25.7 8.52 25.6

25.3 25.4 25.6 25.5

variants and Prior-Joint outperform soft attention and sparse-attention by large margins. Moreover models with posterior attention show improvement over those which use prior attention. This clearly shows the performance advantage of joint modeling and posterior attention. We shall analyze the reasons for these improvements later.
Comparing Attention Coupling Next we explore the impact of different coupling mod-

IWSLT15 EN-VI
IWSLT14 VI-EN

Soft Sparse Prior-Joint Postr-Joint Mono-Postr-Joint Prox-Postr-Joint
Soft Sparse Prior-Joint Postr-Joint Mono-Postr-Joint Prox-Postr-Joint

10.27 10.13
9.67 9.11 9.52 9.59
8.30 8.48 7.57 7.34 7.14 7.26

26.6 26.6 27.4 27.6 27.6 27.5
24.7 24.2 25.7 25.9 25.9 25.9

26.4 26.1 27.3 27.4 27.3 27.3
24.6 23.9 25.6 25.8 25.6 25.9

els discussed in 2.2.1. For that focus Soft 12.46 18.9 18.5

on methods Postr-Joint, Prox-Postr-Joint, and

Sparse

14.18 17.5 16.8

Mono-Postr-Joint in Table 1. We obtain some gains over Postr-Joint by explicitly modeling attention coupling. For language-pairs with a natural monotonic alignment like German-

WAT17 JA-EN

Prior-Joint Postr-Joint Mono-Postr-Joint Prox-Postr-Joint

10.00 9.96 9.98 9.78

20.6 20.5 20.7 20.9

20.2 20.3 20.5 20.5

English, Mono-Postr-Joint slightly outperforms

other models by (0.1-0.2 BLEU points). English- Table 1: Perplexity and test BLEU with two infer-

Vietnamese is a more non-monotonic pair and as ence beam widths (B) on five translation tasks

expected we do not find gains by incorporating

a monotonic bias.

4.2 MORPHOLOGICAL INFLECTION
To demonstrate the use of our model beyond translation, we next consider the task of generating morphological inflections. We use Durrett & DeNero (2013)'s dataset containing inflection forms for German Nouns (de-N) and German Verbs (de-V). The models are trained separately for each type of inflection for each dataset to predict the inflected character sequence. We train a one layer encoder and decoder with 128 hidden LSTM units each with a dropout rate of 0.2 using Adam and measure

6

Under review as a conference paper at ICLR 2019

0/1 accuracy. We also ran the 100 units wide two layer LSTM with hard-monotonic attention model Aharoni & Goldberg (2017) labeled Hard-Mono2.

Data Soft Hard-Mono Prior-Joint Postr-Joint Mono-Postr-Joint Prox-Postr-Joint

de-N 85.50 85.65

85.81

85.88

86.87

85.81

de-V 94.91 95.31

95.52

95.5

95.71

95.4

Table 2: Test accuracy for morphological inflection
Using joint modeling we get significant gains (0.3 points) even against task-specific hard-monotonic attention, showing that our approach is more general than translation. Moreover when we use Mono-Postr-Joint which has a structural bias towards task-specific monotonic attention, we obtain immense improvements (upto 1 accuracy point) over joint models.

4.3 EXPLAINING WHY WE SCORE ABOVE SOFT ATTENTION
We attempt to get more insights on why posterior attention models score over soft attention in end to end accuracy. We show that the main reason is better alignment of input and output because of a more precise attention model. We demonstrate that by first showing some anecdotes of better alignment, then showing that posterior attention is more focused (has lower entropy), provides better alignment accuracy, and better input coverage. For these runs we perform experiments in the teacher forcing setup so as to compare two models' distributions under identical inputs.
Anecdotal Examples Fig2 presents the heatmap of difference between Postr-Joint and SoftAttention on some representative sentences. Thus the red regions represent where Postr-Joint has greater attention and blue where soft-attention has greater focus. One can observe generally that Soft-Attention is far more diffused. More importantly, we can see that Postr-Joint is able to correct mistakes and provides the appropriate context for the next step. For example in Fig2a Soft-Attention (blue) has maximum focus on the source word 'generationen' when the target word is innovation which corresponds to 'innovationen'; on the other hand Postr-Joint is able to correct this. Similarly while producing the phrase 'but the same' Postr-Joint focuses the attention on the source word 'dasselbe' Fig2b. This provides insight into as to how by providing better contexts via incorporating the target, posterior attention can outperform prior attention.

Figure 2: Heatmap of differences between Posterior-Attention (Red) and Soft-Attention (Blue). Mark the corrected red alignments for 'innovation' and 'but the same'
Attention Entropy Vs Accuracy We expect Soft-Attn to be worse hit by high attention uncertainty than other models. This, if true, could illustrate that P (yt|xt) distribution can be learned more easily if the input is 'pure', rather than diffused via pre-aggregation. To this end we plot the accuracy of Postr-Joint, Prior-Joint and Soft-Attn under increasing attention entropy in Figure 3 on the EnglishGerman pair. As one can expect the accuracy drops off quickly as attention uncertainty rises. The plot also presents the histogram of the fraction of cases with different attention uncertainties. Soft attention models (blue) have significantly higher number of cases of high attention uncertainty, leading to low performance. One of the primary means by which joint models outperformed soft-attention is by
2https://github.com/roeeaharoni/morphological-reinflection
7

Under review as a conference paper at ICLR 2019

0.20 Soft

0.9

Prior-Joint

0.8

0.15

Postr-Joint

0.7

0.10 0.6 0.5
0.05 0.4

0.00 0.1 0.3 0.5 0.7 0E.n9tro1p.y1 1.3 1.5 1.7 1.9 0.3

0.25

Soft Prior-Joint

0.8

0.20

Postr-Joint

0.6

0.15 0.4

0.10 0.2

0.05

0.00 0.1

0.3

0.5

0.7

0E.n9tro1p.y1

1.3

1.5

1.7

0.0 1.9

Figure 3: Variation of accuracy and histogram of attention entropy on De-En (left) and En-De (right) . Note the smoother accuracy decay in Postr-Joint and the entropy distibution for Sot-Attention
reducing the number of such cases. These figures also provide insight into another mechanism by which posterior attention boosts performance. One can see that the accuracy drops off much more smoothly wrt attention uncertainty in posterior attention models (green). In fact in cases of high attention certainty (low attention entropy) Postr-Joint slightly underperforms Prior-Joint, however due to relatively stabler behavior gives better performance overall.

Alignment accuracy Failure of attention to produce latent structures which correspond to linguistic

structures has been noted by Koehn & Knowles (2017); Ghader & Monz (2017).Based on few

examples, we hypothesize that Posterior Attention should be able to produce better alignments.

To test this we used the RWTH German-English dataset which provides alignment information

manually tagged by experts, and compare the alignment accuracy for Soft, Prior-Joint and Postr-Joint

attentions. Following the procedure in Ghader & Monz (2017) the most attended source word for

each target word is taken as the aligned word. We used the AER metric Koehn (2010) to compare

these against the expert alignments.

Table 3 presents the AER accuracy for different models. One can read off Attention AER

that Postr-Joint model beats the second best model ( Prior-Joint ) by more Soft

0.449

than 10%, and dwarfs soft-attention by a huge margin, proving that posterior Prior-Joint 0.502

alignments are significantly more compatible with true alignments.

Postr-Joint 0.583

Fraction of covered tokens A natural expectation for translation is that by the time the entire output sentence has been produced, attention would have covered the entire input sequence. A loss based on this precise heuristic was used in Chorowski & Jaitly (2016) to improve the performance of a attention based seq2seq model for speech transcription. In this experiment we try to indirectly assess reliability of different attention models via measuring whether cumulatively attention has focused on the entire input sequence.
We plot the frequency distribution of the coverage in Fig4. Note that in soft attention model, there are many sentences which do not receive enough attention during the entire decoding process. Prior-Joint and Postr-Joint have similar behavior with few instances of one outperforming the other, however both outperform soft attention by huge margins.

5 CONCLUSION
We show in this paper that none of the existing attention models adequately model the dependence of the output and attention along the length of the output for general sequence prediction tasks. We propose a factorization of the joint distribution, and develop practical approximations that allows the joint distribution to decompose over output tokens, much like in existing attention. Our more principled probabilistic joint modeling of the dependency structure leads to three important differences. First, the output token distribution is obtained by aggregating predictions across all attention. Second, the concept of conditioning attention on the current output i.e. a posterior attention for inferring the next output becomes important. Our experiments show that it is sounder, more meaningful and more accurate to condition subsequent attention distribution on the posterior attention. Thirdly, via

8

Under review as a conference paper at ICLR 2019

0.175 0.150

Soft Prior-Joint Postr-Joint

0.20

Soft Prior-Joint

Postr-Joint

0.125 0.15

0.100 0.075 0.10

0.050 0.05 0.025

0.000 0.00

0.5 0.55 0.6 0.65 0.7 0.75 0.8 0.85 0.9 0.95 1.0 0.5 0.55 0.6 0.65 0.7 0.75 0.8 0.85 0.9 0.95 1.0

Coverage

Coverage

Figure 4: Coverage for different attention models on the En-De (left) and De-En(right) tasks

directly exposing attention coupling, we have a principled way to directly incorporate task-specific structural biases and prior knowledge into attention. We experimented with some simple biases and found boosts in related tasks. Our work opens avenues for future work in scaling these techniques to large-scale models and multi-headed attention. Another promising line is to incorporate more complex biases like phrasal structure or image segments into joint attention models.

REFERENCES
Roee Aharoni and Yoav Goldberg. Morphological inflection generation with hard monotonic attention. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, ACL 2017, Vancouver, Canada, July 30 - August 4, Volume 1: Long Papers, pp. 2004­2015, 2017.
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. CoRR, abs/1409.0473, 2014.
Hareesh Bahuleyan, Lili Mou, Olga Vechtomova, and Pascal Poupart. Variational attention for sequence-to-sequence models. CoRR, abs/1712.08207, 2017.
Mauro Cettolo, Jan Niehues, Sebastian Stüker, Luisa Bentivogli, Roldano Cattoni, and Marcello Federico. The iwslt 2015 evaluation campaign. In IWSLT 2015, International Workshop on Spoken Language Translation, 2015.
Huadong Chen, Shujian Huang, David Chiang, and Jiajun Chen. Improved neural machine translation with a syntax-aware encoder and decoder. In ACL, 2017.
Kehai Chen, Rui Wang, Masao Utiyama, Eiichiro Sumita, and Tiejun Zhao. Syntax-directed attention for neural machine translation. CoRR, abs/1711.04231, 2018.
Jan Chorowski and Navdeep Jaitly. Towards better decoding and language model integration in sequence to sequence models. CoRR, abs/1612.02695, 2016.
Greg Durrett and John DeNero. Supervised learning of complete morphological paradigms. In Proceedings of the North American Chapter of the Association for Computational Linguistics, 2013. URL http://aclweb.org/anthology//N/N13/N13-1138.pdf.
Mia Chen et al. The best of both worlds: Combining recent advances in neural machine translation. In ACL, 2018.
Hamidreza Ghader and Christof Monz. What does attention in neural machine translation pay attention to. CoRR, 2017.
Karol Gregor, Ivo Danihelka, Alex Graves, Danilo Rezende, and Daan Wierstra. Draw: A recurrent neural network for image generation. In ICML, 2015.
Y. Kim, C. Denton, L. Hoang, and A. Rush. Structured Attention Networks. In ICLR, 2017.
Philipp Koehn. Statistical Machine Translation. Cambridge University Press, 1st edition, 2010. ISBN 0521874157, 9780521874151.

9

Under review as a conference paper at ICLR 2019
Philipp Koehn and Rebecca Knowles. Six challenges for neural machine translation. CoRR, abs/1706.03872, 2017.
Yang Liu and Mirella Lapata. Learning structured text representations. Transactions of the Association for Computational Linguistics, 2018.
Minh-Thang Luong, Hieu Pham, and Christopher D. Manning. Effective approaches to attentionbased neural machine translation. EMNLP, 2015.
André F. T. Martins and Ramón Fernández Astudillo. From softmax to sparsemax: A sparse model of attention and multi-label classification. In ICML, 2016.
Toshiaki Nakazawa, Manabu Yaguchi, Kiyotaka Uchimoto, Masao Utiyama, Eiichiro Sumita, Sadao Kurohashi, and Hitoshi Isahara. Aspec: Asian scientific paper excerpt corpus. In LREC, 2016.
Vlad Niculae and Mathieu Blondel. A regularized framework for sparse and structured neural attention. In NIPS. 2017.
Philip Schulz, Wilker Aziz, and Trevor Cohn. A stochastic decoder for neural machine translation. Association for Computational Linguistics, 2018. URL http://aclweb.org/anthology/ P18-1115.
Rico Sennrich and Barry Haddow. Linguistic input features improve neural machine translation. In WMT, 2016.
Ronald J. Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Mach. Learn., 1992.
Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhudinov, Rich Zemel, and Yoshua Bengio. Show, attend and tell: Neural image caption generation with visual attention. In Proceedings of the 32nd International Conference on Machine Learning, volume 37 of Proceedings of Machine Learning Research, 2015.
Zichao Yang, Zhiting Hu, Yuntian Deng, Chris Dyer, and Alexander J. Smola. Neural machine translation with recurrent attention modeling. In EACL, 2016.
Lei Yu, Jan Buys, and Phil Blunsom. Online segment to segment neural transduction. In EMNLP, pp. 1307­1316, 2016.
Wojciech Zaremba and Ilya Sutskever. Reinforcement learning neural turing machines. CoRR, abs/1505.00521, 2015.
Chunting Zhou and Graham Neubig. Multi-space variational encoder-decoders for semi-supervised labeled sequence transduction. Association for Computational Linguistics, 2017.
APPENDIX
10

Under review as a conference paper at ICLR 2019

but the same goes for renewables as well
.

but 0.75 the
0.60 same goes
0.45 for
0.30 renewables as
0.15 well
.

0.8 0.6 0.4 0.2

dasselbe gilt
allerdings auch für
erneuerbare energien . dasselbe gilt allerdings auch für
erneuerbare energien .

and we &apos;re building upon innovations of generations who went before us
.

(a)

and

0.75

we &apos;re

building 0.60 upon

innovations

0.45 of

generations

0.30 who

went

0.15

before us

.

(b)

0.60 0.45 0.30 0.15

wir bauen
auf innovationen
von generationen
vor uns auf
. wir bauen auf innovationen von generationen vor uns auf
.

(c) (d)
Figure 5: Individual attention distribution for some sentences, Postr-Jointon left and Soft-Attn on the right

11


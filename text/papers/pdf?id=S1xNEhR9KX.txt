Under review as a conference paper at ICLR 2019
ON THE SENSITIVITY OF ADVERSARIAL ROBUSTNESS TO INPUT DATA DISTRIBUTIONS
Anonymous authors Paper under double-blind review
ABSTRACT
Neural networks are vulnerable to small adversarial perturbations. Existing literature largely focused on understanding and mitigating the vulnerability of learned models. In this paper, we demonstrate an intriguing phenomenon about adversarial training, arguably the most popular robust training method in the literature: Adversarial robustness, unlike clean accuracy, is sensitive to the input data distribution. Even a semantics-preserving transformations on the input data distribution can cause a significantly different robustness for the adversarial trained model that is both trained and evaluated on the new distribution. Our discovery of such sensitivity on data distribution is based on a study which disentangles the behaviors of clean accuracy and robust accuracy of the Bayes classifier. Further empirical investigation on MNIST and CIFAR10 confirms our finding that numerous MNIST and CIFAR10 variants achieve comparable clean accuracies in various different neural nets under standard training but significantly different robustness under adversarial training. This counter-intuitive phenomenon indicates that input data distribution alone can affect the adversarial robustness of trained neural networks, not necessarily the tasks themselves. Lastly, we discuss the practical implications on evaluating adversarial robustness, and make initial attempts to understand this complex phenomenon.
1 INTRODUCTION
Neural networks are vulnerable to adversarial examples. An otherwise highly accurate network can be greatly fooled by artificially constructed small perturbations on the original samples that don't affect human's perception (Szegedy et al., 2013; Biggio et al., 2013). Since the first discovery of adversarial examples, great progresses in constructing stronger adversarial attacks have been made in the literature, from the fast gradient sign method (Goodfellow et al., 2014), DeepFool (Moosavi-Dezfooli et al., 2016), projected gradient descent (PGD) (Kurakin et al., 2016; Madry et al., 2017), to the Carlini and Wagner (2017) optimizer-based attack, among others. In contrast, defense methods for such adversarial attacks are relatively less successful in the literature. Many defense methods were bypassed by the attackers (Carlini and Wagner, 2016; Athalye et al., 2017; 2018), because they do not increase the "intrinsic" robustness of the model. Notable defense methods include provably robust methods (Sinha et al., 2017; Raghunathan et al., 2018; Kolter and Wong, 2017), and adversarial training (Madry et al., 2017; Erraqabi et al., 2018; Huang et al., 2015; Kurakin et al., 2016). Arguably, adversarial training so far is the most popular defense method due to its simplicity, effectiveness and scalability.
This paper starts the investigation on the difficulty in achieving adversarial robustness, motivated by the empirical observations on the MNIST dataset and the CIFAR10 dataset. In particular, while MNIST and CIFAR10 have similar SOTA clean accuracies (difference is less than 3%) (Gastaldi, 2017), CIFAR10 has a much lower achievable robustness than MNIST in practice. After PGD adversarial training (Madry et al., 2017), MNIST has 89.3% accuracy under perturbation  with    0.3 under the strongest PGD attack, but CIFAR10 has only 45.8% accuracy under a much smaller  with    8/255. The accuracy difference is more than 40%, even without considering the difference in perturbation lengths.
We first perform theoretical analysis with the tools from the concentration properties of (probability) measures. We show the invariance of regular Bayes error to invertible transformation of data distribution and illustrate that robust error of the perfect decision boundary do not enjoy this property. We also given an example where robustness is not achievable: if input data is uniformly distributed in a
1

Under review as a conference paper at ICLR 2019
unit cube, then the prefect decision boundary in terms of clean accuracy cannot be robust under 2 attack. In addition, we show provably robust models can be trained to have only 3% provable robust error on binarized MNIST (semantically almost identical to MNIST). At the same time, this cannot be achieved on regular MNIST data. Aligning the theoretical analyses and further empirical studies, we notice the important role of the data distribution in adversarial robustness.
The second contribution of this paper is a series of carefully designed experiments on variants of MNIST and CIFAR10 datasets to study the influence of the data distribution on the achieved adversarial robustness of adversarial training, as shown in Figure 1. Our results lead to an intriguing property of adversarial training:
A semantically lossless shift on the data distribution could result in a significantly different robustness for adversarial trained models.
Such property is really undesirable and raises serious concerns about the deployment of a believedto-be-robust model in a real product. Indeed in a standard development procedure, various models (for example different network architectures) would be prototyped and measured on the existing data. If adversarial training is employed to train the robust models, the truthfulness of the performance estimations then becomes questionable, as one would expect that future data will be slightly shifted. Note that this is different from the transferability of a fixed model that is trained on different data distribution. Even retraining the model on the new data may give you a completely different accuracy and robustness on the same new distribution. This phenomenon was previously unnoticed largely because clean accuracy of standard training is not sensitive to such shifts, as we show in later sections. We particularly illustrate the practical implications with two examples: 1) robust accuracy of PGD trained model is sensitive to gamma values of gamma corrected CIFAR10 images. This indicates that image dataset collected under different conditions will have different robustness properties; 2) both as a "harder" version of MNIST, the fashion-MNIST (Xiao et al., 2017) and edge-fashion-MNIST (an edge detection variant described in Section 4.2) exhibit completely different robust characteristics. This suggests that more care is needed on choosing the right dataset to evaluate ant match adversarial robustness in practical tasks.
The last part is an initial attempt to understand how the data distribution may influence robustness. Such knowledge may help us mitigate the aforementioned "unstable" measurement problem. We specifically look at perturbable volume, inter-class distances, and required model capacity and sample complexity. Our analysis shows that understanding this intriguing phenomenon is non-trivial.
1.1 RELATED WORK
Existing efforts mostly focused on analyzing and improving classification models, and only a few recent papers touch on the issue of data distributions' influences on robustness. Schmidt et al. (2018) proves that it requires significantly fewer samples to achieve adversarially robust generalization on Bernoulli distributed data than on data from mixtures of Gaussian. Empirically, they also show that the lack of data could be the reason that we cannot achieve higher robustness on CIFAR10. Similarly, our focus is on adversarial robustness' sensitivity to changes in input data distribution. We show that robustness difference not only exists in binary v.s. continuous distribution, or MNIST v.s. CIFAR10, but gradual semantics-preserving transformations of data distribution can cause large changes to datasets' achievable robustness. Tsipras et al. (2018) hypothesizes that data has robust and nonrobust features which poses an intrinsic tradeoff between clean accuracy and adversarial robustness. We extend this perspective and show that under standard learning settings (training algorithm, model and training set size), there are different levels of tradeoff depending on the characteristics of input data distribution. Fawzi et al. (2015) defines "distinguishability", the distance between classes, to quantify easiness of achieving robustness on a certain dataset. In later sections, we also define a similar quantity, inter-class distance, exploring the robustness differences between dataset variants. While intuitively plausible, we concretely show counter examples such that datasets with the same inter-class distance could have different robustness properties. And we believe that it is unlikely a simple distance measures can explain the robustness variation across datasets.
1.2 NOTATION AND PROBLEM SETUP
We specifically consider the image classification problem where the input data is inside a high dimensional unit cube. We denote the data distribution as a joint distribution P(X, Y ), where X  [0, 1]d, d is the number of pixels, and Y  {1, 2, . . . , k} is the discrete label. Note that here we assume the support of X is the whole pixel space [0, 1]d. When X is a random noise (or human
2

Under review as a conference paper at ICLR 2019

perceptually unclassifiable image), one can think of P(y | X) being closed to uniform distribution on labels. In the standard setting, the samples (xi, yi) can be interpreted as xi is independently sampled from the marginal distribution P(X), and then yi is sampled from P(Y | X = xi). In this paper, we discuss P(X)'s influences on adversarial robustness, given a fixed P(y|X).
In our experiments, we only discuss the whitebox robustness, as it represents the "intrinsic" robustness. We use models learned by adversarially augmented training (Madry et al., 2017) (PGD training), which has the SOTA whitebox robustness. We consider bounded  attack as the attack for evaluating robustness for 2 reasons: 1) PGD training can defend against  relatively well, while for other attacks, how to train a robust model is still an open question; 2) in the image domain  attack is the mostly widely researched attack.

2 THEORETICAL ANALYSES AND PROVABLE CASES
As mentioned in the introduction, although the SOTA clean accuracies are similar for MNIST and CIFAR10, empirical investigations suggest that robust accuracy on CIFAR10 is much more difficult to achieve, which indicates the different behaviors of clean accuracy and robust accuracy. The first result in this section is to further confirm this indication in a simple setting, where clean accuracy remains the same but robust accuracy completely changes under distribution shift. Based on results from the concentration of measure literature, we then show that under uniform distribution basically no algorithm can achieve good robustness, as long as they have high clean accuracy. Such theoretical results are in sharp contrast to our practical experience. Therefore, we further examine the performance of a verifiable defense method on binarized MNIST (pixels values rounded to 0 and 1), and the result suggests the exact opposite: provable adversarial robustness on MNIST-like dataset is achievable. Such contrast thus clearly suggests the important role of the data distribution in achieving adversarial robustness.

2.1 DISENTANGLE CLEAN ACCURACY AND ROBUST ACCURACY
Let H denote the universal set of all the measurable functions. Given a joint distribution P(x, y) on the space X × Y, we define the Bayes error R = infhH EP(x,y)L(y; h(x)) = R(P(x, y)), where L is the objective function. In other words, Bayes error is the error of the best possible classifier we can have, h, without restriction on the function space of classifiers. We further define (adversarial) robust error RR(h) = EP(x,y) max  < L(y; h(x + )) = RR(P(x, y)). We denote RR = RR(h) to be the robust error achieved by the Bayes classifier h. For simplicity, we assume our algorithm can always learn h, which reduces clean accuracy to be (1 - Bayes error), and robust accuracy of the Bayes classifier to be (1 - RR).

One immediate result is that Bayes error remains the same under any distribution shift induced by an injective map T : X  X . To see that, simply note that T -1 exists and h  T -1 gives the

same Bayes error for the shifted distribution. However, such invariance property does not hold for

the robust error of the Bayes classifier. Furthermore, the following two examples show that Bayes

error can have completely different behavior from its robust error. Although both examples have 0

Bayes error, they have completely different robust errors.

Example 1. Assume x is uniformly distributed in [0, 1]d and y = 1, for all x with x e1 > 1/2 and y = 0, for x e1  1/2, where e1 is the one-hot vector. We use the 0-1 loss here. Note that the Bayes error decision boundaries are given by the following hyperplane: HP1 = {x  [0, 1]d : x1 = 0},

and thus

R = 0; RR = 2 ,

under the budget   < . In this case, the robust error is tolerable and relatively robust measured by the fraction of points that are successfully attacked, 2 .

Moreover, consider an injective map T which maps {x :

x

e1 > 1/2} to {x :

x

1>

d 2

},

and

{x : x

e1  1/2} to {x : x

1



d 2

}1.

The

Bayes

error

on

the

new

distribution

remains

0,

as

T

is

invertible. In contrast, the robust error is much worse. As shown in Proposition 2.1,

RR



1

-

1 4d

2.

Remark 2.1. Note that here the robust error of the Bayes classifier will grow to 1 as the dimensionality increases, for a fixed budget .

1Such map can be easily constructed.

3

Under review as a conference paper at ICLR 2019

Proposition 2.1 (Existence of Non-Adversarially Robust Decision Boundary). Let x be uniformly

distributed on [0, 1]d and y =

1, for all x such that x

1>

d 2

and y

=

0 otherwise.

Consider

adversarial attack under budget   < . Then for zero-one loss L:

RR = EP(X,Y )

max
 <

L(Y

; h(x

+

))



1

-

1 4d

2

2.2 DIFFICULTY IN ACHIEVING ROBUSTNESS

Example 1 shows that good clean accuracy does not necessary lead to good robust accuracy. In contrast, we will show in this section that achieving a good robust accuracy is impossible given uniformly distributed data, as long as we ask for good clean accuracies. Our tool are classical results from the concentration of measure (Ledoux, 2005).

Let A := {x  RN |d(x, A) < } denote the -neighborhood of the nonempty set A. Theorem 2.1

provides a lower bound on the mass in A .

Theorem 2.1 (Concentration of Measure on the Unit Cube and the Unit Ball). Let [0, 1]d denote

the unit d-cube and Bd denote the Euclidean unit d-ball, both equipped with uniform probability

distributions. Let > 0. Then for any A  [0, 1]d with P(A)  1/2, we have:

P(A

)



(

 2

+ -1(P(A)))



1

-

e-

2

(1)

For any B  Bd, with P(B)  1/2,

P(B

)



1-

1 (1 -  P(B)

2(

))2d



1-



1 e-2d(

2- 3

3)

P(B)

2

(2)

where  2 ( ) = 1 -

1-

2
4

and  is the standard normal cumulative distribution function.

Based on Theorem 2.1 we can now show that under some circumstances, no algorithm that achieves
can perfect clean accuracy can also achieve a good robust accuracy.
Example 2 (Vulnerability Guarantee). Consider the joint distribution P(x, y), where the input data x is uniformly distributed on [0, 1]d and label y has 10 classes. Further assume the marginal distribution of y is also uniform2. Theorem 2.1 implies that under 2 adversarial attack with = 0.5, at least 94 % of the samples are ether wrongly classified or can be successfully attacked for a classifier
with perfect clean accuracy.

Furthermore, if d = 3  32  32, A parallel calculation for P(x, y) on the Bd domain gives: under 2 adversarial attack with = 0.09 , at least 97 % of the the samples are ether wrongly classified or can be successfully attacked for a classifier with perfect clean accuracy.

On the one hand, Theorem 2.1 and Example 2 suggest that uniform distribution on a [0, 1]d enjoys
more robustness and is not affected by the high dimensionality. This may partially explain why
MNIST is more adversarially robust than CIFAR10, as the distribution of x in CIFAR10 is "closer" to Bd than to [0, 1]d. On the other hand, while not completely sharp, they also suggest the intrinsic
difficulty in achieving good robust accuracy.

Note that one limit of Theorem 2.1 and Example 2 is the uniform distribution assumption, which is surely not true for natural images. Indeed, although rigorously developed, Theorem 2.1 and Example 2 do not explain certain empirical observations. Following Kolter and Wong (2017), we train a provably3 robust model on a binarized MNIST dataset (bMNIST) 4. Our experiments shows that the learned model is (rigorously) guaranteed to achieve less than 3.00% robust error on bMNIST test data, while maintains 97.65% clean accuracy. Details of this experiments in described in Appendix B.2.

Such experiment clearly suggests the essential role of the data distribution in achieving good robust and clean accuracy. While it is hard to completely answer the question what geometric properties differentiate the concentration rates between the ball/the cube in high dimension, and the distribution of bMNIST. We remark that one obvious difference is the distance distributions in both spaces.

2but their joint distribution is not necessary uniform. 3"Provably" means that the robust accuracy of the model can be rigorously proved. 4It is created by rounding all pixel values to 0 or 1 from the original MNIST

4

Under review as a conference paper at ICLR 2019

Could the distance distributions explain the differences in achievable clean and robust accuracy? Note that the same method can only achieve 37.70% robust error on original MNIST data, and even higher error on CIFAR10, which further supports this hypothesis. In the rest of this paper, we further investigate the dependence of robust accuracy on the distribution of real data.

3 ROBUSTNESS ON DATASETS VARIANTS WITH DIFFERENT INPUT
DISTRIBUTIONS
Section 2.2 clearly suggests that the data distribution plays an essential role in the achievable robust accuracy. In this section we carefully design a series of datasets and experiments to further study its influence. One important property of our new datasets is that they have different P(x)'s while keep P(y|x) reasonably fixed, thus these datasets are only different in a "semantic-lossless" shift. Our experiments reveal an unexpected phenomenon that while standard learning methods manage to achieve stable clean accuracies across different data distributions under "semantic-lossless" shifts, however, adversarial training, arguably the most popular method to achieve robust models, loses this desirable property, in that its robust accuracy becomes unstable even under a "semantic-lossless" shift on the data distribution.
We emphasize that different from preprocessing steps or transfer learning, here we treat the shifted data distribution as a new underlying distribution. We both train the models and test the robust accuracies on the same new distribution.

3.1 SMOOTHING AND SATURATION
We now explain how the new datasets are generated under "semantic-lossless" shifts. In general, MNIST has a more binary distribution of pixels, while CIFAR10 has a more continuous spectrum of pixel values, as shown in Figure 1a and Figure 1b. To bridge the gap between this two datasets that have completely different robust accuracies, we propose two operations to modify their distribution on x: smoothing and saturation, as described below. We apply different levels of "smoothing" on MNIST to create more CIFAR-like datasets, and different levels of "saturation" on CIFAR to create more "binary" ones. Note that we would like to maintain the semantic information of the original data, which means that such operations should be semantics-lossless and not arbitrarily wide.

Smoothing is applied on MNIST images, to make images "less binary". Given an image xi, its smoothed version x~i(s) is generated by first applying average filter of kernel size s to xi to generate an intermediate smooth image, and then take pixel-wise maximum between xi and the intermediate smooth image. Our MNIST variants include the binarized MNIST and smoothed MNIST with
different kernel sizes. As shown in Figure 1c, all MNIST variants still maintain the semantic information in MNIST, which indicates that P(y | x~(s)) should be similar to P(y | x). It is thus reasonable to assume that yi is approximately sampled from P(y | x~(s)), and as such we assign yi as the label of x~(s). Note that all the data points in the binarized MNIST are on the corners of the unit cube. For
the smoothed versions, pixels on the digit boundaries are pushed off the corner of the unit cube.

Saturation of the image xi is denoted by xi(p), and the procedure is defined as below: for each pixel of x, v, its corresponding pixel of x(p) is defined as

sign(2v

-

1)

|2v

-

1|

2 p

1 +

 [0, 1].

22

Saturation is used to generate variants of the CIFAR10 dataset with less centered pixel values. For different saturation level p's, one can see from Figure 1d that x(p) is still semantically similar to x in the same classification task. Similarly we assign yi as the label of x(p). One immediate property about x(p) is that it pushes x to the corners of the data domain where the pixel values are either 0 or 1 when p  2, and pull the data to the center of 0.5 when p  2. When p = 2 it does not change the image, and when p =  it becomes binarization.

3.2 EXPERIMENTAL SETUPS
In this section we use the smoothing and saturation operations to manipulate the data distributions of MNIST and CIFAR10, and show empirical results on how data distributions affects robust accuracies of neural networks trained on them. Since we are only concerned with the intrinsic robustness of neural networks models, we do not consider methods like preprocessing that tries to remove perturbations or randomizing inputs. We perform standard neural network training on clean data to

5

Under review as a conference paper at ICLR 2019

(a) Pixel value histogram (log scale in y) of MNIST (b) Pixel value histogram (log scale in y) of CIFAR10

variants, from left to right: original, smoothed with variants, from left to right: original, saturation level 4,

kernel size 2, 3, 4, 5

8, 16, 64

(c) MNIST variants, from left to right: binarized, original, smoothed with kernel size 2, 3, 4, 5

(d) CIFAR10 variants, from left to right, original, saturation level 4, 8, 16, 64, 

Figure 1: Variants of smoothed MNIST and saturated CIFAR10 datasets.

measure the difficulty of the classification task, and projected gradient descent (PGD) based adversarial training (Madry et al., 2017) to measure the difficulty to achieve robustness.
By default, we use LeNet5 on all the MNIST variants, and use wide residual networks (Zagoruyko and Komodakis, 2016) with widen factor 4 for all the CIFAR10 variants. Unless otherwise specified, PGD training on MNIST variants and CIFAR10 variants all follows the settings in Madry et al. (2017). Details of network structures and training hyperparameters can be found in Appendix B.
We evaluate the classification performance using the test accuracy of standardly trained models on clean unperturbed examples, and the robustness using the robust accuracy of PGD trained model, which is the accuracy on adversarially perturbed examples. Although not directly indicating robustness, we report the clean accuracy on PGD trained models to indicate the tradeoff between being accurate and robust. To understand whether low robust accuracy is due to low clean accuracy or vulnerability of model, we also report robustness wrt predictions, where the attack is used to perturb against the model's clean prediction, instead of the true label. We use  untargeted PGD attacks (Madry et al., 2017) as our adversary, since it is the strongest attack in general based on our experiments. Unless otherwise specified, PGD attack on MNIST variants runs with = 0.3, step size of 0.01 and 40 iterations, and runs with = 8/255, step size of 2/255 and 10 iterations on CIFAR10 variants , same as in Madry et al. (2017).
3.3 SENSITIVITY OF ROBUST ACCURACY TO DATA TRANSFORMATIONS

(a) MNIST results under different smooth levels (b) CIFAR10 results under different saturation levels
Figure 2: Accuracy, Robust Accuracy and Robustness wrt Predictions on different data variants Results on MNIST variants are presented in Figure 2a 5. According to the results, the clean accuracy of standard training is very stable across different MNIST variants. This indicates that their classification tasks have similar difficulties,if the training has no robust considerations. When performing PGD adversarial training, clean accuracy drops only slightly. However, both robust accuracy and robustness wrt predictions drop significantly. This indicates that as smooth level goes up, it is significantly harder to achieve robustness. Note that for binarized MNIST with adversarial training, the clean accuracy and the robust accuracy are almost the same. Indicating that getting high robust accuracy on binarized MNIST does not face a conflict against achieving high clean accuracy. This result conforms with results of provably robust model having high robustness on binarized MNIST described in Section 2.
5Exact numbers are listed in Table 2 in Appendix C.
6

Under review as a conference paper at ICLR 2019
Different results on CIFAR10 variants are reported in Figure 2b. 6, which tells a similar story. For standard training, the clean accuracy maintains almost at the original level until saturation level 16, despite that it is already perceptually very saturated. In contrast, PGD training has a different trend. Before level 16, the robust accuracy significantly increases from 43.2% until 79.7%, while the clean test accuracy drops only in a comparatively small range, from 85.4% to 80.0%. After level 16, PGD training has almost the same clean accuracy and robust accuracy. However, robustness w.r.t. predictions still keeps increasing, which again indicates the instability of the robustness. On the other hand, if the saturation level is smaller than 2, we get worse robust accuracy after PGD training, e.g. at saturation level 1 the robust accuracy is 33.0%. Simultaneously, the clean accuracy maintains almost the same.
Note that after saturation level 64 the standard training accuracies starts to drop significantly. This is likely due to that high degree of saturation has caused "information loss" of the images. Models trained on highly saturated CIFAR10 are quite robust and the gap between robust accuracy and robustness w.r.t. predictions is due to lower clean accuracy. In contrast, In MNIST variants, the robustness wrt predictions is always almost the same as robust accuracy, indicating that drops in robust accuracy is due to adversarial vulnerability.
From these results, we can conclude that robust accuracy under PGD training is much more sensitive than clean accuracy under standard training, to the differences in input data distribution. More importantly, a semantically-lossless shift on the data transformation, while does not introduce any unexpected risk for the clean accuracy of standard training, can lead to large variations in robust accuracy. Such previously unnoticed sensitivity raised serious concerns in practice, as discussed in the next section.
4 PRACTICAL IMPLICATIONS
Given adversarial robustness' sensitivity to input distribution, we further demonstrate two practical implications: 1) Robust accuracy could be sensitive to image acquisition condition and preprocessing. This leads to unreliable benchmarks in practice; 2) When introducing new dataset for benchmarking adversarial robustness, we need to carefully choose datasets with the right characteristics.
4.1 ROBUST ACCURACY IS SENSITIVE TO GAMMA CORRECTION
The natural images are acquired under different lighting conditions, with different cameras and different camera settings. They are also preprocessed in different ways. All these factors could lead to mild shift on input distribution. Therefore, we might get very different performance measures when performing adversarial training on images taken under different conditions. In this section, we use gamma mapping to transform CIFAR10 images to different variants. These variants are then used to represent image dataset acquired under different conditions. Gamma mapping is a simple element-wise operation that takes the original image x, and output the gamma mapped image x~() by performing x~() = x. Gamma mapping is commonly used to adjust the exposure of an images. We refer the readers to Szeliski (2010) on more details about gamma mappings. Figure 3a shows variants of the same image processed with different gamma values. Lower gamma value leads to brighter images and higher gamma values gives darker images, since pixel values range from 0 to 1. Despite the changes in brightness, the semantic information is preserved.
We perform the same experiments as in the saturated CIFAR10 variants experiment in Section 3. The results are displayed in Figure 3a. Accuracies on clean data almost remain the same across different gamma values. However, under PGD training, both accuracy and robust accuracy varies largely following different gamma values.
These results should raise practitioners' attention on how to interpret robustness benchmark "values". For the same adversarial training setting, the robustness measure might change drastically between image datasets with different "exposures". Note that this is not to use a trained model on slightly varied datasets. The discrepancies of robust performance measures is between models trained on slightly varied datasets separately. In other words, if an training algorithm achieves good robustness on one image datasets, it doesn't necessarily achieve similar robustness on another semantically-identical but slightly varied datasets. Therefore, the actual robustness could either be significantly underestimated or overestimated.
6Exact numbers are listed in Table 3 in Appendix C.
7

Under review as a conference paper at ICLR 2019

fMNIST: accuracy, standard training, 92.7% accuracy, PGD training, 81.2%
robust accuracy, PGD training, 65.3% efMNIST: accuracy, standard training, 88.3%
accuracy, PGD training, 87.2% robust accuracy, PGD training, 86.6%

(a) Top: Gamma mapped images from left to right 0.6, (b) Top: Examples of fashion-MNIST images and

0.8, 1.0 (original image), 1.2 , 1.4; Bottom: Robust- edge-fashion-MNIST; bottom: Robustness results on

ness results on gamma mapped CIFAR10 variant

fMNIST and efMNIST

Figure 3: Illustrations on Practical Implications

This raises the questions on whether we are evaluating image classifier robustness in a reliable way, and how we choose benchmark settings that can match the real robustness requirements in practice. This is an important open question and we defer it to future research.

4.2 CHOICE OF DATASETS FOR EVALUATING ROBUSTNESS
As discussed, evaluating robustness on a suitable dataset is important. Here we use fashion-MNIST (fMNIST) (Xiao et al., 2017) and edge-fashion-MNIST (efMNIST) as examples to analyze characteristics of "harder" datasets. The edge-fashion MNIST is generated by running Canny edge detector (Canny, 1986) with  = 1 on the fashion MNIST images. Figure 3b shows examples of fMNIST and efMNIST. We performed the same standard training and PGD training experiments on both fMNIST and efMNIST as we did on MNIST. Figure 3b shows the results. We can see that fMNIST exhibit similar behavior to CIFAR10, where the test accuracy is significantly affected by PGD training and the gap between robust accuracy and accuracy is large. On the other hand, efMNIST is closer to the binarized MNIST: the accuracy is affected very little by PGD training, along with an insignificant difference between robust accuracy and accuracy.
Both fMNIST and efMNIST can be seen as a "harder" MNIST, but they are harder in different ways. One one hand, since efMNIST results from the edge detection run on fMNIST, it contains less information. It is therefore harder to achieve higher accuracy on efMNIST than on fMNIST, where richer semantics is accessible. However, fMNIST's richer semantics makes it better resembles natural images' pixel value distribution, which could lead to increased difficulty in achieving adversarial robustness. efMNIST, on the other hand, can be viewed as a set of "more complex binary symbols" compared to MNIST or binarized MNIST. It is harder to classify these more complex symbols. However, it is easy to achieve high robustness due to the binary pixel value distribution.
To sum up, when introducing new dataset for adversarial robustness, we should not only look for a "harder" one, but we also need to consider whether the dataset is "harder in the right way".
5 ATTEMPTS TO UNDERSTAND THE PHENOMENON
In this section, we make initial attempts to understand the sensitivity of adversarial robustness. We use saturated CIFAR10 as the running example. These analyses also apply to smoothed MNIST. Saturation pushes pixel values towards 0 or 1, thus pushing data points towards the corner of unit cube. This immediately leads to: 1) the "perturbable volume" decreases; 2) distances between data examples increases. Intuitively, both could be related to the increasd robustness. We analyze them and show that although they are correlated with robustness change, none of them can fully explain the observed phenomena. We then further examine the possibility of increasing robust accuracy on less robust datasets by having larger models and more data.

5.1 ON THE INFLUENCE OF PERTURBABLE VOLUME
Saturation moves the pixel values towards 0 and 1, therefore pushing the data points to the corners of the unit cube input domain. This makes the valid perturbation space to be smaller, since the space of perturbation is the intersection between the -  ball and the input domain. Due to high dimen-

8

Under review as a conference paper at ICLR 2019

Table 1: Different robust accuracies on datasets with same inter-class distances

INTER-CLASS DISTANCES
7.12 7.01 6.85

SMOOTH
LEVEL OF
SMOOTHED MNIST
3 4 5

RESILIENCE
OF
SMOOTHED MNIST
91.3 % 90.3 % 89.6 %

SCALE FACTOR OF SCALED ORIGINAL MNIST
0.970 0.955 0.932

RESILIENCE OF SCALED ORIGINAL
MNIST
94.6 % 95.5 % 94.9 %

SCALE FACTOR OF SCALED BINARIZED MNIST
0.821 0.809 0.790

RESILIENCE OF SCALED BINARIZED
MNIST
98.6 % 98.6 % 98.5 %

sionality, the volume of "perturbable region" changes drastically across different saturation levels. For example, the average log perturbable volume 7 of original CIFAR10 images are -12354, and the average log perturbable volume of -saturated CIFAR10 is -15342, which means that the perturbable volume differs by a factor of 22990 = 2(-12352-(-15342)). If the differences in perturbable
volume is a key factor on the robustness' sensitivity, then by allowing the attack to go beyond the domain boundary 8, then the robust accuracies across different saturation levels should behave sim-
ilarly again, or at least significantly differ from the case of box constrained attacks. We performed
PGD attack allowing the perturbation to be outside of the data domain boundary, and compare the
robust accuracy to what we get for normal PGD attack within domain boundary. We found that the
expected difference is not observed, which serves as evidence that differences in perturbable volume
are not causing the differences in robustness.

5.2 ON THE INFLUENCE OF INTER-CLASS DISTANCE
When saturation pushes data points towards data domain boundaries, the distances between data points increase too. Therefore, the margin, the distance from data point to the decision boundary, could also increase. Since the margin is difficult to measure, we use the "inter-class distance" as an approximate measure. Inter-class distance 9 characterizes the distances between each class to rest of classes in each dataset. Intuitively, if the distances between classes are larger, then it should be easier to achieve robustness. We also observed (in Appendix D.2.1 Figure 5) that inter-class distances are positively correlated with robust accuracy. However, we also find counter examples where datasets having the same inter-class distance exhibit different robust accuracies. Specifically, We construct scaled variants of original MNIST and binarized MNIST, such that their inter-class distances are the same as smooth-3, smooth-4, smooth-5 MNIST. The scaling operation is defined as x~() = (x - 0.5) + 0.5, where  is the scaling coefficient. When  < 1. each dimension of x is pushed towards the center with the same rate. Table 1 shows the results. We can see that although having the same interclass distances, the smoothed MNIST is still less robust than the their correspondents of scaled binarized MNIST and original MNIST. This indicates the complexity of the problem, such that a simple measure like inter-class distance cannot fully characterize robustness property of datasets.

5.3 ON THE REQUIRED MODEL CAPACITY AND SAMPLE COMPLEXITY
In practice, it is unclear how far robust accuracy of PGD trained model is from adversarial Bayes error RR for the given data distribution. In the case RR is not yet achieved, this is a non-exhaustive list that we can improve upon: 1) better training/learning algorithm; 2) increase model capacity; 3) train on more data. Finding a better learning algorithm is beyond the scope of this paper. Here we inspect 2) and 3) to see if it is possible to improve robustness by having larger model and more data. For model capacity, we use differently sized LeNet5 by multiplying the number of channels at each layer with different widen factors. These factors include 0.125, 0.25, 0.5, 1, 2, 4. On CIFAR10 variants, we use WideResNet with widen factors 0.25, 1 and 4. For sample complexity, we follow the practice in Section 3 except that we use a weight decay value of 0.002 to prevent overfitting. For both MNIST and CIFAR10, we test on 1000, 3000, 9000, 27000 and entire training set. Both model capacity and sample complexity results are shown in Figure 4.
For MNIST, both training and test accuracies of clean training are invariant to model sizes, even we only use a model with widen factor 0.125. In slight contrast, both the training and test accuracy
7Definition of "log perturbable volume" and other detailed analysis of perturbable volume are given in Appendix D.1 and Table 7.
8So we have a controlled and constant perturbable volume across all cases, where the volume is that of the -  ball
9The calculation of "inter-class distance" and other detailed analyses are delayed to Appendix D.2.1 and Figure 5.

9

Under review as a conference paper at ICLR 2019

(a) MNIST results on model capacity

(b) CIFAR10 results on model capacity

(c) MNIST results on training set size

(d) CIFAR10 results on training set size

Figure 4: Model capacity and training set size's influences on accuracy and robust accuracy. In each subfigure, the top row contains accuracy and robust accuracy measured on training set, the bottom row contains results measured on test set.

of PGD training increase as the model capacity increases, but it plateaus after widen factor 1 at an almost 100% accuracy. For robust accuracy, training robust accuracy kept increasing as model gets larger until the value is close to 100%. However, test robust accuracy stops increasing after widen factor 1, additional model capacity leads to larger (robust) generalization gap. When we vary the size of training set, the model can always fit the training set well to almost 100% clean training accuracy under standard training. The clean test accuracy grows as the training set size get larger. Training set size has more significant impact on robust accuracies of PGD trained models. For most MNIST variants except for binarized MNIST, training robust accuracy gradually drops, and test robust accuracy gradually increases as the training set size increases. This shows that when training set size is small, PGD training overfits to the training set. As training set gets larger, the generalization gap becomes smaller. Both training and test robust accuracies plateau after training set size reaches 27000. Indicating that increasing the training set size might not help in this setting. In conclusion, for MNIST variants, increasing training set size and model capacity does not seem to help beyond a certain point. Therefore, it is not obvious on how to improve robustness on MNIST variants with higher smoothing levels.
CIFAR10 variants exhibit similar trends in general. One notable difference is that for PGD training, the training robust accuracy does not plateau as model size increases. However the test robust accuracy plateaus after wide factor 1. Also when training set size increases, the training robust accuracy drops and test robust accuracy increases with no plateau present. These together suggest that having more training data and training a larger model could potentially improve the robust accuracies on CIFAR10 variants. One interesting phenomenon is that binarized MNIST and -saturated CIFAR10 has different sample complexity property, despite both being "cornered" datasets. This indicates that the existence of complex interaction between the classification tasks and input data distribution.
6 CONCLUSION
Our theoretical analyses shows the significance of input data distribution in adversarial robustness, which further motivates our systematic experiments on MNIST and CIFAR10 variants. We discover that, counter-intuitively, robustness of adversarial trained models are sensitive to semanticallypreserving transformations on data. We then discuss the difficulties on evaluating robust learning algorithms. Finally, we make initial attempts to understand this sensitivity, and believe deeper understanding requires non-trivial future efforts.

10

Under review as a conference paper at ICLR 2019
REFERENCES
Athalye, A., Carlini, N., and Wagner, D. (2018). Obfuscated gradients give a false sense of security: Circumventing defenses to adversarial examples. arXiv preprint arXiv:1802.00420.
Athalye, A., Engstrom, L., Ilyas, A., and Kwok, K. (2017). Synthesizing robust adversarial examples. arXiv preprint arXiv:1707.07397.
Ball, K. (1997). An elementary introduction to modern convex geometry. Flavors of geometry, 31:1­58.
Biggio, B., Corona, I., Maiorca, D., Nelson, B., Srndic´, N., Laskov, P., Giacinto, G., and Roli, F. (2013). Evasion attacks against machine learning at test time. In Joint European conference on machine learning and knowledge discovery in databases, pages 387­402. Springer.
Canny, J. (1986). A computational approach to edge detection. IEEE Transactions on pattern analysis and machine intelligence, (6):679­698.
Carlini, N. and Wagner, D. (2016). Defensive distillation is not robust to adversarial examples. arXiv preprint arXiv:1607.04311.
Carlini, N. and Wagner, D. (2017). Towards evaluating the robustness of neural networks. In Security and Privacy (SP), 2017 IEEE Symposium on, pages 39­57. IEEE.
Erraqabi, A., Baratin, A., Bengio, Y., and Lacoste-Julien, S. (2018). A3t: Adversarially augmented adversarial training. arXiv preprint arXiv:1801.04055.
Fawzi, A., Fawzi, O., and Frossard, P. (2015). Analysis of classifiers' robustness to adversarial perturbations. arXiv preprint arXiv:1502.02590.
Gastaldi, X. (2017). Shake-shake regularization. arXiv preprint arXiv:1705.07485.
Gilmer, J., Metz, L., Faghri, F., Schoenholz, S. S., Raghu, M., Wattenberg, M., and Goodfellow, I. (2018). Adversarial spheres. arXiv preprint arXiv:1801.02774.
Goodfellow, I. J., Shlens, J., and Szegedy, C. (2014). Explaining and harnessing adversarial examples. arXiv preprint arXiv:1412.6572.
Huang, R., Xu, B., Schuurmans, D., and Szepesvári, C. (2015). Learning with a strong adversary. arXiv preprint arXiv:1511.03034.
Kingma, D. P. and Ba, J. (2014). Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980.
Kolter, J. Z. and Wong, E. (2017). Provable defenses against adversarial examples via the convex outer adversarial polytope. arXiv preprint arXiv:1711.00851.
Kurakin, A., Goodfellow, I., and Bengio, S. (2016). Adversarial machine learning at scale. arXiv preprint arXiv:1611.01236.
Ledoux, M. (2005). The concentration of measure phenomenon. Number 89. American Mathematical Soc.
Madry, A., Makelov, A., Schmidt, L., Tsipras, D., and Vladu, A. (2017). Towards deep learning models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083.
Moosavi-Dezfooli, S.-M., Fawzi, A., and Frossard, P. (2016). Deepfool: a simple and accurate method to fool deep neural networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2574­2582.
Raghunathan, A., Steinhardt, J., and Liang, P. (2018). Certified defenses against adversarial examples. arXiv preprint arXiv:1801.09344.
Schmidt, L., Santurkar, S., Tsipras, D., Talwar, K., and Madry, A. (2018). Adversarially robust generalization requires more data. arXiv preprint arXiv:1804.11285.
11

Under review as a conference paper at ICLR 2019
Sinha, A., Namkoong, H., and Duchi, J. (2017). Certifiable distributional robustness with principled adversarial training. arXiv preprint arXiv:1710.10571.
Szegedy, C., Zaremba, W., Sutskever, I., Bruna, J., Erhan, D., Goodfellow, I., and Fergus, R. (2013). Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199.
Szeliski, R. (2010). Computer vision: algorithms and applications. Springer Science & Business Media.
Tsipras, D., Santurkar, S., Engstrom, L., Turner, A., and Madry, A. (2018). There is no free lunch in adversarial robustness (but there are unexpected benefits). arXiv preprint arXiv:1805.12152.
Warde-Farley, D. and Goodfellow, I. (2016). 11 adversarial perturbations of deep neural networks. page 311.
Xiao, H., Rasul, K., and Vollgraf, R. (2017). Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms. arXiv preprint arXiv:1708.07747.
Xu, H., Caramanis, C., and Mannor, S. (2009). Robustness and regularization of support vector machines. Journal of Machine Learning Research, 10(Jul):1485­1510.
Zagoruyko, S. and Komodakis, N. (2016). Wide residual networks. arXiv preprint arXiv:1605.07146.
12

Under review as a conference paper at ICLR 2019

Appendix

A PROOFS

A.1 PROOF FOR PROPOSITION 2.1

Proof. The argument is well-known in concentration of measure. We provide here for the sake of

completeness and adapt it to the context.

The hyperplane HP2

= {x  [0, 1]d

:X

1=

d 2

}

defines the decision boundary. We first compute the orthogonal distance of a given point y =

(y1, y2, · · · , yd) = (x1 + 1, x2 + 2, · · · , xd + d) to HP2. The point y is the perturbed point

within budget   < . The vector 1 is orthogonal to HP2. Pick any point x  HP2, the

orthogonal distance from y to HP2 is:

2(y, HP2) =

P roj1(y - x)

=

(y - x) 1

1

11

= |y 1 - x 1| 1

= | 1| 1

y =|

1- 

d 2

|

d

 The last two equations show that the 2 distance under an  attack can grow at the rate of d, for this particular hyperplane HP2.
Now we take the expectation over [0, 1]d, and note that expectation of the uniform distribution over a product space [0, 1]d is the same as taking expectation on each dimension (Fubini's theorem), picking each random variable coordinatewise uniformly from [0, 1].

E[

2 2

(y,

H

P2)]

=

y E[(

1- d

d 2

)2]

=

1 d E[(

d

yi

-

d )2] 2

i=1

1d

1d

1

= d V[ yi] = d V[yi] = 4

i=1 i=1

Then we apply Markov's inequality, for all real number t > 0:

P(

2(x, H)



 t)

=

P(

2(x, H)2



t)



1 4t

Finally, we observe that the longest (in terms of 2 norm) such  attacks vector to HP2 are parallel to the normal vector 1 to HP2. They have 2 distance d. The set these attacks cover is characterized by {x  [0, 1]d : (x, H)  } = {x  [0, 1]d : 2(x, H)  d}. Let t = 2d, we have:

P(

2(x, H)



 t)

=

P(

2(x, H)2



t)



1 4t

=

1 4 2d



In the case of zero-one loss, RR = P( 2(x, H) 

d)



1

-

4

1
2

d

.

A.2 PROOF FOR THEOREM 2.1

Proof. (First Inequality for Cube) The proof here follows that of Ledoux (2005), but we track of the tight constants so as to give tighter adversarial robustness calculations.

13

Under review as a conference paper at ICLR 2019

Let  be one dimensional standard normal cumulative distribution function and let µd denote d dimensional Gaussian measures. Consider the map T : Rd - (0, 1)d:

T (x1, · · · , xd) = ((x1), · · · , (xd))

T pushes forward µd defined on Rd into a probability measure P on (0, 1)d:

P(A) = µd(T -1(A))

for A  (0, 1)d. Next we have the following Gaussian isoperimetric/concentration inequality

(Ledoux, 2005):

µd(B )  (-1(µd(B)) + )

for all B  Rd measureable.

Now for A  (0, 1)d, we have:



P(A

)

=

µd(T -1(A

))



µd(T -1(A)

 2

)



(-1(µd(T -1(A))

+

2 ))

where the first inequality follows from that T has Lipschitz constant 1 , and thus T -1 has Lips 2
chitz constant 2; and the second one follows from Gaussian isoperimetric inequality.

When P(A)  1/2,

 (-1(µd(T -1(A)) + 2 ))  (-1( 2 ))

Additionally, the inequality (x)



1

-

e

x2 2

implies the last inequality in the theorem.

(Second Inequality for Ball)

We first define the notion of modulus of convexity for a normed space, in this case 2:
x+y  2 ( ) = inf{1 - 2 : x = y = 1, x - y  }
2
=1- 1- 4

The important property about  2 ( ) is that there is a constant C such that:

 2( )  C 2



By

elementary

algebraic

calculuation,

We

can

take

C

=

2- 3

3.

By Equation (2.25) in (Ledoux, 2005),

P(A

)



1-

1 (1 -  P(B)

2(

))2d



1-

1 e-2d P(A)

2(

)

=

1-



1 e-2d(

2- 3

3)

P(A)

2

B DETAILED SETTINGS FOR TRAINING
B.1 DETAILED SETTINGS OF ADVERSARIAL TRAINING
The LeNet5 (widen factor 1) is composed of 32-channel conv filter + ReLU + size 2 max pooling + 64-channel conv filter + ReLU + size 2 max pooling + fc layer with 1024 units + ReLU + fc layer with 10 output classes. We do not preprocess MNIST images before feeding into the model. For training LeNet5 on MNIST variants, we use the Adam optimizer with an initial learning rate of 0.0001 and train for 100000 steps with batch size 50. We use the WideResNet-28-4 as described in Zagoruyko and Komodakis (2016) for our experiments, where 28 is the depth and 4 is the widen factor. We use "per image standardization" 10 to preprocess CIFAR10 images, following Madry et al. (2017).
10https://www.tensorflow.org/api_docs/python/tf/image/per_image_ standardization
14

Under review as a conference paper at ICLR 2019

Table 2: Performance and Robustness of models trained on MNIST variants.

MNIST
VARIANTS
BINARIZED ORIGINAL
SMOOTH 2 SMOOTH 3 SMOOTH 4 SMOOTH 5 SMOOTH 6 SMOOTH 7 SMOOTH 8

STANDARD TRAINING
TEST ACC
98.5 % 99.3 % 99.3 % 99.2 % 99.1 % 99.0 % 99.1 % 99.0 % 99.0 %

TEST ACC
98.9 % 99.2 % 98.9 % 99.0 % 98.8 % 98.7 % 98.5 % 98.3 % 97.9 %

PGD TRAINING

ROBUST ACCURACY = 0.3

ROBUSTNESS WRT PREDICTIONS = 0.3

98.1 % 95.1 % 93.0 % 91.3 % 90.3 % 89.6 % 87.6 % 85.4 % 83.1 %

98.5 % 95.1 % 93.1 % 91.4 % 90.4 % 89.7 % 87.7 % 85.5 % 83.3 %

Table 3: Performance and Robustness of models trained on CIFAR10 variants.

MNIST
VARIANTS
SATURATE 1 SATURATE 1.5 SATURATE 1.75
ORIGINAL
SATURATE 2.25 SATURATE 2.5 SATURATE 3 SATURATE 4 SATURATE 8 SATURATE 16 SATURATE 64 SATURATE 128 SATURATE 256
SATURATE INF

STANDARD TRAINING
TEST ACC
93.8 % 94.7 % 95.2 % 95.0 % 94.8 % 94.8 % 94.5 % 93.8 % 93.3 % 92.9 % 89.6 % 85.3 % 83.0 % 80.3 %

TEST ACC
77.5 % 83.7 % 84.9 % 85.4 % 85.4 % 84.8 % 82.9 % 80.4 % 80.4 % 79.9 % 79.5 % 80.2 % 80.0 % 80.0 %

PGD TRAINING

ROBUST ACCURACY = 0.3

ROBUSTNESS WRT PREDICTIONS = 0.3

33.0 % 38.7 % 41.1 % 43.2 % 44.4 % 46.4 % 51.7 % 64.0 % 78.1 % 79.4 % 79.3 % 79.9 % 79.7 % 79.7 %

33.6 % 39.1 % 41.5 % 43.6 % 44.9 % 47.0 % 52.9 % 68.7 % 93.8 % 98.4 % 99.1 % 99.1 % 99.2 % 99.2 %

For training WideResNet on CIFAR10 variants, we use stochastic gradient descent with momentum 0.9 and weight decay 0.0002. We train 80000 steps in total with batch size 128. The learning rate is set to 0.1 at step 0, 0.01 at step 40000, and 0.001 at step 60000. We performed manual hyperparameter search for our initial experiment and do not observe improvements over the above settings. Therefore we used these settings throughout the all the experiments in the paper unless otherwise indicated.
B.2 LP ROBUST MODEL DESCRIBED IN SECTION 2
For the linear programming based provably robust model (Kolter and Wong, 2017) (LP-robust model). We trained a ConvNet identical to the one in the original paper. It has 2 convolutional layers, with 16 and 32 channels, each with a stride of 2; and 2 fully connected layers, the first one maps the flattened convolution features to hidden dimension 100, the second maps to 10 logit units. We use ReLUs as the nonlinear activation and there is no max pooling in the network. We train for 100 epochs with batch size 50. The first 50 epochs are warm start epochs where epsilon increases from 0.01 to 0.3 linearly. We use Adam optimizer (Kingma and Ba, 2014) with a constant learning rate of 0.001.
C DETAILED EXPERIMENTAL RESULTS
We listed exact numbers of experiments involved in the main body in Table 2, 3, 4 and 5.
15

Under review as a conference paper at ICLR 2019

Table 4: Performance and robustness of different sized LeNet5 models on MNIST variants

WIDEN FACTOR
BINARIZED ORIGINAL
SMOOTH 2 SMOOTH 3 SMOOTH 4 SMOOTH 5 SMOOTH 6 SMOOTH 7 SMOOTH 8
WIDEN FACTOR
BINARIZED ORIGINAL
SMOOTH 2 SMOOTH 3 SMOOTH 4 SMOOTH 5 SMOOTH 6 SMOOTH 7 SMOOTH 8
WIDEN FACTOR
BINARIZED ORIGINAL
SMOOTH 2 SMOOTH 3 SMOOTH 4 SMOOTH 5 SMOOTH 6 SMOOTH 7 SMOOTH 8

0.125
99.9% 100.0% 99.9% 99.9% 99.9% 99.8% 99.8% 99.8% 99.7%

0.25
100.0% 100.0% 100.0% 99.9% 100.0% 100.0% 100.0% 99.9% 100.0%

TRAINING SET

0.5 1

100.0% 100.0% 100.0% 100.0% 100.0% 100.0% 100.0% 100.0% 100.0%

99.6% 100.0% 100.0% 100.0% 100.0% 100.0% 100.0% 100.0% 100.0%

STANDARD TRAINING, ACCURACY

2
100.0% 100.0% 100.0% 100.0% 100.0% 100.0% 100.0% 100.0% 100.0%

4
100.0% 100.0% 100.0% 100.0% 100.0% 100.0% 100.0% 100.0% 100.0%

0.125
98.7% 98.8% 98.8% 98.8% 98.7% 98.5% 98.4% 98.5% 98.4%

0.25
99.0% 99.2% 99.0% 98.8% 99.0% 99.0% 98.9% 98.8% 98.9%

TEST SET

0.5 1

99.2% 99.2% 99.1% 99.2% 99.0% 99.2% 99.0% 99.0% 98.9%

98.5% 99.3% 99.3% 99.2% 99.1% 99.0% 99.1% 99.0% 99.0%

2
99.4% 99.4% 99.3% 99.1% 99.4% 99.3% 99.2% 99.3% 99.2%

4
99.2% 99.3% 99.4% 99.3% 99.4% 99.3% 99.3% 99.3% 99.0%

0.125
97.8% 97.0% 96.1% 96.3% 95.3% 94.9% 93.2% 91.9% 89.4%

0.25
99.6% 98.4% 98.1% 97.8% 97.3% 96.5% 95.6% 95.0% 94.2%

TRAINING SET

0.5 1

100.0% 99.8% 99.0% 98.9% 98.5% 98.0% 97.4% 97.5% 96.5%

100.0% 100.0% 99.9% 99.7% 99.5% 99.3% 99.0% 98.7% 98.4%

PGD TRAINING, ACCURACY

2
100.0% 100.0% 100.0% 99.9% 99.8% 99.6% 99.5% 99.2% 99.0%

4
100.0% 100.0% 100.0% 100.0% 99.9% 99.8% 99.7% 99.4% 99.3%

0.125
97.4% 97.0% 96.1% 96.5% 95.4% 95.0% 93.5% 92.4% 89.7%

0.25
98.3% 98.2% 97.8% 97.6% 97.2% 96.5% 95.7% 95.2% 94.4%

TEST SET

0.5 1

98.8% 98.9% 98.5% 98.6% 98.1% 97.9% 97.1% 97.2% 96.4%

98.9% 99.2% 98.9% 99.0% 98.8% 98.7% 98.5% 98.3% 97.9%

2
99.0% 99.1% 99.0% 99.1% 99.0% 98.9% 98.7% 98.5% 98.2%

4
99.2% 99.2% 99.0% 99.1% 99.0% 98.9% 98.7% 98.7% 98.4%

0.125
95.2% 86.9% 80.5% 75.2% 71.9% 65.7% 58.0% 61.7% 70.3%

0.25
98.5% 90.8% 87.6% 82.0% 77.6% 77.1% 71.5% 74.2% 72.4%

TRAINING SET

0.5 1

100.0% 97.9% 90.9% 90.3% 87.5% 85.7% 80.5% 83.3% 80.3%

100.0% 99.3% 98.0% 95.5% 93.9% 92.5% 90.6% 87.6% 85.3%

PGD TRAINING, ROBUST ACCURACY

2
100.0% 99.6% 99.1% 97.8% 96.8% 94.6% 93.1% 90.5% 90.5%

4
100.0% 99.8% 99.5% 98.7% 97.9% 95.0% 93.8% 92.6% 88.7%

0.125
94.5% 87.1% 81.2% 75.7% 72.7% 66.2% 59.3% 62.8% 71.7%

0.25
96.5% 89.9% 87.0% 81.5% 77.7% 77.1% 72.0% 75.3% 73.2%

TEST SET

0.5 1

98.0% 95.2% 88.7% 88.5% 86.3% 85.1% 80.2% 83.0% 80.3%

98.1% 95.1% 93.0% 91.3% 90.3% 89.6% 87.6% 85.4% 83.1%

2
98.0% 94.8% 92.3% 91.6% 90.6% 89.8% 88.0% 86.7% 86.9%

4
98.0% 94.9% 92.1% 90.8% 90.0% 88.4% 87.2% 87.8% 83.8%

D DETAILED ANALYSES
D.1 DETAILED ANALYSIS OF EFFECTS OF DATA DOMAIN BOUNDARY
One natural hypothesis about the reason of achieving better robustness could be that it is the effect of the boundaries. Indeed, if the data distribution is closer to the data domain boundary, the valid perturbation space, the -  ball may be restricted since it will intersect with the boundary. We then test the correlation between "how close the data distribution is to the boundary" and its achievable robustness, by examining the volume of the allowed perturbed box across different datasets.
The intersection of the data domain, unit cube [0, 1]d, with the allowed perturbation space, -  ball [xi - , xi + ]d, is the hyperrectangle [max{xi - , 0}, min{xi + , 1}]d, where i = 1, · · · , d are the indexes over input dimensions. The size of the available perturbation space at x and is defined by the volume of this hyperrectangle:
d
Vol(x, ) = (min{xi + i, 1} - max{xi - i, 0})
i=1
In high dimensional space, when is fixed, this volume varies greatly based on the location of x. For example, if x is on one of the corners of the unit cube, Vol(xcorner, ) = d. If each dimension of x is at least away from all the data boundaries, then the volume of the hyperrectangle is Vol(xinside, ) = (2 )d. Therefore there can be 2d times difference of perturbable space between different data points. As shown in the average log perturbable volumes Table 6, we can see that different variations of datasets has significantly different perturbable volumes, with the same trend with previously described. It is notable that for the original CIFAR10 datasets has log volume -12354, which is very close to the -12270. The different of 84 bits indicates on average, the perturbation space is 284 smaller than the full -  ball if there is no intersection with the data domain boundary. Volume differences between different saturation or smooth level can be interpreted in the similar

16

Under review as a conference paper at ICLR 2019

Table 5: Performance and robustness of different sized Wide ResNet models on CIFAR10 variants

WIDEN FACTOR
SATURATE 1 SATURATE 1.5 SATURATE 1.75
ORIGINAL
SATURATE 2.25 SATURATE 2.5 SATURATE 3 SATURATE 4 SATURATE 8 SATURATE 16 SATURATE 64 SATURATE 128 SATURATE 256
SATURATE INF

STANDARD TRAINING, ACCURACY

TRAINING SET

TEST SET

0.25 1

4 0.25 1

4

85.5% 87.0% 87.4% 87.2% 87.3% 86.4% 86.2% 85.8% 84.6% 83.5% 80.5% 77.1% 73.7% 73.2%

99.9% 99.9% 99.9% 99.9% 99.9% 99.9% 99.9% 99.9% 99.8% 99.7% 99.4% 98.7% 97.6% 97.3%

100.0% 100.0% 100.0% 100.0% 100.0% 100.0% 100.0% 100.0% 100.0% 100.0% 100.0% 100.0% 100.0% 99.9%

82.4% 84.2% 84.5% 84.4% 84.5% 83.7% 84.0% 83.1% 81.2% 81.0% 79.2% 74.6% 70.7% 70.6%

91.1% 92.1% 93.0% 92.5% 92.5% 92.3% 92.2% 91.1% 90.1% 89.4% 86.9% 83.0% 76.5% 76.3%

93.8% 94.7% 95.2% 95.0% 94.8% 94.8% 94.5% 93.8% 93.3% 92.9% 89.6% 85.3% 83.0% 80.3%

WIDEN FACTOR
SATURATE 1 SATURATE 1.5 SATURATE 1.75
ORIGINAL
SATURATE 2.25 SATURATE 2.5 SATURATE 3 SATURATE 4 SATURATE 8 SATURATE 16 SATURATE 64 SATURATE 128 SATURATE 256
SATURATE INF

PGD TRAINING, ACCURACY

TRAINING SET

TEST SET

0.25 1

4 0.25 1

4

45.4% 52.1% 53.8% 56.1% 56.8% 57.6% 60.0% 62.8% 67.7% 67.2% 70.0% 71.4% 68.6% 71.5%

68.3% 76.5% 79.5% 81.4% 82.7% 83.9% 86.3% 91.3% 96.1% 96.1% 96.5% 96.4% 96.9% 96.9%

93.1% 98.0% 99.2% 99.7% 99.9% 100.0% 100.0% 100.0% 100.0% 99.9% 99.9% 99.9% 99.9% 99.9%

46.8% 53.3% 55.3% 57.1% 58.1% 58.3% 60.8% 63.7% 67.0% 66.0% 68.6% 68.9% 65.7% 69.7%

66.9% 74.1% 77.0% 78.4% 78.8% 79.1% 79.5% 77.9% 76.6% 76.4% 75.8% 76.6% 76.6% 76.1%

77.5% 83.7% 84.9% 85.4% 85.4% 84.8% 82.9% 80.4% 80.4% 79.9% 79.5% 80.2% 80.0% 80.0%

WIDEN FACTOR
SATURATE 1 SATURATE 1.5 SATURATE 1.75
ORIGINAL
SATURATE 2.25 SATURATE 2.5 SATURATE 3 SATURATE 4 SATURATE 8 SATURATE 16 SATURATE 64 SATURATE 128 SATURATE 256
SATURATE INF

PGD TRAINING, ROBUST ACCURACY

TRAINING SET

TEST SET

0.25 1

4 0.25 1

4

24.0% 29.0% 30.9% 32.4% 33.9% 35.5% 38.4% 44.9% 62.3% 66.0% 69.1% 70.7% 68.0% 70.9%

36.9% 44.4% 47.8% 50.4% 52.9% 55.4% 61.5% 77.4% 95.0% 95.5% 96.3% 96.2% 96.7% 96.7%

71.1% 81.3% 86.0% 90.3% 93.4% 96.0% 98.9% 99.7% 99.8% 99.9% 99.9% 99.9% 99.9% 99.9%

25.6% 31.6% 32.7% 35.0% 36.1% 37.5% 40.6% 46.1% 61.9% 65.0% 67.6% 68.2% 65.2% 69.2%

34.4% 40.7% 44.0% 45.5% 47.3% 49.1% 52.5% 60.4% 74.9% 75.5% 75.5% 76.2% 76.3% 75.8%

33.0% 38.7% 41.1% 43.2% 44.4% 46.4% 51.7% 64.0% 78.1% 79.4% 79.3% 79.9% 79.7% 79.7%

Table 6: Perturbable volumes of different variants of MNIST and CIFAR10. Values shown in table are the average log value (in bits) of volumes of test data. For MNIST, = 0.3, for CIFAR10
= 8/255.

MNIST (VALID RANGE -1361 TO -577)

BINARY ORIGINAL

3

5

-1361

-1297

-1265 -1234

ORIGINAL
-12354

CIFAR10 (VALID RANGE -15342 TO -12270) 4 8 16 64 256 -12394 -12477 -12657 -13620 -14747

512 -15028

INF
-15342

way. Note that for CIFAR10 images with large saturation, although they appear similar to human, they actually have very large differences in terms of perturbable volumes.
If the perturbable volume hypothesis holds, then we should observe significantly lower accuracy under PGD attack if we allow perturbation outside of data domain boundary. Since this greatly increases the perturbable volume. We measure the accuracy under PGD attack with and without considering data domain boundary for both MNIST and CIFAR10 variants. The results are shown in Table 7. "With considering boundary" corresponds to regular PGD attacks. We can see that allowing PGD to perturb out of bound do not reduce accuracy under attack. This means that PGD
17

Under review as a conference paper at ICLR 2019

Table 7: PGD attack results with and without domain boundary constraints on MNIST and CIFAR10

MNIST
VARIANTS
BINARIZED ORIGINAL
SMOOTH 2 SMOOTH 3 SMOOTH 4 SMOOTH 5

MNIST
ROBUST ACCURACY W/ BOUND
98.1 % 95.1 % 93.0 % 91.3 % 90.3 % 89.6 %

ROBUST ACCURACY W/O BOUND
96.1 % 95.1 % 92.9 % 91.5 % 90.6 % 89.9 %

CIFAR10
VARIANTS
SATURATE 1
ORIGINAL
SATURATE 4 SATURATE 8 SATURATE 16
SATURATE INF

CIFAR10
ROBUST ACCURACY W/ BOUND
33.0 % 43.2 % 64.0 % 78.1 % 79.4 % 79.7 %

ROBUST ACCURACY W/O BOUND
32.7 % 43.0 % 64.0 % 78.1 % 79.4 % 79.4 %

is not able to use the significantly larger additional volumes even for binarized MNIST or highly saturated CIFAR10, whose data points are on or very close to the corner. In some cases, allowing perturbation outside of domain boundary makes the attack slightly less effective. This might be due to that data domain boundary constrained the perturbation to be in an "easier" region. This might seem surprising considering the huge difference in perturbable volumes, these results conform with empirical results in previous research (Goodfellow et al., 2014; Warde-Farley and Goodfellow, 2016) that adversarial examples appears in certain directions instead of being distributed in small pockets across space. Therefore, the perturbable volume hypothesis is rejected.
D.2 DETAILED ANALYSES OF INTER-CLASS DISTANCE
D.2.1 CALCULATION OF INTER-CLASS DISTANCE
We calculate inter-class distance as follows. Let D = {xi} denote the set of all the input data points, Dc = {xi|yi = c} denote the set of all the data points in class c, and D¬c = {xi|yi = c} denote all the data points not in class c. Our goal is to calculate d(Dc, D¬c) for all the classes, where d(Dc, D¬c) approximates the margin between class c and the rest. To estimate d(Dc, D¬c), we first compute the margin for each data point x in class c. To do that, we calculate the average x - xj 2, where xj  D¬c is one of x's 10% nearest neighbors in D¬c. Lastly, the inter-class distance of class c, d(Dc, D¬c), is then calculated as the average of smallest 10% d(x, D¬c) for x  Dc.
Note that we choose 2 distance for inter-class distance, instead of using the  which measures the robustness. This is because -distance between data examples is essentially max over the per pixel differences, which is always very close to 1. Therefore the -distance between data examples is not really representative / distinguishable.
Figure 5 shows the inter-class distances (averaged over all classes) calculated on MNIST and CIFAR10 variants. The binarized MNIST has a significantly larger inter-class distance. As smoothing kernel size increases, the distance also decrease slightly. On CIFAR10 variants, as the saturation level gets higher, the inter-class distance increases monotonically. We also directly plot inter-class distance vs robust accuracy on MNIST and CIFAR10 variants. In general, inter-class distance shows strong positive correlation with robust accuracy under these transformations. With one exception that original MNIST has smaller inter-class distance, but is sightly more robust than smooth-2 MNIST. This, together with the counter examples we gave in Table 1, suggests that interclass distance cannot fully explain the robust variation across different dataset variants.
D.2.2 INTER-CLASS DISTANCE COULD POTENTIALLY INFLUENCE REQUIRED MODEL CAPACITY
We attempt to understand the relation between the inter-class distance of a dataset and its achievable robustness in this section. We first illustrate our intuition in a synthetic experiment, where a ReLU network is trained to perfectly separate 2 concentric spheres (Gilmer et al., 2018), as shown in Figure 6. Here the inter-class distance is the width of the ring between two spheres. In such example, adversarial training is actually closely related to the inter-class distance of the data. In fact, in the simple setting where the classifier is linear, it has been shown in Xu et al. (2009) that adversarial training, as a particular form of robust optimization, is equivalent to maximizing the classification margins. Following this intuition, one can easily see that the effect of adversarial training is to push two spheres close to each other, and requires the network to perfectly separate the new spheres with much smaller inter-class.

18

Under review as a conference paper at ICLR 2019
(a) Processing levels vs inter-class distances
(b) Inter-class distance vs robust accuracy Figure 5: Inter-class distance's influence on robust accuracy on different MNIST and CIFAR10 variants
Intuitively, when the inter-class distance is large, i.e. the gap between two spheres are large, reasonable model should be able to achieve good standard accuracy. We have also observed such phenomenon on original MNIST and saturated CIFAR (say level 16). As the inter-class gets smaller, although the model capacity could still be enough for the standard training, it may no longer be enough for adversarial training, upon which we would observe that although the test accuracies stay similar, accuracies under adversarial attack significantly would drop. We have also seen similar behavior on smooth MNIST data and smaller level of saturated CIFAR data. Finally, when the inter-class distance is so small such that even a high clean test accuracy may be difficult to achieve. Considering robust accuracy as the clean accuracy with smaller gap between the spheres, the next theorem provides a theoretical guarantee in relating together the difficulty of attaining good accuracy under attack and the model capacity (Ball, 1997), verifying our intuition above. Note that one way to measure the capacity of a ReLU network is by counting the number of its induced piece-wise linear region, which is closely related to the number of facets of its decision boundary. Theorem D.1. Let d(K, L) between symmetric convex bodies K and L denote the least positive d for which there is a linear image L~ of L such that L~  K  dL~. Let K be a (symmetric) polytope in Rn with d(K, B2n) = d. Then K has at least en/(2d2) facets. On the other hand, for each n, there is a polytope with 4n facets whose distance from the ball is at most 2.
Figure 6: Inter-class distance and required model capacity. Left: when distance is small, a small capacity polytope classifier could separate original data; middle: when distance is small, linear classifier not able to separate data points "robustly", but a more complex nonlinear classifier could; right:when distance is large, a a small capacity polytope classifier can separate data "robustly". The above analysis is partially supported by our experiments on model capacity in Section 5.3. However, as we've shown in Section 5.2, the nature of the problem is complex and more conclusive statements requires further research.
19


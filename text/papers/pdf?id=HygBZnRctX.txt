Under review as a conference paper at ICLR 2019
TRANSFERRING KNOWLEDGE ACROSS LEARNING PROCESSES
Anonymous authors Paper under double-blind review
ABSTRACT
In complex transfer learning scenarios new tasks might not be tightly linked to previous tasks. Approaches that transfer information contained only in the final parameters of a source model will therefore struggle. Instead, transfer learning at a higher level of abstraction is needed. We propose Leap, a framework that achieves this by transferring knowledge across learning processes. We associate each task with a manifold on which the training process travels from initialization to final parameters and construct a meta learning objective that minimizes the expected length of this path. Our framework leverages only information obtained during training and can be computed on the fly at negligible cost. We demonstrate that our framework outperforms competing methods, both in meta learning and transfer learning, on a set of computer vision tasks. Finally, we demonstrate that Leap can transfer knowledge across learning processes in demanding Reinforcement learning environments (Atari) that involve millions of gradient steps.
1 INTRODUCTION
Transfer learning is the process of transferring knowledge encoded in one model trained on one set of tasks to another model that is applied to a new task. Since a trained model encodes information in its learned parameters, transfer learning typically transfers knowledge by encouraging the target model's parameters to resemble those of a previous (set of) model(s) (Pan & Yang, 2009). This approach limits transfer learning to settings where good parameters for a new task can be found in the neighborhood of parameters that were learned from a previous task. For this to be a viable assumption, the two tasks must have a high degree of structural affinity, such as when a new task can be learned by extracting features from a pre-trained model (Girshick et al., 2014; He et al., 2017; Mahajan et al., 2018). If not, this approach has been observed to limit knowledge transfer since the training process on one task will discard information during training that is irrelevant for the task at hand, but that would be relevant for another task (Higgins et al., 2017; Achille et al., 2018).
In this paper we argue that such information can be harnessed, even when the downstream task is unknown, by transferring knowledge across the learning process itself. In particular, we propose a meta learning framework for aggregating information across task geometries as they are observed during training. These geometries, formalized as the loss surface over the training data, encode all information seen during training and therefore avoid catastrophic loss of information. Moreover, by transferring knowledge across learning processes, information from previous tasks is distilled to explicitly facilitate the learning of a new task.
Meta learning frames the learning of a new task as a learning problem of itself, typically in the few-shot learning paradigm (Lake et al., 2011; Santoro et al., 2016; Vinyals et al., 2016). In this environment, learning is primarily a problem of rapid adaptation and can optimized by parameterizing and backpropagating through the entire training process (Ravi & Larochelle, 2016; Andrychowicz et al., 2016; Finn et al., 2017). For more demanding tasks, formulating meta learning in this manner is challenging; backpropagating through thousands of gradient updates is both impractical and susceptible to instability.
We argue that as the training process grows longer in terms of the distance traversed on the loss landscape, the geometry of this landscape grows increasingly important. When adapting to a new
1

Under review as a conference paper at ICLR 2019

f ()

2 1
Figure 1: Example of the gradient paths on a manifold described by the loss surface. Leap learns an initialization with shorter expected gradient path that improves performance.

task through a single or a handful of gradient steps, the geometry can largely be ignored. In contrast, for hundreds or thousands of gradient updates, it is the dominant feature of the training process.
To scale meta learning beyond few-shot learning, we propose Leap, a light-weight framework for meta learning over task manifolds that does not need any forward- or backward-passes beyond those of the underlying training process. We demonstrate empirically that Leap is a superior method to similar meta learning frameworks for tasks that require more than a few training steps to learn, and show that it outperforms transfer learning that only use the final parameters of a source model. Finally, we evaluate Leap in a Reinforcement Learning environment (Atari 2600 (Bellemare et al., 2013)), demonstrating that it can transfer knowledge across learning processes that require millions of gradient updates to converge.

2 TRANSFERRING KNOWLEDGE OVER LEARNING PROCESSES

We start section 2.1 by introducing the gradient descent algorithm from a geometric perspective. Section 2.2 builds a framework for transfer learning and explains how we can leverage geometrical quantities to transfer knowledge across learning processes by guiding gradient descent. We focus on the point of initialization for simplicity, but our framework can readily be extended. Section 2.3 presents Leap, our lightweight algorithm for transfer learning across learning processes.

2.1 GRADIENT PATHS ON TASK MANIFOLDS

Central to our framework is the notion of a learning process; the harder a task is to learn, the harder

it is for the learning process to navigate on the loss surface (fig. 1). Our framework is based on the

idea that transfer learning can be achieved by leveraging information contained in similar learning

processes. Exploiting that this information is encoded in the geometry of the loss surface, we leverage

geometrical quantities to facilitate the learning process with respect to new tasks. We focus on the

supervised learning setting for simplicity, though our framework applies more generally. Given a

learning

objective

f

that

maps

a

parameterization





n
R

to

a

scalar

loss

value

given

input

x



m
R

and target y  Rc, we have the gradient descent update as

i+1 = i - iSif (i),

(1)

where f (i) = Ex,yp(x,y) f (i, x, y) . We take the learning rate schedule {i}i and preconditioning matrices {Si}i as given by the chosen optimizer, but these can be learned jointly with the initialization. We assume this process converges to some stationary point  after K gradient steps.
Because the learning process in eq. 1 follows the gradient trajectory, it constantly provides information about the geometry of the loss surface. Gradients that largely point in the same direction indicate a convex loss surface, whereas gradients with frequently opposing directions indicate an ill-conditioned loss landscape, something we would like to avoid. Leveraging this insight, we propose a framework for transfer learning that exploits the accumulation of geometric information by constructing a meta objective that minimizes the expected length of the gradient descent path across tasks. In doing so,

2

Under review as a conference paper at ICLR 2019

1 2

3

0 

0
Figure 2: Left: Illustration of the discrete approximation of the gradient flow. Curvature and step-size determine the fidelity of the approximation. Right: Illustration of the meta objective (eq. 5). Gradient paths from different tasks exert influence on 0 by pulling it forward along their respective path. The learned initialization finds an equilibrium balance between these competing forces.

the meta objective intrinsically balances local geometries across tasks and encourages an initialization as close to task-specific minima as possible, taking the gradient trajectory into account.
The gradient descent update can be seen as the discrete approximation of the gradient flow (t) = -(t)S(t)f ((t)), where we use (t) to differentiate discrete and continuous domains. The gradient flow provides information about movements in parameter space, but does not directly account for performance. To avoid information loss, we embed the gradient flow in a Riemann manifold M defined by the coordinate system (, f ()), i.e. the loss surface (fig. 1). On this manifold, following the gradient flow from an initialization 0 yields a curve  : [0, 1]  M defined by

 (t) = (t), f(t) , (0) = 0, f (0) , (1) =  , f ( ) ,

(2)

where f(t) = (t)T f ((t)). We refer to  as the gradient path from 0 to  on M . The length or energy of the gradient path is defined by accumulating infinitesimal changes along its trajectory,

1

Length() =

 (t),  (t) dt,

0

1
Energy() =  (t),  (t) dt.
0

(3)

Analogously to how the gradient update rule approximates the gradient flow, these measures can
be approximated by the cumulative chordal distance (fig. 2; Ahlberg et al., 1967). Focusing on the energy functional,1 we define the length of the observed gradient path as

K -1

K -1

d(0) =

i+1 - i 2 =

i+1 - i 2 + f (i+1) - f (i) 2.

i=0 i=0

(4)

Note that d involves only terms encountered during the training phase. We exploit this later to construct the gradient using only terms encountered during the training phase, enabling us to perform gradient descent on the meta objective at negligible cost (eq. 8).

We now turn to the transfer learning setting, where we face a set of tasks, each with a distinct loss surface. We can transfer knowledge across these learning process, and in particular information about the local geometry, by aggregating information obtained along the gradient path. This allows us to choose an initialization such that loss surfaces are well behaved and gradient descent converges rapidly in expectation. Not all initializations with short gradient paths are interesting, as some converge to stationary points with bad performance. As such, it is only relevant to consider the set of initializations  whose limit points across tasks achieve some minimum level of final performance.

2.2 META LEARNING ACROSS TASK MANIFOLDS
Formally, we define a task  = (f , p , u ) as the process of learning to approximate the relationship x  y through samples from the data distribution p (x, y). This process is defined by the gradient update rule u (as defined in eq. 1), applied iteratively to minimizes the task objective f . For a given task  , the relevant initializations to choose among are those that converge to stationary points 
1The energy functional is a more stable optimization objective, while every minimizer of the energy functional also minimizes the length.

3

Under review as a conference paper at ICLR 2019

with approximately optimal performance, i.e. an upper bound of the form f ( )  min f ( ) +
for some small > 0. Since this must hold for all tasks, the set of feasible initializations are those that satisfy this performance constraint across tasks,  =  {0 | f ( )  min f ( ) + }. Given a distribution p( ) over tasks, minimizing Ep() d(0) subject to 0   defines our meta
objective,

min
0 
s.t.

K -1

E p( )

i+1 - i 2 +

i=0

i+1 = u (i ),

0 = 0.

f (i+1) - f (i ) 2

,

(5)

Crucially, this meta objective is robust to variations in the geometry of loss surfaces, as it balances complementary and competing learning processes (fig. 2). For instance, there may be an initialization that can solve a small subset of tasks in a handful of gradient steps, but would be catastrophic for a larger set of tasks. When transferring knowledge via the initialization, this means we must trade off commonalities and differences between gradient paths. In eq. 5 these trade-offs arise naturally. For instance, as the number of tasks whose gradient paths move in the same direction increases, so does their pull on the initialization. Conversely, as the initialization moves away from the gradient path of some other set of tasks, these paths act as a springs that exert increasingly strong pressure on the initialization. The solution to eq. 5 is an equilibrium between these competing forces.

In practice, backpropagating through eq. 5 is prohibitively costly. Moreover,  is unknown and extremely hard to model. However, because the relevant upper bound is the performance we can obtain without using prior information, i.e. with a random initialization, we can construct a set of baseline gradient paths and improve the initialization with respect to these. Repeating the process, we obtain an algorithm that is guaranteed to solve eq. 5.

2.3 THE PULL-FORWARD ALGORITHM
To overcome the aforementioned issues, we propose the Pull-Forward objective (algorithm 1), which is guaranteed to find a solution to eq. 5. We take a random initialization, shared across all tasks, as our starting point and assume that this initialization is in . From this initial point (line 1), sample a set of tasks from p( ) and obtain baseline gradient paths for each,  = { } , where  = {i }iK=0 is the baseline gradient path for task  . Note that all tasks share the same initial point, 0 = 0  . These baselines correspond to task-specific learning processes (line 8) that we use to freeze the forward point i+1 in all norm terms in eq. 5. For instance, i+1 - i 2 becomes i+1 - i 2. As such, optimizing 0 with respect to  pulls the initialization forward along each task-specific gradient path, reducing the gradient path while being anchored to baseline gradient paths that converge to good minima. To summarize, we define the Pull-Forward objective as

min
0
s.t.

K -1

F (0; ) = Ep()

i+1 - i 2 +

i=0

i+1 = u (i ),

0 = 0.

f (i+1) - f (i ) 2

,

(6)

Intuitively, because F (0, ) = Ep() d(0) , the pull-forward objective can be understood as a greedy search procedure that incrementally improves upon the observed gradient path. This improved initialization can then be used to obtain a new and more demanding baseline to further improve the initialization. Iterating this process yields a sequence of candidate solutions, all in , with incrementally shorter gradient paths. In theorem 1, we show that the limit point of this sequence is a local minimum of 5.

Theorem 1 (Pull-forward). Define a sequence of initializations {s0}sN by

s0+1 = s0 - sF (s0; s),

00  .

(7)

For s > 0 sufficiently small, there exist learning rates schedules {i }iK=1 for all tasks such that k0 is a local minimum of eq. 5.

4

Under review as a conference paper at ICLR 2019

Algorithm 1 Leap: Transferring Knowledge over Learning Processes

Require: p( ),  = (f , u , p ): distribution over tasks

Require: : step size

1: randomly initialize 0

2: while not done do

3: F  0: initialize meta gradient

4: sample task batch B from p( )

5: for all   B do

6: 0  0: initialize baselines 7: for all i  {0, . . . , K -1} do 8: compute f (i ) and i+1 9: increment F as in eq. 8

10: end for

11: end for

12:

0



0

-

|

 B

|

F

:

update

initialization

13: end while

Proof; see appendix A. Crucially, when F is evaluated at 0, the meta gradient F can be computed analytically using only terms already computed during the task training. As such, the Pull-Forward algorithm can be computed on the fly during training at negligible cost.
In practice we use stochastic gradient descent, and noise can cause f (s+1) - f (i ) > 0. In such cases, the baseline is pulling 0 in the wrong direction and to prevent such distortions, we add regularizing terms that reverse the sign on the loss delta,

µi (i ; s+1) =

0 -2(f (i+1) - f (i ))2

if f (i+1)  f (i ), else.

µi helps ensuring that i is pulled towards a minimum and is constructed to introduce the absolute value |f (i+1) - f (i )| in the meta gradient. Evaluating the meta gradient at 0, we have

K -1

F (0, ) = 2 Ep()

J i(0)T

i=0

|fi | f (i) - i

,

(8)

where J i denotes the Jacobian of i with respect to the initialization, fi = f (i+1) - f (i ) and i = i+1 - i . Ignoring the Jacobian, this expression contains only terms encountered during standard training and can be computed at a negligible cost (line 9 in algorithm 1). The Jacobians
are costly to compute. Empirical evidence suggest that they are largely redundant (Finn et al., 2017;
Nichol et al., 2018). We can understand this by noting that the elements of J can be written on the form 1 - h (see appendix B), where h decreases exponentially fast as the learning rate schedule decreases. As such, for sufficiently small learning rates, Ji = In is a reasonable approximation.

3 RELATED WORK
Transfer learning has been explored in a variety of settings, the most typical approach attempting to infuse knowledge in a target model's parameters by encouraging them to lie close to those of a pretrained source model (Pan & Yang, 2009). Because such approaches can limit knowledge transfer (Higgins et al., 2017; Achille et al., 2018), applying standard transfer learning techniques leads to catastrophic forgetting, by which the model is rendered unable to perform a previously mastered task (McCloskey & Cohen, 1989; Goodfellow et al., 2013). These problems are further accentuated when there is a larger degree of diversity among tasks that push optimal parameterizations further apart. In these cases, transfer learning can in fact be worse than training from scratch.
Recent approaches extend standard finetuning by adding regularizing terms to the training objective that encourage the model to learn parameters that both solve a new task and retain high performance

5

Under review as a conference paper at ICLR 2019
on previous tasks. These regularizers operate by measuring protect the parameters that affect the loss function the most (Miconi et al., 2018; Zenke et al., 2017; Kirkpatrick et al., 2016; Lee et al., 2017; Serrà et al., 2018). Because these approaches use a single model to encode both global task-general information and local task-specific information, they can over-regularize, preventing the model from learning further tasks. More importantly, Schwarz et al. (2018) found that while these approaches mitigate catastrophic forgetting, they are unable to facilitate knowledge transfer on the benchmark they considered. Ultimately, if a single model must encode both task-generic and task-specific information, it must either saturate or grow in size (Rusu et al., 2016).
In contrast, meta learning aims to learn the learning process itself (Schmidhuber, 1987; Bengio et al., 1991; Santoro et al., 2016; Ravi & Larochelle, 2016; Andrychowicz et al., 2016; Vinyals et al., 2016; Finn et al., 2017). Most of the literature focuses on the few-shot learning setting, where a task is some variation on a common theme, such as subsets of classes from a larger pool of data (Lake et al., 2015; Vinyals et al., 2016). The meta learning algorithm adapts a model to a new task given a handful of (if any) samples. Recent attention has been devoted to three main approaches. One trains the meta learner to adapt to a new task by comparing an input to samples from previous tasks (Vinyals et al., 2016; Mishra et al., 2018; Snell et al., 2017). More relevant to our framework are approaches that parameterize the training process through a recurrent neural network that takes the gradient as input and produces a new set of parameters (Ravi & Larochelle, 2016; Santoro et al., 2016; Andrychowicz et al., 2016; Hochreiter et al., 2001). The approach most closely related learns an initialization so that one or a handful of gradient updates allows the model to adapt to a new task (Finn et al., 2017; Nichol et al., 2018; Al-Shedivat et al., 2017; Lee & Choi, 2018). In contrast to our work, these methods focus exclusively on few-shot learning, where the gradient path is trivial as only a single or a handful of training steps are allowed, limiting them to settings where the current task is very similar to previous ones.
It is worth noting that the Model Agnostic Meta Learner (MAML: Finn et al., 2017) can be written as Ep() f (K ) .2 As such it arises as a special case of Leap where only the final parameterization is evaluated in terms of its final performance. Similarly, the Reptile algorithm (Nichol et al., 2018), which proposes to use Ep() K - 0 as the meta update rule, can be seen as a naive version of Leap that assumes all task geometries are Euclidean.
Related work studying models from a geometric perspective have explored how to interpolate in a generative model's learned latent space Tosi et al. (2014); Shao et al. (2017); Arvanitidis et al. (2017); Chen et al. (2017); Kumar et al. (2017). Riemann manifolds have also garnered attention in the context of optimization, as a preconditioning matrix can be understood as the instantiation of some Riemann metric (Amari & Nagaoka, 2007; Abbati et al., 2018; Luk & Grosse, 2018).
4 EMPIRICAL RESULTS
We consider three experiments designed to provide increasingly complex transfer learning environments. We measure transfer learning in terms of final test error and area under the error curve, defined as the error score on the training set during training. We compare Leap to competing meta learning methods on the Omniglot dataset by tasking the meta learning with transferring knowledge across alphabets (section 4.1). We then study Leap's ability to transfer knowledge over more complex and diverse tasks in the Multi-CV experiment (section 4.2). Finally, we evaluate Leap on a much more demanding reinforcement learning problem, demonstrating its ability to scale (section 4.3).
4.1 OMNIGLOT
We transfer knowledge between alphabets in the Omniglot (Lake et al., 2015) dataset. We compare the rate of convergence between different transfer learning methods as we vary the number of pretraining tasks from 1 to 30, holding out 10 tasks for evaluation. As baselines, we compare against no pretraining, fine-tuning in single-headed mode, fine-tuning in multi-headed mode, and against the first-order approximation version of MAML, as well as Reptile. See appendix C for details.
For more than 4 tasks, Leap consistently achieves superior performance (fig. 3). The gap between Leap and other meta learning frameworks is large and Leap is in fact the only meta learner to
2MAML differs from Leap in that it evaluates the meta objective on a held-out test set.
6

Under review as a conference paper at ICLR 2019

Training loss AUC

4
3
2
1
00 100 200 300 400 500 Training steps

0.4

0.3

0.2

0.1

15

10 15 20 25

Number of pretraining tasks

30

Leap Finetune
Reptile MAML Finetune
No pretraining

Figure 3: Transfer learning on Omniglot. Left: Evolution of training curves during meta training of Leap. Blue signifies beginning of training initial and red end of training. Right: AUC under error curve across number of pretraining tasks. Author's suggested first-order approximation; multi-headed finetuning; single-headed finetuning. Shading: standard deviation across 10 seeds.

outperform finetuning. That Leap does better than Reptile, which in turn does better than MAML, underscores the importance of leveraging the task geometry in meta learning. Multi-task finetuning is a tough benchmark to beat as tasks are very similar, but with a sufficiently rich task distribution Leap is superior, and importantly, the performance margin grows with the number of pretraining tasks.
Table 1: Results on Multi-CV benchmark. All methods are trained until convergence on held-out tasks.  Area under training error curve; scaled to 0­100. MNIST results omitted; see appendix C.

Held-out task Facescrub Cifar10 SVHN Cifar100 Traffic Signs

Method
Leap Finetuning No pretraining
Leap Finetuning No pretraining
Leap Finetuning No pretraining
Leap Finetuning No pretraining
Leap Finetuning No pretraining

Test (%)
19.9 32.7 18.2
21.2 27.4 26.2
8.4 10.9 10.3
52.0 59.2 54.8
2.9 5.7 3.6

Train (%)
0.0 0.0 0.0
10.8 13.3 13.1
5.6 6.1 6.9
30.5 31.5 33.1
0.0 0.0 0.0

AUC
11.6 13.2 10.5
17.5 20.7 23.0
7.5 9.3 11.5
43.4 44.1 50.1
1.2 1.7 2.4

4.2 MULTI-CV
Inspired by Serrà et al. (2018), we consider a set of computer vision datasets as distinct tasks. We pretrain on all but one task, which is held out for final evaluation. For details see appendix D. To reduce the computational burden during meta training, we pretrain on each task in the meta batch for ten epochs. We found this to reach equivalent performance to training on longer gradient paths, but with faster convergence as the initialization is updated more frequently. This indicates that it is sufficient for Leap to see a partial trajectory to correctly infer shared structures across task geometries.
The Multi-CV experiment is more challenging both due to greater task diversity and greater complexity among tasks. We report results on held-out tasks in table 1. Leap outperforms both baselines on all but one transfer learning task (Facescrub), where any form of pretraining results in worse performance. Notably, while Leap does marginally worse than a random initialization, finetuning
7

Under review as a conference paper at ICLR 2019

2000 1000
0 SpaceInvaders 0.0 0.5 1.0 1.5 2.0 2.5 3.0
×107 40000
20000 0 RoadRunner 0.0 0.5 1.0 1.5 2.0 ×107

500

250

0 0.0
10000 7500 5000 2500
0

0.5 1.0 1

Breakout 1.5 2.0
×107
Krull 23
×107

40000

20000 0 0
20

KungFuMaster 123
×107

0 -20 Pong
0.0 0.2 0.4 0.6 0.8 ×107

Figure 4: Average episode returns on Atari games over training steps. Scores are reported as moving average over 100 episodes. Except for Pong, all games are held out during meta training. KungFuMaster, RoadRunner and Krull have action state spaces that are twice as large as the largest action state encountered during pretraining. Leap (orange) performs at least on par with a random initialization (blue), and outperforms the baseline in the majority of cases.

leads to a substantial drop in performance. On all other tasks, Leap converges faster to optimal performance and achieves superior final performance.
4.3 ATARI
To demonstrate that Leap can scale to large problems, both in computational terms but also in the complexity of tasks, we apply it in a reinforcement learning domain, specifically Atari 2600 games (Bellemare et al., 2013). We use a variant of the actor-critic architecture for all experiments (Sutton et al., 1998); specifically we learn both the policy  and the value function V from raw pixels, with  and V sharing a convolutional encoder (Schwarz et al., 2018). We apply Leap with respect to this convolutional encoder. During meta training, we sample mini-batches from 27 games in the suite that has an action space dimensionality of at most 10, holding out three of these games for evaluation, along with remaining games. We train on each task in the mini-batch for a million training steps, accumulating the meta gradient as in algorithm 1. See appendix E for details.
Already after 100 meta training steps there is a distinct improvement with respect to the initialization. On games in the pretraining set, Leap learns a useful policy significantly faster (see comparison on Pong in fig. 4). On held-out games with equivalent state spaces (Breakout), Leap learns faster than a random initialization. Notably, in the case of SpaceInvader it reaches a level unattainable by our baseline. Finally on games with almost twice as large an action space as any game seen during meta training, Leap is still able to guarantee performance on par with the baseline, and in fact outperforms it in most cases. In the case of one of the evaluated environments (KungFuMaster), Leap is reaches a significantly higher performance level.
5 CONCLUSIONS
Transfer learning typically ignores the process of learning a task, restricting it to scenarios where target tasks are very similar to source tasks. In this paper, we present Leap, a framework that facilitates transfer learning at a higher level of abstraction. By formalizing transfer learning as minimizing the expected gradient path over task-specific loss surfaces, we present a method for meta learning that scales to highly demanding problems. We find empirically that Leap has superior generalizing properties to finetuning and competing methods. Moreover, by virtue of transferring knowledge across learning processes, it is robust to non-trivial diversity in task distributions.
8

Under review as a conference paper at ICLR 2019
REFERENCES
Gabriele Abbati, Alessandra Tosi, Michael Osborne, and Seth Flaxman. Adageo: Adaptive geometric learning for optimization and sampling. In International Conference on Artificial Intelligence and Statistics, pp. 226­234, 2018.
Alessandro Achille, Tom Eccles, Loic Matthey, Christopher P. Burgess, Nick Watters, Alexander Lerchner, and Irina Higgins. Life-long disentangled representation learning with cross-domain latent homologies. arXiv preprint arXiv:1808.06508, 2018.
J Harold Ahlberg, Edwin Norman Nilson, and Joseph Leonard Walsh. The Theory of Splines and Their Applications. Academic Press, 1967. p. 51.
Maruan Al-Shedivat, Trapit Bansal, Yuri Burda, Ilya Sutskever, Igor Mordatch, and Pieter Abbeel. Continuous Adaptation via Meta-Learning in Nonstationary and Competitive Environments. CoRR, cs.LG, 2017.
Shun-ichi Amari and Hiroshi Nagaoka. Methods of information geometry, volume 191. American Mathematical Society, 2007.
Marcin Andrychowicz, Misha Denil, Sergio Gomez, Matthew W Hoffman, David Pfau, Tom Schaul, Brendan Shillingford, and Nando De Freitas. Learning to learn by gradient descent by gradient descent. In Advances in Neural Information Processing Systems, pp. 3981­3989, 2016.
Georgios Arvanitidis, Lars Kai Hansen, and Søren Hauberg. Latent Space Oddity: on the Curvature of Deep Generative Models. arXiv.org, October 2017.
M. G. Bellemare, Y. Naddaf, J. Veness, and M. Bowling. The arcade learning environment: An evaluation platform for general agents. Journal of Artificial Intelligence Research, 47:253­279, jun 2013.
Yoshua Bengio, Samy Bengio, and Jocelyn Cloutier. Learning a synaptic learning rule. Université de Montréal, Département d'informatique et de recherche opérationnelle, 1991.
Nutan Chen, Alexej Klushyn, Richard Kurle, Xueyan Jiang, Justin Bayer, and Patrick van der Smagt. Metrics for Deep Generative Models. arXiv.org, November 2017.
Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks. arXiv.org, March 2017.
Ross Girshick, Jeff Donahue, Trevor Darrell, and Jitendra Malik. Rich feature hierarchies for accurate object detection and semantic segmentation. In International Conference on Computer Vision and Pattern Recognition, pp. 580­587, 2014.
Ian J Goodfellow, Mehdi Mirza, Da Xiao, Aaron Courville, and Yoshua Bengio. An empirical investigation of catastrophic forgetting in gradient-based neural networks. arXiv preprint arXiv:1312.6211, 2013.
Kaiming He, Georgia Gkioxari, Piotr Dollár, and Ross Girshick. Mask r-cnn. In International Conference on Computer Vision, pp. 2980­2988. IEEE, 2017.
Irina Higgins, Arka Pal, Andrei A Rusu, Loic Matthey, Christopher P Burgess, Alexander Pritzel, Matthew Botvinick, Charles Blundell, and Alexander Lerchner. Darla: Improving zero-shot transfer in reinforcement learning. arXiv preprint arXiv:1707.08475, 2017.
Sepp Hochreiter, A Steven Younger, and Peter R Conwell. Learning to learn using gradient descent. In International Conference on Artificial Neural Networks, pp. 87­94. Springer, 2001.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.
9

Under review as a conference paper at ICLR 2019
James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, Demis Hassabis, Claudia Clopath, Dharshan Kumaran, and Raia Hadsell. Overcoming catastrophic forgetting in neural networks. arXiv preprint arXiv:1612.00796, 2016.
Abhishek Kumar, Prasanna Sattigeri, and P Thomas Fletcher. Improved Semi-supervised Learning with GANs using Manifold Invariances. CoRR, 2017.
Brenden Lake, Ruslan Salakhutdinov, Jason Gross, and Joshua Tenenbaum. One shot learning of simple visual concepts. In Proceedings of the Annual Meeting of the Cognitive Science Society, volume 33, 2011.
Brenden M. Lake, Ruslan Salakhutdinov, and Joshua B. Tenenbaum. Human-level concept learning through probabilistic program induction. Science, 350(6266):1332­1338, 2015. ISSN 0036-8075. doi: 10.1126/science.aab3050. URL http://science.sciencemag.org/content/ 350/6266/1332.
Sang-Woo Lee, Jin-Hwa Kim, JungWoo Ha, and Byoung-Tak Zhang. Overcoming Catastrophic Forgetting by Incremental Moment Matching. CoRR, 2017.
Yoonho Lee and Seungjin Choi. Meta-Learning with Adaptive Layerwise Metric and Subspace. CoRR, 2018.
Kevin Luk and Roger Grosse. A coordinate-free construction of scalable natural gradient. arXiv preprint arXiv:1808.10340, 2018.
Dhruv Mahajan, Ross B. Girshick, Vignesh Ramanathan, Kaiming He, Manohar Paluri, Yixuan Li, Ashwin Bharambe, and Laurens van der Maaten. Exploring the limits of weakly supervised pretraining. Arxiv preprint Arxiv:1805.00932, 2018.
Michael McCloskey and Neal J Cohen. Catastrophic interference in connectionist networks: The sequential learning problem. In Psychology of learning and motivation, volume 24, pp. 109­165. Elsevier, 1989.
Thomas Miconi, Jeff Clune, and Kenneth O. Stanley. Differentiable plasticity: training plastic neural networks with backpropagation. arXiv preprint arXiv:1804.02464, 2018.
Nikhil Mishra, Mostafa Rohaninejad, Xi Chen, and Pieter Abbeel. A Simple Neural Attentive Meta-Learner. In ICLR, 2018.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin Riedmiller. Playing atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602, 2013.
Alex Nichol, Joshua Achiam, and John Schulman. On First-Order Meta-Learning Algorithms. arXiv.org, March 2018.
Sinno Jialin Pan and Qiang Yang. A survey on transfer learning. IEEE Transactions on Knowledge & Data Engineering, (10):1345­1359, 2009.
Sachin Ravi and Hugo Larochelle. Optimization as a model for few-shot learning. In International Conference on Representation Learning, 2016.
Andrei A Rusu, Neil C Rabinowitz, Guillaume Desjardins, Hubert Soyer, James Kirkpatrick, Koray Kavukcuoglu, Razvan Pascanu, and Raia Hadsell. Progressive neural networks. arXiv preprint arXiv:1606.04671, 2016.
Adam Santoro, Sergey Bartunov, Matthew Botvinick, Daan Wierstra, and Timothy Lillicrap. Metalearning with memory-augmented neural networks. In International conference on machine learning, pp. 1842­1850, 2016.
10

Under review as a conference paper at ICLR 2019
Jürgen Schmidhuber. Evolutionary principles in self-referential learning, or on learning how to learn: the meta-meta-... hook. PhD thesis, Technische Universität München, 1987.
Jonathan Schwarz, Jelena Luketina, Wojciech M Czarnecki, Agnieszka Grabska-Barwinska, Yee Whye Teh, Razvan Pascanu, and Raia Hadsell. Progress & compress: A scalable framework for continual learning. arXiv preprint arXiv:1805.06370, 2018.
Joan Serrà, Dídac Surís, Marius Miron, and Alexandros Karatzoglou. Overcoming catastrophic forgetting with hard attention to the task. arXiv preprint arXiv:1801.01423, 2018.
Hang Shao, Abhishek Kumar, and P Thomas Fletcher. The Riemannian Geometry of Deep Generative Models. arXiv.org, November 2017.
Jake Snell, Kevin Swersky, and Richard S Zemel. Prototypical Networks for Few-shot Learning. arXiv.org, March 2017.
Richard S Sutton, Andrew G Barto, et al. Reinforcement learning: An introduction. MIT Press, Cambridge, 1998.
Alessandra Tosi, Søren Hauberg, Alfredo Vellido, and Neil D Lawrence. Metrics for Probabilistic Geometries. UAI, 2014.
Oriol Vinyals, Charles Blundell, Timothy Lillicrap, Koray Kavukcuoglu, and Daan Wierstra. Matching Networks for One Shot Learning. arXiv.org, June 2016.
Friedemann Zenke, Ben Poole, and Surya Ganguli. Continual Learning Through Synaptic Intelligence. arXiv preprint: arXiv:1703.04200v3, 2017.
11

Under review as a conference paper at ICLR 2019

APPENDIX
A PROOF OF THEOREM 1
Proof. We prove the theorem by showing that F (s0+1; s+1) < F (s0; s), with s0+1   for all s. To simplify notation, we define

zi = (s,i, f (s,i)), hi = (s,i+1, f (s,i+1)),

xi = (s+1,i, f (s+1,i)), yi = (s+1,i+1, f (s+1,i+1)).

By assumption, s is sufficiently small to satisfy F (s+1; s)  F (s; s), from which we have

E,i hi - xi 2  E,i hi - zi 2 = E,i zi 2  E,i xi 2 + 2 hi , zi - 2 hi , xi .

where we write E,i = Ep()

K i=1

as

shorthand.

Applying

this

to

yi - zi

2 yields

E,i yi - zi 2 = E,i yi 2 - 2 yi , zi + zi 2  E,i yi 2 - 2 yi , zi + xi 2 + 2 hi , zi - 2 hi , xi = E,i yi - xi 2 + 2 yi , xi - 2 yi , zi + 2 hi , zi - 2 hi , xi = E,i yi - xi 2 + 2 yi - hi , xi - zi .
It remains to show that the expectation over yi - hi , xi - zi is positive for certain choices of learning rates i . Define z^i = s,i and similarly for x^i , h^i and y^i , so that yi - hi , xi - zi = y^i - h^i , x^i - z^i + ci , where ci = (f (y^i ) - f (h^i ))(f (x^i ) - f (z^i )). Now consider ci . Note that we can equally write y^i = x^i - i g(x^i ) with g(x^i ) = Ss+1,if (x^i ) and similarly with h^i . Let i = f (x^i ) - f (z^i ). Using first-order Taylor series expansion, we have

ci = i + f (x^i )T (y^i - x^i ) - f (z^i )T (h^i - z^i ) i = i - i f (x^i )T g(x^i ) + i f (z^i )T g(z^i ) i = i 2 - i i i ,
where i = f (x^i )T g(x^i ) - f (z^i )T g(z^i ). Upon substitution and rearrangement, we find

E,i y^i - h^i , x^i - z^i + ci = E,i xi - zi 2 - i x^i - z^i , g(x^i ) - g(z^i ) + i (x^i - z^i ) .

If the right-most term is positive in expectation, we are done. If not, note that it vanishes as i grows small relative to s. In particular, choose i to satisfy3

where

i  -1 xi - zi 2  (0, ),

 = sup x^i - z^i , g(x^i ) - g(z^i ) + i (x^i - z^i ).
,i
3Generally, this bound is loose since for s small g(x^i ) point in the same direction as g(z^i ).

12

Under review as a conference paper at ICLR 2019

Note that we must have  > 0, since if not there is no need to bound i . This establishes F (s0+1, s+1)  F (s0, s).

Finally, because E,i hi - xi 2  E,i hi - zi 2, it follows that yi-1 is tied to hi by some ball

of the form

hi - yi-1

2  i 2 g(z^i ) 2 +

i 

,

where

i 

captures

noise

from

the

expectation.

For

s small this noise component vanishes, and since {i }i is a converging sequence, the bounds grows

increasingly tight. It follows then that {si+1}i converges to the same stationary point as {si }i,

yielding s0+1   for all s, as desired.

B APPROXIMATING THE JACOBIANS J i(0)

We note that

i

J i+1(0) = In - iSiHf (i) J i(0) =

In - j Sj Hf (j ) ,

j=0

(9)

where Hf () denotes the Hessian of f at . Thus, given i, the only channel 0 acts through is the curvature of the loss landscape, implying that the gradient path is memoryless up to second-order curvature. Moreover, the influence of 0 decays exponentially over time at a rate controlled by
the learning rate schedule. As learning rates shrink, the Jacobians collapse exponentially fast into the identity matrix. As such, for all i small, the identity matrix dominates and can be taken as a
reasonable approximation. Finally, we note that if the Hessians are approximated, the error introduced
by these approximations is compounded in the Jacobians, making the more conservative identity
approximation preferable.

C EXPERIMENT DETAILS: OMNIGLOT
Omniglot contains 50 alphabets, each consisting of a set of characters that in turn have 20 unique samples. We pretrain on up to 30 tasks, holding 10 tasks out for validation and 10 tasks for final evaluation. We use the same convolutional neural network architecture as in previous works (Vinyals et al., 2016; Finn et al., 2017; Schwarz et al., 2018) and train using stochastic gradient descent. For each alphabet, we define a task as a 20-class classification problem. For alphabets with more than 20 characters, we pick 20 characters at random, while alphabets with fewer characters are dropped from the validation set. The validation set is used to for early stopping of the meta training process. On each task, we train the model using stochastic gradient descent with a batch size of 20, and a learning rate of 0.1, except for fine-tuning where we find 0.01 to perform better than 0.1 or 0.001. The initialization is trained using stochastic gradient descent with a learning rate of 0.1, except for in the case of Reptile where a lower learning rate of 0.01 performed best. We use a task batch size of 20, sampled with replacement. We train on a given task until we observe no more improvement on a set of held-out images. Following Schwarz et al. (2018), we augment the dataset with 36 random rotations.

D EXPERIMENT DETAILS: MULTI-CV
In this experiment, we allow different architectures between tasks by using a distinct final linear layers for each task. We use the same neural network architecture as in the Omniglot experiment, and compare Leap against a baseline with no pretraining and one using multitask finetuning. On a given task, we train the model using stochastic gradient descent with a learning rate of 0.1 and a batch size of 32. Training is stopped once the validation accuracy does not improve or once a preset number of epochs is reached. For pretraining, we cap the number of training epochs to 10, and for the held-out task we cap the number of epochs to 100. We train Leap using Adam (Kingma & Ba, 2014) with a learning rate of 0.01 and a task batch size of 10. Once meta training is complete, we train on a held-out dataset until the validation error stops improving, or the training process reaches 100 epochs. We no dataset augmentation; MNIST images are zero padded to have 32 × 32 images, We use the same normalizations as in Serrà et al. (2018).

13

Under review as a conference paper at ICLR 2019

Table 2: Mean test error on held out tasks.Author's suggested first-order approximation; multiheaded finetuning; single-headed finetuning.

Method No. Pretraining tasks
1 2 4 6 8 10 12 14 16 18 20 22 24 26 28 30

Leap
8.7 6.6 4.0 3.0 3.1 2.8 2.7 2.7 2.3 2.8 2.6 2.4 2.4 2.2 2.7 2.1

Finetune
7.5 6.3 4.7 4.4 4.2 4.0 3.6 3.8 3.9 3.7 3.9 3.7 3.5 3.3 3.8 3.6

Reptile
8.4 8.0 7.2 6.3 6.1 5.7 5.4 5.2 5.1 5.1 4.7 4.9 5.0 4.5 4.8 4.6

MAML
8.8 8.8 8.6 8.4 8.7 9.2 8.6 8.6 9.1 8.7 9.4 8.6 8.7 9.2 9.3 8.7

Finetune
15.4 11.7 10.9 11.6 12.3 13.7 15.5 15.1 16.1 17.3 21.6 26.0 30.7 37.7 40.3 43.6

No pretraining
17.0 16.8 17.9 16.1 15.9 16.1 16.2 16.6 16.7 15.9 16.6 16.2 15.9 16.9 17.0 16.1

E EXPERIMENT DETAILS: ATARI
We use the same network as in Mnih et al. (2013), adopting it to actor-critic algorithms by estimating both value and policy through linear layers connected to the final output of a shared convolutional network. As is standard, we use downsampled 84 × 84 × 3 RGB images as input. On each task, we use a batch size of 32, an unroll length of 5 and update the model parameters with RMSProp (using = 10-4,  = 0.99) with a learning rate of 10-4. We set the entropy cost to 0.01 and clip the absolute value of the rewards to maximum 5.0. The discounting factor was set to 0.99.

14

Under review as a conference paper at ICLR 2019

Table 3: Transfer learning results on Multi-CV benchmark. All methods are trained until convergence on held-out tasks.  Area under training error curve; scaled to 0­100.

Held-out task Facescrub NotMNIST MNIST Fashion MNIST Cifar10 SVHN Cifar100 Traffic Signs

Method
Leap Fine-tuning No pre-training
Leap Fine-tuning No pre-training
Leap Fine-tuning No pre-training
Leap Fine-tuning No pre-training
Leap Fine-tuning No pre-training
Leap Fine-tuning No pre-training
Leap Fine-tuning No pre-training
Leap Fine-tuning No pre-training

Test (%)
19.9 32.7 18.2
5.3 5.4 5.4
0.7 0.9 0.9
8.0 8.9 8.4
21.2 27.4 26.2
8.4 10.9 10.3
52.0 59.2 54.8
2.9 5.7 3.6

Train (%)
0.0 0.0 0.0
0.6 2.0 2.6
0.1 0.1 0.2
4.2 3.8 4.7
10.8 13.3 13.1
5.6 6.1 6.9
30.5 31.5 33.1
0.0 0.0 0.0

AUC
11.6 13.2 10.5
2.9 4.4 5.1
0.6 0.8 1.0
6.8 7.0 7.8
17.5 20.7 23.0
7.5 9.3 11.5
43.4 44.1 50.1
1.2 1.7 2.4

       

15


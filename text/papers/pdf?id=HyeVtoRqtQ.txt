Under review as a conference paper at ICLR 2019
TRELLIS NETWORKS FOR SEQUENCE MODELING
Anonymous authors Paper under double-blind review
ABSTRACT
We present trellis networks, a new architecture for sequence modeling. On the one hand, a trellis network is a temporal convolutional network with special structure, characterized by weight tying across depth and direct injection of the input into deep layers. On the other hand, we show that truncated recurrent networks are equivalent to trellis networks with special sparsity structure in their weight matrices. Thus trellis networks with general weight matrices generalize truncated recurrent networks. We leverage these connections to design high-performing trellis networks that absorb structural and algorithmic elements from both recurrent and convolutional models. Experiments demonstrate that trellis networks outperform the current state of the art on a variety of challenging benchmarks, including word-level language modeling on Penn Treebank and WikiText-103, characterlevel language modeling on Penn Treebank, and stress tests designed to evaluate long-term memory retention.
1 INTRODUCTION
What is the best architecture for sequence modeling? Recent research has produced significant progress on multiple fronts. Recurrent networks, such as LSTMs, continue to be optimized and extended (Merity et al., 2018b; Melis et al., 2018; Yang et al., 2018; Trinh et al., 2018). Temporal convolutional networks have demonstrated impressive performance, particularly in modeling longrange context (van den Oord et al., 2016; Dauphin et al., 2017; Bai et al., 2018). And architectures based on self-attention are gaining ground (Vaswani et al., 2017; Santoro et al., 2018).
In this paper, we introduce a new architecture for sequence modeling, the Trellis Network. We aim to both improve empirical performance on sequence modeling benchmarks and shed light on the relationship between two existing model families: recurrent and convolutional networks.
On the one hand, a trellis network is a special temporal convolutional network, distinguished by two unusual characteristics. First, the weights are tied across layers. That is, weights are shared not only by all time steps but also by all network layers, tying them into a regular trellis pattern. Second, the input is injected into all network layers. That is, the input at a given time-step is provided not only to the first layer, but directly to all layers in the network. So far, this may seem merely as a peculiar convolutional network for processing sequences, and not one that would be expected to perform particularly well.
Yet on the other hand, we show that trellis networks generalize truncated recurrent networks (recurrent networks with bounded memory horizon). The precise derivation of this connection is one of the key contributions of our work. It allows trellis networks to serve as bridge between recurrent and convolutional architectures, benefitting from algorithmic and architectural techniques developed in either context. We leverage these relationships to design high-performing trellis networks that absorb ideas from both architectural families. Beyond immediate empirical gains, these connections may serve as a step towards unification in sequence modeling.
We evaluate trellis networks on challenging benchmarks, including word-level language modeling on the standard Penn Treebank and the much larger WikiText-103 datasets; character-level language modeling on Penn Treebank; and standard stress tests designed to evaluate long-term memory retention. Trellis networks outperform state-of-the-art models across the board. On word-level Penn Treebank, a trellis network outperforms by more than a unit of perplexity the recent architecture search work of Pham et al. (2018), as well as the recent results of Melis et al. (2018), which leveraged the Google Vizier service for exhaustive hyperparameter search. On character-level Penn Tree-
1

Under review as a conference paper at ICLR 2019

bank, a trellis network outperforms the thorough optimization work of Merity et al. (2018a). On word-level WikiText-103, a trellis network outperforms by 4% in perplexity the contemporaneous self-attention-based Relational Memory Core (Santoro et al., 2018), and by 8% the work of Merity et al. (2018a). On stress tests, trellis networks outperform recent state-of-the-art results achieved by recurrent networks and self-attention (Transformer) (Trinh et al., 2018). It is notable that the state of the art across these benchmarks was held by models with sometimes dramatic mutual differences.

2 BACKGROUND
Recurrent networks (Elman, 1990; Werbos, 1990; Graves, 2012), particularly with gated cells such as LSTMs (Hochreiter & Schmidhuber, 1997) and GRUs (Cho et al., 2014), are perhaps the most popular architecture for modeling temporal sequences. Recurrent architectures have been used to achieve breakthrough results in natural language processing and other domains (Sutskever et al., 2011; Graves, 2013; Sutskever et al., 2014; Bahdanau et al., 2015; Vinyals et al., 2015; Karpathy & Li, 2015). Convolutional networks have also been widely used for sequence processing (Waibel et al., 1989; Collobert et al., 2011). Recent work indicates that convolutional networks are effective on a variety of sequence modeling tasks, particularly ones that demand long-range information propagation (van den Oord et al., 2016; Kalchbrenner et al., 2016; Dauphin et al., 2017; Gehring et al., 2017; Bai et al., 2018). A third notable approach to sequence processing that has recently gained ground is based on self-attention (Vaswani et al., 2017; Santoro et al., 2018; Chen et al., 2018). Our work is most closely related to the first two approaches. In particular, we establish a strong connection between recurrent and convolutional networks and introduce a model that serves as a bridge between the two. A related recent theoretical investigation showed that under a certain stability condition, recurrent networks can be well-approximated by feed-forward models (Miller & Hardt, 2018).
There have been many combinations of convolutional and recurrent networks (Sainath et al., 2015). For example, convolutional LSTMs combine convolutional and recurrent units (Donahue et al., 2015; Venugopalan et al., 2015; Shi et al., 2015). Quasi-recurrent neural networks interleave convolutional and recurrent layers (Bradbury et al., 2017). Techniques introduced for convolutional networks, such as dilation, have been applied to RNNs (Chang et al., 2017). Our work establishes a deeper connection, deriving a direct mapping across the two architectural families and providing a structural bridge that can incorporate techniques from both sides.

3 SEQUENCE MODELING AND TRELLIS NETWORKS

Sequence modeling. Given an input x1:T = x1, . . . , xT , a sequence model is any function G : X T  YT such that

y1:T = y1, . . . , yT = G(x1, . . . , xT ),

(1)

where yt should only depend on x1:t and not on xt+1:T (i.e. no leakage of information from the future). This causality constraint is essential for autoregressive modeling.

In this section, we describe a new architecture for sequence modeling, referred to as a trellis network or TrellisNet. In particular, we provide an atomic view of TrellisNet, present its fundamental features, and highlight the relationship to convolutional networks. Section 4 will then elaborate on the relationship of trellis networks to convolutional and recurrent models.
Notation. We use x1:T = (x1, . . . , xT ) to denote a length-T input sequence, where vector xt  Rp is the input at time step t. Thus x1:T  RT ×p. We use zt(i)  Rq to represent the hidden unit at time t in layer i of the network. We use Conv1D(x; W ) to denote a 1D convolution with a kernel W applied to input x = x1:T .
A basic trellis network. At the most basic level, a feature vector zt(+i+11) at time step t + 1 and level i + 1 of TrellisNet is computed via three steps, illustrated in Figure 1a:
1. The hidden input comprises the hidden outputs zt(i), zt(+i)1  Rq from the previous layer i, as well as an injection of the input vectors xt, xt+1. At level 0, we initialize to zt(0) = 0.

2

Under review as a conference paper at ICLR 2019

zt(+i+11) <latexit sha1_base64="+VtluC+UvZ7AXfnV6T+0WUjJZNw=">AAAB+HicbVDLSgNBEOz1GeMjqx69DAYhEgg7IiieAl48RjAPSNZldjJJhsw+mJkVkmW/xIsHRbz6Kd78GyfJHjSxoKGo6qa7y48FV9pxvq219Y3Nre3CTnF3b/+gZB8etVSUSMqaNBKR7PhEMcFD1tRcC9aJJSOBL1jbH9/O/PYTk4pH4YOexMwNyDDkA06JNpJnl6Zeqqs4e0wrvIrPM88uOzVnDrRKcE7KkKPh2V+9fkSTgIWaCqJUFzuxdlMiNaeCZcVeolhM6JgMWdfQkARMuen88AydGaWPBpE0FWo0V39PpCRQahL4pjMgeqSWvZn4n9dN9ODaTXkYJ5qFdLFokAikIzRLAfW5ZFSLiSGESm5uRXREJKHaZFU0IeDll1dJ66KGnRq+vyzXb/I4CnACp1ABDFdQhztoQBMoJPAMr/BmTa0X6936WLSuWfnMMfyB9fkDeZiSRg==</latexit>

Activation
<latexit sha1_base64="kugio1po5Z0osXad24S4JMxUx58=">AAAB9XicbZDLSgNBEEVrfMb4irp005gIrsJMEHQZceMygnlAMoaeTk/SpOdBd00kDPkPNy4Uceu/uPNv7ExmoYkXGg63qqjq68VSaLTtb2ttfWNza7uwU9zd2z84LB0dt3SUKMabLJKR6nhUcylC3kSBkndixWngSd72xrfzenvClRZR+IDTmLsBHYbCF4yisR5vGIpJhqTiV/qlsl21M5FVcHIoQ65Gv/TVG0QsCXiITFKtu44do5tShYJJPiv2Es1jysZ0yLsGQxpw7abZ1TNybpwB8SNlXogkc39PpDTQehp4pjOgONLLtbn5X62boH/tpiKME+QhWyzyE0kwIvMIyEAozlBODVCmhLmVsBFVlKEJqmhCcJa/vAqtWtWxq859rVy/zOMowCmcwQU4cAV1uIMGNIGBgmd4hTfryXqx3q2PReualc+cwB9Znz+SgpHW</latexit>

f

xt
<latexit sha1_base64="eEPz6pgZfvj6K8i8F78InKVPN1o=">AAAB6nicbVA9SwNBEJ2LXzF+RS1tFoNgFe5EiIVFwMYyovmA5Ah7m71kyd7esTsnhiM/wcZCEVt/kZ3/xk1yhSY+GHi8N8PMvCCRwqDrfjuFtfWNza3idmlnd2//oHx41DJxqhlvsljGuhNQw6VQvIkCJe8kmtMokLwdjG9mfvuRayNi9YCThPsRHSoRCkbRSvdPfeyXK27VnYOsEi8nFcjR6Je/eoOYpRFXyCQ1puu5CfoZ1SiY5NNSLzU8oWxMh7xrqaIRN342P3VKzqwyIGGsbSkkc/X3REYjYyZRYDsjiiOz7M3E/7xuiuGVnwmVpMgVWywKU0kwJrO/yUBozlBOLKFMC3srYSOqKUObTsmG4C2/vEpaF1XPrXp3l5X6dR5HEU7gFM7BgxrU4RYa0AQGQ3iGV3hzpPPivDsfi9aCk88cwx84nz9vbo3d</latexit>
zt(i) <latexit sha1_base64="2svhF9CpH3GHMAPuUCL8JxYJANA=">AAAB8nicbVBNS8NAEN34WetX1aOXxSLUS0lEUDwVvHisYD8gjWWz3bRLN7thdyLUkJ/hxYMiXv013vw3btsctPXBwOO9GWbmhYngBlz321lZXVvf2Cxtlbd3dvf2KweHbaNSTVmLKqF0NySGCS5ZCzgI1k00I3EoWCcc30z9ziPThit5D5OEBTEZSh5xSsBK/lM/g/whq/GzvF+punV3BrxMvIJUUYFmv/LVGyiaxkwCFcQY33MTCDKigVPB8nIvNSwhdEyGzLdUkpiZIJudnONTqwxwpLQtCXim/p7ISGzMJA5tZ0xgZBa9qfif56cQXQUZl0kKTNL5oigVGBSe/o8HXDMKYmIJoZrbWzEdEU0o2JTKNgRv8eVl0j6ve27du7uoNq6LOEroGJ2gGvLQJWqgW9RELUSRQs/oFb054Lw4787HvHXFKWaO0B84nz9HwZE1</latexit>

W1 <latexit sha1_base64="lXMe2VVJVimguU04W2823Zlih1M=">AAAB6nicbVBNS8NAEJ3Ur1q/qh69LBbBU0lEUDwVvHisaD+gDWWz3bRLN5uwOxFK6E/w4kERr/4ib/4bt2kO2vpg4PHeDDPzgkQKg6777ZTW1jc2t8rblZ3dvf2D6uFR28SpZrzFYhnrbkANl0LxFgqUvJtoTqNA8k4wuZ37nSeujYjVI04T7kd0pEQoGEUrPXQG3qBac+tuDrJKvILUoEBzUP3qD2OWRlwhk9SYnucm6GdUo2CSzyr91PCEsgkd8Z6likbc+Fl+6oycWWVIwljbUkhy9fdERiNjplFgOyOKY7PszcX/vF6K4bWfCZWkyBVbLApTSTAm87/JUGjOUE4toUwLeythY6opQ5tOxYbgLb+8StoXdc+te/eXtcZNEUcZTuAUzsGDK2jAHTShBQxG8Ayv8OZI58V5dz4WrSWnmDmGP3A+fwDW8413</latexit>

z^t(+i+11) <latexit sha1_base64="Am4axvpT/RbzZdU3VQKPoUt9hZE=">AAAB/nicbVDLSgNBEJz1GeNrVTx5WQxCJBB2RFA8Bbx4jGAekKxhdjKbDJl9MNMrxGHBX/HiQRGvfoc3/8ZJsgdNLGgoqrrp7vITwRW47re1tLyyurZe2Chubm3v7Np7+00Vp5KyBo1FLNs+UUzwiDWAg2DtRDIS+oK1/NH1xG89MKl4HN3BOGFeSAYRDzglYKSefdgdEtCPWU9DBWf3uswr+DTr2SW36k7hLBKckxLKUe/ZX91+TNOQRUAFUaqD3QQ8TSRwKlhW7KaKJYSOyIB1DI1IyJSnp+dnzolR+k4QS1MROFP194QmoVLj0DedIYGhmvcm4n9eJ4Xg0tM8SlJgEZ0tClLhQOxMsnD6XDIKYmwIoZKbWx06JJJQMIkVTQh4/uVF0jyrYreKb89Ltas8jgI6QseojDC6QDV0g+qogSjS6Bm9ojfryXqx3q2PWeuSlc8coD+wPn8AaO2VEw==</latexit> W2 <latexit sha1_base64="uQ/4VJ3D9Rsf5zu3tZUkUWQBkuo=">AAAB6nicbVBNS8NAEJ3Ur1q/qh69LBbBU0mKUPFU8OKxov2ANpTNdtIu3WzC7kYooT/BiwdFvPqLvPlv3LY5aOuDgcd7M8zMCxLBtXHdb6ewsbm1vVPcLe3tHxwelY9P2jpOFcMWi0WsugHVKLjEluFGYDdRSKNAYCeY3M79zhMqzWP5aKYJ+hEdSR5yRo2VHjqD2qBccavuAmSdeDmpQI7moPzVH8YsjVAaJqjWPc9NjJ9RZTgTOCv1U40JZRM6wp6lkkao/Wxx6oxcWGVIwljZkoYs1N8TGY20nkaB7YyoGetVby7+5/VSE177GZdJalCy5aIwFcTEZP43GXKFzIipJZQpbm8lbEwVZcamU7IheKsvr5N2req5Ve/+qtK4yeMowhmcwyV4UIcG3EETWsBgBM/wCm+OcF6cd+dj2Vpw8plT+APn8wfYd414</latexit> xt+1 <latexit sha1_base64="nkvIntHmKtHH1HtvVwUH5BQy280=">AAAB7nicbVBNS8NAEJ3Ur1q/qh69LBZBEEoiBT14KHjxWMF+QBvKZrtpl242YXciltAf4cWDIl79Pd78N27bHLT1wcDjvRlm5gWJFAZd99sprK1vbG4Vt0s7u3v7B+XDo5aJU814k8Uy1p2AGi6F4k0UKHkn0ZxGgeTtYHw789uPXBsRqwecJNyP6FCJUDCKVmo/9TO88Kb9csWtunOQVeLlpAI5Gv3yV28QszTiCpmkxnQ9N0E/oxoFk3xa6qWGJ5SN6ZB3LVU04sbP5udOyZlVBiSMtS2FZK7+nshoZMwkCmxnRHFklr2Z+J/XTTG89jOhkhS5YotFYSoJxmT2OxkIzRnKiSWUaWFvJWxENWVoEyrZELzll1dJ67LquVXvvlap3+RxFOEETuEcPLiCOtxBA5rAYAzP8ApvTuK8OO/Ox6K14OQzx/AHzucPDcKPWQ==</latexit> zt(+i)1 <latexit sha1_base64="LuZCTtj0QRFI4tTxozbliK7Tzjg=">AAAB9HicbVDLSgNBEOyNrxhfUY9eBoMQEcKuCIqngBePEcwDkjXMTibJkNmHM72BuOx3ePGgiFc/xpt/4yTZgyYWNBRV3XR3eZEUGm3728qtrK6tb+Q3C1vbO7t7xf2Dhg5jxXidhTJULY9qLkXA6yhQ8lakOPU9yZve6GbqN8dcaREG9ziJuOvTQSD6glE0kvvUTfDMSR+SsjhNu8WSXbFnIMvEyUgJMtS6xa9OL2SxzwNkkmrdduwI3YQqFEzytNCJNY8oG9EBbxsaUJ9rN5kdnZITo/RIP1SmAiQz9fdEQn2tJ75nOn2KQ73oTcX/vHaM/Ss3EUEUIw/YfFE/lgRDMk2A9ITiDOXEEMqUMLcSNqSKMjQ5FUwIzuLLy6RxXnHsinN3UapeZ3Hk4QiOoQwOXEIVbqEGdWDwCM/wCm/W2Hqx3q2PeWvOymYO4Q+szx8jcZGl</latexit>

i!i+1

Padding <latexit sha1_base64="oAHphkycPZbicq7JUsel5512G2E=">AAAB7nicbVBNS8NAEJ3Ur1q/qh69LBbBU0l6sceCF48V7Ae0oWw2m3bpZhN2J4US+iO8eFDEq7/Hm//GbZuDtj4YeLw3w8y8IJXCoOt+O6Wd3b39g/Jh5ej45PSsen7RNUmmGe+wRCa6H1DDpVC8gwIl76ea0ziQvBdM75d+b8a1EYl6wnnK/ZiOlYgEo2ilXpuGoVDjUbXm1t0VyDbxClKDAu1R9WsYJiyLuUImqTEDz03Rz6lGwSRfVIaZ4SllUzrmA0sVjbnx89W5C3JjlZBEibalkKzU3xM5jY2Zx4HtjClOzKa3FP/zBhlGTT8XKs2QK7ZeFGWSYEKWv5NQaM5Qzi2hTAt7K2ETqilDm1DFhuBtvrxNuo2659a9x0at1SziKMMVXMMteHAHLXiANnSAwRSe4RXenNR5cd6dj3VrySlmLuEPnM8fL5WPaw==</latexit>

Padding <latexit sha1_base64="oAHphkycPZbicq7JUsel5512G2E=">AAAB7nicbVBNS8NAEJ3Ur1q/qh69LBbBU0l6sceCF48V7Ae0oWw2m3bpZhN2J4US+iO8eFDEq7/Hm//GbZuDtj4YeLw3w8y8IJXCoOt+O6Wd3b39g/Jh5ej45PSsen7RNUmmGe+wRCa6H1DDpVC8gwIl76ea0ziQvBdM75d+b8a1EYl6wnnK/ZiOlYgEo2ilXpuGoVDjUbXm1t0VyDbxClKDAu1R9WsYJiyLuUImqTEDz03Rz6lGwSRfVIaZ4SllUzrmA0sVjbnx89W5C3JjlZBEibalkKzU3xM5jY2Zx4HtjClOzKa3FP/zBhlGTT8XKs2QK7ZeFGWSYEKWv5NQaM5Qzi2hTAt7K2ETqilDm1DFhuBtvrxNuo2659a9x0at1SziKMMVXMMteHAHLXiANnSAwRSe4RXenNR5cd6dj3VrySlmLuEPnM8fL5WPaw==</latexit>

Activation
<latexit sha1_base64="kugio1po5Z0osXad24S4JMxUx58=">AAAB9XicbZDLSgNBEEVrfMb4irp005gIrsJMEHQZceMygnlAMoaeTk/SpOdBd00kDPkPNy4Uceu/uPNv7ExmoYkXGg63qqjq68VSaLTtb2ttfWNza7uwU9zd2z84LB0dt3SUKMabLJKR6nhUcylC3kSBkndixWngSd72xrfzenvClRZR+IDTmLsBHYbCF4yisR5vGIpJhqTiV/qlsl21M5FVcHIoQ65Gv/TVG0QsCXiITFKtu44do5tShYJJPiv2Es1jysZ0yLsGQxpw7abZ1TNybpwB8SNlXogkc39PpDTQehp4pjOgONLLtbn5X62boH/tpiKME+QhWyzyE0kwIvMIyEAozlBODVCmhLmVsBFVlKEJqmhCcJa/vAqtWtWxq859rVy/zOMowCmcwQU4cAV1uIMGNIGBgmd4hTfryXqx3q2PReualc+cwB9Znz+SgpHW</latexit>

f

x1 <latexit sha1_base64="LHTk4FRcQ4h5K0jvGjsNroBkfok=">AAAB6nicbVBNS8NAEJ3Ur1q/qh69LBbBU0mKUI8FLx4r2g9oQ9lsN+3SzSbsTsQS+hO8eFDEq7/Im//GbZuDtj4YeLw3w8y8IJHCoOt+O4WNza3tneJuaW//4PCofHzSNnGqGW+xWMa6G1DDpVC8hQIl7yaa0yiQvBNMbuZ+55FrI2L1gNOE+xEdKREKRtFK908Db1CuuFV3AbJOvJxUIEdzUP7qD2OWRlwhk9SYnucm6GdUo2CSz0r91PCEsgkd8Z6likbc+Nni1Bm5sMqQhLG2pZAs1N8TGY2MmUaB7Ywojs2qNxf/83ophtd+JlSSIldsuShMJcGYzP8mQ6E5Qzm1hDIt7K2EjammDG06JRuCt/ryOmnXqp5b9e6uKo1aHkcRzuAcLsGDOjTgFprQAgYjeIZXeHOk8+K8Ox/L1oKTz5zCHzifPwbgjZA=</latexit>
z1(i+1) <latexit sha1_base64="Xm7NeWFn8RUrstDxkHIsu2LCQGE=">AAAB8nicbVBNSwMxEJ31s9avqkcvwSJUhLIRQY8FLx4r2A/YriWbpm1oNlmSrFCX/gwvHhTx6q/x5r8xbfegrQ8GHu/NMDMvSgQ31ve/vZXVtfWNzcJWcXtnd2+/dHDYNCrVlDWoEkq3I2KY4JI1LLeCtRPNSBwJ1opGN1O/9ci04Ure23HCwpgMJO9zSqyTgqcufsgq/ByfTbqlsl/1Z0DLBOekDDnq3dJXp6doGjNpqSDGBNhPbJgRbTkVbFLspIYlhI7IgAWOShIzE2azkyfo1Ck91FfalbRopv6eyEhszDiOXGdM7NAselPxPy9Ibf86zLhMUssknS/qpwJZhab/ox7XjFoxdoRQzd2tiA6JJtS6lIouBLz48jJpXlSxX8V3l+Wan8dRgGM4gQpguIIa3EIdGkBBwTO8wptnvRfv3fuYt654+cwR/IH3+QPt8pBM</latexit>

x2 <latexit sha1_base64="Q5m66tjDDgQFM5wTYetSpnS6JPE=">AAAB6nicbVBNS8NAEJ3Ur1q/qh69LBbBU0mKUI8FLx4r2g9oQ9lsJ+3SzSbsbsQS+hO8eFDEq7/Im//GbZuDtj4YeLw3w8y8IBFcG9f9dgobm1vbO8Xd0t7+weFR+fikreNUMWyxWMSqG1CNgktsGW4EdhOFNAoEdoLJzdzvPKLSPJYPZpqgH9GR5CFn1Fjp/mlQG5QrbtVdgKwTLycVyNEclL/6w5ilEUrDBNW657mJ8TOqDGcCZ6V+qjGhbEJH2LNU0gi1ny1OnZELqwxJGCtb0pCF+nsio5HW0yiwnRE1Y73qzcX/vF5qwms/4zJJDUq2XBSmgpiYzP8mQ66QGTG1hDLF7a2EjamizNh0SjYEb/XlddKuVT236t1dVRq1PI4inME5XIIHdWjALTShBQxG8Ayv8OYI58V5dz6WrQUnnzmFP3A+fwAIZI2R</latexit>
z2(i+1) <latexit sha1_base64="ZIwc9u14EnDjItD0L18MuwK9Qck=">AAAB8nicbVBNSwMxEJ2tX7V+VT16CRahIpTdIuix4MVjBfsB27Vk02wbmk2WJCvUpT/DiwdFvPprvPlvTNs9aOuDgcd7M8zMCxPOtHHdb6ewtr6xuVXcLu3s7u0flA+P2lqmitAWkVyqbog15UzQlmGG026iKI5DTjvh+Gbmdx6p0kyKezNJaBDjoWARI9hYyX/q1x+yKrvwzqf9csWtuXOgVeLlpAI5mv3yV28gSRpTYQjHWvuem5ggw8owwum01Es1TTAZ4yH1LRU4pjrI5idP0ZlVBiiSypYwaK7+nshwrPUkDm1njM1IL3sz8T/PT010HWRMJKmhgiwWRSlHRqLZ/2jAFCWGTyzBRDF7KyIjrDAxNqWSDcFbfnmVtOs1z615d5eVhpvHUYQTOIUqeHAFDbiFJrSAgIRneIU3xzgvzrvzsWgtOPnMMfyB8/kD736QTQ==</latexit>

x3 <latexit sha1_base64="vd/3+5BCCHLMHf7hQPZRkv+I3rw=">AAAB6nicbVBNS8NAEJ3Ur1q/qh69LBbBU0mqoMeCF48V7Qe0oWy2k3bpZhN2N2IJ/QlePCji1V/kzX/jts1BWx8MPN6bYWZekAiujet+O4W19Y3NreJ2aWd3b/+gfHjU0nGqGDZZLGLVCahGwSU2DTcCO4lCGgUC28H4Zua3H1FpHssHM0nQj+hQ8pAzaqx0/9S/6JcrbtWdg6wSLycVyNHol796g5ilEUrDBNW667mJ8TOqDGcCp6VeqjGhbEyH2LVU0gi1n81PnZIzqwxIGCtb0pC5+nsio5HWkyiwnRE1I73szcT/vG5qwms/4zJJDUq2WBSmgpiYzP4mA66QGTGxhDLF7a2EjaiizNh0SjYEb/nlVdKqVT236t1dVuq1PI4inMApnIMHV1CHW2hAExgM4Rle4c0Rzovz7nwsWgtOPnMMf+B8/gAJ6I2S</latexit>
z3(i+1) <latexit sha1_base64="ye0c8ZfSouo0FZWCM58p3VWbezk=">AAAB8nicbVBNSwMxEM3Wr1q/qh69BItQEcquCnosePFYwX7Adi3ZNNuGZpMlmRXq0p/hxYMiXv013vw3pu0etPXBwOO9GWbmhYngBlz32ymsrK6tbxQ3S1vbO7t75f2DllGppqxJlVC6ExLDBJesCRwE6ySakTgUrB2ObqZ++5Fpw5W8h3HCgpgMJI84JWAl/6l38ZBV+Zl3OumVK27NnQEvEy8nFZSj0St/dfuKpjGTQAUxxvfcBIKMaOBUsEmpmxqWEDoiA+ZbKknMTJDNTp7gE6v0caS0LQl4pv6eyEhszDgObWdMYGgWvan4n+enEF0HGZdJCkzS+aIoFRgUnv6P+1wzCmJsCaGa21sxHRJNKNiUSjYEb/HlZdI6r3luzbu7rNTdPI4iOkLHqIo8dIXq6BY1UBNRpNAzekVvDjgvzrvzMW8tOPnMIfoD5/MH8QqQTg==</latexit>

x4 <latexit sha1_base64="NhN9u0qKu6RsT8lrhpOfmOFDo20=">AAAB6nicbVBNS8NAEJ3Ur1q/qh69LBbBU0lKQY8FLx4r2g9oQ9lsJ+3SzSbsbsQS+hO8eFDEq7/Im//GbZuDtj4YeLw3w8y8IBFcG9f9dgobm1vbO8Xd0t7+weFR+fikreNUMWyxWMSqG1CNgktsGW4EdhOFNAoEdoLJzdzvPKLSPJYPZpqgH9GR5CFn1Fjp/mlQH5QrbtVdgKwTLycVyNEclL/6w5ilEUrDBNW657mJ8TOqDGcCZ6V+qjGhbEJH2LNU0gi1ny1OnZELqwxJGCtb0pCF+nsio5HW0yiwnRE1Y73qzcX/vF5qwms/4zJJDUq2XBSmgpiYzP8mQ66QGTG1hDLF7a2EjamizNh0SjYEb/XlddKuVT236t3VK41aHkcRzuAcLsGDK2jALTShBQxG8Ayv8OYI58V5dz6WrQUnnzmFP3A+fwALbI2T</latexit>

x5 <latexit sha1_base64="9xOKs6lmcAyyWgXd0SkYRgZhA78=">AAAB6nicbVBNS8NAEJ3Ur1q/qh69LBbBU0mKoseCF48V7Qe0oWy2k3bpZhN2N2IJ/QlePCji1V/kzX/jts1BWx8MPN6bYWZekAiujet+O4W19Y3NreJ2aWd3b/+gfHjU0nGqGDZZLGLVCahGwSU2DTcCO4lCGgUC28H4Zua3H1FpHssHM0nQj+hQ8pAzaqx0/9S/7JcrbtWdg6wSLycVyNHol796g5ilEUrDBNW667mJ8TOqDGcCp6VeqjGhbEyH2LVU0gi1n81PnZIzqwxIGCtb0pC5+nsio5HWkyiwnRE1I73szcT/vG5qwms/4zJJDUq2WBSmgpiYzP4mA66QGTGxhDLF7a2EjaiizNh0SjYEb/nlVdKqVT236t1dVOq1PI4inMApnIMHV1CHW2hAExgM4Rle4c0Rzovz7nwsWgtOPnMMf+B8/gAM8I2U</latexit>

z4(i+1) <latexit sha1_base64="Qsa7udezK+ki1RiBlGdFv56ZU4s=">AAAB8nicbVBNSwMxEJ2tX7V+VT16CRahIpRdKeix4MVjBfsB27Vk02wbmk2WJCvUpT/DiwdFvPprvPlvTNs9aOuDgcd7M8zMCxPOtHHdb6ewtr6xuVXcLu3s7u0flA+P2lqmitAWkVyqbog15UzQlmGG026iKI5DTjvh+Gbmdx6p0kyKezNJaBDjoWARI9hYyX/q1x+yKrvwzqf9csWtuXOgVeLlpAI5mv3yV28gSRpTYQjHWvuem5ggw8owwum01Es1TTAZ4yH1LRU4pjrI5idP0ZlVBiiSypYwaK7+nshwrPUkDm1njM1IL3sz8T/PT010HWRMJKmhgiwWRSlHRqLZ/2jAFCWGTyzBRDF7KyIjrDAxNqWSDcFbfnmVtC9rnlvz7uqVhpvHUYQTOIUqeHAFDbiFJrSAgIRneIU3xzgvzrvzsWgtOPnMMfyB8/kD8paQTw==</latexit>

z5(i+1) <latexit sha1_base64="WxKPp498V8RYZw0RggWUvvfH4lo=">AAAB8nicbVBNSwMxEM3Wr1q/qh69BItQEcquKHosePFYwX7Adi3ZNNuGZpMlmRXq0p/hxYMiXv013vw3pu0etPXBwOO9GWbmhYngBlz32ymsrK6tbxQ3S1vbO7t75f2DllGppqxJlVC6ExLDBJesCRwE6ySakTgUrB2ObqZ++5Fpw5W8h3HCgpgMJI84JWAl/6l3+ZBV+Zl3OumVK27NnQEvEy8nFZSj0St/dfuKpjGTQAUxxvfcBIKMaOBUsEmpmxqWEDoiA+ZbKknMTJDNTp7gE6v0caS0LQl4pv6eyEhszDgObWdMYGgWvan4n+enEF0HGZdJCkzS+aIoFRgUnv6P+1wzCmJsCaGa21sxHRJNKNiUSjYEb/HlZdI6r3luzbu7qNTdPI4iOkLHqIo8dIXq6BY1UBNRpNAzekVvDjgvzrvzMW8tOPnMIfoD5/MH9CKQUA==</latexit>

Filter <latexit sha1_base64="t/B8MdoeaDPtk2WxPCtkm9aAmV4=">AAAB8XicbVBNS8NAEJ3Ur1q/qh69LLaCp5IUQY8FQTxWsK3YhrLZTtqlm03Y3Qgl9F948aCIV/+NN/+N2zYHbX0w8Hhvhpl5QSK4Nq777RTW1jc2t4rbpZ3dvf2D8uFRW8epYthisYjVQ0A1Ci6xZbgR+JAopFEgsBOMr2d+5wmV5rG8N5ME/YgOJQ85o8ZKjzdcGFSk2qn2yxW35s5BVomXkwrkaPbLX71BzNIIpWGCat313MT4GVWGM4HTUi/VmFA2pkPsWipphNrP5hdPyZlVBiSMlS1pyFz9PZHRSOtJFNjOiJqRXvZm4n9eNzXhlZ9xmaQGJVssClNBTExm75MBV8iMmFhCmeL2VsJGVFFmY9AlG4K3/PIqaddrnlvz7uqVxkUeRxFO4BTOwYNLaMAtNKEFDCQ8wyu8Odp5cd6dj0VrwclnjuEPnM8fTGeP8w==</latexit>

W

x6 <latexit sha1_base64="3mJ56c0McJp8Sj2EekOsRFMnB5Q=">AAAB6nicbVBNS8NAEJ3Ur1q/qh69LBbBU0mKqMeCF48V7Qe0oWy2k3bpZhN2N2IJ/QlePCji1V/kzX/jts1BWx8MPN6bYWZekAiujet+O4W19Y3NreJ2aWd3b/+gfHjU0nGqGDZZLGLVCahGwSU2DTcCO4lCGgUC28H4Zua3H1FpHssHM0nQj+hQ8pAzaqx0/9S/7JcrbtWdg6wSLycVyNHol796g5ilEUrDBNW667mJ8TOqDGcCp6VeqjGhbEyH2LVU0gi1n81PnZIzqwxIGCtb0pC5+nsio5HWkyiwnRE1I73szcT/vG5qwms/4zJJDUq2WBSmgpiYzP4mA66QGTGxhDLF7a2EjaiizNh0SjYEb/nlVdKqVT236t1dVOq1PI4inMApnIMHV1CHW2hAExgM4Rle4c0Rzovz7nwsWgtOPnMMf+B8/gAOdI2V</latexit>

x7 <latexit sha1_base64="QaRF83Q7awwnGlt55EAYIgtUZw8=">AAAB6nicbVBNS8NAEJ3Ur1q/qh69LBbBU0mKUI8FLx4r2g9oQ9lsN+3SzSbsTsQS+hO8eFDEq7/Im//GbZuDtj4YeLw3w8y8IJHCoOt+O4WNza3tneJuaW//4PCofHzSNnGqGW+xWMa6G1DDpVC8hQIl7yaa0yiQvBNMbuZ+55FrI2L1gNOE+xEdKREKRtFK90+D+qBccavuAmSdeDmpQI7moPzVH8YsjbhCJqkxPc9N0M+oRsEkn5X6qeEJZRM64j1LFY248bPFqTNyYZUhCWNtSyFZqL8nMhoZM40C2xlRHJtVby7+5/VSDK/9TKgkRa7YclGYSoIxmf9NhkJzhnJqCWVa2FsJG1NNGdp0SjYEb/XlddKuVT236t1dVRq1PI4inME5XIIHdWjALTShBQxG8Ayv8OZI58V5dz6WrQUnnzmFP3A+fwAP+I2W</latexit>

z6(i+1) <latexit sha1_base64="rpOIsZ2QAafkFco+dPaJtiSG0lg=">AAAB8nicbVBNSwMxEM3Wr1q/qh69BItQEcquiHosePFYwX7Adi3ZNNuGZpMlmRXq0p/hxYMiXv013vw3pu0etPXBwOO9GWbmhYngBlz32ymsrK6tbxQ3S1vbO7t75f2DllGppqxJlVC6ExLDBJesCRwE6ySakTgUrB2ObqZ++5Fpw5W8h3HCgpgMJI84JWAl/6l3+ZBV+Zl3OumVK27NnQEvEy8nFZSj0St/dfuKpjGTQAUxxvfcBIKMaOBUsEmpmxqWEDoiA+ZbKknMTJDNTp7gE6v0caS0LQl4pv6eyEhszDgObWdMYGgWvan4n+enEF0HGZdJCkzS+aIoFRgUnv6P+1wzCmJsCaGa21sxHRJNKNiUSjYEb/HlZdI6r3luzbu7qNTdPI4iOkLHqIo8dIXq6BY1UBNRpNAzekVvDjgvzrvzMW8tOPnMIfoD5/MH9a6QUQ==</latexit>

z7(i+1) <latexit sha1_base64="7F9nxBXfc+NOMi8KgPyEJHwYOWk=">AAAB8nicbVBNSwMxEM3Wr1q/qh69BItQEcquCPVY8OKxgv2A7VqyabYNzSZLMivUpT/DiwdFvPprvPlvTNs9aOuDgcd7M8zMCxPBDbjut1NYW9/Y3Cpul3Z29/YPyodHbaNSTVmLKqF0NySGCS5ZCzgI1k00I3EoWCcc38z8ziPThit5D5OEBTEZSh5xSsBK/lO//pBV+YV3Pu2XK27NnQOvEi8nFZSj2S9/9QaKpjGTQAUxxvfcBIKMaOBUsGmplxqWEDomQ+ZbKknMTJDNT57iM6sMcKS0LQl4rv6eyEhszCQObWdMYGSWvZn4n+enEF0HGZdJCkzSxaIoFRgUnv2PB1wzCmJiCaGa21sxHRFNKNiUSjYEb/nlVdK+rHluzbu7qjTcPI4iOkGnqIo8VEcNdIuaqIUoUugZvaI3B5wX5935WLQWnHzmGP2B8/kD9zqQUg==</latexit>

x8 <latexit sha1_base64="LxQhhprzm81NnfnhLVIHmSPbSpM=">AAAB6nicbVBNS8NAEJ3Ur1q/qh69LBbBU0mKYI8FLx4r2g9oQ9lsN+3SzSbsTsQS+hO8eFDEq7/Im//GbZuDtj4YeLw3w8y8IJHCoOt+O4WNza3tneJuaW//4PCofHzSNnGqGW+xWMa6G1DDpVC8hQIl7yaa0yiQvBNMbuZ+55FrI2L1gNOE+xEdKREKRtFK90+D+qBccavuAmSdeDmpQI7moPzVH8YsjbhCJqkxPc9N0M+oRsEkn5X6qeEJZRM64j1LFY248bPFqTNyYZUhCWNtSyFZqL8nMhoZM40C2xlRHJtVby7+5/VSDOt+JlSSIldsuShMJcGYzP8mQ6E5Qzm1hDIt7K2EjammDG06JRuCt/ryOmnXqp5b9e6uKo1aHkcRzuAcLsGDa2jALTShBQxG8Ayv8OZI58V5dz6WrQUnnzmFP3A+fwARfI2X</latexit>
z8(i+1) <latexit sha1_base64="CFEDdErB2vRkfQsGYocM72Zk/wA=">AAAB8nicbVBNSwMxEM3Wr1q/qh69BItQEcquCPZY8OKxgv2A7VqyabYNzSZLMivUpT/DiwdFvPprvPlvTNs9aOuDgcd7M8zMCxPBDbjut1NYW9/Y3Cpul3Z29/YPyodHbaNSTVmLKqF0NySGCS5ZCzgI1k00I3EoWCcc38z8ziPThit5D5OEBTEZSh5xSsBK/lO//pBV+YV3Pu2XK27NnQOvEi8nFZSj2S9/9QaKpjGTQAUxxvfcBIKMaOBUsGmplxqWEDomQ+ZbKknMTJDNT57iM6sMcKS0LQl4rv6eyEhszCQObWdMYGSWvZn4n+enENWDjMskBSbpYlGUCgwKz/7HA64ZBTGxhFDN7a2YjogmFGxKJRuCt/zyKmlf1jy35t1dVRpuHkcRnaBTVEUeukYNdIuaqIUoUugZvaI3B5wX5935WLQWnHzmGP2B8/kD+MaQUw==</latexit>

Activation
<latexit sha1_base64="kugio1po5Z0osXad24S4JMxUx58=">AAAB9XicbZDLSgNBEEVrfMb4irp005gIrsJMEHQZceMygnlAMoaeTk/SpOdBd00kDPkPNy4Uceu/uPNv7ExmoYkXGg63qqjq68VSaLTtb2ttfWNza7uwU9zd2z84LB0dt3SUKMabLJKR6nhUcylC3kSBkndixWngSd72xrfzenvClRZR+IDTmLsBHYbCF4yisR5vGIpJhqTiV/qlsl21M5FVcHIoQ65Gv/TVG0QsCXiITFKtu44do5tShYJJPiv2Es1jysZ0yLsGQxpw7abZ1TNybpwB8SNlXogkc39PpDTQehp4pjOgONLLtbn5X62boH/tpiKME+QhWyzyE0kwIvMIyEAozlBODVCmhLmVsBFVlKEJqmhCcJa/vAqtWtWxq859rVy/zOMowCmcwQU4cAV1uIMGNIGBgmd4hTfryXqx3q2PReualc+cwB9Znz+SgpHW</latexit>

f

x1 <latexit sha1_base64="LHTk4FRcQ4h5K0jvGjsNroBkfok=">AAAB6nicbVBNS8NAEJ3Ur1q/qh69LBbBU0mKUI8FLx4r2g9oQ9lsN+3SzSbsTsQS+hO8eFDEq7/Im//GbZuDtj4YeLw3w8y8IJHCoOt+O4WNza3tneJuaW//4PCofHzSNnGqGW+xWMa6G1DDpVC8hQIl7yaa0yiQvBNMbuZ+55FrI2L1gNOE+xEdKREKRtFK908Db1CuuFV3AbJOvJxUIEdzUP7qD2OWRlwhk9SYnucm6GdUo2CSz0r91PCEsgkd8Z6likbc+Nni1Bm5sMqQhLG2pZAs1N8TGY2MmUaB7Ywojs2qNxf/83ophtd+JlSSIldsuShMJcGYzP8mQ6E5Qzm1hDIt7K2EjammDG06JRuCt/ryOmnXqp5b9e6uKo1aHkcRzuAcLsGDOjTgFprQAgYjeIZXeHOk8+K8Ox/L1oKTz5zCHzifPwbgjZA=</latexit>
z1(i) <latexit sha1_base64="8aFrIZpyNszu8100UYCIuIio7Bo=">AAAB8HicbVBNSwMxEJ2tX7V+VT16CRahXsquFPRY8OKxgv2Qdi3ZNNuGJtklyQrr0l/hxYMiXv053vw3pu0etPXBwOO9GWbmBTFn2rjut1NYW9/Y3Cpul3Z29/YPyodHbR0litAWiXikugHWlDNJW4YZTruxolgEnHaCyfXM7zxSpVkk70waU1/gkWQhI9hY6f5p4D1kVXY+HZQrbs2dA60SLycVyNEclL/6w4gkgkpDONa657mx8TOsDCOcTkv9RNMYkwke0Z6lEguq/Wx+8BSdWWWIwkjZkgbN1d8TGRZapyKwnQKbsV72ZuJ/Xi8x4ZWfMRknhkqyWBQmHJkIzb5HQ6YoMTy1BBPF7K2IjLHCxNiMSjYEb/nlVdK+qHluzbutVxr1PI4inMApVMGDS2jADTShBQQEPMMrvDnKeXHenY9Fa8HJZ47hD5zPHxQBj+A=</latexit>

x2 <latexit sha1_base64="Q5m66tjDDgQFM5wTYetSpnS6JPE=">AAAB6nicbVBNS8NAEJ3Ur1q/qh69LBbBU0mKUI8FLx4r2g9oQ9lsJ+3SzSbsbsQS+hO8eFDEq7/Im//GbZuDtj4YeLw3w8y8IBFcG9f9dgobm1vbO8Xd0t7+weFR+fikreNUMWyxWMSqG1CNgktsGW4EdhOFNAoEdoLJzdzvPKLSPJYPZpqgH9GR5CFn1Fjp/mlQG5QrbtVdgKwTLycVyNEclL/6w5ilEUrDBNW657mJ8TOqDGcCZ6V+qjGhbEJH2LNU0gi1ny1OnZELqwxJGCtb0pCF+nsio5HW0yiwnRE1Y73qzcX/vF5qwms/4zJJDUq2XBSmgpiYzP8mQ66QGTG1hDLF7a2EjamizNh0SjYEb/XlddKuVT236t1dVRq1PI4inME5XIIHdWjALTShBQxG8Ayv8OYI58V5dz6WrQUnnzmFP3A+fwAIZI2R</latexit>
z2(i) <latexit sha1_base64="utZ60D7VnEBY9obYyD/39kKsKCM=">AAAB8HicbVBNSwMxEJ2tX7V+VT16CRahXspuKdRjwYvHCvZD2rVk02wbmmSXJCvUpb/CiwdFvPpzvPlvTNs9aOuDgcd7M8zMC2LOtHHdbye3sbm1vZPfLeztHxweFY9P2jpKFKEtEvFIdQOsKWeStgwznHZjRbEIOO0Ek+u533mkSrNI3plpTH2BR5KFjGBjpfunQfUhLbPL2aBYcivuAmideBkpQYbmoPjVH0YkEVQawrHWPc+NjZ9iZRjhdFboJ5rGmEzwiPYslVhQ7aeLg2fowipDFEbKljRoof6eSLHQeioC2ymwGetVby7+5/USE175KZNxYqgky0VhwpGJ0Px7NGSKEsOnlmCimL0VkTFWmBibUcGG4K2+vE7a1YrnVrzbWqlRy+LIwxmcQxk8qEMDbqAJLSAg4Ble4c1Rzovz7nwsW3NONnMKf+B8/gAVi4/h</latexit>

x3 <latexit sha1_base64="vd/3+5BCCHLMHf7hQPZRkv+I3rw=">AAAB6nicbVBNS8NAEJ3Ur1q/qh69LBbBU0mqoMeCF48V7Qe0oWy2k3bpZhN2N2IJ/QlePCji1V/kzX/jts1BWx8MPN6bYWZekAiujet+O4W19Y3NreJ2aWd3b/+gfHjU0nGqGDZZLGLVCahGwSU2DTcCO4lCGgUC28H4Zua3H1FpHssHM0nQj+hQ8pAzaqx0/9S/6JcrbtWdg6wSLycVyNHol796g5ilEUrDBNW667mJ8TOqDGcCp6VeqjGhbEyH2LVU0gi1n81PnZIzqwxIGCtb0pC5+nsio5HWkyiwnRE1I73szcT/vG5qwms/4zJJDUq2WBSmgpiYzP4mA66QGTGxhDLF7a2EjaiizNh0SjYEb/nlVdKqVT236t1dVuq1PI4inMApnIMHV1CHW2hAExgM4Rle4c0Rzovz7nwsWgtOPnMMf+B8/gAJ6I2S</latexit>
z3(i) <latexit sha1_base64="kfXEEDeaeeqBXGdWKWo652TLB6A=">AAAB8HicbVBNSwMxEJ2tX7V+VT16CRahXsquLeix4MVjBfsh7VqyabYNTbJLkhXq0l/hxYMiXv053vw3pu0etPXBwOO9GWbmBTFn2rjut5NbW9/Y3MpvF3Z29/YPiodHLR0litAmiXikOgHWlDNJm4YZTjuxolgEnLaD8fXMbz9SpVkk78wkpr7AQ8lCRrCx0v1Tv/qQltn5tF8suRV3DrRKvIyUIEOjX/zqDSKSCCoN4VjrrufGxk+xMoxwOi30Ek1jTMZ4SLuWSiyo9tP5wVN0ZpUBCiNlSxo0V39PpFhoPRGB7RTYjPSyNxP/87qJCa/8lMk4MVSSxaIw4chEaPY9GjBFieETSzBRzN6KyAgrTIzNqGBD8JZfXiWti4rnVrzbWqley+LIwwmcQhk8uIQ63EADmkBAwDO8wpujnBfn3flYtOacbOYY/sD5/AEXFY/i</latexit>

x4 <latexit sha1_base64="NhN9u0qKu6RsT8lrhpOfmOFDo20=">AAAB6nicbVBNS8NAEJ3Ur1q/qh69LBbBU0lKQY8FLx4r2g9oQ9lsJ+3SzSbsbsQS+hO8eFDEq7/Im//GbZuDtj4YeLw3w8y8IBFcG9f9dgobm1vbO8Xd0t7+weFR+fikreNUMWyxWMSqG1CNgktsGW4EdhOFNAoEdoLJzdzvPKLSPJYPZpqgH9GR5CFn1Fjp/mlQH5QrbtVdgKwTLycVyNEclL/6w5ilEUrDBNW657mJ8TOqDGcCZ6V+qjGhbEJH2LNU0gi1ny1OnZELqwxJGCtb0pCF+nsio5HW0yiwnRE1Y73qzcX/vF5qwms/4zJJDUq2XBSmgpiYzP8mQ66QGTG1hDLF7a2EjamizNh0SjYEb/XlddKuVT236t3VK41aHkcRzuAcLsGDK2jALTShBQxG8Ayv8OYI58V5dz6WrQUnnzmFP3A+fwALbI2T</latexit>
z4(i) <latexit sha1_base64="DI+ULDLoOk823ZYgE5ED3TWrYXI=">AAAB8HicbVBNSwMxEJ2tX7V+VT16CRahXsquLOix4MVjBfsh7VqyabYNTbJLkhXq0l/hxYMiXv053vw3pu0etPXBwOO9GWbmhQln2rjut1NYW9/Y3Cpul3Z29/YPyodHLR2nitAmiXmsOiHWlDNJm4YZTjuJoliEnLbD8fXMbz9SpVks78wkoYHAQ8kiRrCx0v1T33/Iqux82i9X3Jo7B1olXk4qkKPRL3/1BjFJBZWGcKx113MTE2RYGUY4nZZ6qaYJJmM8pF1LJRZUB9n84Ck6s8oARbGyJQ2aq78nMiy0nojQdgpsRnrZm4n/ed3URFdBxmSSGirJYlGUcmRiNPseDZiixPCJJZgoZm9FZIQVJsZmVLIheMsvr5LWRc1za96tX6n7eRxFOIFTqIIHl1CHG2hAEwgIeIZXeHOU8+K8Ox+L1oKTzxzDHzifPxifj+M=</latexit>

x5 <latexit sha1_base64="9xOKs6lmcAyyWgXd0SkYRgZhA78=">AAAB6nicbVBNS8NAEJ3Ur1q/qh69LBbBU0mKoseCF48V7Qe0oWy2k3bpZhN2N2IJ/QlePCji1V/kzX/jts1BWx8MPN6bYWZekAiujet+O4W19Y3NreJ2aWd3b/+gfHjU0nGqGDZZLGLVCahGwSU2DTcCO4lCGgUC28H4Zua3H1FpHssHM0nQj+hQ8pAzaqx0/9S/7JcrbtWdg6wSLycVyNHol796g5ilEUrDBNW667mJ8TOqDGcCp6VeqjGhbEyH2LVU0gi1n81PnZIzqwxIGCtb0pC5+nsio5HWkyiwnRE1I73szcT/vG5qwms/4zJJDUq2WBSmgpiYzP4mA66QGTGxhDLF7a2EjaiizNh0SjYEb/nlVdKqVT236t1dVOq1PI4inMApnIMHV1CHW2hAExgM4Rle4c0Rzovz7nwsWgtOPnMMf+B8/gAM8I2U</latexit>
z5(i) <latexit sha1_base64="M3ir7frekpR+nTtCJ8EkaP7kyiM=">AAAB8HicbVBNSwMxEJ2tX7V+VT16CRahXsqutOix4MVjBfsh7VqyabYNTbJLkhXq0l/hxYMiXv053vw3pu0etPXBwOO9GWbmBTFn2rjut5NbW9/Y3MpvF3Z29/YPiodHLR0litAmiXikOgHWlDNJm4YZTjuxolgEnLaD8fXMbz9SpVkk78wkpr7AQ8lCRrCx0v1Tv/aQltn5tF8suRV3DrRKvIyUIEOjX/zqDSKSCCoN4VjrrufGxk+xMoxwOi30Ek1jTMZ4SLuWSiyo9tP5wVN0ZpUBCiNlSxo0V39PpFhoPRGB7RTYjPSyNxP/87qJCa/8lMk4MVSSxaIw4chEaPY9GjBFieETSzBRzN6KyAgrTIzNqGBD8JZfXiWti4rnVrzbaqlezeLIwwmcQhk8uIQ63EADmkBAwDO8wpujnBfn3flYtOacbOYY/sD5/AEaKY/k</latexit>

x6 <latexit sha1_base64="3mJ56c0McJp8Sj2EekOsRFMnB5Q=">AAAB6nicbVBNS8NAEJ3Ur1q/qh69LBbBU0mKqMeCF48V7Qe0oWy2k3bpZhN2N2IJ/QlePCji1V/kzX/jts1BWx8MPN6bYWZekAiujet+O4W19Y3NreJ2aWd3b/+gfHjU0nGqGDZZLGLVCahGwSU2DTcCO4lCGgUC28H4Zua3H1FpHssHM0nQj+hQ8pAzaqx0/9S/7JcrbtWdg6wSLycVyNHol796g5ilEUrDBNW667mJ8TOqDGcCp6VeqjGhbEyH2LVU0gi1n81PnZIzqwxIGCtb0pC5+nsio5HWkyiwnRE1I73szcT/vG5qwms/4zJJDUq2WBSmgpiYzP4mA66QGTGxhDLF7a2EjaiizNh0SjYEb/nlVdKqVT236t1dVOq1PI4inMApnIMHV1CHW2hAExgM4Rle4c0Rzovz7nwsWgtOPnMMf+B8/gAOdI2V</latexit>
z6(i) <latexit sha1_base64="diNED6XE6KUBoBne4HCg9kcQHOk=">AAAB8HicbVBNSwMxEJ2tX7V+VT16CRahXsqulOqx4MVjBfsh7VqyabYNTbJLkhXq0l/hxYMiXv053vw3pu0etPXBwOO9GWbmBTFn2rjut5NbW9/Y3MpvF3Z29/YPiodHLR0litAmiXikOgHWlDNJm4YZTjuxolgEnLaD8fXMbz9SpVkk78wkpr7AQ8lCRrCx0v1Tv/aQltn5tF8suRV3DrRKvIyUIEOjX/zqDSKSCCoN4VjrrufGxk+xMoxwOi30Ek1jTMZ4SLuWSiyo9tP5wVN0ZpUBCiNlSxo0V39PpFhoPRGB7RTYjPSyNxP/87qJCa/8lMk4MVSSxaIw4chEaPY9GjBFieETSzBRzN6KyAgrTIzNqGBD8JZfXiWti4rnVrzbaqlezeLIwwmcQhk8uIQ63EADmkBAwDO8wpujnBfn3flYtOacbOYY/sD5/AEbs4/l</latexit>

x7 <latexit sha1_base64="QaRF83Q7awwnGlt55EAYIgtUZw8=">AAAB6nicbVBNS8NAEJ3Ur1q/qh69LBbBU0mKUI8FLx4r2g9oQ9lsN+3SzSbsTsQS+hO8eFDEq7/Im//GbZuDtj4YeLw3w8y8IJHCoOt+O4WNza3tneJuaW//4PCofHzSNnGqGW+xWMa6G1DDpVC8hQIl7yaa0yiQvBNMbuZ+55FrI2L1gNOE+xEdKREKRtFK90+D+qBccavuAmSdeDmpQI7moPzVH8YsjbhCJqkxPc9N0M+oRsEkn5X6qeEJZRM64j1LFY248bPFqTNyYZUhCWNtSyFZqL8nMhoZM40C2xlRHJtVby7+5/VSDK/9TKgkRa7YclGYSoIxmf9NhkJzhnJqCWVa2FsJG1NNGdp0SjYEb/XlddKuVT236t1dVRq1PI4inME5XIIHdWjALTShBQxG8Ayv8OZI58V5dz6WrQUnnzmFP3A+fwAP+I2W</latexit>
z7(i) <latexit sha1_base64="6mtN1MRcsODxEqKdp+7t2960S2g=">AAAB8HicbVBNSwMxEJ2tX7V+tOrRS7AI9VJ2pVCPBS8eK9gPadeSTbNtaJJdkqxQl/4KLx4U8erP8ea/MW33oK0PBh7vzTAzL4g508Z1v53cxubW9k5+t7C3f3BYLB0dt3WUKEJbJOKR6gZYU84kbRlmOO3GimIRcNoJJtdzv/NIlWaRvDPTmPoCjyQLGcHGSvdPg/pDWmEXs0Gp7FbdBdA68TJShgzNQemrP4xIIqg0hGOte54bGz/FyjDC6azQTzSNMZngEe1ZKrGg2k8XB8/QuVWGKIyULWnQQv09kWKh9VQEtlNgM9ar3lz8z+slJrzyUybjxFBJlovChCMTofn3aMgUJYZPLcFEMXsrImOsMDE2o4INwVt9eZ20L6ueW/Vua+VGLYsjD6dwBhXwoA4NuIEmtICAgGd4hTdHOS/Ou/OxbM052cwJ/IHz+QMdPY/m</latexit>

Filter <latexit sha1_base64="t/B8MdoeaDPtk2WxPCtkm9aAmV4=">AAAB8XicbVBNS8NAEJ3Ur1q/qh69LLaCp5IUQY8FQTxWsK3YhrLZTtqlm03Y3Qgl9F948aCIV/+NN/+N2zYHbX0w8Hhvhpl5QSK4Nq777RTW1jc2t4rbpZ3dvf2D8uFRW8epYthisYjVQ0A1Ci6xZbgR+JAopFEgsBOMr2d+5wmV5rG8N5ME/YgOJQ85o8ZKjzdcGFSk2qn2yxW35s5BVomXkwrkaPbLX71BzNIIpWGCat313MT4GVWGM4HTUi/VmFA2pkPsWipphNrP5hdPyZlVBiSMlS1pyFz9PZHRSOtJFNjOiJqRXvZm4n9eNzXhlZ9xmaQGJVssClNBTExm75MBV8iMmFhCmeL2VsJGVFFmY9AlG4K3/PIqaddrnlvz7uqVxkUeRxFO4BTOwYNLaMAtNKEFDCQ8wyu8Odp5cd6dj0VrwclnjuEPnM8fTGeP8w==</latexit>

W

x8 <latexit sha1_base64="LxQhhprzm81NnfnhLVIHmSPbSpM=">AAAB6nicbVBNS8NAEJ3Ur1q/qh69LBbBU0mKYI8FLx4r2g9oQ9lsN+3SzSbsTsQS+hO8eFDEq7/Im//GbZuDtj4YeLw3w8y8IJHCoOt+O4WNza3tneJuaW//4PCofHzSNnGqGW+xWMa6G1DDpVC8hQIl7yaa0yiQvBNMbuZ+55FrI2L1gNOE+xEdKREKRtFK90+D+qBccavuAmSdeDmpQI7moPzVH8YsjbhCJqkxPc9N0M+oRsEkn5X6qeEJZRM64j1LFY248bPFqTNyYZUhCWNtSyFZqL8nMhoZM40C2xlRHJtVby7+5/VSDOt+JlSSIldsuShMJcGYzP8mQ6E5Qzm1hDIt7K2EjammDG06JRuCt/ryOmnXqp5b9e6uKo1aHkcRzuAcLsGDa2jALTShBQxG8Ayv8OZI58V5dz6WrQUnnzmFP3A+fwARfI2X</latexit>

Injected
<latexit sha1_base64="GZi1cn1EK93iBvUDCBO84RA5aM0=">AAAB9XicbVDLSgMxFL1TX7W+qi7dBIvgqswUQZcFN3ZXwT6grSWTuW1jM5khySil9D/cuFDErf/izr8x085CWw8EDufcS+45fiy4Nq777eTW1jc2t/LbhZ3dvf2D4uFRU0eJYthgkYhU26caBZfYMNwIbMcKaegLbPnj69RvPaLSPJJ3ZhJjL6RDyQecUWOl+5p8QGYwIDUZJ6ZfLLlldw6ySryMlCBDvV/86gYRS0KUhgmqdcdzY9ObUmU4EzgrdBONMWVjOsSOpZKGqHvT+dUzcmaVgAwiZZ80ZK7+3pjSUOtJ6NvJkJqRXvZS8T+vk5jBVW/K00Qo2eKjQSKIiUhaAQm4sqHFxBLKFLe3EjaiitoilC7YErzlyKukWSl7btm7rZSqF1kdeTiBUzgHDy6hCjdQhwYwUPAMr/DmPDkvzrvzsRjNOdnOMfyB8/kDRJeSTA==</latexit>

Input

z8(i) <latexit sha1_base64="T5ySP4lJybM6vZ2YZAHJPO65/Nw=">AAAB8HicbVBNSwMxEJ2tX7V+VT16CRahXsquFNpjwYvHCvZD2rVk02wbmmSXJCvUpb/CiwdFvPpzvPlvTNs9aOuDgcd7M8zMC2LOtHHdbye3sbm1vZPfLeztHxweFY9P2jpKFKEtEvFIdQOsKWeStgwznHZjRbEIOO0Ek+u533mkSrNI3plpTH2BR5KFjGBjpfunQf0hLbPL2aBYcivuAmideBkpQYbmoPjVH0YkEVQawrHWPc+NjZ9iZRjhdFboJ5rGmEzwiPYslVhQ7aeLg2fowipDFEbKljRoof6eSLHQeioC2ymwGetVby7+5/USE9b9lMk4MVSS5aIw4chEaP49GjJFieFTSzBRzN6KyBgrTIzNqGBD8FZfXiftq4rnVrzbaqlRzeLIwxmcQxk8qEEDbqAJLSAg4Ble4c1Rzovz7nwsW3NONnMKf+B8/gAex4/n</latexit>

Hidden <latexit sha1_base64="bIWS/9lY0PwmUGvF0F1xjxMKS3k=">AAAB83icbVBNS8NAEN3Ur1q/qh69LBbBU0mKoMeClx4rmLbQhrLZTNqlm03YnQil9G948aCIV/+MN/+N2zYHbX0w8Hhvhpl5YSaFQdf9dkpb2zu7e+X9ysHh0fFJ9fSsY9Jcc/B5KlPdC5kBKRT4KFBCL9PAklBCN5zcL/zuE2gjUvWI0wyChI2UiAVnaKVBS0QRKOorgWZYrbl1dwm6SbyC1EiB9rD6NYhSniegkEtmTN9zMwxmTKPgEuaVQW4gY3zCRtC3VLEETDBb3jynV1aJaJxqWwrpUv09MWOJMdMktJ0Jw7FZ9xbif14/x/gumAmV5QiKrxbFuaSY0kUANBIaOMqpJYxrYW+lfMw042hjqtgQvPWXN0mnUffcuvfQqDVvijjK5IJckmvikVvSJC3SJj7hJCPP5JW8Obnz4rw7H6vWklPMnJM/cD5/AKpikWE=</latexit>

Units

Layer <latexit sha1_base64="Omx146Fv69pKxCTzcUXdA+GkZwY=">AAACAnicbVBNS8NAEN3Ur1q/op7Ey2IrCEJJelE8Fbx48FDBfkAbyma7aZdusmF3ooRQvPhXvHhQxKu/wpv/xm2bg1YfDDzem2Fmnh8LrsFxvqzC0vLK6lpxvbSxubW9Y+/utbRMFGVNKoVUHZ9oJnjEmsBBsE6sGAl9wdr++HLqt++Y0lxGt5DGzAvJMOIBpwSM1LcPrknKFK5w3FN8OAKilLzH/NSt9O2yU3VmwH+Jm5MyytHo25+9gaRJyCKggmjddZ0YvIwo4FSwSamXaBYTOiZD1jU0IiHTXjZ7YYKPjTLAgVSmIsAz9edERkKt09A3nSGBkV70puJ/XjeB4NzLeBQnwCI6XxQkAoPE0zzwgCtGQaSGEKq4uRXTEVGEgkmtZEJwF1/+S1q1qutU3ZtauX6Rx1FEh+gInSAXnaE6ukIN1EQUPaAn9IJerUfr2Xqz3uetBSuf2Ue/YH18A14BlhM=</latexit>

(a) TrellisNet at an atomic level

(b) TrellisNet on a sequence of units

Figure 1: The interlayer transformation of TrellisNet, at an atomic level (time steps t and t + 1, layers i and i + 1) and on a longer sequence (time steps 1 to 8, layers i and i + 1).

2. A pre-activation output z^t(+i+11)  Rr is produced by a feed-forward linear transformation:

z^t(+i+11) = W1

xt zt(i)

+ W2

xt+1 zt(+i)1

(2)

where W1, W2  Rr×(p+q) are weights, and r is the size of the pre-activation output z^t(+i+11). (Here and throughout the paper, all linear transformations can include additive biases. We omit these for clarity.)

3. The output zt(+i+11) is produced by a nonlinear activation function f : Rr × Rq  Rq applied to the pre-activation output z^t(+i+11) and the output zt(i) from the previous layer. More formally,
zt(+i+11) = f z^t(+i+11), zt(i) .

A full trellis network can be built by tiling this elementary procedure across time and depth. Given an input sequence x1:T , we apply the same production procedure across all time steps and all layers, using the same weights. The transformation is the same for all elements in the temporal dimen-
sion and in the depth dimension. This is illustrated in Figure 1b. Note that since we inject the
same input sequence at every layer of the TrellisNet, we can precompute the linear transformation x~t+1 = W1xxt + W2xxt+1 for all layers i. This identical linear combination of the input can then be added in each layer i to the appropriate linear combination of the hidden units, W1zzt(i) + W2zzt(+i)1, where Wjx  Rr×p, Wjz  Rr×q.

Now observe that in each level of the network, we are in effect performing a 1D convolution over the

hidden units z1(i:T) . Formally, with W

The output of this convolution is then  Rr×q as the kernel weight matrix, the

passed through computation in

the activation function f . layer i can be summarized

as follows (Figure 1b):

z^1(i:T+1) = Conv1D z1(i:T) ; W + x~1:T ,

z1(i:T+1) = f z^1(i:T+1), z1(i:T) -1 .

(3)

The resulting network operates in feed-forward fashion, with deeper elements having progressively larger receptive fields. There are, however, important differences from typical (temporal) convolutional networks. Notably, the filter matrix is shared across all layers. That is, the weights are tied not only across time but also across depth. (Vogel & Pock (2017) have previously tied weights across depth in image processing.) Another difference is that the transformed input sequence x~1:T is directly injected into each hidden layer. These differences and their importance will be analyzed further in Section 4.
The activation function f in Equation (3) can be any nonlinearity that processes the pre-activation output z^1(i:T+1)and the output from the previous layer z1(i:T) -1. We will later describe an activation function based on the LSTM cell. The rationale for its use will become clearer in light of the analysis presented in the next section.

3

Under review as a conference paper at ICLR 2019

4 TRELLISNET, TCN, AND RNN

In this section we analyze the relationships between trellis networks, convolutional networks, and recurrent networks. In particular, we show that trellis networks can serve as a bridge between convolutional and recurrent networks. On the one hand, TrellisNet is a special form of TCN; this has already been clear in Section 3 and will be discussed further in Section 4.1. On the other hand, any truncated RNN can be represented as a TrellisNet with special structure in the interlayer transformations; this will be the subject of Section 4.2. These connections allow TrellisNet to harness architectural elements and regularization techniques from both TCNs and RNNs; this will be summarized in Section 4.3.

4.1 TRELLISNET AND TCN
In essence, TrellisNet is a special kind of temporal convolutional network. TCNs have two distinctive characteristics: 1) causal convolution in each layer to satisfy the causality constraint and 2) deep stacking of layers to increase the effective history length (i.e. receptive field). Trellis networks have both of these characteristics. The basic model presented in Section 3 can easily be elaborated with larger kernel sizes, dilated convolutions, and other architectural elements used in TCNs; some of these are reviewed further in Section 4.3.
However, TrellisNet is not a general TCN. As mentioned in Section 3, two important differences are: 1) the weights are tied across layers and 2) the linearly transformed input x~1:T is injected into each layer. Weight tying can be viewed as a form of regularization that can stabilize training, support generalization, and significantly reduce the size of the model. Input injection mixes deep features with the original sequence. These structural characteristics will be further illuminated by the connection between trellis networks and recurrent networks, presented next.

4.2 TRELLISNET AND RNN

Recurrent networks appear fundamentally different from convolutional networks. Instead of operat-
ing on all elements of a sequence in parallel in each layer, an RNN processes one input element at a time and unrolls in the time dimension. Given a non-linearity g (which could be a sigmoid or a more elaborate cell), we can summarize the transformations in an L-layer RNN at time-step t as follows:

ht(i) = g Wh(ix)ht(i-1) + Wh(ih)ht(-i)1

for 1  i  L,

ht(0) = xt.

(4)

Despite the apparent differences, we will now show that any RNN unrolled to a finite length is equivalent to a TrellisNet with special sparsity structure in the kernel matrix W . We begin by formally defining the notion of a truncated (i.e. finite-horizon) RNN.
Definition 1. Given an RNN , a corresponding M-truncated RNN M , applied to the sequence x1:T , produces at time step t the output yt by applying  to the sequence xt-M+1:t (here x<0 = 0).

Theorem 1. Let M be an M -truncated RNN with L layers and hidden unit dimensionality d. Then there exists an equivalent TrellisNet  with depth (M + L - 1) and layer width (i.e. number of channels in each hidden layer) Ld. That is, for any x1:T ,  (x1:T ) = M (x1:T ).

Theorem 1 states that any M -truncated RNN can be represented as a TrellisNet. How severe of a restriction is M -truncation? Note that M -truncation is intimately related to truncated backpropagation-through-time (BPTT), used pervasively in training recurrent networks on long sequences. While RNNs can in principle retain unlimited history, there is both empirical and theoretical evidence that the memory horizon of RNNs is bounded (Bai et al., 2018; Khandelwal et al., 2018; Miller & Hardt, 2018). Furthermore, if desired, TrellisNets can recover exactly a common method of applying RNNs to long sequences ­ hidden state repackaging, i.e. copying the hidden state across subsequences. This is accomplished using an analogous form of hidden state repackaging, detailed in Appendix B.
Proof of Theorem 1. We define ht,t  Rd as the RNN hidden state at time t but when the history started at time t . Note that in typical RNNs, we assume the history starts at time t = 1, so h(t,i1) = ht(i). When t > t, we assume ht,t = 0 (i.e., no history information if the clock starts in the future).

4

Under review as a conference paper at ICLR 2019

By assumption, M is an RNN defined by the following parameters: {Wh(ix), Wh(ih), g, M }, where Wh(ih)  Rw×d for all i, Wh(1x)  Rw×p, and Wh(ix)  Rw×d for all i = 2, . . . , L are the weight matrices at each layer (w is the dimension of pre-activation output). We now construct a TrellisNet  according to the exact definition in Section 3, with parameters {W1, W2, f }, where

 0

Wh(1h)

0

...

 0

 Wh(1x)

0

...

0

 0

W1



=

 





0 ...

0 ...

Wh(2h) . . . ... . . .

0 ...



 

,

W2

=

 





0 ...

Wh(2x) . . . ... . . .

0 ...

0

 

...

  

00

0 . . . Wh(Lh)

0 0 . . . Wh(Lx) 0

(5)

and where W1, W2  RLw×(p+Ld). Meanwhile, we define non-linearity f by f (, ) = g() (i.e., applying g only on the first entry).

Let t  [T ] be arbitrary and fixed. We now claim that the hidden unit at time t and layer j of TrellisNet  can be expressed in terms of hidden units in RNN M (note: j is the layer in  , while i in ht(,it) is the layer index in M ):

zt(j) = h(t,1t)-j+1 ht(,2t)-j+2 . . . h(t,Lt-) j+L  RLd

(6)

We prove this claim by induction on j. As a base case, consider j = 0; i.e., the input level of  . Since ht,t = 0 when t > t, we have that zj(0) = [0 0 . . . 0] indeed (recall that in the input level
of TrellisNet we initialize zt(0) = 0). For the inductive step, suppose Eq. (6) holds for level j, and consider level j + 1. By the feed-forward transformation of TrellisNet defined in Eq. (2) and the nonlinearity f we defined above, we have that:

z^t(j+1) = W1

xt-1 zt(-j)1

+ W2

xt zt(j)

(7)

 0

=

    

0 ...

Wh(1h) 0 ...

0 Wh(2h)
...

...
... ...

0
0 ...

 
 
 
 
 


xt-1 h(t-1)1,t-j
...

  Wh(1x)







+

 









0 ...

0 Wh(2x)
...

...
... ...

0
0 ...

 0

xt



0

 



ht(,1t)-j

+1



...

 
 
 

...

  

00

0 . . . Wh(Lh)

h(t-L)1,t-j+L-1

0

0 . . . Wh(Lx) 0

h(t,Lt-) j+L

(8)



Wh(1h)h(t-1)1,t-j + Wh(1x)xt



Conv1D([h(t-1)1,t-j , xt]; W (1))



=

 

...

 

=

 

...

 

 



Wh(Lh)ht(-L)1,t-j+L-1 + Wh(Lx )ht(,Lt--j1+) L-1

Conv1D([h(t-L)1,t-j+L-1, ht(,Lt--j1+) L-1]; W (L))

(9)

= h^t(,1t)-j h^(t,2t)-j+1 . . . h^(t,Lt-) j+L-1

(10)

zt(j+1) = f (z^t(j+1), zt(-j)1) = g(z^t(j+1)) = ht(,1t)-j h(t,2t)-j+1 . . . h(t,Lt-) j+L-1

(11)

where W (i) = (Wh(ih), Wh(ix)). Therefore, by induction, we have shown that Eq. (6) holds for all j  0. If TrellisNet  has M + L - 1 layers, then at the final layer we would eventually have zt(M+L-1) = [. . . . . . ht(,Lt)+1-M ] . Since M is an L-layer M -truncated RNN, this (taking the last d channels) is exactly the output of M at time t.

In other words, we have shown that M is equivalent to a TrellisNet with a sparse kernel matrix. This completes the proof.

Note that the convolutions in the TrellisNet  constructed in Theorem 1 are sparse, as shown in Eq. (5). They are related to group convolutions (Krizhevsky et al., 2012), but have an unusual form because group k at time t is convolved with group k - 1 at time t + 1. We refer to these as mixed group convolutions.
For didactic purposes, we recap and illustrate the construction in the case of a 2-layer RNN. The key challenge is that a na¨ive unrolling of the RNN into a feed-forward network does not produce a convolutional network, since the linear transformation weights are not constant across a layer. The solution, illustrated in Figure 2a, is to organize each hidden unit into groups of channels, such

5

Under review as a conference paper at ICLR 2019

Output <latexit sha1_base64="cFEyDXjwdwHG/SZM/GY/02ncwMw=">AAAB7XicbVBNTwIxFHyLX4hfqEcvjcTEE9nlIkcSL97ERMAENqRbulDptpv2rQkh/AcvHjTGq//Hm//GAntQcJImk5n30jcTpVJY9P1vr7CxubW9U9wt7e0fHB6Vj0/aVmeG8RbTUpuHiFouheItFCj5Q2o4TSLJO9H4eu53nrixQqt7nKQ8TOhQiVgwik5q32aYZtgvV/yqvwBZJ0FOKpCj2S9/9QaaZQlXyCS1thv4KYZTalAwyWelXmZ5StmYDnnXUUUTbsPp4toZuXDKgMTauKeQLNTfG1OaWDtJIjeZUBzZVW8u/ud1M4zr4VQol4grtvwoziRBTebRyUAYzlBOHKHMCHcrYSNqKENXUMmVEKxGXiftWjXwq8FdrdKo53UU4QzO4RICuIIG3EATWsDgEZ7hFd487b14797HcrTg5Tun8Afe5w/PT487</latexit>

y1 <latexit sha1_base64="peugGuPaqBRykXI6euNtldN8Rpw=">AAAB6nicbVBNS8NAEJ3Ur1q/qh69LBbBU0lEUDwVvHisaD+gDWWz3bRLN5uwOxFC6E/w4kERr/4ib/4bt20O2vpg4PHeDDPzgkQKg6777ZTW1jc2t8rblZ3dvf2D6uFR28SpZrzFYhnrbkANl0LxFgqUvJtoTqNA8k4wuZ35nSeujYjVI2YJ9yM6UiIUjKKVHrKBN6jW3Lo7B1klXkFqUKA5qH71hzFLI66QSWpMz3MT9HOqUTDJp5V+anhC2YSOeM9SRSNu/Hx+6pScWWVIwljbUkjm6u+JnEbGZFFgOyOKY7PszcT/vF6K4bWfC5WkyBVbLApTSTAms7/JUGjOUGaWUKaFvZWwMdWUoU2nYkPwll9eJe2LuufWvfvLWuOmiKMMJ3AK5+DBFTTgDprQAgYjeIZXeHOk8+K8Ox+L1pJTzBzDHzifPwrOjZk=</latexit>

y2 <latexit sha1_base64="eFwq5zrRdmOaDitcLAhIwMZsWx8=">AAAB6nicbVBNS8NAEJ34WetX1aOXxSJ4KkkRFE8FLx4r2g9oQ9lsJ+3SzSbsboQQ+hO8eFDEq7/Im//GbZuDtj4YeLw3w8y8IBFcG9f9dtbWNza3tks75d29/YPDytFxW8epYthisYhVN6AaBZfYMtwI7CYKaRQI7AST25nfeUKleSwfTZagH9GR5CFn1FjpIRvUB5WqW3PnIKvEK0gVCjQHla/+MGZphNIwQbXueW5i/Jwqw5nAabmfakwom9AR9iyVNELt5/NTp+TcKkMSxsqWNGSu/p7IaaR1FgW2M6JmrJe9mfif10tNeO3nXCapQckWi8JUEBOT2d9kyBUyIzJLKFPc3krYmCrKjE2nbEPwll9eJe16zXNr3v1ltXFTxFGCUziDC/DgChpwB01oAYMRPMMrvDnCeXHenY9F65pTzJzAHzifPwxSjZo=</latexit>

y3 <latexit sha1_base64="2oOh5aCxItxgY+GGybajIYrS5mc=">AAAB6nicbVBNS8NAEJ3Ur1q/qh69LBbBU0lUUDwVvHisaG2hDWWz3bRLN5uwOxFC6E/w4kERr/4ib/4bt20O2vpg4PHeDDPzgkQKg6777ZRWVtfWN8qbla3tnd296v7Bo4lTzXiLxTLWnYAaLoXiLRQoeSfRnEaB5O1gfDP1209cGxGrB8wS7kd0qEQoGEUr3Wf983615tbdGcgy8QpSgwLNfvWrN4hZGnGFTFJjup6boJ9TjYJJPqn0UsMTysZ0yLuWKhpx4+ezUyfkxCoDEsbalkIyU39P5DQyJosC2xlRHJlFbyr+53VTDK/8XKgkRa7YfFGYSoIxmf5NBkJzhjKzhDIt7K2EjaimDG06FRuCt/jyMnk8q3tu3bu7qDWuizjKcATHcAoeXEIDbqEJLWAwhGd4hTdHOi/Ou/Mxby05xcwh/IHz+QMN1o2b</latexit>

y4 <latexit sha1_base64="gX4hlq9vLiCWk4bmwe/c8+XXqhQ=">AAAB6nicbVBNS8NAEJ3Ur1q/qh69LBbBU0lEUDwVvHisaD+gDWWznbRLN5uwuxFC6E/w4kERr/4ib/4bt20O2vpg4PHeDDPzgkRwbVz32ymtrW9sbpW3Kzu7e/sH1cOjto5TxbDFYhGrbkA1Ci6xZbgR2E0U0igQ2AkmtzO/84RK81g+mixBP6IjyUPOqLHSQza4HFRrbt2dg6wSryA1KNAcVL/6w5ilEUrDBNW657mJ8XOqDGcCp5V+qjGhbEJH2LNU0gi1n89PnZIzqwxJGCtb0pC5+nsip5HWWRTYzoiasV72ZuJ/Xi814bWfc5mkBiVbLApTQUxMZn+TIVfIjMgsoUxxeythY6ooMzadig3BW355lbQv6p5b9+4va42bIo4ynMApnIMHV9CAO2hCCxiM4Ble4c0Rzovz7nwsWktOMXMMf+B8/gAPWo2c</latexit>

y5 <latexit sha1_base64="GeK5g0wUUTFJTmw4YyIxu9hOJ0k=">AAAB6nicbVBNS8NAEJ3Ur1q/qh69LBbBU0lEUTwVvHisaG2hDWWz3bRLN5uwOxFC6E/w4kERr/4ib/4bt20O2vpg4PHeDDPzgkQKg6777ZRWVtfWN8qbla3tnd296v7Bo4lTzXiLxTLWnYAaLoXiLRQoeSfRnEaB5O1gfDP1209cGxGrB8wS7kd0qEQoGEUr3Wf9i3615tbdGcgy8QpSgwLNfvWrN4hZGnGFTFJjup6boJ9TjYJJPqn0UsMTysZ0yLuWKhpx4+ezUyfkxCoDEsbalkIyU39P5DQyJosC2xlRHJlFbyr+53VTDK/8XKgkRa7YfFGYSoIxmf5NBkJzhjKzhDIt7K2EjaimDG06FRuCt/jyMnk8q3tu3bs7rzWuizjKcATHcAoeXEIDbqEJLWAwhGd4hTdHOi/Ou/Mxby05xcwh/IHz+QMQ3o2d</latexit> Grey means unimportant here <latexit sha1_base64="bYQi+Q0xLtPUYZqCtTkbkPLF5+A=">AAACBHicbVA9SwNBEN3zM8avU8s0i0GwCndpFKuAhZYRzAckR9jbzCVLdveO3T3hOFLY+FdsLBSx9UfY+W/cJFdo4oOBx3szzMwLE8608bxvZ219Y3Nru7RT3t3bPzh0j47bOk4VhRaNeay6IdHAmYSWYYZDN1FARMihE06uZ37nAZRmsbw3WQKBICPJIkaJsdLArdwoyLAAIjVOJRNJrAyRBo9BwcCtejVvDrxK/IJUUYHmwP3qD2OaCpCGcqJ1z/cSE+REGUY5TMv9VENC6ISMoGepJAJ0kM+fmOIzqwxxFCtb9oC5+nsiJ0LrTIS2UxAz1sveTPzP66UmugxyJpPUgKSLRVHKsYnxLBE8ZAqo4ZklhCpmb8V0TBShxuZWtiH4yy+vkna95ns1/65ebVwVcZRQBZ2ic+SjC9RAt6iJWoiiR/SMXtGb8+S8OO/Ox6J1zSlmTtAfOJ8/wHmYHw==</latexit>

h4(1,1) <latexit sha1_base64="TmSuGQwwU2b3SlNQsHGvrAzQSjs=">AAAB9HicbVBNSwMxEJ2tX7V+VT16CRahgpSNFBRPBS8eK9gPaNeSTdM2NJtdk2yhLPs7vHhQxKs/xpv/xrTdg7Y+GHi8N8PMPD8SXBvX/XZya+sbm1v57cLO7t7+QfHwqKnDWFHWoKEIVdsnmgkuWcNwI1g7UowEvmAtf3w781sTpjQP5YOZRswLyFDyAafEWMkb9ZLqBU4fkzI+T3vFkltx50CrBGekBBnqveJXtx/SOGDSUEG07mA3Ml5ClOFUsLTQjTWLCB2TIetYKknAtJfMj07RmVX6aBAqW9Kgufp7IiGB1tPAt50BMSO97M3E/7xObAbXXsJlFBsm6WLRIBbIhGiWAOpzxagRU0sIVdzeiuiIKEKNzalgQ8DLL6+S5mUFuxV8Xy3VbrI48nACp1AGDFdQgzuoQwMoPMEzvMKbM3FenHfnY9Gac7KZY/gD5/MHUD6RHA==</latexit>

h(42,2) <latexit sha1_base64="OOlZ6cogwGom2ewBQMcQe91MNfE=">AAAB9HicbVBNS8NAEJ3Ur1q/qh69LBahgpSkFOyx4MVjBfsBbSyb7aZdutmku5tCCfkdXjwo4tUf481/47bNQVsfDDzem2FmnhdxprRtf1u5re2d3b38fuHg8Oj4pHh61lZhLAltkZCHsuthRTkTtKWZ5rQbSYoDj9OON7lb+J0ZlYqF4lHPI+oGeCSYzwjWRnLHg6R2U02fknL1Oh0US3bFXgJtEicjJcjQHBS/+sOQxAEVmnCsVM+xI+0mWGpGOE0L/VjRCJMJHtGeoQIHVLnJ8ugUXRlliPxQmhIaLdXfEwkOlJoHnukMsB6rdW8h/uf1Yu3X3YSJKNZUkNUiP+ZIh2iRABoySYnmc0MwkczcisgYS0y0yalgQnDWX94k7WrFsSvOQ63UqGdx5OECLqEMDtxCA+6hCS0gMIVneIU3a2a9WO/Wx6o1Z2Uz5/AH1ucPUrWRHA==</latexit>

x3 <latexit sha1_base64="FsFp5SDaWDjvwmxpdt2PBoWBq3Q=">AAAB6nicbVBNS8NAEJ3Ur1q/qh69LBbBU0lUUDwVvHisaD+gDWWz3bRLN5uwOxFL6E/w4kERr/4ib/4bt20O2vpg4PHeDDPzgkQKg6777RRWVtfWN4qbpa3tnd298v5B08SpZrzBYhnrdkANl0LxBgqUvJ1oTqNA8lYwupn6rUeujYjVA44T7kd0oEQoGEUr3T/1znvlilt1ZyDLxMtJBXLUe+Wvbj9macQVMkmN6Xhugn5GNQom+aTUTQ1PKBvRAe9YqmjEjZ/NTp2QE6v0SRhrWwrJTP09kdHImHEU2M6I4tAselPxP6+TYnjlZ0IlKXLF5ovCVBKMyfRv0heaM5RjSyjTwt5K2JBqytCmU7IheIsvL5PmWdVzq97dRaV2ncdRhCM4hlPw4BJqcAt1aACDATzDK7w50nlx3p2PeWvByWcO4Q+czx8MUI2a</latexit>
x3 <latexit sha1_base64="FsFp5SDaWDjvwmxpdt2PBoWBq3Q=">AAAB6nicbVBNS8NAEJ3Ur1q/qh69LBbBU0lUUDwVvHisaD+gDWWz3bRLN5uwOxFL6E/w4kERr/4ib/4bt20O2vpg4PHeDDPzgkQKg6777RRWVtfWN4qbpa3tnd298v5B08SpZrzBYhnrdkANl0LxBgqUvJ1oTqNA8lYwupn6rUeujYjVA44T7kd0oEQoGEUr3T/1znvlilt1ZyDLxMtJBXLUe+Wvbj9macQVMkmN6Xhugn5GNQom+aTUTQ1PKBvRAe9YqmjEjZ/NTp2QE6v0SRhrWwrJTP09kdHImHEU2M6I4tAselPxP6+TYnjlZ0IlKXLF5ovCVBKMyfRv0heaM5RjSyjTwt5K2JBqytCmU7IheIsvL5PmWdVzq97dRaV2ncdRhCM4hlPw4BJqcAt1aACDATzDK7w50nlx3p2PeWvByWcO4Q+czx8MUI2a</latexit>

y3 <latexit sha1_base64="2oOh5aCxItxgY+GGybajIYrS5mc=">AAAB6nicbVBNS8NAEJ3Ur1q/qh69LBbBU0lUUDwVvHisaG2hDWWz3bRLN5uwOxFC6E/w4kERr/4ib/4bt20O2vpg4PHeDDPzgkQKg6777ZRWVtfWN8qbla3tnd296v7Bo4lTzXiLxTLWnYAaLoXiLRQoeSfRnEaB5O1gfDP1209cGxGrB8wS7kd0qEQoGEUr3Wf983615tbdGcgy8QpSgwLNfvWrN4hZGnGFTFJjup6boJ9TjYJJPqn0UsMTysZ0yLuWKhpx4+ezUyfkxCoDEsbalkIyU39P5DQyJosC2xlRHJlFbyr+53VTDK/8XKgkRa7YfFGYSoIxmf5NBkJzhjKzhDIt7K2EjaimDG06FRuCt/jyMnk8q3tu3bu7qDWuizjKcATHcAoeXEIDbqEJLWAwhGd4hTdHOi/Ou/Mxby05xcwh/IHz+QMN1o2b</latexit>

h(31,1) <latexit sha1_base64="BSuWFrtzYtL0oXVJvebhtFFULj8=">AAAB9HicbVBNSwMxEJ2tX7V+VT16CRahgpSNCvZY8OKxgv2Adi3ZNG1Ds9k1yRbKsr/DiwdFvPpjvPlvTNs9aOuDgcd7M8zM8yPBtXHdbye3tr6xuZXfLuzs7u0fFA+PmjqMFWUNGopQtX2imeCSNQw3grUjxUjgC9byx7czvzVhSvNQPphpxLyADCUfcEqMlbxRL7m6wOljUsbnaa9YcivuHGiV4IyUIEO9V/zq9kMaB0waKojWHexGxkuIMpwKlha6sWYRoWMyZB1LJQmY9pL50Sk6s0ofDUJlSxo0V39PJCTQehr4tjMgZqSXvZn4n9eJzaDqJVxGsWGSLhYNYoFMiGYJoD5XjBoxtYRQxe2tiI6IItTYnAo2BLz88ippXlawW8H316VaNYsjDydwCmXAcAM1uIM6NIDCEzzDK7w5E+fFeXc+Fq05J5s5hj9wPn8ATheRGQ==</latexit>

h3(2,1) <latexit sha1_base64="zckN78LW7w8EyUSENFbjCUAG0LU=">AAAB9HicbVBNS8NAEJ3Ur1q/qh69LBahgpSkCvZY8OKxgv2ANpbNdtMu3WzS3U2hhPwOLx4U8eqP8ea/cdvmoK0PBh7vzTAzz4s4U9q2v63cxubW9k5+t7C3f3B4VDw+aakwloQ2SchD2fGwopwJ2tRMc9qJJMWBx2nbG9/N/faUSsVC8ahnEXUDPBTMZwRrI7mjfnJ95aRPSbl6mfaLJbtiL4DWiZOREmRo9ItfvUFI4oAKTThWquvYkXYTLDUjnKaFXqxohMkYD2nXUIEDqtxkcXSKLowyQH4oTQmNFurviQQHSs0Cz3QGWI/UqjcX//O6sfZrbsJEFGsqyHKRH3OkQzRPAA2YpETzmSGYSGZuRWSEJSba5FQwITirL6+TVrXi2BXn4aZUr2Vx5OEMzqEMDtxCHe6hAU0gMIFneIU3a2q9WO/Wx7I1Z2Uzp/AH1ucPT52RGg==</latexit>
h3(2,2) <latexit sha1_base64="1DvkMWKABldvZRihPtKnXv75Fk4=">AAAB9HicbVBNS8NAEJ3Ur1q/qh69LBahgpSkCvZY8OKxgv2ANpbNdtMu3WzS3U2hhPwOLx4U8eqP8ea/cdvmoK0PBh7vzTAzz4s4U9q2v63cxubW9k5+t7C3f3B4VDw+aakwloQ2SchD2fGwopwJ2tRMc9qJJMWBx2nbG9/N/faUSsVC8ahnEXUDPBTMZwRrI7mjfnJ9VU2fknL1Mu0XS3bFXgCtEycjJcjQ6Be/eoOQxAEVmnCsVNexI+0mWGpGOE0LvVjRCJMxHtKuoQIHVLnJ4ugUXRhlgPxQmhIaLdTfEwkOlJoFnukMsB6pVW8u/ud1Y+3X3ISJKNZUkOUiP+ZIh2ieABowSYnmM0MwkczcisgIS0y0yalgQnBWX14nrWrFsSvOw02pXsviyMMZnEMZHLiFOtxDA5pAYALP8Apv1tR6sd6tj2VrzspmTuEPrM8fUSiRGw==</latexit>

x4 <latexit sha1_base64="S4f+bq/VrUW0JYj64fGswbvelkw=">AAAB6nicbVBNS8NAEJ3Ur1q/qh69LBbBU0mkoHgqePFY0X5AG8pmO2mXbjZhdyOW0J/gxYMiXv1F3vw3btsctPXBwOO9GWbmBYng2rjut1NYW9/Y3Cpul3Z29/YPyodHLR2nimGTxSJWnYBqFFxi03AjsJMopFEgsB2Mb2Z++xGV5rF8MJME/YgOJQ85o8ZK90/9Wr9ccavuHGSVeDmpQI5Gv/zVG8QsjVAaJqjWXc9NjJ9RZTgTOC31Uo0JZWM6xK6lkkao/Wx+6pScWWVAwljZkobM1d8TGY20nkSB7YyoGellbyb+53VTE175GZdJalCyxaIwFcTEZPY3GXCFzIiJJZQpbm8lbEQVZcamU7IheMsvr5LWRdVzq95drVK/zuMowgmcwjl4cAl1uIUGNIHBEJ7hFd4c4bw4787HorXg5DPH8AfO5w8N1I2b</latexit>
x4 <latexit sha1_base64="S4f+bq/VrUW0JYj64fGswbvelkw=">AAAB6nicbVBNS8NAEJ3Ur1q/qh69LBbBU0mkoHgqePFY0X5AG8pmO2mXbjZhdyOW0J/gxYMiXv1F3vw3btsctPXBwOO9GWbmBYng2rjut1NYW9/Y3Cpul3Z29/YPyodHLR2nimGTxSJWnYBqFFxi03AjsJMopFEgsB2Mb2Z++xGV5rF8MJME/YgOJQ85o8ZK90/9Wr9ccavuHGSVeDmpQI5Gv/zVG8QsjVAaJqjWXc9NjJ9RZTgTOC31Uo0JZWM6xK6lkkao/Wx+6pScWWVAwljZkobM1d8TGY20nkSB7YyoGellbyb+53VTE175GZdJalCyxaIwFcTEZPY3GXCFzIiJJZQpbm8lbEQVZcamU7IheMsvr5LWRdVzq95drVK/zuMowgmcwjl4cAl1uIUGNIHBEJ7hFd4c4bw4787HorXg5DPH8AfO5w8N1I2b</latexit>

y4 <latexit sha1_base64="gX4hlq9vLiCWk4bmwe/c8+XXqhQ=">AAAB6nicbVBNS8NAEJ3Ur1q/qh69LBbBU0lEUDwVvHisaD+gDWWznbRLN5uwuxFC6E/w4kERr/4ib/4bt20O2vpg4PHeDDPzgkRwbVz32ymtrW9sbpW3Kzu7e/sH1cOjto5TxbDFYhGrbkA1Ci6xZbgR2E0U0igQ2AkmtzO/84RK81g+mixBP6IjyUPOqLHSQza4HFRrbt2dg6wSryA1KNAcVL/6w5ilEUrDBNW657mJ8XOqDGcCp5V+qjGhbEJH2LNU0gi1n89PnZIzqwxJGCtb0pC5+nsip5HWWRTYzoiasV72ZuJ/Xi814bWfc5mkBiVbLApTQUxMZn+TIVfIjMgsoUxxeythY6ooMzadig3BW355lbQv6p5b9+4va42bIo4ynMApnIMHV9CAO2hCCxiM4Ble4c0Rzovz7nwsWktOMXMMf+B8/gAPWo2c</latexit>

Output <latexit sha1_base64="cFEyDXjwdwHG/SZM/GY/02ncwMw=">AAAB7XicbVBNTwIxFHyLX4hfqEcvjcTEE9nlIkcSL97ERMAENqRbulDptpv2rQkh/AcvHjTGq//Hm//GAntQcJImk5n30jcTpVJY9P1vr7CxubW9U9wt7e0fHB6Vj0/aVmeG8RbTUpuHiFouheItFCj5Q2o4TSLJO9H4eu53nrixQqt7nKQ8TOhQiVgwik5q32aYZtgvV/yqvwBZJ0FOKpCj2S9/9QaaZQlXyCS1thv4KYZTalAwyWelXmZ5StmYDnnXUUUTbsPp4toZuXDKgMTauKeQLNTfG1OaWDtJIjeZUBzZVW8u/ud1M4zr4VQol4grtvwoziRBTebRyUAYzlBOHKHMCHcrYSNqKENXUMmVEKxGXiftWjXwq8FdrdKo53UU4QzO4RICuIIG3EATWsDgEZ7hFd487b14797HcrTg5Tun8Afe5w/PT487</latexit>

h4(1,2) <latexit sha1_base64="vVMM6NQmebZLbtMrmVQFlCn9YfI=">AAAB9HicbVBNS8NAEJ3Ur1q/qh69LBahgpSkFOyx4MVjBfsBbSyb7aZdutmku5tCCfkdXjwo4tUf481/47bNQVsfDDzem2FmnhdxprRtf1u5re2d3b38fuHg8Oj4pHh61lZhLAltkZCHsuthRTkTtKWZ5rQbSYoDj9OON7lb+J0ZlYqF4lHPI+oGeCSYzwjWRnLHg6R2U02fkrJznQ6KJbtiL4E2iZOREmRoDopf/WFI4oAKTThWqufYkXYTLDUjnKaFfqxohMkEj2jPUIEDqtxkeXSKrowyRH4oTQmNlurviQQHSs0Dz3QGWI/VurcQ//N6sfbrbsJEFGsqyGqRH3OkQ7RIAA2ZpETzuSGYSGZuRWSMJSba5FQwITjrL2+SdrXi2BXnoVZq1LM48nABl1AGB26hAffQhBYQmMIzvMKbNbNerHfrY9Was7KZc/gD6/MHUS+RGw==</latexit>

h4(2,2) <latexit sha1_base64="OOlZ6cogwGom2ewBQMcQe91MNfE=">AAAB9HicbVBNS8NAEJ3Ur1q/qh69LBahgpSkFOyx4MVjBfsBbSyb7aZdutmku5tCCfkdXjwo4tUf481/47bNQVsfDDzem2FmnhdxprRtf1u5re2d3b38fuHg8Oj4pHh61lZhLAltkZCHsuthRTkTtKWZ5rQbSYoDj9OON7lb+J0ZlYqF4lHPI+oGeCSYzwjWRnLHg6R2U02fknL1Oh0US3bFXgJtEicjJcjQHBS/+sOQxAEVmnCsVM+xI+0mWGpGOE0L/VjRCJMJHtGeoQIHVLnJ8ugUXRlliPxQmhIaLdXfEwkOlJoHnukMsB6rdW8h/uf1Yu3X3YSJKNZUkNUiP+ZIh2iRABoySYnmc0MwkczcisgYS0y0yalgQnDWX94k7WrFsSvOQ63UqGdx5OECLqEMDtxCA+6hCS0gMIVneIU3a2a9WO/Wx6o1Z2Uz5/AH1ucPUrWRHA==</latexit> h4(2,3) <latexit sha1_base64="dGYkbyf0/PJ/5VHaXsLOHckNvsY=">AAAB9HicbVBNS8NAEJ3Ur1q/qh69LBahgpSkFuyx4MVjBfsBbSyb7aZdutnE3U2hhPwOLx4U8eqP8ea/cdvmoK0PBh7vzTAzz4s4U9q2v63cxubW9k5+t7C3f3B4VDw+aaswloS2SMhD2fWwopwJ2tJMc9qNJMWBx2nHm9zO/c6USsVC8aBnEXUDPBLMZwRrI7njQVK7uk4fk3L1Mh0US3bFXgCtEycjJcjQHBS/+sOQxAEVmnCsVM+xI+0mWGpGOE0L/VjRCJMJHtGeoQIHVLnJ4ugUXRhliPxQmhIaLdTfEwkOlJoFnukMsB6rVW8u/uf1Yu3X3YSJKNZUkOUiP+ZIh2ieABoySYnmM0MwkczcisgYS0y0yalgQnBWX14n7WrFsSvOfa3UqGdx5OEMzqEMDtxAA+6gCS0g8ATP8Apv1tR6sd6tj2VrzspmTuEPrM8fVECRHQ==</latexit>

x4 <latexit sha1_base64="S4f+bq/VrUW0JYj64fGswbvelkw=">AAAB6nicbVBNS8NAEJ3Ur1q/qh69LBbBU0mkoHgqePFY0X5AG8pmO2mXbjZhdyOW0J/gxYMiXv1F3vw3btsctPXBwOO9GWbmBYng2rjut1NYW9/Y3Cpul3Z29/YPyodHLR2nimGTxSJWnYBqFFxi03AjsJMopFEgsB2Mb2Z++xGV5rF8MJME/YgOJQ85o8ZK90/9Wr9ccavuHGSVeDmpQI5Gv/zVG8QsjVAaJqjWXc9NjJ9RZTgTOC31Uo0JZWM6xK6lkkao/Wx+6pScWWVAwljZkobM1d8TGY20nkSB7YyoGellbyb+53VTE175GZdJalCyxaIwFcTEZPY3GXCFzIiJJZQpbm8lbEQVZcamU7IheMsvr5LWRdVzq95drVK/zuMowgmcwjl4cAl1uIUGNIHBEJ7hFd4c4bw4787HorXg5DPH8AfO5w8N1I2b</latexit>

h(41,2) <latexit sha1_base64="vVMM6NQmebZLbtMrmVQFlCn9YfI=">AAAB9HicbVBNS8NAEJ3Ur1q/qh69LBahgpSkFOyx4MVjBfsBbSyb7aZdutmku5tCCfkdXjwo4tUf481/47bNQVsfDDzem2FmnhdxprRtf1u5re2d3b38fuHg8Oj4pHh61lZhLAltkZCHsuthRTkTtKWZ5rQbSYoDj9OON7lb+J0ZlYqF4lHPI+oGeCSYzwjWRnLHg6R2U02fkrJznQ6KJbtiL4E2iZOREmRoDopf/WFI4oAKTThWqufYkXYTLDUjnKaFfqxohMkEj2jPUIEDqtxkeXSKrowyRH4oTQmNlurviQQHSs0Dz3QGWI/VurcQ//N6sfbrbsJEFGsqyGqRH3OkQ7RIAA2ZpETzuSGYSGZuRWSMJSba5FQwITjrL2+SdrXi2BXnoVZq1LM48nABl1AGB26hAffQhBYQmMIzvMKbNbNerHfrY9Was7KZc/gD6/MHUS+RGw==</latexit>

h(42,3) <latexit sha1_base64="dGYkbyf0/PJ/5VHaXsLOHckNvsY=">AAAB9HicbVBNS8NAEJ3Ur1q/qh69LBahgpSkFuyx4MVjBfsBbSyb7aZdutnE3U2hhPwOLx4U8eqP8ea/cdvmoK0PBh7vzTAzz4s4U9q2v63cxubW9k5+t7C3f3B4VDw+aaswloS2SMhD2fWwopwJ2tJMc9qNJMWBx2nHm9zO/c6USsVC8aBnEXUDPBLMZwRrI7njQVK7uk4fk3L1Mh0US3bFXgCtEycjJcjQHBS/+sOQxAEVmnCsVM+xI+0mWGpGOE0L/VjRCJMJHtGeoQIHVLnJ4ugUXRhliPxQmhIaLdTfEwkOlJoFnukMsB6rVW8u/uf1Yu3X3YSJKNZUkOUiP+ZIh2ieABoySYnmM0MwkczcisgYS0y0yalgQnBWX14n7WrFsSvOfa3UqGdx5OEMzqEMDtxAA+6gCS0g8ATP8Apv1tR6sd6tj2VrzspmTuEPrM8fVECRHQ==</latexit>

x4 <latexit sha1_base64="S4f+bq/VrUW0JYj64fGswbvelkw=">AAAB6nicbVBNS8NAEJ3Ur1q/qh69LBbBU0mkoHgqePFY0X5AG8pmO2mXbjZhdyOW0J/gxYMiXv1F3vw3btsctPXBwOO9GWbmBYng2rjut1NYW9/Y3Cpul3Z29/YPyodHLR2nimGTxSJWnYBqFFxi03AjsJMopFEgsB2Mb2Z++xGV5rF8MJME/YgOJQ85o8ZK90/9Wr9ccavuHGSVeDmpQI5Gv/zVG8QsjVAaJqjWXc9NjJ9RZTgTOC31Uo0JZWM6xK6lkkao/Wx+6pScWWVAwljZkobM1d8TGY20nkSB7YyoGellbyb+53VTE175GZdJalCyxaIwFcTEZPY3GXCFzIiJJZQpbm8lbEQVZcamU7IheMsvr5LWRdVzq95drVK/zuMowgmcwjl4cAl1uIUGNIHBEJ7hFd4c4bw4787HorXg5DPH8AfO5w8N1I2b</latexit>

h4(1,3) <latexit sha1_base64="UkfQFulH2ho3iM0sKHL3OHhkk50=">AAAB9HicbVBNS8NAEJ3Ur1q/qh69LBahgpSkFuyx4MVjBfsBbSyb7aZdutnE3U2hhPwOLx4U8eqP8ea/cdvmoK0PBh7vzTAzz4s4U9q2v63cxubW9k5+t7C3f3B4VDw+aaswloS2SMhD2fWwopwJ2tJMc9qNJMWBx2nHm9zO/c6USsVC8aBnEXUDPBLMZwRrI7njQVK7uk4fk7JzmQ6KJbtiL4DWiZOREmRoDopf/WFI4oAKTThWqufYkXYTLDUjnKaFfqxohMkEj2jPUIEDqtxkcXSKLowyRH4oTQmNFurviQQHSs0Cz3QGWI/VqjcX//N6sfbrbsJEFGsqyHKRH3OkQzRPAA2ZpETzmSGYSGZuRWSMJSba5FQwITirL6+TdrXi2BXnvlZq1LM48nAG51AGB26gAXfQhBYQeIJneIU3a2q9WO/Wx7I1Z2Uzp/AH1ucPUrqRHA==</latexit>

h4(2,4) <latexit sha1_base64="OmbDXGhuiYn5DphxRl3OEseBfW4=">AAAB9HicbVBNS8NAEJ3Ur1q/qh69LBahgpSkFOyx4MVjBfsBbSyb7aZdutmku5tCCfkdXjwo4tUf481/47bNQVsfDDzem2FmnhdxprRtf1u5re2d3b38fuHg8Oj4pHh61lZhLAltkZCHsuthRTkTtKWZ5rQbSYoDj9OON7lb+J0ZlYqF4lHPI+oGeCSYzwjWRnLHg6R2U0ufknL1Oh0US3bFXgJtEicjJcjQHBS/+sOQxAEVmnCsVM+xI+0mWGpGOE0L/VjRCJMJHtGeoQIHVLnJ8ugUXRlliPxQmhIaLdXfEwkOlJoHnukMsB6rdW8h/uf1Yu3X3YSJKNZUkNUiP+ZIh2iRABoySYnmc0MwkczcisgYS0y0yalgQnDWX94k7WrFsSvOQ63UqGdx5OECLqEMDtxCA+6hCS0gMIVneIU3a2a9WO/Wx6o1Z2Uz5/AH1ucPVcuRHg==</latexit>

Input <latexit sha1_base64="EYDz37zSf0z72fUQ3SRP8AL1/3U=">AAAB7HicbVBNT8JAFHzFL8Qv1KOXjcTEE2m5yJHEi94wsUACDdkuW9iw3Ta7ryaE8Bu8eNAYr/4gb/4bt9CDgpNsMpl5L/tmwlQKg6777ZS2tnd298r7lYPDo+OT6ulZxySZZtxniUx0L6SGS6G4jwIl76Wa0ziUvBtOb3O/+8S1EYl6xFnKg5iOlYgEo2gl/16lGQ6rNbfuLkE2iVeQGhRoD6tfg1HCspgrZJIa0/fcFIM51SiY5IvKIDM8pWxKx7xvqaIxN8F8eeyCXFllRKJE26eQLNXfG3MaGzOLQzsZU5yYdS8X//P6GUbNYC7yRFyx1UdRJgkmJE9ORkJzhnJmCWVa2FsJm1BNGdp+KrYEbz3yJuk06p5b9x4atVazqKMMF3AJ1+DBDbTgDtrgAwMBz/AKb45yXpx352M1WnKKnXP4A+fzB+SDjrA=</latexit>

x1 <latexit sha1_base64="Gx1rB9K2pA68P1aOYP+U250HTz0=">AAAB6nicbVBNS8NAEJ3Ur1q/qh69LBbBU0lEqHgqePFY0X5AG8pmO2mXbjZhdyOW0J/gxYMiXv1F3vw3btsctPXBwOO9GWbmBYng2rjut1NYW9/Y3Cpul3Z29/YPyodHLR2nimGTxSJWnYBqFFxi03AjsJMopFEgsB2Mb2Z++xGV5rF8MJME/YgOJQ85o8ZK9099r1+uuFV3DrJKvJxUIEejX/7qDWKWRigNE1Trrucmxs+oMpwJnJZ6qcaEsjEdYtdSSSPUfjY/dUrOrDIgYaxsSUPm6u+JjEZaT6LAdkbUjPSyNxP/87qpCa/8jMskNSjZYlGYCmJiMvubDLhCZsTEEsoUt7cSNqKKMmPTKdkQvOWXV0nrouq5Ve/uslK/zuMowgmcwjl4UIM63EIDmsBgCM/wCm+OcF6cd+dj0Vpw8plj+APn8wcJSI2Y</latexit>

x2 <latexit sha1_base64="958LDsa5w4EedPbIxPcFgVddcGo=">AAAB6nicbVBNS8NAEJ3Ur1q/qh69LBbBU0mKoHgqePFY0X5AG8pmO2mXbjZhdyOW0J/gxYMiXv1F3vw3btsctPXBwOO9GWbmBYng2rjut1NYW9/Y3Cpul3Z29/YPyodHLR2nimGTxSJWnYBqFFxi03AjsJMopFEgsB2Mb2Z++xGV5rF8MJME/YgOJQ85o8ZK90/9Wr9ccavuHGSVeDmpQI5Gv/zVG8QsjVAaJqjWXc9NjJ9RZTgTOC31Uo0JZWM6xK6lkkao/Wx+6pScWWVAwljZkobM1d8TGY20nkSB7YyoGellbyb+53VTE175GZdJalCyxaIwFcTEZPY3GXCFzIiJJZQpbm8lbEQVZcamU7IheMsvr5JWreq5Ve/uolK/zuMowgmcwjl4cAl1uIUGNIHBEJ7hFd4c4bw4787HorXg5DPH8AfO5w8KzI2Z</latexit>

x3 <latexit sha1_base64="FsFp5SDaWDjvwmxpdt2PBoWBq3Q=">AAAB6nicbVBNS8NAEJ3Ur1q/qh69LBbBU0lUUDwVvHisaD+gDWWz3bRLN5uwOxFL6E/w4kERr/4ib/4bt20O2vpg4PHeDDPzgkQKg6777RRWVtfWN4qbpa3tnd298v5B08SpZrzBYhnrdkANl0LxBgqUvJ1oTqNA8lYwupn6rUeujYjVA44T7kd0oEQoGEUr3T/1znvlilt1ZyDLxMtJBXLUe+Wvbj9macQVMkmN6Xhugn5GNQom+aTUTQ1PKBvRAe9YqmjEjZ/NTp2QE6v0SRhrWwrJTP09kdHImHEU2M6I4tAselPxP6+TYnjlZ0IlKXLF5ovCVBKMyfRv0heaM5RjSyjTwt5K2JBqytCmU7IheIsvL5PmWdVzq97dRaV2ncdRhCM4hlPw4BJqcAt1aACDATzDK7w50nlx3p2PeWvByWcO4Q+czx8MUI2a</latexit>

x4 <latexit sha1_base64="S4f+bq/VrUW0JYj64fGswbvelkw=">AAAB6nicbVBNS8NAEJ3Ur1q/qh69LBbBU0mkoHgqePFY0X5AG8pmO2mXbjZhdyOW0J/gxYMiXv1F3vw3btsctPXBwOO9GWbmBYng2rjut1NYW9/Y3Cpul3Z29/YPyodHLR2nimGTxSJWnYBqFFxi03AjsJMopFEgsB2Mb2Z++xGV5rF8MJME/YgOJQ85o8ZK90/9Wr9ccavuHGSVeDmpQI5Gv/zVG8QsjVAaJqjWXc9NjJ9RZTgTOC31Uo0JZWM6xK6lkkao/Wx+6pScWWVAwljZkobM1d8TGY20nkSB7YyoGellbyb+53VTE175GZdJalCyxaIwFcTEZPY3GXCFzIiJJZQpbm8lbEQVZcamU7IheMsvr5LWRdVzq95drVK/zuMowgmcwjl4cAl1uIUGNIHBEJ7hFd4c4bw4787HorXg5DPH8AfO5w8N1I2b</latexit>

x5 <latexit sha1_base64="dPT//6RAyPZ9Ol0ZAjwg7n/hgME=">AAAB6nicbVBNS8NAEJ3Ur1q/qh69LBbBU0lEUTwVvHisaD+gDWWz3bRLN5uwOxFL6E/w4kERr/4ib/4bt20O2vpg4PHeDDPzgkQKg6777RRWVtfWN4qbpa3tnd298v5B08SpZrzBYhnrdkANl0LxBgqUvJ1oTqNA8lYwupn6rUeujYjVA44T7kd0oEQoGEUr3T/1Lnrlilt1ZyDLxMtJBXLUe+Wvbj9macQVMkmN6Xhugn5GNQom+aTUTQ1PKBvRAe9YqmjEjZ/NTp2QE6v0SRhrWwrJTP09kdHImHEU2M6I4tAselPxP6+TYnjlZ0IlKXLF5ovCVBKMyfRv0heaM5RjSyjTwt5K2JBqytCmU7IheIsvL5PmWdVzq97deaV2ncdRhCM4hlPw4BJqcAt1aACDATzDK7w50nlx3p2PeWvByWcO4Q+czx8PWI2c</latexit>

x4 <latexit sha1_base64="S4f+bq/VrUW0JYj64fGswbvelkw=">AAAB6nicbVBNS8NAEJ3Ur1q/qh69LBbBU0mkoHgqePFY0X5AG8pmO2mXbjZhdyOW0J/gxYMiXv1F3vw3btsctPXBwOO9GWbmBYng2rjut1NYW9/Y3Cpul3Z29/YPyodHLR2nimGTxSJWnYBqFFxi03AjsJMopFEgsB2Mb2Z++xGV5rF8MJME/YgOJQ85o8ZK90/9Wr9ccavuHGSVeDmpQI5Gv/zVG8QsjVAaJqjWXc9NjJ9RZTgTOC31Uo0JZWM6xK6lkkao/Wx+6pScWWVAwljZkobM1d8TGY20nkSB7YyoGellbyb+53VTE175GZdJalCyxaIwFcTEZPY3GXCFzIiJJZQpbm8lbEQVZcamU7IheMsvr5LWRdVzq95drVK/zuMowgmcwjl4cAl1uIUGNIHBEJ7hFd4c4bw4787HorXg5DPH8AfO5w8N1I2b</latexit>

h(41,4) <latexit sha1_base64="1ACFCIeWCu6xW798dM+rHizW1lU=">AAAB9HicbVBNS8NAEJ3Ur1q/qh69LBahgpRECvZY8OKxgv2ANpbNdtMu3Wzi7qZQQn6HFw+KePXHePPfuGlz0NYHA4/3ZpiZ50WcKW3b31ZhY3Nre6e4W9rbPzg8Kh+fdFQYS0LbJOSh7HlYUc4EbWumOe1FkuLA47TrTW8zvzujUrFQPOh5RN0AjwXzGcHaSO5kmNSv6uljUnUu02G5YtfsBdA6cXJSgRytYflrMApJHFChCcdK9R070m6CpWaE07Q0iBWNMJniMe0bKnBAlZssjk7RhVFGyA+lKaHRQv09keBAqXngmc4A64la9TLxP68fa7/hJkxEsaaCLBf5MUc6RFkCaMQkJZrPDcFEMnMrIhMsMdEmp5IJwVl9eZ10rmuOXXPu65VmI4+jCGdwDlVw4AaacActaAOBJ3iGV3izZtaL9W59LFsLVj5zCn9gff4AVEWRHQ==</latexit>

(a) Representing RNN units as channel groups

x3 <latexit sha1_base64="FsFp5SDaWDjvwmxpdt2PBoWBq3Q=">AAAB6nicbVBNS8NAEJ3Ur1q/qh69LBbBU0lUUDwVvHisaD+gDWWz3bRLN5uwOxFL6E/w4kERr/4ib/4bt20O2vpg4PHeDDPzgkQKg6777RRWVtfWN4qbpa3tnd298v5B08SpZrzBYhnrdkANl0LxBgqUvJ1oTqNA8lYwupn6rUeujYjVA44T7kd0oEQoGEUr3T/1znvlilt1ZyDLxMtJBXLUe+Wvbj9macQVMkmN6Xhugn5GNQom+aTUTQ1PKBvRAe9YqmjEjZ/NTp2QE6v0SRhrWwrJTP09kdHImHEU2M6I4tAselPxP6+TYnjlZ0IlKXLF5ovCVBKMyfRv0heaM5RjSyjTwt5K2JBqytCmU7IheIsvL5PmWdVzq97dRaV2ncdRhCM4hlPw4BJqcAt1aACDATzDK7w50nlx3p2PeWvByWcO4Q+czx8MUI2a</latexit>

Wh(1h) <latexit sha1_base64="x2OBbsckn1W2P4vh6U1bjIpjqk4=">AAAB83icbVDLSsNAFL2pr1pfVZduBotQNyURwS4LblxWsA9oY5lMJ83QySTMQyghv+HGhSJu/Rl3/o3TNgttPXDhcM693HtPkHKmtOt+O6WNza3tnfJuZW//4PCoenzSVYmRhHZIwhPZD7CinAna0Uxz2k8lxXHAaS+Y3s793hOViiXiQc9S6sd4IljICNZWGvZGWRTlj1ndu8xH1ZrbcBdA68QrSA0KtEfVr+E4ISamQhOOlRp4bqr9DEvNCKd5ZWgUTTGZ4gkdWCpwTJWfLW7O0YVVxihMpC2h0UL9PZHhWKlZHNjOGOtIrXpz8T9vYHTY9DMmUqOpIMtFoeFIJ2geABozSYnmM0swkczeikiEJSbaxlSxIXirL6+T7lXDcxve/XWt1SziKMMZnEMdPLiBFtxBGzpAIIVneIU3xzgvzrvzsWwtOcXMKfyB8/kDb/KRPg==</latexit>
h3(1,2) <latexit sha1_base64="0H7qvoQ0a4CrNfFjHFa0eFascjU=">AAAB9HicbVBNS8NAEJ3Ur1q/qh69LBahgpSkCvZY8OKxgv2ANpbNdtMu3WzS3U2hhPwOLx4U8eqP8ea/cdvmoK0PBh7vzTAzz4s4U9q2v63cxubW9k5+t7C3f3B4VDw+aakwloQ2SchD2fGwopwJ2tRMc9qJJMWBx2nbG9/N/faUSsVC8ahnEXUDPBTMZwRrI7mjfnJ9VU2fkrJzmfaLJbtiL4DWiZOREmRo9ItfvUFI4oAKTThWquvYkXYTLDUjnKaFXqxohMkYD2nXUIEDqtxkcXSKLowyQH4oTQmNFurviQQHSs0Cz3QGWI/UqjcX//O6sfZrbsJEFGsqyHKRH3OkQzRPAA2YpETzmSGYSGZuRWSEJSba5FQwITirL6+TVrXi2BXn4aZUr2Vx5OEMzqEMDtxCHe6hAU0gMIFneIU3a2q9WO/Wx7I1Z2Uzp/AH1ucPT6KRGg==</latexit>

Wh(2h) <latexit sha1_base64="mgAPACgi9cwAWa+uc/wY4KQubbQ=">AAAB83icbVDLSsNAFL3xWeur6tLNYBHqpiRFsMuCG5cV7APaWCbTSTN0MgnzEErIb7hxoYhbf8adf+O0zUJbD1w4nHMv994TpJwp7brfzsbm1vbObmmvvH9weHRcOTntqsRIQjsk4YnsB1hRzgTtaKY57aeS4jjgtBdMb+d+74lKxRLxoGcp9WM8ESxkBGsrDXujLIryx6zWuMpHlapbdxdA68QrSBUKtEeVr+E4ISamQhOOlRp4bqr9DEvNCKd5eWgUTTGZ4gkdWCpwTJWfLW7O0aVVxihMpC2h0UL9PZHhWKlZHNjOGOtIrXpz8T9vYHTY9DMmUqOpIMtFoeFIJ2geABozSYnmM0swkczeikiEJSbaxlS2IXirL6+TbqPuuXXv/rraahZxlOAcLqAGHtxAC+6gDR0gkMIzvMKbY5wX5935WLZuOMXMGfyB8/kDcXiRPw==</latexit>
h3(2,3) <latexit sha1_base64="mRX14vlACU54SoLYxl9WNS7fi6E=">AAAB9HicbVBNS8NAEJ3Ur1q/qh69LBahgpSkFeyx4MVjBfsBbSyb7aZdutnE3U2hhPwOLx4U8eqP8ea/cdvmoK0PBh7vzTAzz4s4U9q2v63cxubW9k5+t7C3f3B4VDw+aaswloS2SMhD2fWwopwJ2tJMc9qNJMWBx2nHm9zO/c6USsVC8aBnEXUDPBLMZwRrI7njQVK7qqWPSbl6mQ6KJbtiL4DWiZOREmRoDopf/WFI4oAKTThWqufYkXYTLDUjnKaFfqxohMkEj2jPUIEDqtxkcXSKLowyRH4oTQmNFurviQQHSs0Cz3QGWI/VqjcX//N6sfbrbsJEFGsqyHKRH3OkQzRPAA2ZpETzmSGYSGZuRWSMJSba5FQwITirL6+TdrXi2BXn/rrUqGdx5OEMzqEMDtxAA+6gCS0g8ATP8Apv1tR6sd6tj2VrzspmTuEPrM8fUrORHA==</latexit>

I
<latexit sha1_base64="stm077CEDV4DCNoSinYA1n/DU9E=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lEsMeCF721YD+gDWWznbRrN5uwuxFK6C/w4kERr/4kb/4bt20O2vpg4PHeDDPzgkRwbVz32ylsbG5t7xR3S3v7B4dH5eOTto5TxbDFYhGrbkA1Ci6xZbgR2E0U0igQ2Akmt3O/84RK81g+mGmCfkRHkoecUWOl5v2gXHGr7gJknXg5qUCOxqD81R/GLI1QGiao1j3PTYyfUWU4Ezgr9VONCWUTOsKepZJGqP1sceiMXFhlSMJY2ZKGLNTfExmNtJ5Gge2MqBnrVW8u/uf1UhPW/IzLJDUo2XJRmApiYjL/mgy5QmbE1BLKFLe3EjamijJjsynZELzVl9dJ+6rquVWveV2p1/I4inAG53AJHtxAHe6gAS1ggPAMr/DmPDovzrvzsWwtOPnMKfyB8/kDm/uMww==</latexit>
x4 <latexit sha1_base64="S4f+bq/VrUW0JYj64fGswbvelkw=">AAAB6nicbVBNS8NAEJ3Ur1q/qh69LBbBU0mkoHgqePFY0X5AG8pmO2mXbjZhdyOW0J/gxYMiXv1F3vw3btsctPXBwOO9GWbmBYng2rjut1NYW9/Y3Cpul3Z29/YPyodHLR2nimGTxSJWnYBqFFxi03AjsJMopFEgsB2Mb2Z++xGV5rF8MJME/YgOJQ85o8ZK90/9Wr9ccavuHGSVeDmpQI5Gv/zVG8QsjVAaJqjWXc9NjJ9RZTgTOC31Uo0JZWM6xK6lkkao/Wx+6pScWWVAwljZkobM1d8TGY20nkSB7YyoGellbyb+53VTE175GZdJalCyxaIwFcTEZPY3GXCFzIiJJZQpbm8lbEQVZcamU7IheMsvr5LWRdVzq95drVK/zuMowgmcwjl4cAl1uIUGNIHBEJ7hFd4c4bw4787HorXg5DPH8AfO5w8N1I2b</latexit>

Whx <latexit sha1_base64="IJIOAqhC6Q8Cp8RD0brgjknBzM4=">AAAB7XicbVDLSgNBEOz1GeMr6tHLYBA8hV0RzDHgxWME84AkhNnJbDJmdmaZ6RXDkn/w4kERr/6PN//GSbIHTSxoKKq66e4KEyks+v63t7a+sbm1Xdgp7u7tHxyWjo6bVqeG8QbTUpt2SC2XQvEGCpS8nRhO41DyVji+mfmtR26s0OoeJwnvxXSoRCQYRSc1W/1s9DTtl8p+xZ+DrJIgJ2XIUe+XvroDzdKYK2SSWtsJ/AR7GTUomOTTYje1PKFsTIe846iiMbe9bH7tlJw7ZUAibVwpJHP190RGY2snceg6Y4oju+zNxP+8TopRtZcJlaTIFVssilJJUJPZ62QgDGcoJ45QZoS7lbARNZShC6joQgiWX14lzctK4FeCu6tyrZrHUYBTOIMLCOAaanALdWgAgwd4hld487T34r17H4vWNS+fOYE/8D5/AMs7jzo=</latexit> h4(1,3) <latexit sha1_base64="UkfQFulH2ho3iM0sKHL3OHhkk50=">AAAB9HicbVBNS8NAEJ3Ur1q/qh69LBahgpSkFuyx4MVjBfsBbSyb7aZdutnE3U2hhPwOLx4U8eqP8ea/cdvmoK0PBh7vzTAzz4s4U9q2v63cxubW9k5+t7C3f3B4VDw+aaswloS2SMhD2fWwopwJ2tJMc9qNJMWBx2nHm9zO/c6USsVC8aBnEXUDPBLMZwRrI7njQVK7uk4fk7JzmQ6KJbtiL4DWiZOREmRoDopf/WFI4oAKTThWqufYkXYTLDUjnKaFfqxohMkEj2jPUIEDqtxkcXSKLowyRH4oTQmNFurviQQHSs0Cz3QGWI/VqjcX//N6sfbrbsJEFGsqyHKRH3OkQzRPAA2ZpETzmSGYSGZuRWSMJSba5FQwITirL6+TdrXi2BXnvlZq1LM48nAG51AGB26gAXfQhBYQeIJneIU3a2q9WO/Wx7I1Z2Uzp/AH1ucPUrqRHA==</latexit>

Wh(1h/2) <latexit sha1_base64="JjF1DGrOxzHMl3z1RZJcwP5f34o=">AAAB9XicbVBNS8NAEJ3Ur1q/qh69LBahXmpSBHssePFYwX5Am5bNdtMs3WzC7kYpIf/DiwdFvPpfvPlv3LY5aOuDgcd7M8zM82LOlLbtb6uwsbm1vVPcLe3tHxwelY9POipKJKFtEvFI9jysKGeCtjXTnPZiSXHocdr1prdzv/tIpWKReNCzmLohngjmM4K1kYbdURoE2TCtOlf1y2xUrtg1ewG0TpycVCBHa1T+GowjkoRUaMKxUn3HjrWbYqkZ4TQrDRJFY0ymeEL7hgocUuWmi6szdGGUMfIjaUpotFB/T6Q4VGoWeqYzxDpQq95c/M/rJ9pvuCkTcaKpIMtFfsKRjtA8AjRmkhLNZ4ZgIpm5FZEAS0y0CapkQnBWX14nnXrNsWvO/XWl2cjjKMIZnEMVHLiBJtxBC9pAQMIzvMKb9WS9WO/Wx7K1YOUzp/AH1ucPVUyRsw==</latexit> h(42,4) <latexit sha1_base64="OmbDXGhuiYn5DphxRl3OEseBfW4=">AAAB9HicbVBNS8NAEJ3Ur1q/qh69LBahgpSkFOyx4MVjBfsBbSyb7aZdutmku5tCCfkdXjwo4tUf481/47bNQVsfDDzem2FmnhdxprRtf1u5re2d3b38fuHg8Oj4pHh61lZhLAltkZCHsuthRTkTtKWZ5rQbSYoDj9OON7lb+J0ZlYqF4lHPI+oGeCSYzwjWRnLHg6R2U0ufknL1Oh0US3bFXgJtEicjJcjQHBS/+sOQxAEVmnCsVM+xI+0mWGpGOE0L/VjRCJMJHtGeoQIHVLnJ8ugUXRlliPxQmhIaLdXfEwkOlJoHnukMsB6rdW8h/uf1Yu3X3YSJKNZUkNUiP+ZIh2iRABoySYnmc0MwkczcisgYS0y0yalgQnDWX94k7WrFsSvOQ63UqGdx5OECLqEMDtxCA+6hCS0gMIVneIU3a2a9WO/Wx6o1Z2Uz5/AH1ucPVcuRHg==</latexit>

x3 <latexit sha1_base64="FsFp5SDaWDjvwmxpdt2PBoWBq3Q=">AAAB6nicbVBNS8NAEJ3Ur1q/qh69LBbBU0lUUDwVvHisaD+gDWWz3bRLN5uwOxFL6E/w4kERr/4ib/4bt20O2vpg4PHeDDPzgkQKg6777RRWVtfWN4qbpa3tnd298v5B08SpZrzBYhnrdkANl0LxBgqUvJ1oTqNA8lYwupn6rUeujYjVA44T7kd0oEQoGEUr3T/1znvlilt1ZyDLxMtJBXLUe+Wvbj9macQVMkmN6Xhugn5GNQom+aTUTQ1PKBvRAe9YqmjEjZ/NTp2QE6v0SRhrWwrJTP09kdHImHEU2M6I4tAselPxP6+TYnjlZ0IlKXLF5ovCVBKMyfRv0heaM5RjSyjTwt5K2JBqytCmU7IheIsvL5PmWdVzq97dRaV2ncdRhCM4hlPw4BJqcAt1aACDATzDK7w50nlx3p2PeWvByWcO4Q+czx8MUI2a</latexit>

h(31,3) <latexit sha1_base64="mKeSq2YelqNEkY6UC+a5nVGXYDM=">AAAB9HicbVBNS8NAEJ3Ur1q/qh69LBahgpSkFeyx4MVjBfsBbSyb7aZdutnE3U2hhPwOLx4U8eqP8ea/cdvmoK0PBh7vzTAzz4s4U9q2v63cxubW9k5+t7C3f3B4VDw+aaswloS2SMhD2fWwopwJ2tJMc9qNJMWBx2nHm9zO/c6USsVC8aBnEXUDPBLMZwRrI7njQVK7qqWPSdm5TAfFkl2xF0DrxMlICTI0B8Wv/jAkcUCFJhwr1XPsSLsJlpoRTtNCP1Y0wmSCR7RnqMABVW6yODpFF0YZIj+UpoRGC/X3RIIDpWaBZzoDrMdq1ZuL/3m9WPt1N2EiijUVZLnIjznSIZongIZMUqL5zBBMJDO3IjLGEhNtciqYEJzVl9dJu1px7Ipzf11q1LM48nAG51AGB26gAXfQhBYQeIJneIU3a2q9WO/Wx7I1Z2Uzp/AH1ucPUS2RGw==</latexit>

0 <latexit sha1_base64="+kYPchScNyik1n1ulpTEzE1fdeA=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lEqHgqePHYgv2ANpTNdtKu3WzC7kYoob/AiwdFvPqTvPlv3LY5aOuDgcd7M8zMCxLBtXHdb6ewsbm1vVPcLe3tHxwelY9P2jpOFcMWi0WsugHVKLjEluFGYDdRSKNAYCeY3M39zhMqzWP5YKYJ+hEdSR5yRo2Vmu6gXHGr7gJknXg5qUCOxqD81R/GLI1QGiao1j3PTYyfUWU4Ezgr9VONCWUTOsKepZJGqP1sceiMXFhlSMJY2ZKGLNTfExmNtJ5Gge2MqBnrVW8u/uf1UhPe+BmXSWpQsuWiMBXExGT+NRlyhcyIqSWUKW5vJWxMFWXGZlOyIXirL6+T9lXVc6te87pSv83jKMIZnMMleFCDOtxDA1rAAOEZXuHNeXRenHfnY9lacPKZU/gD5/MHdrGMrA==</latexit>

x4 <latexit sha1_base64="S4f+bq/VrUW0JYj64fGswbvelkw=">AAAB6nicbVBNS8NAEJ3Ur1q/qh69LBbBU0mkoHgqePFY0X5AG8pmO2mXbjZhdyOW0J/gxYMiXv1F3vw3btsctPXBwOO9GWbmBYng2rjut1NYW9/Y3Cpul3Z29/YPyodHLR2nimGTxSJWnYBqFFxi03AjsJMopFEgsB2Mb2Z++xGV5rF8MJME/YgOJQ85o8ZK90/9Wr9ccavuHGSVeDmpQI5Gv/zVG8QsjVAaJqjWXc9NjJ9RZTgTOC31Uo0JZWM6xK6lkkao/Wx+6pScWWVAwljZkobM1d8TGY20nkSB7YyoGellbyb+53VTE175GZdJalCyxaIwFcTEZPY3GXCFzIiJJZQpbm8lbEQVZcamU7IheMsvr5LWRdVzq95drVK/zuMowgmcwjl4cAl1uIUGNIHBEJ7hFd4c4bw4787HorXg5DPH8AfO5w8N1I2b</latexit>

h(41,4) <latexit sha1_base64="1ACFCIeWCu6xW798dM+rHizW1lU=">AAAB9HicbVBNS8NAEJ3Ur1q/qh69LBahgpRECvZY8OKxgv2ANpbNdtMu3Wzi7qZQQn6HFw+KePXHePPfuGlz0NYHA4/3ZpiZ50WcKW3b31ZhY3Nre6e4W9rbPzg8Kh+fdFQYS0LbJOSh7HlYUc4EbWumOe1FkuLA47TrTW8zvzujUrFQPOh5RN0AjwXzGcHaSO5kmNSv6uljUnUu02G5YtfsBdA6cXJSgRytYflrMApJHFChCcdK9R070m6CpWaE07Q0iBWNMJniMe0bKnBAlZssjk7RhVFGyA+lKaHRQv09keBAqXngmc4A64la9TLxP68fa7/hJkxEsaaCLBf5MUc6RFkCaMQkJZrPDcFEMnMrIhMsMdEmp5IJwVl9eZ10rmuOXXPu65VmI4+jCGdwDlVw4AaacActaAOBJ3iGV3izZtaL9W59LFsLVj5zCn9gff4AVEWRHQ==</latexit>

0 <latexit sha1_base64="+kYPchScNyik1n1ulpTEzE1fdeA=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lEqHgqePHYgv2ANpTNdtKu3WzC7kYoob/AiwdFvPqTvPlv3LY5aOuDgcd7M8zMCxLBtXHdb6ewsbm1vVPcLe3tHxwelY9P2jpOFcMWi0WsugHVKLjEluFGYDdRSKNAYCeY3M39zhMqzWP5YKYJ+hEdSR5yRo2Vmu6gXHGr7gJknXg5qUCOxqD81R/GLI1QGiao1j3PTYyfUWU4Ezgr9VONCWUTOsKepZJGqP1sceiMXFhlSMJY2ZKGLNTfExmNtJ5Gge2MqBnrVW8u/uf1UhPe+BmXSWpQsuWiMBXExGT+NRlyhcyIqSWUKW5vJWxMFWXGZlOyIXirL6+T9lXVc6te87pSv83jKMIZnMMleFCDOtxDA1rAAOEZXuHNeXRenHfnY9lacPKZU/gD5/MHdrGMrA==</latexit>

x3 <latexit sha1_base64="FsFp5SDaWDjvwmxpdt2PBoWBq3Q=">AAAB6nicbVBNS8NAEJ3Ur1q/qh69LBbBU0lUUDwVvHisaD+gDWWz3bRLN5uwOxFL6E/w4kERr/4ib/4bt20O2vpg4PHeDDPzgkQKg6777RRWVtfWN4qbpa3tnd298v5B08SpZrzBYhnrdkANl0LxBgqUvJ1oTqNA8lYwupn6rUeujYjVA44T7kd0oEQoGEUr3T/1znvlilt1ZyDLxMtJBXLUe+Wvbj9macQVMkmN6Xhugn5GNQom+aTUTQ1PKBvRAe9YqmjEjZ/NTp2QE6v0SRhrWwrJTP09kdHImHEU2M6I4tAselPxP6+TYnjlZ0IlKXLF5ovCVBKMyfRv0heaM5RjSyjTwt5K2JBqytCmU7IheIsvL5PmWdVzq97dRaV2ncdRhCM4hlPw4BJqcAt1aACDATzDK7w50nlx3p2PeWvByWcO4Q+czx8MUI2a</latexit>

0 <latexit sha1_base64="+kYPchScNyik1n1ulpTEzE1fdeA=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lEqHgqePHYgv2ANpTNdtKu3WzC7kYoob/AiwdFvPqTvPlv3LY5aOuDgcd7M8zMCxLBtXHdb6ewsbm1vVPcLe3tHxwelY9P2jpOFcMWi0WsugHVKLjEluFGYDdRSKNAYCeY3M39zhMqzWP5YKYJ+hEdSR5yRo2Vmu6gXHGr7gJknXg5qUCOxqD81R/GLI1QGiao1j3PTYyfUWU4Ezgr9VONCWUTOsKepZJGqP1sceiMXFhlSMJY2ZKGLNTfExmNtJ5Gge2MqBnrVW8u/uf1UhPe+BmXSWpQsuWiMBXExGT+NRlyhcyIqSWUKW5vJWxMFWXGZlOyIXirL6+T9lXVc6te87pSv83jKMIZnMMleFCDOtxDA1rAAOEZXuHNeXRenHfnY9lacPKZU/gD5/MHdrGMrA==</latexit>

0 <latexit sha1_base64="+kYPchScNyik1n1ulpTEzE1fdeA=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lEqHgqePHYgv2ANpTNdtKu3WzC7kYoob/AiwdFvPqTvPlv3LY5aOuDgcd7M8zMCxLBtXHdb6ewsbm1vVPcLe3tHxwelY9P2jpOFcMWi0WsugHVKLjEluFGYDdRSKNAYCeY3M39zhMqzWP5YKYJ+hEdSR5yRo2Vmu6gXHGr7gJknXg5qUCOxqD81R/GLI1QGiao1j3PTYyfUWU4Ezgr9VONCWUTOsKepZJGqP1sceiMXFhlSMJY2ZKGLNTfExmNtJ5Gge2MqBnrVW8u/uf1UhPe+BmXSWpQsuWiMBXExGT+NRlyhcyIqSWUKW5vJWxMFWXGZlOyIXirL6+T9lXVc6te87pSv83jKMIZnMMleFCDOtxDA1rAAOEZXuHNeXRenHfnY9lacPKZU/gD5/MHdrGMrA==</latexit>

x4 <latexit sha1_base64="S4f+bq/VrUW0JYj64fGswbvelkw=">AAAB6nicbVBNS8NAEJ3Ur1q/qh69LBbBU0mkoHgqePFY0X5AG8pmO2mXbjZhdyOW0J/gxYMiXv1F3vw3btsctPXBwOO9GWbmBYng2rjut1NYW9/Y3Cpul3Z29/YPyodHLR2nimGTxSJWnYBqFFxi03AjsJMopFEgsB2Mb2Z++xGV5rF8MJME/YgOJQ85o8ZK90/9Wr9ccavuHGSVeDmpQI5Gv/zVG8QsjVAaJqjWXc9NjJ9RZTgTOC31Uo0JZWM6xK6lkkao/Wx+6pScWWVAwljZkobM1d8TGY20nkSB7YyoGellbyb+53VTE175GZdJalCyxaIwFcTEZPY3GXCFzIiJJZQpbm8lbEQVZcamU7IheMsvr5LWRdVzq95drVK/zuMowgmcwjl4cAl1uIUGNIHBEJ7hFd4c4bw4787HorXg5DPH8AfO5w8N1I2b</latexit>

0 <latexit sha1_base64="+kYPchScNyik1n1ulpTEzE1fdeA=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lEqHgqePHYgv2ANpTNdtKu3WzC7kYoob/AiwdFvPqTvPlv3LY5aOuDgcd7M8zMCxLBtXHdb6ewsbm1vVPcLe3tHxwelY9P2jpOFcMWi0WsugHVKLjEluFGYDdRSKNAYCeY3M39zhMqzWP5YKYJ+hEdSR5yRo2Vmu6gXHGr7gJknXg5qUCOxqD81R/GLI1QGiao1j3PTYyfUWU4Ezgr9VONCWUTOsKepZJGqP1sceiMXFhlSMJY2ZKGLNTfExmNtJ5Gge2MqBnrVW8u/uf1UhPe+BmXSWpQsuWiMBXExGT+NRlyhcyIqSWUKW5vJWxMFWXGZlOyIXirL6+T9lXVc6te87pSv83jKMIZnMMleFCDOtxDA1rAAOEZXuHNeXRenHfnY9lacPKZU/gD5/MHdrGMrA==</latexit>

0 <latexit sha1_base64="+kYPchScNyik1n1ulpTEzE1fdeA=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lEqHgqePHYgv2ANpTNdtKu3WzC7kYoob/AiwdFvPqTvPlv3LY5aOuDgcd7M8zMCxLBtXHdb6ewsbm1vVPcLe3tHxwelY9P2jpOFcMWi0WsugHVKLjEluFGYDdRSKNAYCeY3M39zhMqzWP5YKYJ+hEdSR5yRo2Vmu6gXHGr7gJknXg5qUCOxqD81R/GLI1QGiao1j3PTYyfUWU4Ezgr9VONCWUTOsKepZJGqP1sceiMXFhlSMJY2ZKGLNTfExmNtJ5Gge2MqBnrVW8u/uf1UhPe+BmXSWpQsuWiMBXExGT+NRlyhcyIqSWUKW5vJWxMFWXGZlOyIXirL6+T9lXVc6te87pSv83jKMIZnMMleFCDOtxDA1rAAOEZXuHNeXRenHfnY9lacPKZU/gD5/MHdrGMrA==</latexit>

Group <latexit sha1_base64="unb6uJEAAE0vKz3VYbQv/lQUE7Y=">AAAB7nicdVDLSgMxFM3UV62vqks3wSK4Gmb6mi4LLnRZwT6gHUomzbShmSQkGaEM/Qg3LhRx6/e4829MH4KKHrhwOOde7r0nkoxq43kfTm5jc2t7J79b2Ns/ODwqHp90tEgVJm0smFC9CGnCKCdtQw0jPakISiJGutH0auF374nSVPA7M5MkTNCY05hiZKzUvVYildAfFkueGwTValCBnluu1+r1wBK/UQn8MvRdb4kSWKM1LL4PRgKnCeEGM6R13/ekCTOkDMWMzAuDVBOJ8BSNSd9SjhKiw2x57hxeWGUEY6FscQOX6veJDCVaz5LIdibITPRvbyH+5fVTEzfCjHKZGsLxalGcMmgEXPwOR1QRbNjMEoQVtbdCPEEKYWMTKtgQvj6F/5NO2fVtMrflUrO2jiMPzsA5uAQ+CEAT3IAWaAMMpuABPIFnRzqPzovzumrNOeuZU/ADztsnJNyPZQ==</latexit>

1

Group <latexit sha1_base64="yjatuhr3MjSgJghVfJ+Ws4Qvqgs=">AAAB7nicdVDLSgMxFM3UV62vqks3wSK4Gmamj+my4EKXFewD2qFk0kwbmklCkhFK6Ue4caGIW7/HnX9j+hBU9MCFwzn3cu89sWRUG8/7cHIbm1vbO/ndwt7+weFR8fikrUWmMGlhwYTqxkgTRjlpGWoY6UpFUBoz0oknVwu/c0+UpoLfmakkUYpGnCYUI2OlzrUSmYTBoFjy3DCsVMIy9NygVq3VQkv8ejn0A+i73hIlsEZzUHzvDwXOUsINZkjrnu9JE82QMhQzMi/0M00kwhM0Ij1LOUqJjmbLc+fwwipDmAhlixu4VL9PzFCq9TSNbWeKzFj/9hbiX14vM0k9mlEuM0M4Xi1KMgaNgIvf4ZAqgg2bWoKwovZWiMdIIWxsQgUbwten8H/SDlzfJnMblBrVdRx5cAbOwSXwQQga4AY0QQtgMAEP4Ak8O9J5dF6c11VrzlnPnIIfcN4+ASZgj2Y=</latexit>

2

Group <latexit sha1_base64="oyjkR/EN89ntYxi1HXoib+9qWWg=">AAAB7nicdVDLSgMxFM3UV62vqks3wSK4Gmb6mi4LLnRZwT6gHUomzbShmSQkGaEM/Qg3LhRx6/e4829MH4KKHrhwOOde7r0nkoxq43kfTm5jc2t7J79b2Ns/ODwqHp90tEgVJm0smFC9CGnCKCdtQw0jPakISiJGutH0auF374nSVPA7M5MkTNCY05hiZKzUvVYilbAyLJY8Nwiq1aACPbdcr9XrgSV+oxL4Zei73hIlsEZrWHwfjAROE8INZkjrvu9JE2ZIGYoZmRcGqSYS4Skak76lHCVEh9ny3Dm8sMoIxkLZ4gYu1e8TGUq0niWR7UyQmejf3kL8y+unJm6EGeUyNYTj1aI4ZdAIuPgdjqgi2LCZJQgram+FeIIUwsYmVLAhfH0K/yedsuvbZG7LpWZtHUcenIFzcAl8EIAmuAEt0AYYTMEDeALPjnQenRfnddWac9Yzp+AHnLdPJ+SPZw==</latexit>

3

Channels <latexit sha1_base64="FXxsPe5Tf+Z84SJWrFZwJvz1goo=">AAAB/3icdVA9SwNBEN3zM8avqGBjsxgFq3CXxFwaIWBjqWBUiEfY28wlS/b2jt05IcQU/hUbC0Vs/Rt2/hs3MYKKPhh4vDfDzLwwlcKg6747M7Nz8wuLuaX88srq2nphY/PCJJnm0OSJTPRVyAxIoaCJAiVcpRpYHEq4DPvHY//yBrQRiTrHQQpBzLpKRIIztFK7sH3cY0qBNJQhRRED3cOj6l67UHRLvl+t+hXqlsq1w1rNt8SrV3yvTL2SO0GRTHHaLrxddxKexaCQS2ZMy3NTDIZMo+ASRvnrzEDKeJ91oWWpYjGYYDi5f0T3rdKhUaJtKaQT9fvEkMXGDOLQdsYMe+a3Nxb/8loZRvVgKFSaISj+uSjKJMWEjsOgHaGBoxxYwrgW9lbKe0wzjjayvA3h61P6P7kolzybzFm52KhP48iRHbJLDohHfNIgJ+SUNAknt+SePJIn5855cJ6dl8/WGWc6s0V+wHn9AHbnlQ4=</latexit>

at

time

t=4

(b) Mixed group convolution

Input <latexit sha1_base64="EYDz37zSf0z72fUQ3SRP8AL1/3U=">AAAB7HicbVBNT8JAFHzFL8Qv1KOXjcTEE2m5yJHEi94wsUACDdkuW9iw3Ta7ryaE8Bu8eNAYr/4gb/4bt9CDgpNsMpl5L/tmwlQKg6777ZS2tnd298r7lYPDo+OT6ulZxySZZtxniUx0L6SGS6G4jwIl76Wa0ziUvBtOb3O/+8S1EYl6xFnKg5iOlYgEo2gl/16lGQ6rNbfuLkE2iVeQGhRoD6tfg1HCspgrZJIa0/fcFIM51SiY5IvKIDM8pWxKx7xvqaIxN8F8eeyCXFllRKJE26eQLNXfG3MaGzOLQzsZU5yYdS8X//P6GUbNYC7yRFyx1UdRJgkmJE9ORkJzhnJmCWVa2FsJm1BNGdp+KrYEbz3yJuk06p5b9x4atVazqKMMF3AJ1+DBDbTgDtrgAwMBz/AKb45yXpx352M1WnKKnXP4A+fzB+SDjrA=</latexit>

Figure 2: Representing a truncated 2-layer RNN M as a trellis network  . (a) Each unit of  has three groups, which house the input, first-layer hidden vector, and second-layer hidden vector of M , respectively. (b) Each group in the hidden unit of  in level i + 1 at time step t + 1 is computed by a linear combination of appropriate groups of hidden units in level i at time steps t and t + 1. The linear transformations form a mixed group convolution that reproduces computation in M . (Nonlinearities not shown for clarity.)

that each TrellisNet unit represents 3 RNN units simultaneously (for xt, h(t1), h(t2)). Each TrellisNet unit thus has (p + 2d) channels. The interlayer transformation can then be expressed as a mixed group convolution, illustrated in Figure 2b. This can be represented as a sparse convolution with the structure given in Eq. (5) (with L = 2). Applying the nonlinearity g on the pre-activation output, this exactly reproduces the transformations in the original 2-layer RNN.
The TrellisNet that emerges from this construction has special sparsity structure in the weight matrix. It stands to reason that a general TrellisNet with an unconstrained (dense) weight matrix W may have greater expressive power: it can model a broader class of transformations than the original RNN M . Note that while the hidden channels of the TrellisNet  constructed in the proof of Theorem 1 are naturally arranged into groups that represent different layers of the RNN M (Eq. (6), an unconstrained dense weight matrix W no longer admits such an interpretation. A model defined by a dense weight matrix is fundamentally distinct from the RNN M that served as our point of departure. We take advantage of this expressivity and use general weight matrices W , as presented in Section 3, in our experiments. Our ablation analysis will show that such generalized dense transformations are beneficial, even when model capacity is controlled for.
The proof of Theorem 1 did not delve into the inner structure of the nonlinear transformation g in RNN (or f in the constructed TrellisNet). For a vanilla RNN, for instance, f is usually an elementwise sigmoid or tanh function that is applied to h^t. But the construction in Theorem 1 applies just as well to RNNs with structured cells, such as LSTMs and GRUs. We adopt LSTM cells for the TrellisNets in our experiments and provide a detailed treatment of this nonlinearity in Section 5.1 and Appendix A.

4.3 TRELLISNET AS A BRIDGE BETWEEN RECURRENT AND CONVOLUTIONAL MODELS
In Section 4.1 we concluded that TrellisNet is a special kind of TCN, characterized by weight tying and input injection. In Section 4.2 we established that TrellisNet is a generalization of truncated RNNs. These connections along with the construction in our proof of Theorem 1 allow TrellisNets to benefit significantly from techniques developed originally for RNNs, while also incorporating architectural and algorithmic motifs developed for convolutional networks. We summarize a number of techniques here. From recurrent networks, we can integrate 1) structured nonlinear activations (e.g. LSTM and GRU gates); 2) variational RNN dropout (Gal & Ghahramani, 2016); 3) recurrent DropConnect (Merity et al., 2018b); and 4) history compression and repackaging. From convolutional networks, we can adapt 1) larger kernels and dilated convolutions (Yu & Koltun, 2016); 2) auxiliary losses at intermediate layers (Lee et al., 2015; Xie & Tu, 2015); 3) weight normalization (Salimans & Kingma, 2016); and 4) parallel convolutional processing. Being able to directly incorporate techniques from both streams of research is one of the benefits of trellis networks. We leverage this in our experiments and provide a more comprehensive treatment of these adaptations in Appendix B.

6

Under review as a conference paper at ICLR 2019

5 EXPERIMENTS
5.1 A TRELLISNET WITH GATED ACTIVATION

zt(+i+11) <latexit sha1_base64="+VtluC+UvZ7AXfnV6T+0WUjJZNw=">AAAB+HicbVDLSgNBEOz1GeMjqx69DAYhEgg7IiieAl48RjAPSNZldjJJhsw+mJkVkmW/xIsHRbz6Kd78GyfJHjSxoKGo6qa7y48FV9pxvq219Y3Nre3CTnF3b/+gZB8etVSUSMqaNBKR7PhEMcFD1tRcC9aJJSOBL1jbH9/O/PYTk4pH4YOexMwNyDDkA06JNpJnl6Zeqqs4e0wrvIrPM88uOzVnDrRKcE7KkKPh2V+9fkSTgIWaCqJUFzuxdlMiNaeCZcVeolhM6JgMWdfQkARMuen88AydGaWPBpE0FWo0V39PpCRQahL4pjMgeqSWvZn4n9dN9ODaTXkYJ5qFdLFokAikIzRLAfW5ZFSLiSGESm5uRXREJKHaZFU0IeDll1dJ66KGnRq+vyzXb/I4CnACp1ABDFdQhztoQBMoJPAMr/BmTa0X6936WLSuWfnMMfyB9fkDeZiSRg==</latexit>

zt(+i+11,1) <latexit sha1_base64="HwsUkBgByuOgfWaANPnhipbeukE=">AAAB+nicbVDLSsNAFJ34rPWV6tLNYBEqlZIRQd0V3LisYB/QxjCZTtqhk0mYmSg15lPcuFDErV/izr9x2mahrQcuHM65l3vv8WPOlHacb2tpeWV1bb2wUdzc2t7ZtUt7LRUlktAmiXgkOz5WlDNBm5ppTjuxpDj0OW37o6uJ376nUrFI3OpxTN0QDwQLGMHaSJ5devRSXUUnKLtLK6yKjjPPLjs1Zwq4SFBOyiBHw7O/ev2IJCEVmnCsVBc5sXZTLDUjnGbFXqJojMkID2jXUIFDqtx0enoGj4zSh0EkTQkNp+rviRSHSo1D33SGWA/VvDcR//O6iQ4u3JSJONFUkNmiIOFQR3CSA+wzSYnmY0MwkczcCskQS0y0SatoQkDzLy+S1mkNOTV0c1auX+ZxFMABOAQVgMA5qINr0ABNQMADeAav4M16sl6sd+tj1rpk5TP74A+szx9YR5K2</latexit>
zt(+i+11,2) <latexit sha1_base64="G0XcPH55M3XTZdbxur3zMiK0otc=">AAAB+nicbVDLSgMxFM3UV62vqS7dBItQqZRJEdRdwY3LCvYB7Thk0rQNzWSGJKPUcT7FjQtF3Pol7vwb03YW2nrgwuGce7n3Hj/iTGnH+bZyK6tr6xv5zcLW9s7unl3cb6kwloQ2SchD2fGxopwJ2tRMc9qJJMWBz2nbH19N/fY9lYqF4lZPIuoGeCjYgBGsjeTZxUcv0RV0WkvvkjKroJPUs0tO1ZkBLhOUkRLI0PDsr14/JHFAhSYcK9VFTqTdBEvNCKdpoRcrGmEyxkPaNVTggCo3mZ2ewmOj9OEglKaEhjP190SCA6UmgW86A6xHatGbiv953VgPLtyEiSjWVJD5okHMoQ7hNAfYZ5ISzSeGYCKZuRWSEZaYaJNWwYSAFl9eJq1aFTlVdHNWql9mceTBITgCZYDAOaiDa9AATUDAA3gGr+DNerJerHfrY96as7KZA/AH1ucPWdSStw==</latexit>

<latexit sha1_base64="lSOnCFWQCuvxbU0oQl4fw4kzbuc=">AAACGHicbVDLSsNAFJ34rPUVdelmsAgVpSYiqLuCG5cV7AOaWCbTaTt0MgkzN0IN+Qw3/oobF4q47c6/cfpY2NYDFw7n3Mu99wSx4Boc58daWl5ZXVvPbeQ3t7Z3du29/ZqOEkVZlUYiUo2AaCa4ZFXgIFgjVoyEgWD1oH878utPTGkeyQcYxMwPSVfyDqcEjNSyzz3NuyHBHuWK4udWCmdu9pgW+UmGT/GM6QGRvZZdcErOGHiRuFNSQFNUWvbQa0c0CZkEKojWTdeJwU+JAk4Fy/JeollMaJ90WdNQSUKm/XT8WIaPjdLGnUiZkoDH6t+JlIRaD8LAdIYEenreG4n/ec0EOtd+ymWcAJN0sqiTCAwRHqWE21wxCmJgCKGKm1sx7RFFKJgs8yYEd/7lRVK7KLlOyb2/LJRvpnHk0CE6QkXkoitURneogqqIohf0hj7Qp/VqvVtf1vekdcmazhygGVjDX8dSnvA=</latexit>

zt(,i1) +

tanh

In our description of generic trellis networks in Section 3, the
activation function f can be any nonlinearity that computes z1(i:T+1) based on z^1(i:T+1) and z1(i:T) -1. In experiments, we use a gated activation based on the LSTM cell. Gated activations have

z^t(+i+11) <latexit sha1_base64="Am4axvpT/RbzZdU3VQKPoUt9hZE=">AAAB/nicbVDLSgNBEJz1GeNrVTx5WQxCJBB2RFA8Bbx4jGAekKxhdjKbDJl9MNMrxGHBX/HiQRGvfoc3/8ZJsgdNLGgoqrrp7vITwRW47re1tLyyurZe2Chubm3v7Np7+00Vp5KyBo1FLNs+UUzwiDWAg2DtRDIS+oK1/NH1xG89MKl4HN3BOGFeSAYRDzglYKSefdgdEtCPWU9DBWf3uswr+DTr2SW36k7hLBKckxLKUe/ZX91+TNOQRUAFUaqD3QQ8TSRwKlhW7KaKJYSOyIB1DI1IyJSnp+dnzolR+k4QS1MROFP194QmoVLj0DedIYGhmvcm4n9eJ4Xg0tM8SlJgEZ0tClLhQOxMsnD6XDIKYmwIoZKbWx06JJJQMIkVTQh4/uVF0jyrYreKb89Ltas8jgI6QseojDC6QDV0g+qogSjS6Bm9ojfryXqx3q2PWeuSlc8coD+wPn8AaO2VEw==</latexit>

been used before in convolutional networks for sequence modeling (van den Oord et al., 2016; Dauphin et al., 2017). Our choice is inspired directly by Theorem 1, which suggests incorporating

i!i+1

xt
<latexit sha1_base64="eEPz6pgZfvj6K8i8F78InKVPN1o=">AAAB6nicbVA9SwNBEJ2LXzF+RS1tFoNgFe5EiIVFwMYyovmA5Ah7m71kyd7esTsnhiM/wcZCEVt/kZ3/xk1yhSY+GHi8N8PMvCCRwqDrfjuFtfWNza3idmlnd2//oHx41DJxqhlvsljGuhNQw6VQvIkCJe8kmtMokLwdjG9mfvuRayNi9YCThPsRHSoRCkbRSvdPfeyXK27VnYOsEi8nFcjR6Je/eoOYpRFXyCQ1puu5CfoZ1SiY5NNSLzU8oWxMh7xrqaIRN342P3VKzqwyIGGsbSkkc/X3REYjYyZRYDsjiiOz7M3E/7xuiuGVnwmVpMgVWywKU0kwJrO/yUBozlBOLKFMC3srYSOqKUObTsmG4C2/vEpaF1XPrXp3l5X6dR5HEU7gFM7BgxrU4RYa0AQGQ3iGV3hzpPPivDsfi9aCk88cwx84nz9vbo3d</latexit>

W1 <latexit sha1_base64="lXMe2VVJVimguU04W2823Zlih1M=">AAAB6nicbVBNS8NAEJ3Ur1q/qh69LBbBU0lEUDwVvHisaD+gDWWz3bRLN5uwOxFK6E/w4kERr/4ib/4bt2kO2vpg4PHeDDPzgkQKg6777ZTW1jc2t8rblZ3dvf2D6uFR28SpZrzFYhnrbkANl0LxFgqUvJtoTqNA8k4wuZ37nSeujYjVI04T7kd0pEQoGEUrPXQG3qBac+tuDrJKvILUoEBzUP3qD2OWRlwhk9SYnucm6GdUo2CSzyr91PCEsgkd8Z6likbc+Fl+6oycWWVIwljbUkhy9fdERiNjplFgOyOKY7PszcX/vF6K4bWfCZWkyBVbLApTSTAm87/JUGjOUE4toUwLeythY6opQ5tOxYbgLb+8StoXdc+te/eXtcZNEUcZTuAUzsGDK2jAHTShBQxG8Ayv8OZI58V5dz4WrSWnmDmGP3A+fwDW8413</latexit>

W2 <latexit sha1_base64="uQ/4VJ3D9Rsf5zu3tZUkUWQBkuo=">AAAB6nicbVBNS8NAEJ3Ur1q/qh69LBbBU0mKUPFU8OKxov2ANpTNdtIu3WzC7kYooT/BiwdFvPqLvPlv3LY5aOuDgcd7M8zMCxLBtXHdb6ewsbm1vVPcLe3tHxwelY9P2jpOFcMWi0WsugHVKLjEluFGYDdRSKNAYCeY3M79zhMqzWP5aKYJ+hEdSR5yRo2VHjqD2qBccavuAmSdeDmpQI7moPzVH8YsjVAaJqjWPc9NjJ9RZTgTOCv1U40JZRM6wp6lkkao/Wxx6oxcWGVIwljZkoYs1N8TGY20nkaB7YyoGetVby7+5/VSE177GZdJalCy5aIwFcTEZP43GXKFzIipJZQpbm8lbEwVZcamU7IheKsvr5N2req5Ve/+qtK4yeMowhmcwyV4UIcG3EETWsBgBM/wCm+OcF6cd+dj2Vpw8plT+APn8wfYd414</latexit>
xt+1 <latexit sha1_base64="nkvIntHmKtHH1HtvVwUH5BQy280=">AAAB7nicbVBNS8NAEJ3Ur1q/qh69LBZBEEoiBT14KHjxWMF+QBvKZrtpl242YXciltAf4cWDIl79Pd78N27bHLT1wcDjvRlm5gWJFAZd99sprK1vbG4Vt0s7u3v7B+XDo5aJU814k8Uy1p2AGi6F4k0UKHkn0ZxGgeTtYHw789uPXBsRqwecJNyP6FCJUDCKVmo/9TO88Kb9csWtunOQVeLlpAI5Gv3yV28QszTiCpmkxnQ9N0E/oxoFk3xa6qWGJ5SN6ZB3LVU04sbP5udOyZlVBiSMtS2FZK7+nshoZMwkCmxnRHFklr2Z+J/XTTG89jOhkhS5YotFYSoJxmT2OxkIzRnKiSWUaWFvJWxENWVoEyrZELzll1dJ67LquVXvvlap3+RxFOEETuEcPLiCOtxBA5rAYAzP8ApvTuK8OO/Ox6K14OQzx/AHzucPDcKPWQ==</latexit>

an existing RNN cell into TrellisNet. We use the LSTM cell due to its effectiveness in recurrent networks (Jozefowicz et al., 2015; Greff et al., 2017; Melis et al., 2018). We summarize the construc-

Layer <latexit sha1_base64="Omx146Fv69pKxCTzcUXdA+GkZwY=">AAACAnicbVBNS8NAEN3Ur1q/op7Ey2IrCEJJelE8Fbx48FDBfkAbyma7aZdusmF3ooRQvPhXvHhQxKu/wpv/xm2bg1YfDDzem2Fmnh8LrsFxvqzC0vLK6lpxvbSxubW9Y+/utbRMFGVNKoVUHZ9oJnjEmsBBsE6sGAl9wdr++HLqt++Y0lxGt5DGzAvJMOIBpwSM1LcPrknKFK5w3FN8OAKilLzH/NSt9O2yU3VmwH+Jm5MyytHo25+9gaRJyCKggmjddZ0YvIwo4FSwSamXaBYTOiZD1jU0IiHTXjZ7YYKPjTLAgVSmIsAz9edERkKt09A3nSGBkV70puJ/XjeB4NzLeBQnwCI6XxQkAoPE0zzwgCtGQaSGEKq4uRXTEVGEgkmtZEJwF1/+S1q1qutU3ZtauX6Rx1FEh+gInSAXnaE6ukIN1EQUPaAn9IJerUfr2Xqz3uetBSuf2Ue/YH18A14BlhM=</latexit>

zt(i) <latexit sha1_base64="2svhF9CpH3GHMAPuUCL8JxYJANA=">AAAB8nicbVBNS8NAEN34WetX1aOXxSLUS0lEUDwVvHisYD8gjWWz3bRLN7thdyLUkJ/hxYMiXv013vw3btsctPXBwOO9GWbmhYngBlz321lZXVvf2Cxtlbd3dvf2KweHbaNSTVmLKqF0NySGCS5ZCzgI1k00I3EoWCcc30z9ziPThit5D5OEBTEZSh5xSsBK/lM/g/whq/GzvF+punV3BrxMvIJUUYFmv/LVGyiaxkwCFcQY33MTCDKigVPB8nIvNSwhdEyGzLdUkpiZIJudnONTqwxwpLQtCXim/p7ISGzMJA5tZ0xgZBa9qfif56cQXQUZl0kKTNL5oigVGBSe/o8HXDMKYmIJoZrbWzEdEU0o2JTKNgRv8eVl0j6ve27du7uoNq6LOEroGJ2gGvLQJWqgW9RELUSRQs/oFb054Lw4787HvHXFKWaO0B84nz9HwZE1</latexit>

zt(,i1) <latexit sha1_base64="hEqsC+awpPdZyeIQrbA/Py+DhL4=">AAAB9HicbVDLSgNBEOyNrxhfUY9eBoMQQcKuCOot4MVjBPOAZA2zk0kyZPbhTG8gLvsdXjwo4tWP8ebfOEn2oIkFDUVVN91dXiSFRtv+tnIrq2vrG/nNwtb2zu5ecf+gocNYMV5noQxVy6OaSxHwOgqUvBUpTn1P8qY3upn6zTFXWoTBPU4i7vp0EIi+YBSN5D51Ezxz0oekLE7TbrFkV+wZyDJxMlKCDLVu8avTC1ns8wCZpFq3HTtCN6EKBZM8LXRizSPKRnTA24YG1OfaTWZHp+TEKD3SD5WpAMlM/T2RUF/rie+ZTp/iUC96U/E/rx1j/8pNRBDFyAM2X9SPJcGQTBMgPaE4QzkxhDIlzK2EDamiDE1OBROCs/jyMmmcVxy74txdlKrXWRx5OIJjKIMDl1CFW6hBHRg8wjO8wps1tl6sd+tj3pqzsplD+APr8wcksJGl</latexit>
zt(,i2) <latexit sha1_base64="CM90o1WRrsRKWE4t3BC62PomYXs=">AAAB9HicbVDLSgNBEOz1GeMr6tHLYBAiSNgNgnoLePEYwTwgWcPsZDYZMvtwpjcQl3yHFw+KePVjvPk3TpI9aGJBQ1HVTXeXF0uh0ba/rZXVtfWNzdxWfntnd2+/cHDY0FGiGK+zSEaq5VHNpQh5HQVK3ooVp4EnedMb3kz95ogrLaLwHscxdwPaD4UvGEUjuU/dFM8rk4e0JM4m3ULRLtszkGXiZKQIGWrdwlenF7Ek4CEySbVuO3aMbkoVCib5JN9JNI8pG9I+bxsa0oBrN50dPSGnRukRP1KmQiQz9fdESgOtx4FnOgOKA73oTcX/vHaC/pWbijBOkIdsvshPJMGITBMgPaE4Qzk2hDIlzK2EDaiiDE1OeROCs/jyMmlUyo5ddu4uitXrLI4cHMMJlMCBS6jCLdSgDgwe4Rle4c0aWS/Wu/Uxb12xspkj+APr8wcmO5Gm</latexit>

zt(+i)1 <latexit sha1_base64="LuZCTtj0QRFI4tTxozbliK7Tzjg=">AAAB9HicbVDLSgNBEOyNrxhfUY9eBoMQEcKuCIqngBePEcwDkjXMTibJkNmHM72BuOx3ePGgiFc/xpt/4yTZgyYWNBRV3XR3eZEUGm3728qtrK6tb+Q3C1vbO7t7xf2Dhg5jxXidhTJULY9qLkXA6yhQ8lakOPU9yZve6GbqN8dcaREG9ziJuOvTQSD6glE0kvvUTfDMSR+SsjhNu8WSXbFnIMvEyUgJMtS6xa9OL2SxzwNkkmrdduwI3YQqFEzytNCJNY8oG9EBbxsaUJ9rN5kdnZITo/RIP1SmAiQz9fdEQn2tJ75nOn2KQ73oTcX/vHaM/Ss3EUEUIw/YfFE/lgRDMk2A9ITiDOXEEMqUMLcSNqSKMjQ5FUwIzuLLy6RxXnHsinN3UapeZ3Hk4QiOoQwOXEIVbqEGdWDwCM/wCm/W2Hqx3q2PeWvOymYO4Q+szx8jcZGl</latexit>

zt(+i)1,1 <latexit sha1_base64="3QxhVQRV4MPfAQCGJeDKkKtO4Q8=">AAAB+HicbVDLSsNAFL2pr1ofjbp0M1iEilISEdRdwY3LCvYBbQyT6aQdOpmEmYnQhnyJGxeKuPVT3Pk3Th8LrR64cDjnXu69J0g4U9pxvqzCyura+kZxs7S1vbNbtvf2WypOJaFNEvNYdgKsKGeCNjXTnHYSSXEUcNoORjdTv/1IpWKxuNfjhHoRHggWMoK1kXy7PPEzfeqeuflDVmUnuW9XnJozA/pL3AWpwAIN3/7s9WOSRlRowrFSXddJtJdhqRnhNC/1UkUTTEZ4QLuGChxR5WWzw3N0bJQ+CmNpSmg0U39OZDhSahwFpjPCeqiWvan4n9dNdXjlZUwkqaaCzBeFKUc6RtMUUJ9JSjQfG4KJZOZWRIZYYqJNViUTgrv88l/SOq+5Ts29u6jUrxdxFOEQjqAKLlxCHW6hAU0gkMITvMCrNbGerTfrfd5asBYzB/AL1sc3eNWSRg==</latexit>
zt(+i)1,2 <latexit sha1_base64="jJAvqfFQwy/CDu8cCn5IwVKXhnU=">AAAB+HicbVDLSsNAFJ3UV62PRl26GSxCRSlJEdRdwY3LCvYBbQyT6aQdOnkwcyO0IV/ixoUibv0Ud/6N0zYLbT1w4XDOvdx7jxcLrsCyvo3C2vrG5lZxu7Szu7dfNg8O2ypKJGUtGolIdj2imOAhawEHwbqxZCTwBOt449uZ33liUvEofIBJzJyADEPuc0pAS65ZnropnNsX9ewxrfKzzDUrVs2aA68SOycVlKPpml/9QUSTgIVABVGqZ1sxOCmRwKlgWamfKBYTOiZD1tM0JAFTTjo/PMOnWhlgP5K6QsBz9fdESgKlJoGnOwMCI7XszcT/vF4C/rWT8jBOgIV0schPBIYIz1LAAy4ZBTHRhFDJ9a2YjogkFHRWJR2CvfzyKmnXa7ZVs+8vK42bPI4iOkYnqIpsdIUa6A41UQtRlKBn9IrejKnxYrwbH4vWgpHPHKE/MD5/AHpgkkc=</latexit>

Figure 3: A gated activation based on the LSTM cell.

tion here; a more detailed treatment can be found in Appendix A.
In an LSTM cell, three information-controlling gates are computed at time t. Moreover, there is a cell state that does not participate in the hidden-to-hidden transformations but is updated in

every step using the result from the gated activations. We integrate the LSTM cell into the TrellisNet

as follows (Figure 3):

z^t(+i+11) = W1

xt zt(,i2)

+ W2

xt+1 zt(+i)1,2

= [z^t+1,1

z^t+1,2

z^t+1,3

z^t+1,4]

(12)

zt(+i+11,1) = (z^t+1,1)  zt(,i1) + (z^t+1,2)  tanh(z^t+1,3) zt(+i+11,2) = (z^t+1,4)  tanh(zt(+i+11,1))

(13; Gated activation f )

Thus the linear transformation in each layer of the TrellisNet produces a pre-activation feature

z^t+1 with r = 4q feature channels, which are then processed by elementwise transformations and Hadamard products to yield the final output zt(+i+11) = zt(+i+11,1), zt(+i+11,2) of the layer.

5.2 RESULTS
We evaluate trellis networks on word-level and character-level language modeling on the standard Penn Treebank (PTB) dataset (Marcus et al., 1993; Mikolov et al., 2010), large-scale word-level modeling on WikiText-103 (WT103) (Merity et al., 2017), and standard stress tests used to study long-range information propagation in sequence models: sequential MNIST, permuted MNIST (PMNIST), and sequential CIFAR-10 (Chang et al., 2017; Bai et al., 2018; Trinh et al., 2018). Note that these tasks are on very different scales, with unique properties that challenge sequence models in different ways. For example, word-level PTB is a small dataset that a typical model easily overfits, so judicious regularization is essential. WT103 is a hundred times larger, with less danger of overfitting, but with a vocabulary size of 268K that makes training more challenging (and precludes the application of techniques such mixture of softmaxes (Yang et al., 2018)). A more complete description of these tasks and their characteristics can be found in Appendix C.
The prior state of the art on these tasks was set by completely different models, such as AWD-LSTM on character-level PTB (Merity et al., 2018a), neural architecture search on word-level PTB (Pham et al., 2018), and the self-attention-based Relational Memory Core on WikiText-103 (Santoro et al., 2018). We use trellis networks on all tasks and outperform the respective state-of-the-art models on each. For example, on word-level Penn Treebank, TrellisNet outperforms by a good margin the recent results of Melis et al. (2018), which used the Google Vizier service for exhaustive hyperparameter tuning, as well as the recent neural architecture search work of Pham et al. (2018). On WikiText-103, a trellis network outperforms by 4% the Relational Memory Core (Santoro et al., 2018) and by 8% the thorough optimization work of Merity et al. (2018a).
Many hyperparameters we use are adapted directly from prior work on recurrent networks. (As highlighted in Section 4.3, many techniques can be carried over directly from RNNs). For others, we perform a basic grid search. We decay the learning rate by a fixed factor once validation error plateaus. All hyperparameters are reported in Appendix D, along with an ablation study. A complete implementation will be released.

7

Under review as a conference paper at ICLR 2019

Table 1: Test perplexities (ppl) on word-level language modeling with the Penn Treebank (PTB) corpus (with and without mixture of softmaxes (MoS) (Yang et al., 2018)). means lower is better.

Word-level PTB without MoS

Model

Size Test ppl

Word-level PTB with MoS

Model

Size Test ppl

NAS Cell (Zoph & Le, 2017) AWD-LSTM (Merity et al., 2018b) (Black-box tuned) NAS (Melis et al., 2018) (Black-box tuned) LSTM + skip conn.
(Melis et al., 2018)
Ours - TrellisNet Ours - TrellisNet (1.4x larger)

54M 24M 24M
24M
24M 33M

62.4 58.8 59.7
58.3
56.97 56.80

AWD-LSTM-MoC (Yang et al., 2018) AWD-LSTM-MoS (Yang et al., 2018)
DARTS (Liu et al., 2018)

22M 24M 23M

ENAS (Pham et al., 2018)

24M

Ours - TrellisNet-MoS

25M

Ours - TrellisNet-MoS (1.4x larger) 34M

57.55 55.97 56.10
55.80
54.67 54.19

Table 2: Test perplexities (ppl) on word-level WikiText-103 and test bits-per-character (bpc) on character-level Penn Treebank. means lower is better.

Word-level WikiText-103

Model

Size Test ppl Epo.

Character-level PTB

Model

Size Test bpc

LSTM (Grave et al., 2017b) LSTM+cont. cache (Grave et al., 2017b)
Generic TCN (Bai et al., 2018) Gated Linear ConvNet (Dauphin et al., 2017)
AWD-QRNN (Merity et al., 2018a) Relational Memory Core (Santoro et al., 2018)
Ours - TrellisNet

150M 230M 159M 195M 180M

48.7 40.8 45.2 37.2 33.0 31.6 30.35

60 24 90 22

IndRNN (Li et al., 2018) Hyper LSTM (Ha et al., 2017) NAS Cell (Zoph & Le, 2017) FS-LSTM-2 (Mujika et al., 2017) Quasi-RNN (Merity et al., 2018a) AWD-LSTM (Merity et al., 2018a)
Ours - TrellisNet

12.0M 14.4M 16.3M 7.2M 13.8M 13.8M 13.4M

1.23 1.219 1.214 1.19 1.187 1.175 1.159

Word-level language modeling. For word-level language modeling, we use PTB and WT103. The results on PTB are listed in Table 1. TrellisNet sets a new state of the art on PTB, both with and without mixture of softmaxes (Yang et al., 2018), outperforming all previously published results by more than one unit of perplexity.
WT103 is 110 times larger than PTB, with vocabulary size 268K. We follow prior work and use the adaptive softmax (Grave et al., 2017a), which improves memory efficiency by assigning higher capacity to more frequent words. The results are listed in Table 2 (left). TrellisNet sets a new state of the art on this dataset as well, with perplexity 30.35: about 4% better than the contemporaneous self-attention-based Relational Memory Core (RMC) (Santoro et al., 2018). TrellisNet achieves this better accuracy with much faster convergence: 22 epochs, versus 90 for RMC.
Character-level language modeling. When used for character-level modeling, PTB is a mediumscale dataset. We thus use a deeper network as well as techniques such as weight normalization (Salimans & Kingma, 2016) and deep supervision (Lee et al., 2015; Xie & Tu, 2015). The results are listed in Table 2 (right). TrellisNet sets a new state of the art, outperforming the recent results of Merity et al. (2018a) by a comfortable margin.
Long-range modeling with Sequential MNIST, PMNIST, and CIFAR-10. We also evaluate the TrellisNet for ability to model long-term dependencies. In the Sequential MNIST, PMNIST, and CIFAR-10 tasks, images are processed as long sequences, one pixel at a time (Chang et al., 2017; Bai et al., 2018; Trinh et al., 2018). Our model has 8M parameters, in alignment with prior work. To cover the larger context, we use dilated convolutions in intermediate layers, adopting a common architectural element from TCNs (Yu & Koltun, 2016; van den Oord et al., 2016; Bai et al., 2018). The results are listed in Table 3. Note that the performance of prior models is inconsistent. The Transformer works well on MNIST but fairs poorly on CIFAR-10, while r-LSTM with unsupervised auxiliary losses achieves good results on CIFAR-10 but underperforms on Permuted MNIST. TrellisNet outperforms all these models on all three tasks.

6 DISCUSSION
We presented trellis networks, a new architecture for sequence modeling. Trellis networks form a structural bridge between convolutional and recurrent models. This enables direct assimilation of many techniques designed for either of these two architectural families. We leverage these connections to train high-performing trellis networks that set a new state of the art on highly competitive language modeling benchmarks. Beyond the empirical gains, we hope that trellis networks will serve as a step towards deeper and more unified understanding of sequence modeling.

8

Under review as a conference paper at ICLR 2019

Table 3: Test accuracies on long-range modeling benchmarks. h means higher is better.

Model

Seq. MNIST Permuted MNIST Seq. CIFAR-10

Test acc.h

Test acc.h

Test acc.h

Dilated GRU (Chang et al., 2017) IndRNN (Li et al., 2018)
Generic TCN (Bai et al., 2018) r-LSTM w/ Aux. Loss (Trinh et al., 2018) Transformer (self-attention) (Trinh et al., 2018)
Ours - TrellisNet

99.0 99.0 99.0 98.4 98.9 99.20

94.6 96.0 97.2 95.2 97.9 98.13

72.2 62.2 73.42

There are many exciting opportunities for future work. First, we have not conducted thorough performance optimizations on trellis networks. For example, architecture search on the structure of the gated activation f may yield a higher-performing activation function than the classic LSTM cell we used (Zoph & Le, 2017; Pham et al., 2018). Likewise, principled hyperparameter tuning will likely improve modeling accuracy beyond the levels we have observed (Melis et al., 2018). Future work can also explore acceleration schemes that speed up training and inference. Another significant opportunity is to establish connections between trellis networks and self-attention-based architectures (Transformers) (Vaswani et al., 2017; Santoro et al., 2018; Chen et al., 2018), thus unifying all three major contemporary approaches to sequence modeling. Finally, we look forward to seeing applications of trellis networks to industrial-scale challenges such as machine translation.
REFERENCES
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. In International Conference on Learning Representations (ICLR), 2015.
Shaojie Bai, J Zico Kolter, and Vladlen Koltun. An empirical evaluation of generic convolutional and recurrent networks for sequence modeling. arXiv:1803.01271, 2018.
James Bradbury, Stephen Merity, Caiming Xiong, and Richard Socher. Quasi-recurrent neural networks. In International Conference on Learning Representations (ICLR), 2017.
Shiyu Chang, Yang Zhang, Wei Han, Mo Yu, Xiaoxiao Guo, Wei Tan, Xiaodong Cui, Michael Witbrock, Mark Hasegawa-Johnson, and Thomas Huang. Dilated recurrent neural networks. In Neural Information Processing Systems (NIPS), 2017.
Mia Xu Chen, Orhan Firat, Ankur Bapna, Melvin Johnson, Wolfgang Macherey, George Foster, Llion Jones, Niki Parmar, Noam Shazeer, Ashish Vaswani, Jakob Uszkoreit, Lukasz Kaiser, Mike Schuster, Zhifeng Chen, Yonghui Wu, and Macduff Hughes. The best of both worlds: Combining recent advances in neural machine translation. In Annual Meeting of the Association for Computational Linguistics (ACL), 2018.
Kyunghyun Cho, Bart Van Merrie¨nboer, Dzmitry Bahdanau, and Yoshua Bengio. On the properties of neural machine translation: Encoder-decoder approaches. arXiv:1409.1259, 2014.
Ronan Collobert, Jason Weston, Le´on Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel P. Kuksa. Natural language processing (almost) from scratch. Journal of Machine Learning Research (JMLR), 12, 2011.
Yann N. Dauphin, Angela Fan, Michael Auli, and David Grangier. Language modeling with gated convolutional networks. In International Conference on Machine Learning (ICML), 2017.
Jeff Donahue, Lisa Anne Hendricks, Sergio Guadarrama, Marcus Rohrbach, Subhashini Venugopalan, Trevor Darrell, and Kate Saenko. Long-term recurrent convolutional networks for visual recognition and description. In Computer Vision and Pattern Recognition (CVPR), 2015.
Jeffrey L Elman. Finding structure in time. Cognitive Science, 14(2), 1990.
Yarin Gal and Zoubin Ghahramani. A theoretically grounded application of dropout in recurrent neural networks. In Neural Information Processing Systems (NIPS), 2016.
Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolutional sequence to sequence learning. In International Conference on Machine Learning (ICML), 2017.

9

Under review as a conference paper at ICLR 2019
Edouard Grave, Armand Joulin, Moustapha Cisse´, David Grangier, and Herve´ Je´gou. Efficient softmax approximation for GPUs. In International Conference on Machine Learning (ICML), 2017a.
Edouard Grave, Armand Joulin, and Nicolas Usunier. Improving neural language models with a continuous cache. In International Conference on Learning Representations (ICLR), 2017b.
Alex Graves. Supervised Sequence Labelling with Recurrent Neural Networks. Springer, 2012.
Alex Graves. Generating sequences with recurrent neural networks. arXiv:1308.0850, 2013.
Klaus Greff, Rupesh Kumar Srivastava, Jan Koutn´ik, Bas R. Steunebrink, and Ju¨rgen Schmidhuber. LSTM: A search space odyssey. IEEE Transactions on Neural Networks and Learning Systems, 28(10), 2017.
David Ha, Andrew Dai, and Quoc V Le. HyperNetworks. In International Conference on Learning Representations (ICLR), 2017.
Sepp Hochreiter and Ju¨rgen Schmidhuber. Long short-term memory. Neural Computation, 9(8), 1997.
Rafal Jozefowicz, Wojciech Zaremba, and Ilya Sutskever. An empirical exploration of recurrent network architectures. In International Conference on Machine Learning (ICML), 2015.
Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aa¨ron van den Oord, Alex Graves, and Koray Kavukcuoglu. Neural machine translation in linear time. arXiv:1610.10099, 2016.
Andrej Karpathy and Fei-Fei Li. Deep visual-semantic alignments for generating image descriptions. In Computer Vision and Pattern Recognition (CVPR), 2015.
Urvashi Khandelwal, He He, Peng Qi, and Dan Jurafsky. Sharp nearby, fuzzy far away: How neural language models use context. In Annual Meeting of the Association for Computational Linguistics (ACL), 2018.
Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. Technical report, University of Toronto, 2009.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. ImageNet classification with deep convolutional neural networks. In Neural Information Processing Systems (NIPS), 2012.
Yann LeCun, Bernhard Boser, John S. Denker, Donnie Henderson, Richard E. Howard, Wayne Hubbard, and Lawrence D. Jackel. Backpropagation applied to handwritten zip code recognition. Neural Computation, 1(4), 1989.
Chen-Yu Lee, Saining Xie, Patrick Gallagher, Zhengyou Zhang, and Zhuowen Tu. Deeplysupervised nets. In AISTATS, 2015.
Shuai Li, Wanqing Li, Chris Cook, Ce Zhu, and Yanbo Gao. Independently recurrent neural network (IndRNN): Building a longer and deeper RNN. In Computer Vision and Pattern Recognition (CVPR), 2018.
Hanxiao Liu, Karen Simonyan, and Yiming Yang. DARTS: Differentiable architecture search. arXiv:1806.09055, 2018.
Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated corpus of English: The Penn treebank. Computational Linguistics, 19(2), 1993.
Ga´bor Melis, Chris Dyer, and Phil Blunsom. On the state of the art of evaluation in neural language models. In International Conference on Learning Representations (ICLR), 2018.
Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models. In International Conference on Learning Representations (ICLR), 2017.
Stephen Merity, Nitish Shirish Keskar, and Richard Socher. An analysis of neural language modeling at multiple scales. arXiv:1803.08240, 2018a.
10

Under review as a conference paper at ICLR 2019
Stephen Merity, Nitish Shirish Keskar, and Richard Socher. Regularizing and optimizing LSTM language models. In International Conference on Learning Representations (ICLR), 2018b.
Tomas Mikolov, Martin Karafia´t, Luka´s Burget, Jan Cernocky´, and Sanjeev Khudanpur. Recurrent neural network based language model. In Interspeech, 2010.
John Miller and Moritz Hardt. When recurrent models don't need to be recurrent. arXiv:1805.10369, 2018.
Yasumasa Miyamoto and Kyunghyun Cho. Gated word-character recurrent language model. arXiv:1606.01700, 2016.
Asier Mujika, Florian Meier, and Angelika Steger. Fast-slow recurrent neural networks. In Neural Information Processing Systems (NIPS), 2017.
Hieu Pham, Melody Y Guan, Barret Zoph, Quoc V Le, and Jeff Dean. Efficient neural architecture search via parameters sharing. In International Conference on Machine Learning (ICML), 2018.
Tara N. Sainath, Oriol Vinyals, Andrew W. Senior, and Hasim Sak. Convolutional, long short-term memory, fully connected deep neural networks. In International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2015.
Tim Salimans and Diederik P Kingma. Weight normalization: A simple reparameterization to accelerate training of deep neural networks. In Neural Information Processing Systems (NIPS), 2016.
Adam Santoro, Ryan Faulkner, David Raposo, Jack Rae, Mike Chrzanowski, Theophane Weber, Daan Wierstra, Oriol Vinyals, Razvan Pascanu, and Timothy Lillicrap. Relational recurrent neural networks. In Neural Information Processing Systems (NIPS), 2018.
Xingjian Shi, Zhourong Chen, Hao Wang, Dit-Yan Yeung, Wai-Kin Wong, and Wang-chun Woo. Convolutional LSTM network: A machine learning approach for precipitation nowcasting. In Neural Information Processing Systems (NIPS), 2015.
Ilya Sutskever, James Martens, and Geoffrey E. Hinton. Generating text with recurrent neural networks. In International Conference on Machine Learning (ICML), 2011.
Ilya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to sequence learning with neural networks. In Neural Information Processing Systems (NIPS), 2014.
Trieu H Trinh, Andrew M Dai, Thang Luong, and Quoc V Le. Learning longer-term dependencies in RNNs with auxiliary losses. In International Conference on Machine Learning (ICML), 2018.
Aa¨ron van den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves, Nal Kalchbrenner, Andrew W. Senior, and Koray Kavukcuoglu. WaveNet: A generative model for raw audio. arXiv:1609.03499, 2016.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Neural Information Processing Systems (NIPS), 2017.
Subhashini Venugopalan, Marcus Rohrbach, Jeffrey Donahue, Raymond J. Mooney, Trevor Darrell, and Kate Saenko. Sequence to sequence ­ video to text. In International Conference on Computer Vision (ICCV), 2015.
Oriol Vinyals, Alexander Toshev, Samy Bengio, and Dumitru Erhan. Show and tell: A neural image caption generator. In Computer Vision and Pattern Recognition (CVPR), 2015.
Christoph Vogel and Thomas Pock. A primal dual network for low-level vision problems. In German Conference on Pattern Recognition, 2017.
Alex Waibel, Toshiyuki Hanazawa, Geoffrey Hinton, Kiyohiro Shikano, and Kevin J Lang. Phoneme recognition using time-delay neural networks. IEEE Transactions on Acoustics, Speech, and Signal Processing, 37(3), 1989.
11

Under review as a conference paper at ICLR 2019 Paul J Werbos. Backpropagation through time: What it does and how to do it. Proceedings of the
IEEE, 78(10), 1990. Saining Xie and Zhuowen Tu. Holistically-nested edge detection. In International Conference on
Computer Vision (ICCV), 2015. Zhilin Yang, Zihang Dai, Ruslan Salakhutdinov, and William W. Cohen. Breaking the softmax
bottleneck: A high-rank RNN language model. International Conference on Learning Representations (ICLR), 2018. Fisher Yu and Vladlen Koltun. Multi-scale context aggregation by dilated convolutions. In International Conference on Learning Representations (ICLR), 2016. Julian Georg Zilly, Rupesh Kumar Srivastava, Jan Koutn´ik, and Ju¨rgen Schmidhuber. Recurrent highway networks. In International Conference on Machine Learning (ICML), 2017. Barret Zoph and Quoc V Le. Neural architecture search with reinforcement learning. In International Conference on Learning Representations (ICLR), 2017.
12

Under review as a conference paper at ICLR 2019

A EXPRESSING AN LSTM AS A TRELLISNET

zt(+i+11) <latexit sha1_base64="+VtluC+UvZ7AXfnV6T+0WUjJZNw=">AAAB+HicbVDLSgNBEOz1GeMjqx69DAYhEgg7IiieAl48RjAPSNZldjJJhsw+mJkVkmW/xIsHRbz6Kd78GyfJHjSxoKGo6qa7y48FV9pxvq219Y3Nre3CTnF3b/+gZB8etVSUSMqaNBKR7PhEMcFD1tRcC9aJJSOBL1jbH9/O/PYTk4pH4YOexMwNyDDkA06JNpJnl6Zeqqs4e0wrvIrPM88uOzVnDrRKcE7KkKPh2V+9fkSTgIWaCqJUFzuxdlMiNaeCZcVeolhM6JgMWdfQkARMuen88AydGaWPBpE0FWo0V39PpCRQahL4pjMgeqSWvZn4n9dN9ODaTXkYJ5qFdLFokAikIzRLAfW5ZFSLiSGESm5uRXREJKHaZFU0IeDll1dJ66KGnRq+vyzXb/I4CnACp1ABDFdQhztoQBMoJPAMr/BmTa0X6936WLSuWfnMMfyB9fkDeZiSRg==</latexit>

zt(+i+11,1) <latexit sha1_base64="HwsUkBgByuOgfWaANPnhipbeukE=">AAAB+nicbVDLSsNAFJ34rPWV6tLNYBEqlZIRQd0V3LisYB/QxjCZTtqhk0mYmSg15lPcuFDErV/izr9x2mahrQcuHM65l3vv8WPOlHacb2tpeWV1bb2wUdzc2t7ZtUt7LRUlktAmiXgkOz5WlDNBm5ppTjuxpDj0OW37o6uJ376nUrFI3OpxTN0QDwQLGMHaSJ5devRSXUUnKLtLK6yKjjPPLjs1Zwq4SFBOyiBHw7O/ev2IJCEVmnCsVBc5sXZTLDUjnGbFXqJojMkID2jXUIFDqtx0enoGj4zSh0EkTQkNp+rviRSHSo1D33SGWA/VvDcR//O6iQ4u3JSJONFUkNmiIOFQR3CSA+wzSYnmY0MwkczcCskQS0y0SatoQkDzLy+S1mkNOTV0c1auX+ZxFMABOAQVgMA5qINr0ABNQMADeAav4M16sl6sd+tj1rpk5TP74A+szx9YR5K2</latexit>
zt(+i+11,2) <latexit sha1_base64="G0XcPH55M3XTZdbxur3zMiK0otc=">AAAB+nicbVDLSgMxFM3UV62vqS7dBItQqZRJEdRdwY3LCvYB7Thk0rQNzWSGJKPUcT7FjQtF3Pol7vwb03YW2nrgwuGce7n3Hj/iTGnH+bZyK6tr6xv5zcLW9s7unl3cb6kwloQ2SchD2fGxopwJ2tRMc9qJJMWBz2nbH19N/fY9lYqF4lZPIuoGeCjYgBGsjeTZxUcv0RV0WkvvkjKroJPUs0tO1ZkBLhOUkRLI0PDsr14/JHFAhSYcK9VFTqTdBEvNCKdpoRcrGmEyxkPaNVTggCo3mZ2ewmOj9OEglKaEhjP190SCA6UmgW86A6xHatGbiv953VgPLtyEiSjWVJD5okHMoQ7hNAfYZ5ISzSeGYCKZuRWSEZaYaJNWwYSAFl9eJq1aFTlVdHNWql9mceTBITgCZYDAOaiDa9AATUDAA3gGr+DNerJerHfrY96as7KZA/AH1ucPWdSStw==</latexit>

<latexit sha1_base64="lSOnCFWQCuvxbU0oQl4fw4kzbuc=">AAACGHicbVDLSsNAFJ34rPUVdelmsAgVpSYiqLuCG5cV7AOaWCbTaTt0MgkzN0IN+Qw3/oobF4q47c6/cfpY2NYDFw7n3Mu99wSx4Boc58daWl5ZXVvPbeQ3t7Z3du29/ZqOEkVZlUYiUo2AaCa4ZFXgIFgjVoyEgWD1oH878utPTGkeyQcYxMwPSVfyDqcEjNSyzz3NuyHBHuWK4udWCmdu9pgW+UmGT/GM6QGRvZZdcErOGHiRuFNSQFNUWvbQa0c0CZkEKojWTdeJwU+JAk4Fy/JeollMaJ90WdNQSUKm/XT8WIaPjdLGnUiZkoDH6t+JlIRaD8LAdIYEenreG4n/ec0EOtd+ymWcAJN0sqiTCAwRHqWE21wxCmJgCKGKm1sx7RFFKJgs8yYEd/7lRVK7KLlOyb2/LJRvpnHk0CE6QkXkoitURneogqqIohf0hj7Qp/VqvVtf1vekdcmazhygGVjDX8dSnvA=</latexit>

zt(,i1) +

tanh

z^t(+i+11) <latexit sha1_base64="Am4axvpT/RbzZdU3VQKPoUt9hZE=">AAAB/nicbVDLSgNBEJz1GeNrVTx5WQxCJBB2RFA8Bbx4jGAekKxhdjKbDJl9MNMrxGHBX/HiQRGvfoc3/8ZJsgdNLGgoqrrp7vITwRW47re1tLyyurZe2Chubm3v7Np7+00Vp5KyBo1FLNs+UUzwiDWAg2DtRDIS+oK1/NH1xG89MKl4HN3BOGFeSAYRDzglYKSefdgdEtCPWU9DBWf3uswr+DTr2SW36k7hLBKckxLKUe/ZX91+TNOQRUAFUaqD3QQ8TSRwKlhW7KaKJYSOyIB1DI1IyJSnp+dnzolR+k4QS1MROFP194QmoVLj0DedIYGhmvcm4n9eJ4Xg0tM8SlJgEZ0tClLhQOxMsnD6XDIKYmwIoZKbWx06JJJQMIkVTQh4/uVF0jyrYreKb89Ltas8jgI6QseojDC6QDV0g+qogSjS6Bm9ojfryXqx3q2PWeuSlc8coD+wPn8AaO2VEw==</latexit>

W1 <latexit sha1_base64="lXMe2VVJVimguU04W2823Zlih1M=">AAAB6nicbVBNS8NAEJ3Ur1q/qh69LBbBU0lEUDwVvHisaD+gDWWz3bRLN5uwOxFK6E/w4kERr/4ib/4bt2kO2vpg4PHeDDPzgkQKg6777ZTW1jc2t8rblZ3dvf2D6uFR28SpZrzFYhnrbkANl0LxFgqUvJtoTqNA8k4wuZ37nSeujYjVI04T7kd0pEQoGEUrPXQG3qBac+tuDrJKvILUoEBzUP3qD2OWRlwhk9SYnucm6GdUo2CSzyr91PCEsgkd8Z6likbc+Fl+6oycWWVIwljbUkhy9fdERiNjplFgOyOKY7PszcX/vF6K4bWfCZWkyBVbLApTSTAm87/JUGjOUE4toUwLeythY6opQ5tOxYbgLb+8StoXdc+te/eXtcZNEUcZTuAUzsGDK2jAHTShBQxG8Ayv8OZI58V5dz4WrSWnmDmGP3A+fwDW8413</latexit>

W2 <latexit sha1_base64="uQ/4VJ3D9Rsf5zu3tZUkUWQBkuo=">AAAB6nicbVBNS8NAEJ3Ur1q/qh69LBbBU0mKUPFU8OKxov2ANpTNdtIu3WzC7kYooT/BiwdFvPqLvPlv3LY5aOuDgcd7M8zMCxLBtXHdb6ewsbm1vVPcLe3tHxwelY9P2jpOFcMWi0WsugHVKLjEluFGYDdRSKNAYCeY3M79zhMqzWP5aKYJ+hEdSR5yRo2VHjqD2qBccavuAmSdeDmpQI7moPzVH8YsjVAaJqjWPc9NjJ9RZTgTOCv1U40JZRM6wp6lkkao/Wxx6oxcWGVIwljZkoYs1N8TGY20nkaB7YyoGetVby7+5/VSE177GZdJalCy5aIwFcTEZP43GXKFzIipJZQpbm8lbEwVZcamU7IheKsvr5N2req5Ve/+qtK4yeMowhmcwyV4UIcG3EETWsBgBM/wCm+OcF6cd+dj2Vpw8plT+APn8wfYd414</latexit>

xt
<latexit sha1_base64="eEPz6pgZfvj6K8i8F78InKVPN1o=">AAAB6nicbVA9SwNBEJ2LXzF+RS1tFoNgFe5EiIVFwMYyovmA5Ah7m71kyd7esTsnhiM/wcZCEVt/kZ3/xk1yhSY+GHi8N8PMvCCRwqDrfjuFtfWNza3idmlnd2//oHx41DJxqhlvsljGuhNQw6VQvIkCJe8kmtMokLwdjG9mfvuRayNi9YCThPsRHSoRCkbRSvdPfeyXK27VnYOsEi8nFcjR6Je/eoOYpRFXyCQ1puu5CfoZ1SiY5NNSLzU8oWxMh7xrqaIRN342P3VKzqwyIGGsbSkkc/X3REYjYyZRYDsjiiOz7M3E/7xuiuGVnwmVpMgVWywKU0kwJrO/yUBozlBOLKFMC3srYSOqKUObTsmG4C2/vEpaF1XPrXp3l5X6dR5HEU7gFM7BgxrU4RYa0AQGQ3iGV3hzpPPivDsfi9aCk88cwx84nz9vbo3d</latexit>

zt(i) <latexit sha1_base64="2svhF9CpH3GHMAPuUCL8JxYJANA=">AAAB8nicbVBNS8NAEN34WetX1aOXxSLUS0lEUDwVvHisYD8gjWWz3bRLN7thdyLUkJ/hxYMiXv013vw3btsctPXBwOO9GWbmhYngBlz321lZXVvf2Cxtlbd3dvf2KweHbaNSTVmLKqF0NySGCS5ZCzgI1k00I3EoWCcc30z9ziPThit5D5OEBTEZSh5xSsBK/lM/g/whq/GzvF+punV3BrxMvIJUUYFmv/LVGyiaxkwCFcQY33MTCDKigVPB8nIvNSwhdEyGzLdUkpiZIJudnONTqwxwpLQtCXim/p7ISGzMJA5tZ0xgZBa9qfif56cQXQUZl0kKTNL5oigVGBSe/o8HXDMKYmIJoZrbWzEdEU0o2JTKNgRv8eVl0j6ve27du7uoNq6LOEroGJ2gGvLQJWqgW9RELUSRQs/oFb054Lw4787HvHXFKWaO0B84nz9HwZE1</latexit>

zt(,i1) <latexit sha1_base64="hEqsC+awpPdZyeIQrbA/Py+DhL4=">AAAB9HicbVDLSgNBEOyNrxhfUY9eBoMQQcKuCOot4MVjBPOAZA2zk0kyZPbhTG8gLvsdXjwo4tWP8ebfOEn2oIkFDUVVN91dXiSFRtv+tnIrq2vrG/nNwtb2zu5ecf+gocNYMV5noQxVy6OaSxHwOgqUvBUpTn1P8qY3upn6zTFXWoTBPU4i7vp0EIi+YBSN5D51Ezxz0oekLE7TbrFkV+wZyDJxMlKCDLVu8avTC1ns8wCZpFq3HTtCN6EKBZM8LXRizSPKRnTA24YG1OfaTWZHp+TEKD3SD5WpAMlM/T2RUF/rie+ZTp/iUC96U/E/rx1j/8pNRBDFyAM2X9SPJcGQTBMgPaE4QzkxhDIlzK2EDamiDE1OBROCs/jyMmmcVxy74txdlKrXWRx5OIJjKIMDl1CFW6hBHRg8wjO8wps1tl6sd+tj3pqzsplD+APr8wcksJGl</latexit>
zt(,i2) <latexit sha1_base64="CM90o1WRrsRKWE4t3BC62PomYXs=">AAAB9HicbVDLSgNBEOz1GeMr6tHLYBAiSNgNgnoLePEYwTwgWcPsZDYZMvtwpjcQl3yHFw+KePVjvPk3TpI9aGJBQ1HVTXeXF0uh0ba/rZXVtfWNzdxWfntnd2+/cHDY0FGiGK+zSEaq5VHNpQh5HQVK3ooVp4EnedMb3kz95ogrLaLwHscxdwPaD4UvGEUjuU/dFM8rk4e0JM4m3ULRLtszkGXiZKQIGWrdwlenF7Ek4CEySbVuO3aMbkoVCib5JN9JNI8pG9I+bxsa0oBrN50dPSGnRukRP1KmQiQz9fdESgOtx4FnOgOKA73oTcX/vHaC/pWbijBOkIdsvshPJMGITBMgPaE4Qzk2hDIlzK2EDaiiDE1OeROCs/jyMmlUyo5ddu4uitXrLI4cHMMJlMCBS6jCLdSgDgwe4Rle4c0aWS/Wu/Uxb12xspkj+APr8wcmO5Gm</latexit>

xt+1 <latexit sha1_base64="nkvIntHmKtHH1HtvVwUH5BQy280=">AAAB7nicbVBNS8NAEJ3Ur1q/qh69LBZBEEoiBT14KHjxWMF+QBvKZrtpl242YXciltAf4cWDIl79Pd78N27bHLT1wcDjvRlm5gWJFAZd99sprK1vbG4Vt0s7u3v7B+XDo5aJU814k8Uy1p2AGi6F4k0UKHkn0ZxGgeTtYHw789uPXBsRqwecJNyP6FCJUDCKVmo/9TO88Kb9csWtunOQVeLlpAI5Gv3yV28QszTiCpmkxnQ9N0E/oxoFk3xa6qWGJ5SN6ZB3LVU04sbP5udOyZlVBiSMtS2FZK7+nshoZMwkCmxnRHFklr2Z+J/XTTG89jOhkhS5YotFYSoJxmT2OxkIzRnKiSWUaWFvJWxENWVoEyrZELzll1dJ67LquVXvvlap3+RxFOEETuEcPLiCOtxBA5rAYAzP8ApvTuK8OO/Ox6K14OQzx/AHzucPDcKPWQ==</latexit>

zt(+i)1 <latexit sha1_base64="LuZCTtj0QRFI4tTxozbliK7Tzjg=">AAAB9HicbVDLSgNBEOyNrxhfUY9eBoMQEcKuCIqngBePEcwDkjXMTibJkNmHM72BuOx3ePGgiFc/xpt/4yTZgyYWNBRV3XR3eZEUGm3728qtrK6tb+Q3C1vbO7t7xf2Dhg5jxXidhTJULY9qLkXA6yhQ8lakOPU9yZve6GbqN8dcaREG9ziJuOvTQSD6glE0kvvUTfDMSR+SsjhNu8WSXbFnIMvEyUgJMtS6xa9OL2SxzwNkkmrdduwI3YQqFEzytNCJNY8oG9EBbxsaUJ9rN5kdnZITo/RIP1SmAiQz9fdEQn2tJ75nOn2KQ73oTcX/vHaM/Ss3EUEUIw/YfFE/lgRDMk2A9ITiDOXEEMqUMLcSNqSKMjQ5FUwIzuLLy6RxXnHsinN3UapeZ3Hk4QiOoQwOXEIVbqEGdWDwCM/wCm/W2Hqx3q2PeWvOymYO4Q+szx8jcZGl</latexit>

zt(+i)1,1 <latexit sha1_base64="3QxhVQRV4MPfAQCGJeDKkKtO4Q8=">AAAB+HicbVDLSsNAFL2pr1ofjbp0M1iEilISEdRdwY3LCvYBbQyT6aQdOpmEmYnQhnyJGxeKuPVT3Pk3Th8LrR64cDjnXu69J0g4U9pxvqzCyura+kZxs7S1vbNbtvf2WypOJaFNEvNYdgKsKGeCNjXTnHYSSXEUcNoORjdTv/1IpWKxuNfjhHoRHggWMoK1kXy7PPEzfeqeuflDVmUnuW9XnJozA/pL3AWpwAIN3/7s9WOSRlRowrFSXddJtJdhqRnhNC/1UkUTTEZ4QLuGChxR5WWzw3N0bJQ+CmNpSmg0U39OZDhSahwFpjPCeqiWvan4n9dNdXjlZUwkqaaCzBeFKUc6RtMUUJ9JSjQfG4KJZOZWRIZYYqJNViUTgrv88l/SOq+5Ts29u6jUrxdxFOEQjqAKLlxCHW6hAU0gkMITvMCrNbGerTfrfd5asBYzB/AL1sc3eNWSRg==</latexit>
zt(+i)1,2 <latexit sha1_base64="jJAvqfFQwy/CDu8cCn5IwVKXhnU=">AAAB+HicbVDLSsNAFJ3UV62PRl26GSxCRSlJEdRdwY3LCvYBbQyT6aQdOnkwcyO0IV/ixoUibv0Ud/6N0zYLbT1w4XDOvdx7jxcLrsCyvo3C2vrG5lZxu7Szu7dfNg8O2ypKJGUtGolIdj2imOAhawEHwbqxZCTwBOt449uZ33liUvEofIBJzJyADEPuc0pAS65ZnropnNsX9ewxrfKzzDUrVs2aA68SOycVlKPpml/9QUSTgIVABVGqZ1sxOCmRwKlgWamfKBYTOiZD1tM0JAFTTjo/PMOnWhlgP5K6QsBz9fdESgKlJoGnOwMCI7XszcT/vF4C/rWT8jBOgIV0schPBIYIz1LAAy4ZBTHRhFDJ9a2YjogkFHRWJR2CvfzyKmnXa7ZVs+8vK42bPI4iOkYnqIpsdIUa6A41UQtRlKBn9IrejKnxYrwbH4vWgpHPHKE/MD5/AHpgkkc=</latexit>

(a) An atomic view

Layer <latexit sha1_base64="Omx146Fv69pKxCTzcUXdA+GkZwY=">AAACAnicbVBNS8NAEN3Ur1q/op7Ey2IrCEJJelE8Fbx48FDBfkAbyma7aZdusmF3ooRQvPhXvHhQxKu/wpv/xm2bg1YfDDzem2Fmnh8LrsFxvqzC0vLK6lpxvbSxubW9Y+/utbRMFGVNKoVUHZ9oJnjEmsBBsE6sGAl9wdr++HLqt++Y0lxGt5DGzAvJMOIBpwSM1LcPrknKFK5w3FN8OAKilLzH/NSt9O2yU3VmwH+Jm5MyytHo25+9gaRJyCKggmjddZ0YvIwo4FSwSamXaBYTOiZD1jU0IiHTXjZ7YYKPjTLAgVSmIsAz9edERkKt09A3nSGBkV70puJ/XjeB4NzLeBQnwCI6XxQkAoPE0zzwgCtGQaSGEKq4uRXTEVGEgkmtZEJwF1/+S1q1qutU3ZtauX6Rx1FEh+gInSAXnaE6ukIN1EQUPaAn9IJerUfr2Xqz3uetBSuf2Ue/YH18A14BlhM=</latexit>

i!i+1

z1(i,2+1)

z2(i,2+1)

z3(i,2+1)

z4(i,2+1)

z5(i,2+1)

z6(i,2+1)

z7(i,2+1)

z8(i,2+1)

tanh

x1 <latexit sha1_base64="LHTk4FRcQ4h5K0jvGjsNroBkfok=">AAAB6nicbVBNS8NAEJ3Ur1q/qh69LBbBU0mKUI8FLx4r2g9oQ9lsN+3SzSbsTsQS+hO8eFDEq7/Im//GbZuDtj4YeLw3w8y8IJHCoOt+O4WNza3tneJuaW//4PCofHzSNnGqGW+xWMa6G1DDpVC8hQIl7yaa0yiQvBNMbuZ+55FrI2L1gNOE+xEdKREKRtFK908Db1CuuFV3AbJOvJxUIEdzUP7qD2OWRlwhk9SYnucm6GdUo2CSz0r91PCEsgkd8Z6likbc+Nni1Bm5sMqQhLG2pZAs1N8TGY2MmUaB7Ywojs2qNxf/83ophtd+JlSSIldsuShMJcGYzP8mQ6E5Qzm1hDIt7K2EjammDG06JRuCt/ryOmnXqp5b9e6uKo1aHkcRzuAcLsGDOjTgFprQAgYjeIZXeHOk8+K8Ox/L1oKTz5zCHzifPwbgjZA=</latexit>
z1(i,1) z1(i,2)

x2 <latexit sha1_base64="Q5m66tjDDgQFM5wTYetSpnS6JPE=">AAAB6nicbVBNS8NAEJ3Ur1q/qh69LBbBU0mKUI8FLx4r2g9oQ9lsJ+3SzSbsbsQS+hO8eFDEq7/Im//GbZuDtj4YeLw3w8y8IBFcG9f9dgobm1vbO8Xd0t7+weFR+fikreNUMWyxWMSqG1CNgktsGW4EdhOFNAoEdoLJzdzvPKLSPJYPZpqgH9GR5CFn1Fjp/mlQG5QrbtVdgKwTLycVyNEclL/6w5ilEUrDBNW657mJ8TOqDGcCZ6V+qjGhbEJH2LNU0gi1ny1OnZELqwxJGCtb0pCF+nsio5HW0yiwnRE1Y73qzcX/vF5qwms/4zJJDUq2XBSmgpiYzP8mQ66QGTG1hDLF7a2EjamizNh0SjYEb/XlddKuVT236t1dVRq1PI4inME5XIIHdWjALTShBQxG8Ayv8OYI58V5dz6WrQUnnzmFP3A+fwAIZI2R</latexit>
z2(i,1) z2(i,2)

x3 <latexit sha1_base64="vd/3+5BCCHLMHf7hQPZRkv+I3rw=">AAAB6nicbVBNS8NAEJ3Ur1q/qh69LBbBU0mqoMeCF48V7Qe0oWy2k3bpZhN2N2IJ/QlePCji1V/kzX/jts1BWx8MPN6bYWZekAiujet+O4W19Y3NreJ2aWd3b/+gfHjU0nGqGDZZLGLVCahGwSU2DTcCO4lCGgUC28H4Zua3H1FpHssHM0nQj+hQ8pAzaqx0/9S/6JcrbtWdg6wSLycVyNHol796g5ilEUrDBNW667mJ8TOqDGcCp6VeqjGhbEyH2LVU0gi1n81PnZIzqwxIGCtb0pC5+nsio5HWkyiwnRE1I73szcT/vG5qwms/4zJJDUq2WBSmgpiYzP4mA66QGTGxhDLF7a2EjaiizNh0SjYEb/nlVdKqVT236t1dVuq1PI4inMApnIMHV1CHW2hAExgM4Rle4c0Rzovz7nwsWgtOPnMMf+B8/gAJ6I2S</latexit>
z3(i,1) z3(i,2)

x4 <latexit sha1_base64="NhN9u0qKu6RsT8lrhpOfmOFDo20=">AAAB6nicbVBNS8NAEJ3Ur1q/qh69LBbBU0lKQY8FLx4r2g9oQ9lsJ+3SzSbsbsQS+hO8eFDEq7/Im//GbZuDtj4YeLw3w8y8IBFcG9f9dgobm1vbO8Xd0t7+weFR+fikreNUMWyxWMSqG1CNgktsGW4EdhOFNAoEdoLJzdzvPKLSPJYPZpqgH9GR5CFn1Fjp/mlQH5QrbtVdgKwTLycVyNEclL/6w5ilEUrDBNW657mJ8TOqDGcCZ6V+qjGhbEJH2LNU0gi1ny1OnZELqwxJGCtb0pCF+nsio5HW0yiwnRE1Y73qzcX/vF5qwms/4zJJDUq2XBSmgpiYzP8mQ66QGTG1hDLF7a2EjamizNh0SjYEb/XlddKuVT236t3VK41aHkcRzuAcLsGDK2jALTShBQxG8Ayv8OYI58V5dz6WrQUnnzmFP3A+fwALbI2T</latexit>
z4(i,1) z4(i,2)

x5 <latexit sha1_base64="9xOKs6lmcAyyWgXd0SkYRgZhA78=">AAAB6nicbVBNS8NAEJ3Ur1q/qh69LBbBU0mKoseCF48V7Qe0oWy2k3bpZhN2N2IJ/QlePCji1V/kzX/jts1BWx8MPN6bYWZekAiujet+O4W19Y3NreJ2aWd3b/+gfHjU0nGqGDZZLGLVCahGwSU2DTcCO4lCGgUC28H4Zua3H1FpHssHM0nQj+hQ8pAzaqx0/9S/7JcrbtWdg6wSLycVyNHol796g5ilEUrDBNW667mJ8TOqDGcCp6VeqjGhbEyH2LVU0gi1n81PnZIzqwxIGCtb0pC5+nsio5HWkyiwnRE1I73szcT/vG5qwms/4zJJDUq2WBSmgpiYzP4mA66QGTGxhDLF7a2EjaiizNh0SjYEb/nlVdKqVT236t1dVOq1PI4inMApnIMHV1CHW2hAExgM4Rle4c0Rzovz7nwsWgtOPnMMf+B8/gAM8I2U</latexit>
z5(i,1) z5(i,2)

x6 <latexit sha1_base64="3mJ56c0McJp8Sj2EekOsRFMnB5Q=">AAAB6nicbVBNS8NAEJ3Ur1q/qh69LBbBU0mKqMeCF48V7Qe0oWy2k3bpZhN2N2IJ/QlePCji1V/kzX/jts1BWx8MPN6bYWZekAiujet+O4W19Y3NreJ2aWd3b/+gfHjU0nGqGDZZLGLVCahGwSU2DTcCO4lCGgUC28H4Zua3H1FpHssHM0nQj+hQ8pAzaqx0/9S/7JcrbtWdg6wSLycVyNHol796g5ilEUrDBNW667mJ8TOqDGcCp6VeqjGhbEyH2LVU0gi1n81PnZIzqwxIGCtb0pC5+nsio5HWkyiwnRE1I73szcT/vG5qwms/4zJJDUq2WBSmgpiYzP4mA66QGTGxhDLF7a2EjaiizNh0SjYEb/nlVdKqVT236t1dVOq1PI4inMApnIMHV1CHW2hAExgM4Rle4c0Rzovz7nwsWgtOPnMMf+B8/gAOdI2V</latexit>
z6(i,1) z6(i,2)

x7 <latexit sha1_base64="QaRF83Q7awwnGlt55EAYIgtUZw8=">AAAB6nicbVBNS8NAEJ3Ur1q/qh69LBbBU0mKUI8FLx4r2g9oQ9lsN+3SzSbsTsQS+hO8eFDEq7/Im//GbZuDtj4YeLw3w8y8IJHCoOt+O4WNza3tneJuaW//4PCofHzSNnGqGW+xWMa6G1DDpVC8hQIl7yaa0yiQvBNMbuZ+55FrI2L1gNOE+xEdKREKRtFK90+D+qBccavuAmSdeDmpQI7moPzVH8YsjbhCJqkxPc9N0M+oRsEkn5X6qeEJZRM64j1LFY248bPFqTNyYZUhCWNtSyFZqL8nMhoZM40C2xlRHJtVby7+5/VSDK/9TKgkRa7YclGYSoIxmf9NhkJzhnJqCWVa2FsJG1NNGdp0SjYEb/XlddKuVT236t1dVRq1PI4inME5XIIHdWjALTShBQxG8Ayv8OZI58V5dz6WrQUnnzmFP3A+fwAP+I2W</latexit>
z7(i,1) z7(i,2)

Filter <latexit sha1_base64="t/B8MdoeaDPtk2WxPCtkm9aAmV4=">AAAB8XicbVBNS8NAEJ3Ur1q/qh69LLaCp5IUQY8FQTxWsK3YhrLZTtqlm03Y3Qgl9F948aCIV/+NN/+N2zYHbX0w8Hhvhpl5QSK4Nq777RTW1jc2t4rbpZ3dvf2D8uFRW8epYthisYjVQ0A1Ci6xZbgR+JAopFEgsBOMr2d+5wmV5rG8N5ME/YgOJQ85o8ZKjzdcGFSk2qn2yxW35s5BVomXkwrkaPbLX71BzNIIpWGCat313MT4GVWGM4HTUi/VmFA2pkPsWipphNrP5hdPyZlVBiSMlS1pyFz9PZHRSOtJFNjOiJqRXvZm4n9eNzXhlZ9xmaQGJVssClNBTExm75MBV8iMmFhCmeL2VsJGVFFmY9AlG4K3/PIqaddrnlvz7uqVxkUeRxFO4BTOwYNLaMAtNKEFDCQ8wyu8Odp5cd6dj0VrwclnjuEPnM8fTGeP8w==</latexit>

W

x8 <latexit sha1_base64="LxQhhprzm81NnfnhLVIHmSPbSpM=">AAAB6nicbVBNS8NAEJ3Ur1q/qh69LBbBU0mKYI8FLx4r2g9oQ9lsN+3SzSbsTsQS+hO8eFDEq7/Im//GbZuDtj4YeLw3w8y8IJHCoOt+O4WNza3tneJuaW//4PCofHzSNnGqGW+xWMa6G1DDpVC8hQIl7yaa0yiQvBNMbuZ+55FrI2L1gNOE+xEdKREKRtFK90+D+qBccavuAmSdeDmpQI7moPzVH8YsjbhCJqkxPc9N0M+oRsEkn5X6qeEJZRM64j1LFY248bPFqTNyYZUhCWNtSyFZqL8nMhoZM40C2xlRHJtVby7+5/VSDOt+JlSSIldsuShMJcGYzP8mQ6E5Qzm1hDIt7K2EjammDG06JRuCt/ryOmnXqp5b9e6uKo1aHkcRzuAcLsGDa2jALTShBQxG8Ayv8OZI58V5dz6WrQUnnzmFP3A+fwARfI2X</latexit>

z8(i,1) z1(i:T) = (z1(i:T) ,1, z1(i:T) ,2)

z8(i,2)

Convolution <latexit sha1_base64="jtqGPPNegS7AgtLFAcl4qe2bDCI=">AAAB8nicbZBNS8NAEIYnftb6VfXoJVgETyUpgh4LvXisYD8gDWWz3bRLN7thd1IopT/DiwdFvPprvPlv3LQ5aOsLCw/vzLAzb5QKbtDzvp2t7Z3dvf3SQfnw6PjktHJ23jEq05S1qRJK9yJimOCStZGjYL1UM5JEgnWjSTOvd6dMG67kE85SFiZkJHnMKUFrBU0lp0pkOQ8qVa/mLeVugl9AFQq1BpWv/lDRLGESqSDGBL6XYjgnGjkVbFHuZ4alhE7IiAUWJUmYCefLlRfutXWGbqy0fRLdpft7Yk4SY2ZJZDsTgmOzXsvN/2pBhvF9OOcyzZBJuvoozoSLys3vd4dcM4piZoFQze2uLh0TTSjalMo2BH/95E3o1Gu+V/Mf69XGbRFHCS7hCm7AhztowAO0oA0UFDzDK7w56Lw4787HqnXLKWYu4I+czx+2E5F4</latexit>

(b) A sequence view

Figure 4: A TrellisNet with an LSTM nonlinearity, at an atomic level and on a longer sequence.

Here we trace in more detail the transformation of an LSTM into a TrellisNet. This is an application of Theorem 1. The nonlinear activation has been examined in Section 5.1. We will walk through the construction again here.

In each time step, an LSTM cell computes the following:

ft( ) = (Wf h(t -1) + Uf h(t-)1) ot( ) = (Woh(t -1) + Uoht(-)1)

it( ) = (Wiht( -1) + Uih(t-)1) c(t ) = ft( )  c(t-)1 + i(t )  gt( )

gt( ) = tanh(Wght( -1) + Ugh(t-)1) h(t ) = o(t )  tanh(ct( ))

(14)

where ht(0) = xt, and ft, it, ot are typically called the forget, input, and output gates. By a similar construction to how we defined  in Theorem 1, to recover an LSTM the mixed group convolution needs to produce 3q more channels for these gated outputs, which have the form ft,t , it,t and gt,t (see Figure 5 for an example). In addition, at each layer of the mixed group convolution, the network
also needs to maintain a group of channels for cell states ct,t . Note that in an LSTM network ct is updated "synchronously" with ht, so we can similarly write

c(t,1t) = ft(,1t)  c(t-1)1,t + it(,1t)  gt(,1t)

h(t,1t) = ot(,1t)  tanh(ct(,1t) )

(15)

Based on these changes, we show in Figure 4 an atomic and a sequence view of TrellisNet with the
LSTM activation. The hidden units z1:T consist of two parts: z1:T,1, which gets updated directly via the gated activations (akin to LSTM cell states), and z1:T,2, which is processed by parameterized convolutions (akin to LSTM hidden states). Formally, in layer i:

z^1(i:T+1) = Conv1D(z1(i:T) ,2; W ) + x~1:T = [z^1:T,1 z^1:T,2 z^1:T,3 z1(i:T+,11) = (z^1:T,1)  z0(i:T) -1,1 + (z^1:T,2)  tanh(z^1:T,3) z1(i:T+,12) = (z^1:T,4)  tanh(z1(i:T+,11))

z^1:T ,4 ]T

x4 <latexit sha1_base64="S4f+bq/VrUW0JYj64fGswbvelkw=">AAAB6nicbVBNS8NAEJ3Ur1q/qh69LBbBU0mkoHgqePFY0X5AG8pmO2mXbjZhdyOW0J/gxYMiXv1F3vw3btsctPXBwOO9GWbmBYng2rjut1NYW9/Y3Cpul3Z29/YPyodHLR2nimGTxSJWnYBqFFxi03AjsJMopFEgsB2Mb2Z++xGV5rF8MJME/YgOJQ85o8ZK90/9Wr9ccavuHGSVeDmpQI5Gv/zVG8QsjVAaJqjWXc9NjJ9RZTgTOC31Uo0JZWM6xK6lkkao/Wx+6pScWWVAwljZkobM1d8TGY20nkSB7YyoGellbyb+53VTE175GZdJalCyxaIwFcTEZPY3GXCFzIiJJZQpbm8lbEQVZcamU7IheMsvr5LWRdVzq95drVK/zuMowgmcwjl4cAl1uIUGNIHBEJ7hFd4c4bw4787HorXg5DPH8AfO5w8N1I2b</latexit>

h4(1,2) <latexit sha1_base64="3lxonuKMIB4SCIyUSKv2CAH48Oo=">AAAB9HicbVBNS8NAEJ3Ur1q/qh69LBahgpSkFBRPBS8eK9gPaGPZbDft0s0m3d0USsjv8OJBEa/+GG/+G7dtDtr6YODx3gwz87yIM6Vt+9vKbWxube/kdwt7+weHR8Xjk5YKY0lok4Q8lB0PK8qZoE3NNKedSFIceJy2vfHd3G9PqVQsFI96FlE3wEPBfEawNpI76ie1q2r6lJSdy7RfLNkVewG0TpyMlCBDo1/86g1CEgdUaMKxUl3HjrSbYKkZ4TQt9GJFI0zGeEi7hgocUOUmi6NTdGGUAfJDaUpotFB/TyQ4UGoWeKYzwHqkVr25+J/XjbV/4yZMRLGmgiwX+TFHOkTzBNCASUo0nxmCiWTmVkRGWGKiTU4FE4Kz+vI6aVUrjl1xHmql+m0WRx7O4BzK4MA11OEeGtAEAhN4hld4s6bWi/VufSxbc1Y2cwp/YH3+AFHJkR0=</latexit>

h(42,3) <latexit sha1_base64="vqiCb3PG69e6p/XY5Mj5vqrqMyg=">AAAB9HicbVBNS8NAEJ3Ur1q/qh69LBahgpSkFhRPBS8eK9gPaGPZbDft0s0m7m4KJeR3ePGgiFd/jDf/jds2B219MPB4b4aZeV7EmdK2/W3l1tY3Nrfy24Wd3b39g+LhUUuFsSS0SUIeyo6HFeVM0KZmmtNOJCkOPE7b3vh25rcnVCoWigc9jagb4KFgPiNYG8kd9ZPaxWX6mJSr52m/WLIr9hxolTgZKUGGRr/41RuEJA6o0IRjpbqOHWk3wVIzwmla6MWKRpiM8ZB2DRU4oMpN5ken6MwoA+SH0pTQaK7+nkhwoNQ08ExngPVILXsz8T+vG2v/2k2YiGJNBVks8mOOdIhmCaABk5RoPjUEE8nMrYiMsMREm5wKJgRn+eVV0qpWHLvi3NdK9ZssjjycwCmUwYErqMMdNKAJBJ7gGV7hzZpYL9a79bFozVnZzDH8gfX5A1TakR8=</latexit>

f4(,12) g4(1,2)

<latexit sha1_base64="UW6Ynxw5Asr6zFj8tVR6soRyBzE=">AAAB9HicbVBNS8NAEJ3Ur1q/qh69LBahgpSkFPRY8OKxgv2ANpbNdtMu3Wzi7qZQQn6HFw+KePXHePPfuGlz0NYHA4/3ZpiZ50WcKW3b31ZhY3Nre6e4W9rbPzg8Kh+fdFQYS0LbJOSh7HlYUc4EbWumOe1FkuLA47TrTW8zvzujUrFQPOh5RN0AjwXzGcHaSK4/TBpX9fQxqTqX6bBcsWv2AmidODmpQI7WsPw1GIUkDqjQhGOl+o4daTfBUjPCaVoaxIpGmEzxmPYNFTigyk0WR6fowigj5IfSlNBoof6eSHCg1DzwTGeA9UStepn4n9ePtX/jJkxEsaaCLBf5MUc6RFkCaMQkJZrPDcFEMnMrIhMsMdEmp5IJwVl9eZ106jXHrjn3jUqzkcdRhDM4hyo4cA1NuIMWtIHAEzzDK7xZM+vFerc+lq0FK585hT+wPn8ATNuRFQ==</latexit>

<latexit sha1_base64="D0M0RmciCwUCxQ+dAQspOaDfsk8=">AAAB9HicbVBNS8NAEJ3Ur1q/qh69LBahgpSkFPRY8OKxgv2ANpbNdtMu3Wzi7qZQQn6HFw+KePXHePPfuGlz0NYHA4/3ZpiZ50WcKW3b31ZhY3Nre6e4W9rbPzg8Kh+fdFQYS0LbJOSh7HlYUc4EbWumOe1FkuLA47TrTW8zvzujUrFQPOh5RN0AjwXzGcHaSO54mDSu6uljUnUu02G5YtfsBdA6cXJSgRytYflrMApJHFChCcdK9R070m6CpWaE07Q0iBWNMJniMe0bKnBAlZssjk7RhVFGyA+lKaHRQv09keBAqXngmc4A64la9TLxP68fa//GTZiIYk0FWS7yY450iLIE0IhJSjSfG4KJZOZWRCZYYqJNTiUTgrP68jrp1GuOXXPuG5VmI4+jCGdwDlVw4BqacActaAOBJ3iGV3izZtaL9W59LFsLVj5zCn9gff4ATmuRFg==</latexit>

i(41,2) <latexit sha1_base64="DB39SHN0WkBFm4dQHT44RXj97YU=">AAAB9HicbVBNS8NAEJ3Ur1q/qh69LBahgpSkFPRY8OKxgv2ANpbNdtMu3Wzi7qZQQn6HFw+KePXHePPfuGlz0NYHA4/3ZpiZ50WcKW3b31ZhY3Nre6e4W9rbPzg8Kh+fdFQYS0LbJOSh7HlYUc4EbWumOe1FkuLA47TrTW8zvzujUrFQPOh5RN0AjwXzGcHaSC4bJo2revqYVJ3LdFiu2DV7AbROnJxUIEdrWP4ajEISB1RowrFSfceOtJtgqRnhNC0NYkUjTKZ4TPuGChxQ5SaLo1N0YZQR8kNpSmi0UH9PJDhQah54pjPAeqJWvUz8z+vH2r9xEyaiWFNBlov8mCMdoiwBNGKSEs3nhmAimbkVkQmWmGiTU8mE4Ky+vE469Zpj15z7RqXZyOMowhmcQxUcuIYm3EEL2kDgCZ7hFd6smfVivVsfy9aClc+cwh9Ynz9Ri5EY</latexit>

o4(1,2) <latexit sha1_base64="ZUNBi54AoIROhece1eOpH4Ibxa4=">AAAB9HicbVBNSwMxEJ2tX7V+VT16CRahgpTdUtBjwYvHCvYD2rVk02wbmk3WJFsoS3+HFw+KePXHePPfmLZ70NYHA4/3ZpiZF8ScaeO6305uY3Nreye/W9jbPzg8Kh6ftLRMFKFNIrlUnQBrypmgTcMMp51YURwFnLaD8e3cb0+o0kyKBzONqR/hoWAhI9hYyZf9tHZVnT2mZe9y1i+W3Iq7AFonXkZKkKHRL371BpIkERWGcKx113Nj46dYGUY4nRV6iaYxJmM8pF1LBY6o9tPF0TN0YZUBCqWyJQxaqL8nUhxpPY0C2xlhM9Kr3lz8z+smJrzxUybixFBBlovChCMj0TwBNGCKEsOnlmCimL0VkRFWmBibU8GG4K2+vE5a1YrnVrz7Wqley+LIwxmcQxk8uIY63EEDmkDgCZ7hFd6cifPivDsfy9ack82cwh84nz9a65Ee</latexit>

x3 <latexit sha1_base64="FsFp5SDaWDjvwmxpdt2PBoWBq3Q=">AAAB6nicbVBNS8NAEJ3Ur1q/qh69LBbBU0lUUDwVvHisaD+gDWWz3bRLN5uwOxFL6E/w4kERr/4ib/4bt20O2vpg4PHeDDPzgkQKg6777RRWVtfWN4qbpa3tnd298v5B08SpZrzBYhnrdkANl0LxBgqUvJ1oTqNA8lYwupn6rUeujYjVA44T7kd0oEQoGEUr3T/1znvlilt1ZyDLxMtJBXLUe+Wvbj9macQVMkmN6Xhugn5GNQom+aTUTQ1PKBvRAe9YqmjEjZ/NTp2QE6v0SRhrWwrJTP09kdHImHEU2M6I4tAselPxP6+TYnjlZ0IlKXLF5ovCVBKMyfRv0heaM5RjSyjTwt5K2JBqytCmU7IheIsvL5PmWdVzq97dRaV2ncdRhCM4hlPw4BJqcAt1aACDATzDK7w50nlx3p2PeWvByWcO4Q+czx8MUI2a</latexit>

Wh(1h) <latexit sha1_base64="LnNhvssgwH5Y1zscKY4KK91oeeM=">AAACE3icbVDLSsNAFJ34rPUVdelmsAjVRUmkoLgquHFZxT6giWEynbRDJ5N0ZiKUkH9w46+4caGIWzfu/BsnbRbaeuDC4Zx7ufceP2ZUKsv6NpaWV1bX1ksb5c2t7Z1dc2+/LaNEYNLCEYtE10eSMMpJS1HFSDcWBIU+Ix1/dJX7nQciJI34nZrExA3RgNOAYqS05JmnHS8dDrP7tGqfZNChHDohUkPfT2+1WB9DR9GQSDjOoGdWrJo1BVwkdkEqoEDTM7+cfoSTkHCFGZKyZ1uxclMkFMWMZGUnkSRGeIQGpKcpR3qRm05/yuCxVvowiIQuruBU/T2RolDKSejrzvxgOe/l4n9eL1HBhZtSHieKcDxbFCQMqgjmAcE+FQQrNtEEYUH1rRAPkUBY6RjLOgR7/uVF0j6r2VbNvqlXGpdFHCVwCI5AFdjgHDTANWiCFsDgETyDV/BmPBkvxrvxMWtdMoqZA/AHxucPyFGdbg==</latexit>

2 R4qq

h(31,2) <latexit sha1_base64="cmeYO9GIvlR3qVKEq+UPjp24ug0=">AAAB9HicbVBNS8NAEJ3Ur1q/qh69LBahgpSkFRRPBS8eK9gPaGPZbDft0s0m7m4KJeR3ePGgiFd/jDf/jds2B219MPB4b4aZeV7EmdK2/W3l1tY3Nrfy24Wd3b39g+LhUUuFsSS0SUIeyo6HFeVM0KZmmtNOJCkOPE7b3vh25rcnVCoWigc9jagb4KFgPiNYG8kd9ZPaRTV9TMrOedovluyKPQdaJU5GSpCh0S9+9QYhiQMqNOFYqa5jR9pNsNSMcJoWerGiESZjPKRdQwUOqHKT+dEpOjPKAPmhNCU0mqu/JxIcKDUNPNMZYD1Sy95M/M/rxtq/dhMmolhTQRaL/JgjHaJZAmjAJCWaTw3BRDJzKyIjLDHRJqeCCcFZfnmVtKoVx64495el+k0WRx5O4BTK4MAV1OEOGtAEAk/wDK/wZk2sF+vd+li05qxs5hj+wPr8AVA8kRw=</latexit>

h(32,3) <latexit sha1_base64="Vqksm/FbYcr3C+zPinso3H1oWQ8=">AAAB9HicbVBNS8NAEJ3Ur1q/qh69LBahgpSkFRRPBS8eK9gPaGPZbDft0s0m7m4KJeR3ePGgiFd/jDf/jds2B219MPB4b4aZeV7EmdK2/W3l1tY3Nrfy24Wd3b39g+LhUUuFsSS0SUIeyo6HFeVM0KZmmtNOJCkOPE7b3vh25rcnVCoWigc9jagb4KFgPiNYG8kd9ZPaRS19TMrV87RfLNkVew60SpyMlCBDo1/86g1CEgdUaMKxUl3HjrSbYKkZ4TQt9GJFI0zGeEi7hgocUOUm86NTdGaUAfJDaUpoNFd/TyQ4UGoaeKYzwHqklr2Z+J/XjbV/7SZMRLGmgiwW+TFHOkSzBNCASUo0nxqCiWTmVkRGWGKiTU4FE4Kz/PIqaVUrjl1x7i9L9ZssjjycwCmUwYErqMMdNKAJBJ7gGV7hzZpYL9a79bFozVnZzDH8gfX5A1NNkR4=</latexit>

Wh(1x) <latexit sha1_base64="beO+16Lls4eMB4x57m+xaPPbB7A=">AAACE3icbVDLSsNAFJ34rPUVdelmsAjVRUmkoLgquHFZxT6giWEynbRDJ5M4MxFLyD+48VfcuFDErRt3/o2TNgttPXDhcM693HuPHzMqlWV9GwuLS8srq6W18vrG5ta2ubPbllEiMGnhiEWi6yNJGOWkpahipBsLgkKfkY4/usj9zj0Rkkb8Ro1j4oZowGlAMVJa8szjjpcOH7LbtGofZdChHDohUkPfT6+1WL+DjqIhkTDOoGdWrJo1AZwndkEqoEDTM7+cfoSTkHCFGZKyZ1uxclMkFMWMZGUnkSRGeIQGpKcpR3qRm05+yuChVvowiIQuruBE/T2RolDKcejrzvxgOevl4n9eL1HBmZtSHieKcDxdFCQMqgjmAcE+FQQrNtYEYUH1rRAPkUBY6RjLOgR79uV50j6p2VbNvqpXGudFHCWwDw5AFdjgFDTAJWiCFsDgETyDV/BmPBkvxrvxMW1dMIqZPfAHxucP4VudfQ==</latexit>

2 R4qp

x4 <latexit sha1_base64="S4f+bq/VrUW0JYj64fGswbvelkw=">AAAB6nicbVBNS8NAEJ3Ur1q/qh69LBbBU0mkoHgqePFY0X5AG8pmO2mXbjZhdyOW0J/gxYMiXv1F3vw3btsctPXBwOO9GWbmBYng2rjut1NYW9/Y3Cpul3Z29/YPyodHLR2nimGTxSJWnYBqFFxi03AjsJMopFEgsB2Mb2Z++xGV5rF8MJME/YgOJQ85o8ZK90/9Wr9ccavuHGSVeDmpQI5Gv/zVG8QsjVAaJqjWXc9NjJ9RZTgTOC31Uo0JZWM6xK6lkkao/Wx+6pScWWVAwljZkobM1d8TGY20nkSB7YyoGellbyb+53VTE175GZdJalCyxaIwFcTEZPY3GXCFzIiJJZQpbm8lbEQVZcamU7IheMsvr5LWRdVzq95drVK/zuMowgmcwjl4cAl1uIUGNIHBEJ7hFd4c4bw4787HorXg5DPH8AfO5w8N1I2b</latexit>

h(41,3) <latexit sha1_base64="YpdeYAxYLdXBL/s6CFgxDNWoDc0=">AAAB9HicbVBNS8NAEJ3Ur1q/qh69LBahgpSkFhRPBS8eK9gPaGPZbDft0s0m7m4KJeR3ePGgiFd/jDf/jds2B219MPB4b4aZeV7EmdK2/W3l1tY3Nrfy24Wd3b39g+LhUUuFsSS0SUIeyo6HFeVM0KZmmtNOJCkOPE7b3vh25rcnVCoWigc9jagb4KFgPiNYG8kd9ZPaxWX6mJSd87RfLNkVew60SpyMlCBDo1/86g1CEgdUaMKxUl3HjrSbYKkZ4TQt9GJFI0zGeEi7hgocUOUm86NTdGaUAfJDaUpoNFd/TyQ4UGoaeKYzwHqklr2Z+J/XjbV/7SZMRLGmgiwW+TFHOkSzBNCASUo0nxqCiWTmVkRGWGKiTU4FE4Kz/PIqaVUrjl1x7mul+k0WRx5O4BTK4MAV1OEOGtAEAk/wDK/wZk2sF+vd+li05qxs5hj+wPr8AVNUkR4=</latexit>

h4(2,4) <latexit sha1_base64="Z8cZRxayNwJ9SGVDbYXdwsMfPno=">AAAB9HicbVBNS8NAEJ3Ur1q/qh69LBahgpSkFBRPBS8eK9gPaGPZbDft0s0m3d0USsjv8OJBEa/+GG/+G7dtDtr6YODx3gwz87yIM6Vt+9vKbWxube/kdwt7+weHR8Xjk5YKY0lok4Q8lB0PK8qZoE3NNKedSFIceJy2vfHd3G9PqVQsFI96FlE3wEPBfEawNpI76ie1q1r6lJSrl2m/WLIr9gJonTgZKUGGRr/41RuEJA6o0IRjpbqOHWk3wVIzwmla6MWKRpiM8ZB2DRU4oMpNFken6MIoA+SH0pTQaKH+nkhwoNQs8ExngPVIrXpz8T+vG2v/xk2YiGJNBVku8mOOdIjmCaABk5RoPjMEE8nMrYiMsMREm5wKJgRn9eV10qpWHLviPNRK9dssjjycwTmUwYFrqMM9NKAJBCbwDK/wZk2tF+vd+li25qxs5hT+wPr8AVZlkSA=</latexit>

Figure 5: A 2-layer LSTM is expressed as a trellis network with mixed group convolutions on four groups of feature channels. (Partial view.)

13

Under review as a conference paper at ICLR 2019

B

OPTIMIZING AND REGULARIZING TRELLISNET WITH RNN AND TCN

METHODS

Truncated <latexit sha1_base64="Mk2iSt4BcKdyVsVjsGdq5sFJR0s=">AAAB/nicbVDLSgNBEJyNrxhfUfHkZTARPIXdXPQY9OIxQl6QLGF2tpMMmZ1dZnqFsAT8FS8eFPHqd3jzb5w8DppY0FBT1c10V5BIYdB1v53cxubW9k5+t7C3f3B4VDw+aZk41RyaPJax7gTMgBQKmihQQifRwKJAQjsY38389iNoI2LVwEkCfsSGSgwEZ2ilfvGsoVNlHxBShhRFBLTcKPeLJbfizkHXibckJbJEvV/86oUxTyNQyCUzpuu5CfoZ0yi4hGmhlxpIGB+zIXQtVSwC42fz9af00iohHcTalkI6V39PZCwyZhIFtjNiODKr3kz8z+umOLjxM6GSFEHxxUeDVFKM6SwLGgoNHOXEEsa1sLtSPmKacbSJFWwI3urJ66RVrXhuxXuolmq3yzjy5JxckCvikWtSI/ekTpqEk4w8k1fy5jw5L86787FozTnLmVPyB87nD7cVlKU=</latexit>

at

time

T

History repackaging corresponds to
a kind of non-zero history padding in
TrellisNet (and TCNs in general).
<latexit sha1_base64="7bB0qr+84xms1mTu0nN0C+zf8sE=">AAACZHicdVHPS9xAFJ6kttqtrbHSU0EeXQr20JCsuvEo9eJJFHZV2CzLZPKyO+xkJsxMhG3wn+ytx178O5ysKbSlPhh4fD94732TVYIbG0U/Pf/FxstXm1uve2+2377bCXbfXxtVa4ZjpoTStxk1KLjEseVW4G2lkZaZwJtsedbyN3eoDVdyZFcVTks6l7zgjFoHzYImlYrLHKWFczdN6RVorChb0jmXc2BKazSVkrkBqyBNexSWTg+qAKnk1++oFSw6Y0XzvDVx2QpHGoU74AItHFDnGJ1dmJaao0RNxZcQZkE/CpPk6Cg5hCgcDI+Hw8Q18clhEg8gDqN19UlXl7PgR5orVpduWyaoMZM4quy0odpyJvC+l9am2x0nrpW0RDNt1iHdw2eH5FAo7Z67do3+6WhoacyqzJyypHZh/uVa8H/cpLbFybThsqotSvY0qKhFm1ebOORcI7Ni5RrKNHe7AltQTZl1/9JzIfy+FJ5vrgdh7JK5GvRPv3VxbJGP5BM5IDFJyCk5J5dkTBj55W16gbfrPfjb/p7/4Unqe51nj/xV/v4jo3a2zw==</latexit>

xT +1 <latexit sha1_base64="qetRKztGPtBFR5l7XpLVxqsh6Nc=">AAAB7nicbVBNS8NAEJ3Ur1q/qh69LBZBEEoigj0WvHis0C9oQ9lsN+3SzSbsTsQS+iO8eFDEq7/Hm//GbZuDtj4YeLw3w8y8IJHCoOt+O4WNza3tneJuaW//4PCofHzSNnGqGW+xWMa6G1DDpVC8hQIl7yaa0yiQvBNM7uZ+55FrI2LVxGnC/YiOlAgFo2ilztMga155s0G54lbdBcg68XJSgRyNQfmrP4xZGnGFTFJjep6boJ9RjYJJPiv1U8MTyiZ0xHuWKhpx42eLc2fkwipDEsbalkKyUH9PZDQyZhoFtjOiODar3lz8z+ulGNb8TKgkRa7YclGYSoIxmf9OhkJzhnJqCWVa2FsJG1NNGdqESjYEb/XlddK+rnpu1Xu4qdRreRxFOINzuAQPbqEO99CAFjCYwDO8wpuTOC/Ou/OxbC04+cwp/IHz+QPbn481</latexit> hT(1+) 1,1 <latexit sha1_base64="hNFfSyxk3GWlKtd2UXv7dwutHjA=">AAAB+HicbVBNS8NAEJ34WetHox69BItQUUpWBHssePFYoV/QxrDZbtqlm03Y3Qg19Jd48aCIV3+KN/+N2zYHbX0w8Hhvhpl5QcKZ0q77ba2tb2xubRd2irt7+wcl+/CoreJUEtoiMY9lN8CKciZoSzPNaTeRFEcBp51gfDvzO49UKhaLpp4k1IvwULCQEayN5NulkZ81L9Almj5kFXQ+9e2yW3XncFYJykkZcjR8+6s/iEkaUaEJx0r1kJtoL8NSM8LptNhPFU0wGeMh7RkqcESVl80PnzpnRhk4YSxNCe3M1d8TGY6UmkSB6YywHqllbyb+5/VSHda8jIkk1VSQxaIw5Y6OnVkKzoBJSjSfGIKJZOZWh4ywxESbrIomBLT88ippX1WRW0X31+V6LY+jACdwChVAcAN1uIMGtIBACs/wCm/Wk/VivVsfi9Y1K585hj+wPn8A1QWR2w==</latexit> hT(2+) 1,1 <latexit sha1_base64="nU9WoWCYWa7Q0ZNrnOd1M/6u7oc=">AAAB+HicbVDLSsNAFJ3UV62PRl26GSxCRSmZIiiuCm5cVugL2hgm00k7dDIJMxOhhnyJGxeKuPVT3Pk3TtsstPXAhcM593LvPX7MmdKO820V1tY3NreK26Wd3b39sn1w2FFRIgltk4hHsudjRTkTtK2Z5rQXS4pDn9OuP7md+d1HKhWLREtPY+qGeCRYwAjWRvLs8thLW+foAmUPabV+lnl2xak5c8BVgnJSATmanv01GEYkCanQhGOl+siJtZtiqRnhNCsNEkVjTCZ4RPuGChxS5abzwzN4apQhDCJpSmg4V39PpDhUahr6pjPEeqyWvZn4n9dPdHDtpkzEiaaCLBYFCYc6grMU4JBJSjSfGoKJZOZWSMZYYqJNViUTAlp+eZV06jXk1ND9ZaVxk8dRBMfgBFQBAlegAe5AE7QBAQl4Bq/gzXqyXqx362PRWrDymSPwB9bnD9clkd4=</latexit>

... ...

hT(2) 1,1 <latexit sha1_base64="fE1cCTapsK3zIqw8Pg24HnxpwMw=">AAAB+HicbVDLSsNAFJ3UV62PRl26GSxCBS2ZIiiuCm5cVugL2hgm00k7dDIJMxOhhnyJGxeKuPVT3Pk3TtsstPXAhcM593LvPX7MmdKO820V1tY3NreK26Wd3b39sn1w2FFRIgltk4hHsudjRTkTtK2Z5rQXS4pDn9OuP7md+d1HKhWLREtPY+qGeCRYwAjWRvLs8thLWxfoHGUPabV+lnl2xak5c8BVgnJSATmanv01GEYkCanQhGOl+siJtZtiqRnhNCsNEkVjTCZ4RPuGChxS5abzwzN4apQhDCJpSmg4V39PpDhUahr6pjPEeqyWvZn4n9dPdHDtpkzEiaaCLBYFCYc6grMU4JBJSjSfGoKJZOZWSMZYYqJNViUTAlp+eZV06jXk1ND9ZaVxk8dRBMfgBFQBAlegAe5AE7QBAQl4Bq/gzXqyXqx362PRWrDymSPwB9bnD9pBkeA=</latexit>

h(T2,)1 <latexit sha1_base64="EV47VmZDPBs9/1DEoVX6lpLCP3c=">AAAB9HicbVBNS8NAEJ3Ur1q/qh69LBahgpSkCPZY8OKxQr+gjWWz3bRLN5t0d1MoIb/DiwdFvPpjvPlv3LY5aOuDgcd7M8zM8yLOlLbtbyu3tb2zu5ffLxwcHh2fFE/P2iqMJaEtEvJQdj2sKGeCtjTTnHYjSXHgcdrxJvcLvzOjUrFQNPU8om6AR4L5jGBtJHc8SJo3TvqUlKvX6aBYsiv2EmiTOBkpQYbGoPjVH4YkDqjQhGOleo4daTfBUjPCaVrox4pGmEzwiPYMFTigyk2WR6foyihD5IfSlNBoqf6eSHCg1DzwTGeA9VitewvxP68Xa7/mJkxEsaaCrBb5MUc6RIsE0JBJSjSfG4KJZOZWRMZYYqJNTgUTgrP+8iZpVyuOXXEeb0v1WhZHHi7gEsrgwB3U4QEa0AICU3iGV3izZtaL9W59rFpzVjZzDn9gff4AgsqROw==</latexit>

Layer <latexit sha1_base64="JU08TK1DtIt6F5DuEM9JfoFGuoU=">AAAB7nicbVA9SwNBEJ2LXzF+RS1tFoNgFe5SqGXAxsIigvmA5Ah7m7lkyd7esbsnHEd+hI2FIrb+Hjv/jZvkCk18MPB4b4aZeUEiuDau++2UNja3tnfKu5W9/YPDo+rxSUfHqWLYZrGIVS+gGgWX2DbcCOwlCmkUCOwG09u5331CpXksH02WoB/RseQhZ9RYqXtPM1SkMazW3Lq7AFknXkFqUKA1rH4NRjFLI5SGCap133MT4+dUGc4EziqDVGNC2ZSOsW+ppBFqP1+cOyMXVhmRMFa2pCEL9fdETiOtsyiwnRE1E73qzcX/vH5qwhs/5zJJDUq2XBSmgpiYzH8nI66QGZFZQpni9lbCJlRRZmxCFRuCt/ryOuk06p5b9x4ateZVEUcZzuAcLsGDa2jCHbSgDQym8Ayv8OYkzovz7nwsW0tOMXMKf+B8/gCRT48B</latexit>

2

hT(1) 1,1 <latexit sha1_base64="y/FVYb6QZ9a7R1R98R13vjdYFc0=">AAAB+HicbVDJSgNBEK2JW4xLRj16aQxCBA3TIiieAl48RsgGyTj0dHqSJj0L3T1CHOZLvHhQxKuf4s2/sbMcNPqg4PFeFVX1/ERwpR3nyyqsrK6tbxQ3S1vbO7tle2+/reJUUtaisYhl1yeKCR6xluZasG4iGQl9wTr++Gbqdx6YVDyOmnqSMDckw4gHnBJtJM8uj7yseYZPcX6fVfFJ7tkVp+bMgP4SvCAVWKDh2Z/9QUzTkEWaCqJUDzuJdjMiNaeC5aV+qlhC6JgMWc/QiIRMudns8BwdG2WAgliaijSaqT8nMhIqNQl90xkSPVLL3lT8z+ulOrhyMx4lqWYRnS8KUoF0jKYpoAGXjGoxMYRQyc2tiI6IJFSbrEomBLz88l/SPq9hp4bvLir160UcRTiEI6gChkuowy00oAUUUniCF3i1Hq1n6816n7cWrMXMAfyC9fEN2LuR3w==</latexit>

hT(1,)1 <latexit sha1_base64="h4/JN0FCwd/16T8uYmGW0hiOxLk=">AAAB9HicbVBNSwMxEJ2tX7V+VT16CRahgpSNCIqnghePFfoF7VqyadqGZrNrki2UZX+HFw+KePXHePPfmLZ70NYHA4/3ZpiZ50eCa+O6305ubX1jcyu/XdjZ3ds/KB4eNXUYK8oaNBShavtEM8ElaxhuBGtHipHAF6zlj+9mfmvClOahrJtpxLyADCUfcEqMlbxRL6lf4PQxKePztFcsuRV3DrRKcEZKkKHWK351+yGNAyYNFUTrDnYj4yVEGU4FSwvdWLOI0DEZso6lkgRMe8n86BSdWaWPBqGyJQ2aq78nEhJoPQ182xkQM9LL3kz8z+vEZnDjJVxGsWGSLhYNYoFMiGYJoD5XjBoxtYRQxe2tiI6IItTYnAo2BLz88ippXlawW8EPV6XqbRZHHk7gFMqA4RqqcA81aACFJ3iGV3hzJs6L8+58LFpzTjZzDH/gfP4Agd6RPA==</latexit>

Layer <latexit sha1_base64="ZxQuDh5uth0sILyD6mLZELll5sw=">AAAB7nicbVA9SwNBEJ2LXzF+RS1tFoNgFe5SqGXAxsIigvmA5Ah7m7lkyd7esbsnHEd+hI2FIrb+Hjv/jZvkCk18MPB4b4aZeUEiuDau++2UNja3tnfKu5W9/YPDo+rxSUfHqWLYZrGIVS+gGgWX2DbcCOwlCmkUCOwG09u5331CpXksH02WoB/RseQhZ9RYqXtPM1TEG1Zrbt1dgKwTryA1KNAaVr8Go5ilEUrDBNW677mJ8XOqDGcCZ5VBqjGhbErH2LdU0gi1ny/OnZELq4xIGCtb0pCF+nsip5HWWRTYzoiaiV715uJ/Xj814Y2fc5mkBiVbLgpTQUxM5r+TEVfIjMgsoUxxeythE6ooMzahig3BW315nXQadc+tew+NWvOqiKMMZ3AOl+DBNTThDlrQBgZTeIZXeHMS58V5dz6WrSWnmDmFP3A+fwCPy48A</latexit>

1

hT(2,)1 <latexit sha1_base64="EV47VmZDPBs9/1DEoVX6lpLCP3c=">AAAB9HicbVBNS8NAEJ3Ur1q/qh69LBahgpSkCPZY8OKxQr+gjWWz3bRLN5t0d1MoIb/DiwdFvPpjvPlv3LY5aOuDgcd7M8zM8yLOlLbtbyu3tb2zu5ffLxwcHh2fFE/P2iqMJaEtEvJQdj2sKGeCtjTTnHYjSXHgcdrxJvcLvzOjUrFQNPU8om6AR4L5jGBtJHc8SJo3TvqUlKvX6aBYsiv2EmiTOBkpQYbGoPjVH4YkDqjQhGOleo4daTfBUjPCaVrox4pGmEzwiPYMFTigyk2WR6foyihD5IfSlNBoqf6eSHCg1DzwTGeA9VitewvxP68Xa7/mJkxEsaaCrBb5MUc6RIsE0JBJSjSfG4KJZOZWRMZYYqJNTgUTgrP+8iZpVyuOXXEeb0v1WhZHHi7gEsrgwB3U4QEa0AICU3iGV3izZtaL9W59rFpzVjZzDn9gff4AgsqROw==</latexit>

h(T2+) 1,1 <latexit sha1_base64="kKYH7p8a+9A6lDPD12H+MQBbr8Y=">AAAB+HicbVDLSsNAFJ3UV62PRl26GSxCRSmZIthlwY3LCn1BG8NkOmmHTiZhZiLUkC9x40IRt36KO//GaZuFth64cDjnXu69x485U9pxvq3CxubW9k5xt7S3f3BYto+OuypKJKEdEvFI9n2sKGeCdjTTnPZjSXHoc9rzp7dzv/dIpWKRaOtZTN0QjwULGMHaSJ5dnnhp+xJdoewhrdYvMs+uODVnAbhOUE4qIEfLs7+Go4gkIRWacKzUADmxdlMsNSOcZqVhomiMyRSP6cBQgUOq3HRxeAbPjTKCQSRNCQ0X6u+JFIdKzULfdIZYT9SqNxf/8waJDhpuykScaCrIclGQcKgjOE8BjpikRPOZIZhIZm6FZIIlJtpkVTIhoNWX10m3XkNODd1fV5qNPI4iOAVnoAoQuAFNcAdaoAMISMAzeAVv1pP1Yr1bH8vWgpXPnIA/sD5/ANaLkdw=</latexit>

h(T1,)1 <latexit sha1_base64="h4/JN0FCwd/16T8uYmGW0hiOxLk=">AAAB9HicbVBNSwMxEJ2tX7V+VT16CRahgpSNCIqnghePFfoF7VqyadqGZrNrki2UZX+HFw+KePXHePPfmLZ70NYHA4/3ZpiZ50eCa+O6305ubX1jcyu/XdjZ3ds/KB4eNXUYK8oaNBShavtEM8ElaxhuBGtHipHAF6zlj+9mfmvClOahrJtpxLyADCUfcEqMlbxRL6lf4PQxKePztFcsuRV3DrRKcEZKkKHWK351+yGNAyYNFUTrDnYj4yVEGU4FSwvdWLOI0DEZso6lkgRMe8n86BSdWaWPBqGyJQ2aq78nEhJoPQ182xkQM9LL3kz8z+vEZnDjJVxGsWGSLhYNYoFMiGYJoD5XjBoxtYRQxe2tiI6IItTYnAo2BLz88ippXlawW8EPV6XqbRZHHk7gFMqA4RqqcA81aACFJ3iGV3hzJs6L8+58LFpzTjZzDH/gfP4Agd6RPA==</latexit>

hT(1+) 1,1 <latexit sha1_base64="hNFfSyxk3GWlKtd2UXv7dwutHjA=">AAAB+HicbVBNS8NAEJ34WetHox69BItQUUpWBHssePFYoV/QxrDZbtqlm03Y3Qg19Jd48aCIV3+KN/+N2zYHbX0w8Hhvhpl5QcKZ0q77ba2tb2xubRd2irt7+wcl+/CoreJUEtoiMY9lN8CKciZoSzPNaTeRFEcBp51gfDvzO49UKhaLpp4k1IvwULCQEayN5NulkZ81L9Almj5kFXQ+9e2yW3XncFYJykkZcjR8+6s/iEkaUaEJx0r1kJtoL8NSM8LptNhPFU0wGeMh7RkqcESVl80PnzpnRhk4YSxNCe3M1d8TGY6UmkSB6YywHqllbyb+5/VSHda8jIkk1VSQxaIw5Y6OnVkKzoBJSjSfGIKJZOZWh4ywxESbrIomBLT88ippX1WRW0X31+V6LY+jACdwChVAcAN1uIMGtIBACs/wCm/Wk/VivVsfi9Y1K585hj+wPn8A1QWR2w==</latexit>

Layer <latexit sha1_base64="JU08TK1DtIt6F5DuEM9JfoFGuoU=">AAAB7nicbVA9SwNBEJ2LXzF+RS1tFoNgFe5SqGXAxsIigvmA5Ah7m7lkyd7esbsnHEd+hI2FIrb+Hjv/jZvkCk18MPB4b4aZeUEiuDau++2UNja3tnfKu5W9/YPDo+rxSUfHqWLYZrGIVS+gGgWX2DbcCOwlCmkUCOwG09u5331CpXksH02WoB/RseQhZ9RYqXtPM1SkMazW3Lq7AFknXkFqUKA1rH4NRjFLI5SGCap133MT4+dUGc4EziqDVGNC2ZSOsW+ppBFqP1+cOyMXVhmRMFa2pCEL9fdETiOtsyiwnRE1E73qzcX/vH5qwhs/5zJJDUq2XBSmgpiYzH8nI66QGZFZQpni9lbCJlRRZmxCFRuCt/ryOuk06p5b9x4ateZVEUcZzuAcLsGDa2jCHbSgDQym8Ayv8OYkzovz7nwsW0tOMXMKf+B8/gCRT48B</latexit>

2

Layer <latexit sha1_base64="ZxQuDh5uth0sILyD6mLZELll5sw=">AAAB7nicbVA9SwNBEJ2LXzF+RS1tFoNgFe5SqGXAxsIigvmA5Ah7m7lkyd7esbsnHEd+hI2FIrb+Hjv/jZvkCk18MPB4b4aZeUEiuDau++2UNja3tnfKu5W9/YPDo+rxSUfHqWLYZrGIVS+gGgWX2DbcCOwlCmkUCOwG09u5331CpXksH02WoB/RseQhZ9RYqXtPM1TEG1Zrbt1dgKwTryA1KNAaVr8Go5ilEUrDBNW677mJ8XOqDGcCZ5VBqjGhbErH2LdU0gi1ny/OnZELq4xIGCtb0pCF+nsip5HWWRTYzoiaiV715uJ/Xj814Y2fc5mkBiVbLgpTQUxM5r+TEVfIjMgsoUxxeythE6ooMzahig3BW315nXQadc+tew+NWvOqiKMMZ3AOl+DBNTThDlrQBgZTeIZXeHMS58V5dz6WrSWnmDmFP3A+fwCPy48A</latexit>

1

xT <latexit sha1_base64="dXjy5/4DK4Wimh7Sn1xvl8IKfMc=">AAAB7nicbVBNS8NAEJ34WetX1aOXxSJ4sSRFUDwVvHis0C9oQ9lsN+3SzSbsTsQS+iO8eFDEq7/Hm//GbZuDtj4YeLw3w8y8IJHCoOt+O2vrG5tb24Wd4u7e/sFh6ei4ZeJUM95ksYx1J6CGS6F4EwVK3kk0p1EgeTsY38389iPXRsSqgZOE+xEdKhEKRtFK7ad+1risTvulsltx5yCrxMtJGXLU+6Wv3iBmacQVMkmN6Xpugn5GNQom+bTYSw1PKBvTIe9aqmjEjZ/Nz52Sc6sMSBhrWwrJXP09kdHImEkU2M6I4sgsezPxP6+bYnjjZ0IlKXLFFovCVBKMyex3MhCaM5QTSyjTwt5K2IhqytAmVLQheMsvr5JWteK5Fe/hqly7zeMowCmcwQV4cA01uIc6NIHBGJ7hFd6cxHlx3p2PReuak8+cwB84nz/gyo86</latexit>

2

xT <latexit sha1_base64="Cbw1qmWaO0p0CMB0l93d9rvur2U=">AAAB7nicbVBNS8NAEJ3Ur1q/qh69LBbBiyURQfFU8OKxQr+gDWWz3bRLN5uwOxFL6I/w4kERr/4eb/4bt20O2vpg4PHeDDPzgkQKg6777RTW1jc2t4rbpZ3dvf2D8uFRy8SpZrzJYhnrTkANl0LxJgqUvJNoTqNA8nYwvpv57UeujYhVAycJ9yM6VCIUjKKV2k/9rHHhTfvlilt15yCrxMtJBXLU++Wv3iBmacQVMkmN6Xpugn5GNQom+bTUSw1PKBvTIe9aqmjEjZ/Nz52SM6sMSBhrWwrJXP09kdHImEkU2M6I4sgsezPxP6+bYnjjZ0IlKXLFFovCVBKMyex3MhCaM5QTSyjTwt5K2IhqytAmVLIheMsvr5LWZdVzq97DVaV2m8dRhBM4hXPw4BpqcA91aAKDMTzDK7w5ifPivDsfi9aCk88cwx84nz/fRY85</latexit>

1

xT <latexit sha1_base64="mxhW5YZNSmp2iLlswqU2446rSoM=">AAAB6nicbVBNS8NAEJ3Ur1q/qh69LBbBU0lEUDwVvHis2C9oQ9lsJ+3SzSbsbsQS+hO8eFDEq7/Im//GbZuDtj4YeLw3w8y8IBFcG9f9dgpr6xubW8Xt0s7u3v5B+fCopeNUMWyyWMSqE1CNgktsGm4EdhKFNAoEtoPx7cxvP6LSPJYNM0nQj+hQ8pAzaqz08NRv9MsVt+rOQVaJl5MK5Kj3y1+9QczSCKVhgmrd9dzE+BlVhjOB01Iv1ZhQNqZD7FoqaYTaz+anTsmZVQYkjJUtachc/T2R0UjrSRTYzoiakV72ZuJ/Xjc14bWfcZmkBiVbLApTQUxMZn+TAVfIjJhYQpni9lbCRlRRZmw6JRuCt/zyKmldVD236t1fVmo3eRxFOIFTOAcPrqAGd1CHJjAYwjO8wpsjnBfn3flYtBacfOYY/sD5/AE+VI27</latexit>

xT +1 <latexit sha1_base64="qetRKztGPtBFR5l7XpLVxqsh6Nc=">AAAB7nicbVBNS8NAEJ3Ur1q/qh69LBZBEEoigj0WvHis0C9oQ9lsN+3SzSbsTsQS+iO8eFDEq7/Hm//GbZuDtj4YeLw3w8y8IJHCoOt+O4WNza3tneJuaW//4PCofHzSNnGqGW+xWMa6G1DDpVC8hQIl7yaa0yiQvBNM7uZ+55FrI2LVxGnC/YiOlAgFo2ilztMga155s0G54lbdBcg68XJSgRyNQfmrP4xZGnGFTFJjep6boJ9RjYJJPiv1U8MTyiZ0xHuWKhpx42eLc2fkwipDEsbalkKyUH9PZDQyZhoFtjOiODar3lz8z+ulGNb8TKgkRa7YclGYSoIxmf9OhkJzhnJqCWVa2FsJG1NNGdqESjYEb/XlddK+rnpu1Xu4qdRreRxFOINzuAQPbqEO99CAFjCYwDO8wpuTOC/Ou/OxbC04+cwp/IHz+QPbn481</latexit>

Backpropagation <latexit sha1_base64="LncFmNq2rqbPQkuMtnveWZabwyc=">AAAB+HicbVBNTwIxEJ3FL8QPUI9eGomJJ7LLRY8ELx4xkY8ENqRbutDQbZu2a4IbfokXDxrj1Z/izX9jgT0o+JJJXt6bycy8SHFmrO9/e4Wt7Z3dveJ+6eDw6LhcOTntGJlqQttEcql7ETaUM0HblllOe0pTnEScdqPp7cLvPlJtmBQPdqZomOCxYDEj2DppWCk3MZkqLRUe50rVr/lLoE0S5KQKOVrDytdgJEmaUGEJx8b0A1/ZMMPaMsLpvDRIDVVuBx7TvqMCJ9SE2fLwObp0ygjFUrsSFi3V3xMZToyZJZHrTLCdmHVvIf7n9VMb34QZEyq1VJDVojjlyEq0SAGNmKbE8pkjmGjmbkVkgjUm1mVVciEE6y9vkk69Fvi14L5ebTTzOIpwDhdwBQFcQwPuoAVtIJDCM7zCm/fkvXjv3seqteDlM2fwB97nDw4+k1Q=</latexit>

Backpropagation <latexit sha1_base64="LncFmNq2rqbPQkuMtnveWZabwyc=">AAAB+HicbVBNTwIxEJ3FL8QPUI9eGomJJ7LLRY8ELx4xkY8ENqRbutDQbZu2a4IbfokXDxrj1Z/izX9jgT0o+JJJXt6bycy8SHFmrO9/e4Wt7Z3dveJ+6eDw6LhcOTntGJlqQttEcql7ETaUM0HblllOe0pTnEScdqPp7cLvPlJtmBQPdqZomOCxYDEj2DppWCk3MZkqLRUe50rVr/lLoE0S5KQKOVrDytdgJEmaUGEJx8b0A1/ZMMPaMsLpvDRIDVVuBx7TvqMCJ9SE2fLwObp0ygjFUrsSFi3V3xMZToyZJZHrTLCdmHVvIf7n9VMb34QZEyq1VJDVojjlyEq0SAGNmKbE8pkjmGjmbkVkgjUm1mVVciEE6y9vkk69Fvi14L5ebTTzOIpwDhdwBQFcQwPuoAVtIJDCM7zCm/fkvXjv3seqteDlM2fwB97nDw4+k1Q=</latexit>

(a) History repackaging between truncated sequences in recurrent networks.

xT <latexit sha1_base64="dXjy5/4DK4Wimh7Sn1xvl8IKfMc=">AAAB7nicbVBNS8NAEJ34WetX1aOXxSJ4sSRFUDwVvHis0C9oQ9lsN+3SzSbsTsQS+iO8eFDEq7/Hm//GbZuDtj4YeLw3w8y8IJHCoOt+O2vrG5tb24Wd4u7e/sFh6ei4ZeJUM95ksYx1J6CGS6F4EwVK3kk0p1EgeTsY38389iPXRsSqgZOE+xEdKhEKRtFK7ad+1risTvulsltx5yCrxMtJGXLU+6Wv3iBmacQVMkmN6Xpugn5GNQom+bTYSw1PKBvTIe9aqmjEjZ/Nz52Sc6sMSBhrWwrJXP09kdHImEkU2M6I4sgsezPxP6+bYnjjZ0IlKXLFFovCVBKMyex3MhCaM5QTSyjTwt5K2IhqytAmVLQheMsvr5JWteK5Fe/hqly7zeMowCmcwQV4cA01uIc6NIHBGJ7hFd6cxHlx3p2PReuak8+cwB84nz/gyo86</latexit>

2

...

xT <latexit sha1_base64="Cbw1qmWaO0p0CMB0l93d9rvur2U=">AAAB7nicbVBNS8NAEJ3Ur1q/qh69LBbBiyURQfFU8OKxQr+gDWWz3bRLN5uwOxFL6I/w4kERr/4eb/4bt20O2vpg4PHeDDPzgkQKg6777RTW1jc2t4rbpZ3dvf2D8uFRy8SpZrzJYhnrTkANl0LxJgqUvJNoTqNA8nYwvpv57UeujYhVAycJ9yM6VCIUjKKV2k/9rHHhTfvlilt15yCrxMtJBXLU++Wv3iBmacQVMkmN6Xpugn5GNQom+bTUSw1PKBvTIe9aqmjEjZ/Nz52SM6sMSBhrWwrJXP09kdHImEkU2M6I4sgsezPxP6+bYnjjZ0IlKXLFFovCVBKMyex3MhCaM5QTSyjTwt5K2IhqytAmVLIheMsvr5LWZdVzq97DVaV2m8dRhBM4hXPw4BpqcA91aAKDMTzDK7w5ifPivDsfi9aCk88cwx84nz/fRY85</latexit>

1

hT(1) 1,1 <latexit sha1_base64="y/FVYb6QZ9a7R1R98R13vjdYFc0=">AAAB+HicbVDJSgNBEK2JW4xLRj16aQxCBA3TIiieAl48RsgGyTj0dHqSJj0L3T1CHOZLvHhQxKuf4s2/sbMcNPqg4PFeFVX1/ERwpR3nyyqsrK6tbxQ3S1vbO7tle2+/reJUUtaisYhl1yeKCR6xluZasG4iGQl9wTr++Gbqdx6YVDyOmnqSMDckw4gHnBJtJM8uj7yseYZPcX6fVfFJ7tkVp+bMgP4SvCAVWKDh2Z/9QUzTkEWaCqJUDzuJdjMiNaeC5aV+qlhC6JgMWc/QiIRMudns8BwdG2WAgliaijSaqT8nMhIqNQl90xkSPVLL3lT8z+ulOrhyMx4lqWYRnS8KUoF0jKYpoAGXjGoxMYRQyc2tiI6IJFSbrEomBLz88l/SPq9hp4bvLir160UcRTiEI6gChkuowy00oAUUUniCF3i1Hq1n6816n7cWrMXMAfyC9fEN2LuR3w==</latexit>
hT(2) 1,1 <latexit sha1_base64="fE1cCTapsK3zIqw8Pg24HnxpwMw=">AAAB+HicbVDLSsNAFJ3UV62PRl26GSxCBS2ZIiiuCm5cVugL2hgm00k7dDIJMxOhhnyJGxeKuPVT3Pk3TtsstPXAhcM593LvPX7MmdKO820V1tY3NreK26Wd3b39sn1w2FFRIgltk4hHsudjRTkTtK2Z5rQXS4pDn9OuP7md+d1HKhWLREtPY+qGeCRYwAjWRvLs8thLWxfoHGUPabV+lnl2xak5c8BVgnJSATmanv01GEYkCanQhGOl+siJtZtiqRnhNCsNEkVjTCZ4RPuGChxS5abzwzN4apQhDCJpSmg4V39PpDhUahr6pjPEeqyWvZn4n9dPdHDtpkzEiaaCLBYFCYc6grMU4JBJSjSfGoKJZOZWSMZYYqJNViUTAlp+eZV06jXk1ND9ZaVxk8dRBMfgBFQBAlegAe5AE7QBAQl4Bq/gzXqyXqx362PRWrDymSPwB9bnD9pBkeA=</latexit>

xT <latexit sha1_base64="mxhW5YZNSmp2iLlswqU2446rSoM=">AAAB6nicbVBNS8NAEJ3Ur1q/qh69LBbBU0lEUDwVvHis2C9oQ9lsJ+3SzSbsbsQS+hO8eFDEq7/Im//GbZuDtj4YeLw3w8y8IBFcG9f9dgpr6xubW8Xt0s7u3v5B+fCopeNUMWyyWMSqE1CNgktsGm4EdhKFNAoEtoPx7cxvP6LSPJYNM0nQj+hQ8pAzaqz08NRv9MsVt+rOQVaJl5MK5Kj3y1+9QczSCKVhgmrd9dzE+BlVhjOB01Iv1ZhQNqZD7FoqaYTaz+anTsmZVQYkjJUtachc/T2R0UjrSRTYzoiakV72ZuJ/Xjc14bWfcZmkBiVbLApTQUxMZn+TAVfIjJhYQpni9lbCRlRRZmw6JRuCt/zyKmldVD236t1fVmo3eRxFOIFTOAcPrqAGd1CHJjAYwjO8wpsjnBfn3flYtBacfOYY/sD5/AE+VI27</latexit>
h(T1,)1 <latexit sha1_base64="h4/JN0FCwd/16T8uYmGW0hiOxLk=">AAAB9HicbVBNSwMxEJ2tX7V+VT16CRahgpSNCIqnghePFfoF7VqyadqGZrNrki2UZX+HFw+KePXHePPfmLZ70NYHA4/3ZpiZ50eCa+O6305ubX1jcyu/XdjZ3ds/KB4eNXUYK8oaNBShavtEM8ElaxhuBGtHipHAF6zlj+9mfmvClOahrJtpxLyADCUfcEqMlbxRL6lf4PQxKePztFcsuRV3DrRKcEZKkKHWK351+yGNAyYNFUTrDnYj4yVEGU4FSwvdWLOI0DEZso6lkgRMe8n86BSdWaWPBqGyJQ2aq78nEhJoPQ182xkQM9LL3kz8z+vEZnDjJVxGsWGSLhYNYoFMiGYJoD5XjBoxtYRQxe2tiI6IItTYnAo2BLz88ippXlawW8EPV6XqbRZHHk7gFMqA4RqqcA81aACFJ3iGV3hzJs6L8+58LFpzTjZzDH/gfP4Agd6RPA==</latexit>
h(T2,)1 <latexit sha1_base64="EV47VmZDPBs9/1DEoVX6lpLCP3c=">AAAB9HicbVBNS8NAEJ3Ur1q/qh69LBahgpSkCPZY8OKxQr+gjWWz3bRLN5t0d1MoIb/DiwdFvPpjvPlv3LY5aOuDgcd7M8zM8yLOlLbtbyu3tb2zu5ffLxwcHh2fFE/P2iqMJaEtEvJQdj2sKGeCtjTTnHYjSXHgcdrxJvcLvzOjUrFQNPU8om6AR4L5jGBtJHc8SJo3TvqUlKvX6aBYsiv2EmiTOBkpQYbGoPjVH4YkDqjQhGOleo4daTfBUjPCaVrox4pGmEzwiPYMFTigyk2WR6foyihD5IfSlNBoqf6eSHCg1DzwTGeA9VitewvxP68Xa7/mJkxEsaaCrBb5MUc6RIsE0JBJSjSfG4KJZOZWRMZYYqJNTgUTgrP+8iZpVyuOXXEeb0v1WhZHHi7gEsrgwB3U4QEa0AICU3iGV3izZtaL9W59rFpzVjZzDn9gff4AgsqROw==</latexit>

Layer <latexit sha1_base64="SAl8ZIDAGqF6+OQtJeM24sLD3zo=">AAAB8nicbVA9SwNBEN2LXzF+RS1tFhNBEMJdCrUM2FhYRMgXXI6wt5kkS/Z2j9094TjyM2wsFLH119j5b9wkV2jig4HHezPMzAtjzrRx3W+nsLG5tb1T3C3t7R8cHpWPTzpaJopCm0ouVS8kGjgT0DbMcOjFCkgUcuiG07u5330CpZkULZPGEERkLNiIUWKs5D+QFBSutq686qBccWvuAnideDmpoBzNQfmrP5Q0iUAYyonWvufGJsiIMoxymJX6iYaY0CkZg2+pIBHoIFucPMMXVhnikVS2hMEL9fdERiKt0yi0nRExE73qzcX/PD8xo9sgYyJODAi6XDRKODYSz//HQ6aAGp5aQqhi9lZMJ0QRamxKJRuCt/ryOunUa55b8x7rlcZ1HkcRnaFzdIk8dIMa6B41URtRJNEzekVvjnFenHfnY9lacPKZU/QHzucPW9iP7w==</latexit>

T

+1

0 <latexit sha1_base64="SnGAWWDvcKOm5XCgld8TjcUaeUk=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lEaI8FLx5bsB/QhrLZTtq1m03Y3Qgl9Bd48aCIV3+SN/+N2zYHbX0w8Hhvhpl5QSK4Nq777RS2tnd294r7pYPDo+OT8ulZR8epYthmsYhVL6AaBZfYNtwI7CUKaRQI7AbTu4XffUKleSwfzCxBP6JjyUPOqLFSyx2WK27VXYJsEi8nFcjRHJa/BqOYpRFKwwTVuu+5ifEzqgxnAuelQaoxoWxKx9i3VNIItZ8tD52TK6uMSBgrW9KQpfp7IqOR1rMosJ0RNRO97i3E/7x+asK6n3GZpAYlWy0KU0FMTBZfkxFXyIyYWUKZ4vZWwiZUUWZsNiUbgrf+8ibp3FQ9t+q1biuNeh5HES7gEq7Bgxo04B6a0AYGCM/wCm/Oo/PivDsfq9aCk8+cwx84nz92F4yq</latexit>
hT(1,)1 <latexit sha1_base64="h4/JN0FCwd/16T8uYmGW0hiOxLk=">AAAB9HicbVBNSwMxEJ2tX7V+VT16CRahgpSNCIqnghePFfoF7VqyadqGZrNrki2UZX+HFw+KePXHePPfmLZ70NYHA4/3ZpiZ50eCa+O6305ubX1jcyu/XdjZ3ds/KB4eNXUYK8oaNBShavtEM8ElaxhuBGtHipHAF6zlj+9mfmvClOahrJtpxLyADCUfcEqMlbxRL6lf4PQxKePztFcsuRV3DrRKcEZKkKHWK351+yGNAyYNFUTrDnYj4yVEGU4FSwvdWLOI0DEZso6lkgRMe8n86BSdWaWPBqGyJQ2aq78nEhJoPQ182xkQM9LL3kz8z+vEZnDjJVxGsWGSLhYNYoFMiGYJoD5XjBoxtYRQxe2tiI6IItTYnAo2BLz88ippXlawW8EPV6XqbRZHHk7gFMqA4RqqcA81aACFJ3iGV3hzJs6L8+58LFpzTjZzDH/gfP4Agd6RPA==</latexit> hT(2,)1 <latexit sha1_base64="EV47VmZDPBs9/1DEoVX6lpLCP3c=">AAAB9HicbVBNS8NAEJ3Ur1q/qh69LBahgpSkCPZY8OKxQr+gjWWz3bRLN5t0d1MoIb/DiwdFvPpjvPlv3LY5aOuDgcd7M8zM8yLOlLbtbyu3tb2zu5ffLxwcHh2fFE/P2iqMJaEtEvJQdj2sKGeCtjTTnHYjSXHgcdrxJvcLvzOjUrFQNPU8om6AR4L5jGBtJHc8SJo3TvqUlKvX6aBYsiv2EmiTOBkpQYbGoPjVH4YkDqjQhGOleo4daTfBUjPCaVrox4pGmEzwiPYMFTigyk2WR6foyihD5IfSlNBoqf6eSHCg1DzwTGeA9VitewvxP68Xa7/mJkxEsaaCrBb5MUc6RIsE0JBJSjSfG4KJZOZWRMZYYqJNTgUTgrP+8iZpVyuOXXEeb0v1WhZHHi7gEsrgwB3U4QEa0AICU3iGV3izZtaL9W59rFpzVjZzDn9gff4AgsqROw==</latexit>

xT +1 <latexit sha1_base64="qetRKztGPtBFR5l7XpLVxqsh6Nc=">AAAB7nicbVBNS8NAEJ3Ur1q/qh69LBZBEEoigj0WvHis0C9oQ9lsN+3SzSbsTsQS+iO8eFDEq7/Hm//GbZuDtj4YeLw3w8y8IJHCoOt+O4WNza3tneJuaW//4PCofHzSNnGqGW+xWMa6G1DDpVC8hQIl7yaa0yiQvBNM7uZ+55FrI2LVxGnC/YiOlAgFo2ilztMga155s0G54lbdBcg68XJSgRyNQfmrP4xZGnGFTFJjep6boJ9RjYJJPiv1U8MTyiZ0xHuWKhpx42eLc2fkwipDEsbalkKyUH9PZDQyZhoFtjOiODar3lz8z+ulGNb8TKgkRa7YclGYSoIxmf9OhkJzhnJqCWVa2FsJG1NNGdqESjYEb/XlddK+rnpu1Xu4qdRreRxFOINzuAQPbqEO99CAFjCYwDO8wpuTOC/Ou/OxbC04+cwp/IHz+QPbn481</latexit> hT(1+) 1,1 <latexit sha1_base64="hNFfSyxk3GWlKtd2UXv7dwutHjA=">AAAB+HicbVBNS8NAEJ34WetHox69BItQUUpWBHssePFYoV/QxrDZbtqlm03Y3Qg19Jd48aCIV3+KN/+N2zYHbX0w8Hhvhpl5QcKZ0q77ba2tb2xubRd2irt7+wcl+/CoreJUEtoiMY9lN8CKciZoSzPNaTeRFEcBp51gfDvzO49UKhaLpp4k1IvwULCQEayN5NulkZ81L9Almj5kFXQ+9e2yW3XncFYJykkZcjR8+6s/iEkaUaEJx0r1kJtoL8NSM8LptNhPFU0wGeMh7RkqcESVl80PnzpnRhk4YSxNCe3M1d8TGY6UmkSB6YywHqllbyb+5/VSHda8jIkk1VSQxaIw5Y6OnVkKzoBJSjSfGIKJZOZWh4ywxESbrIomBLT88ippX1WRW0X31+V6LY+jACdwChVAcAN1uIMGtIBACs/wCm/Wk/VivVsfi9Y1K585hj+wPn8A1QWR2w==</latexit>

Layer <latexit sha1_base64="JU08TK1DtIt6F5DuEM9JfoFGuoU=">AAAB7nicbVA9SwNBEJ2LXzF+RS1tFoNgFe5SqGXAxsIigvmA5Ah7m7lkyd7esbsnHEd+hI2FIrb+Hjv/jZvkCk18MPB4b4aZeUEiuDau++2UNja3tnfKu5W9/YPDo+rxSUfHqWLYZrGIVS+gGgWX2DbcCOwlCmkUCOwG09u5331CpXksH02WoB/RseQhZ9RYqXtPM1SkMazW3Lq7AFknXkFqUKA1rH4NRjFLI5SGCap133MT4+dUGc4EziqDVGNC2ZSOsW+ppBFqP1+cOyMXVhmRMFa2pCEL9fdETiOtsyiwnRE1E73qzcX/vH5qwhs/5zJJDUq2XBSmgpiYzH8nI66QGZFZQpni9lbCJlRRZmxCFRuCt/ryOuk06p5b9x4ateZVEUcZzuAcLsGDa2jCHbSgDQym8Ayv8OYkzovz7nwsW0tOMXMKf+B8/gCRT48B</latexit>

2

...

xT <latexit sha1_base64="dXjy5/4DK4Wimh7Sn1xvl8IKfMc=">AAAB7nicbVBNS8NAEJ34WetX1aOXxSJ4sSRFUDwVvHis0C9oQ9lsN+3SzSbsTsQS+iO8eFDEq7/Hm//GbZuDtj4YeLw3w8y8IJHCoOt+O2vrG5tb24Wd4u7e/sFh6ei4ZeJUM95ksYx1J6CGS6F4EwVK3kk0p1EgeTsY38389iPXRsSqgZOE+xEdKhEKRtFK7ad+1risTvulsltx5yCrxMtJGXLU+6Wv3iBmacQVMkmN6Xpugn5GNQom+bTYSw1PKBvTIe9aqmjEjZ/Nz52Sc6sMSBhrWwrJXP09kdHImEkU2M6I4sgsezPxP6+bYnjjZ0IlKXLFFovCVBKMyex3MhCaM5QTSyjTwt5K2IhqytAmVLQheMsvr5JWteK5Fe/hqly7zeMowCmcwQV4cA01uIc6NIHBGJ7hFd6cxHlx3p2PReuak8+cwB84nz/gyo86</latexit>

2

xT <latexit sha1_base64="Cbw1qmWaO0p0CMB0l93d9rvur2U=">AAAB7nicbVBNS8NAEJ3Ur1q/qh69LBbBiyURQfFU8OKxQr+gDWWz3bRLN5uwOxFL6I/w4kERr/4eb/4bt20O2vpg4PHeDDPzgkQKg6777RTW1jc2t4rbpZ3dvf2D8uFRy8SpZrzJYhnrTkANl0LxJgqUvJNoTqNA8nYwvpv57UeujYhVAycJ9yM6VCIUjKKV2k/9rHHhTfvlilt15yCrxMtJBXLU++Wv3iBmacQVMkmN6Xpugn5GNQom+bTUSw1PKBvTIe9aqmjEjZ/Nz52SM6sMSBhrWwrJXP09kdHImEkU2M6I4sgsezPxP6+bYnjjZ0IlKXLFFovCVBKMyex3MhCaM5QTSyjTwt5K2IhqytAmVLIheMsvr5LWZdVzq97DVaV2m8dRhBM4hXPw4BpqcA91aAKDMTzDK7w5ifPivDsfi9aCk88cwx84nz/fRY85</latexit>

1

hT(1) 1,1 <latexit sha1_base64="y/FVYb6QZ9a7R1R98R13vjdYFc0=">AAAB+HicbVDJSgNBEK2JW4xLRj16aQxCBA3TIiieAl48RsgGyTj0dHqSJj0L3T1CHOZLvHhQxKuf4s2/sbMcNPqg4PFeFVX1/ERwpR3nyyqsrK6tbxQ3S1vbO7tle2+/reJUUtaisYhl1yeKCR6xluZasG4iGQl9wTr++Gbqdx6YVDyOmnqSMDckw4gHnBJtJM8uj7yseYZPcX6fVfFJ7tkVp+bMgP4SvCAVWKDh2Z/9QUzTkEWaCqJUDzuJdjMiNaeC5aV+qlhC6JgMWc/QiIRMudns8BwdG2WAgliaijSaqT8nMhIqNQl90xkSPVLL3lT8z+ulOrhyMx4lqWYRnS8KUoF0jKYpoAGXjGoxMYRQyc2tiI6IJFSbrEomBLz88l/SPq9hp4bvLir160UcRTiEI6gChkuowy00oAUUUniCF3i1Hq1n6816n7cWrMXMAfyC9fEN2LuR3w==</latexit>
hT(2) 1,1 <latexit sha1_base64="fE1cCTapsK3zIqw8Pg24HnxpwMw=">AAAB+HicbVDLSsNAFJ3UV62PRl26GSxCBS2ZIiiuCm5cVugL2hgm00k7dDIJMxOhhnyJGxeKuPVT3Pk3TtsstPXAhcM593LvPX7MmdKO820V1tY3NreK26Wd3b39sn1w2FFRIgltk4hHsudjRTkTtK2Z5rQXS4pDn9OuP7md+d1HKhWLREtPY+qGeCRYwAjWRvLs8thLWxfoHGUPabV+lnl2xak5c8BVgnJSATmanv01GEYkCanQhGOl+siJtZtiqRnhNCsNEkVjTCZ4RPuGChxS5abzwzN4apQhDCJpSmg4V39PpDhUahr6pjPEeqyWvZn4n9dPdHDtpkzEiaaCLBYFCYc6grMU4JBJSjSfGoKJZOZWSMZYYqJNViUTAlp+eZV06jXk1ND9ZaVxk8dRBMfgBFQBAlegAe5AE7QBAQl4Bq/gzXqyXqx362PRWrDymSPwB9bnD9pBkeA=</latexit> ...

xT <latexit sha1_base64="mxhW5YZNSmp2iLlswqU2446rSoM=">AAAB6nicbVBNS8NAEJ3Ur1q/qh69LBbBU0lEUDwVvHis2C9oQ9lsJ+3SzSbsbsQS+hO8eFDEq7/Im//GbZuDtj4YeLw3w8y8IBFcG9f9dgpr6xubW8Xt0s7u3v5B+fCopeNUMWyyWMSqE1CNgktsGm4EdhKFNAoEtoPx7cxvP6LSPJYNM0nQj+hQ8pAzaqz08NRv9MsVt+rOQVaJl5MK5Kj3y1+9QczSCKVhgmrd9dzE+BlVhjOB01Iv1ZhQNqZD7FoqaYTaz+anTsmZVQYkjJUtachc/T2R0UjrSRTYzoiakV72ZuJ/Xjc14bWfcZmkBiVbLApTQUxMZn+TAVfIjJhYQpni9lbCRlRRZmw6JRuCt/zyKmldVD236t1fVmo3eRxFOIFTOAcPrqAGd1CHJjAYwjO8wpsjnBfn3flYtBacfOYY/sD5/AE+VI27</latexit>
hT(1,)1 <latexit sha1_base64="h4/JN0FCwd/16T8uYmGW0hiOxLk=">AAAB9HicbVBNSwMxEJ2tX7V+VT16CRahgpSNCIqnghePFfoF7VqyadqGZrNrki2UZX+HFw+KePXHePPfmLZ70NYHA4/3ZpiZ50eCa+O6305ubX1jcyu/XdjZ3ds/KB4eNXUYK8oaNBShavtEM8ElaxhuBGtHipHAF6zlj+9mfmvClOahrJtpxLyADCUfcEqMlbxRL6lf4PQxKePztFcsuRV3DrRKcEZKkKHWK351+yGNAyYNFUTrDnYj4yVEGU4FSwvdWLOI0DEZso6lkgRMe8n86BSdWaWPBqGyJQ2aq78nEhJoPQ182xkQM9LL3kz8z+vEZnDjJVxGsWGSLhYNYoFMiGYJoD5XjBoxtYRQxe2tiI6IItTYnAo2BLz88ippXlawW8EPV6XqbRZHHk7gFMqA4RqqcA81aACFJ3iGV3hzJs6L8+58LFpzTjZzDH/gfP4Agd6RPA==</latexit> h(T2,)2 <latexit sha1_base64="dBbrzC8c00u+Haqy68kHIHlR20M=">AAAB9HicbVBNS8NAEJ3Ur1q/qh69LBahgpSkCIqnghePFfoFbSyb7aZdutmku5tCCfkdXjwo4tUf481/47bNQVsfDDzem2FmnhdxprRtf1u5jc2t7Z38bmFv/+DwqHh80lJhLAltkpCHsuNhRTkTtKmZ5rQTSYoDj9O2N76f++0plYqFoqFnEXUDPBTMZwRrI7mjftK4qqZPSbl6mfaLJbtiL4DWiZOREmSo94tfvUFI4oAKTThWquvYkXYTLDUjnKaFXqxohMkYD2nXUIEDqtxkcXSKLowyQH4oTQmNFurviQQHSs0Cz3QGWI/UqjcX//O6sfZv3YSJKNZUkOUiP+ZIh2ieABowSYnmM0MwkczcisgIS0y0yalgQnBWX14nrWrFsSvO43WpdpfFkYczOIcyOHADNXiAOjSBwASe4RXerKn1Yr1bH8vWnJXNnMIfWJ8/hO+RPg==</latexit>

Layer <latexit sha1_base64="95kS2GDLkzDmHwiYUt/qqictcx8=">AAAB8HicbVA9SwNBEN2LXzF+RS1tFhPBKtylUMuAjYVFhHxJcoS9zVyyZHfv2N0TjiO/wsZCEVt/jp3/xk1yhSY+GHi8N8PMvCDmTBvX/XYKG5tb2zvF3dLe/sHhUfn4pKOjRFFo04hHqhcQDZxJaBtmOPRiBUQEHLrB9Hbud59AaRbJlklj8AUZSxYySoyVHu9JCgpXW9VhueLW3AXwOvFyUkE5msPy12AU0USANJQTrfueGxs/I8owymFWGiQaYkKnZAx9SyURoP1scfAMX1hlhMNI2ZIGL9TfExkRWqcisJ2CmIle9ebif14/MeGNnzEZJwYkXS4KE45NhOff4xFTQA1PLSFUMXsrphOiCDU2o5INwVt9eZ106jXPrXkP9UrjKo+jiM7QObpEHrpGDXSHmqiNKBLoGb2iN0c5L86787FsLTj5zCn6A+fzB4C+j38=</latexit>

T

0 <latexit sha1_base64="SnGAWWDvcKOm5XCgld8TjcUaeUk=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lEaI8FLx5bsB/QhrLZTtq1m03Y3Qgl9Bd48aCIV3+SN/+N2zYHbX0w8Hhvhpl5QSK4Nq777RS2tnd294r7pYPDo+OT8ulZR8epYthmsYhVL6AaBZfYNtwI7CUKaRQI7AbTu4XffUKleSwfzCxBP6JjyUPOqLFSyx2WK27VXYJsEi8nFcjRHJa/BqOYpRFKwwTVuu+5ifEzqgxnAuelQaoxoWxKx9i3VNIItZ8tD52TK6uMSBgrW9KQpfp7IqOR1rMosJ0RNRO97i3E/7x+asK6n3GZpAYlWy0KU0FMTBZfkxFXyIyYWUKZ4vZWwiZUUWZsNiUbgrf+8ibp3FQ9t+q1biuNeh5HES7gEq7Bgxo04B6a0AYGCM/wCm/Oo/PivDsfq9aCk8+cwx84nz92F4yq</latexit>
h(T1,)1 <latexit sha1_base64="h4/JN0FCwd/16T8uYmGW0hiOxLk=">AAAB9HicbVBNSwMxEJ2tX7V+VT16CRahgpSNCIqnghePFfoF7VqyadqGZrNrki2UZX+HFw+KePXHePPfmLZ70NYHA4/3ZpiZ50eCa+O6305ubX1jcyu/XdjZ3ds/KB4eNXUYK8oaNBShavtEM8ElaxhuBGtHipHAF6zlj+9mfmvClOahrJtpxLyADCUfcEqMlbxRL6lf4PQxKePztFcsuRV3DrRKcEZKkKHWK351+yGNAyYNFUTrDnYj4yVEGU4FSwvdWLOI0DEZso6lkgRMe8n86BSdWaWPBqGyJQ2aq78nEhJoPQ182xkQM9LL3kz8z+vEZnDjJVxGsWGSLhYNYoFMiGYJoD5XjBoxtYRQxe2tiI6IItTYnAo2BLz88ippXlawW8EPV6XqbRZHHk7gFMqA4RqqcA81aACFJ3iGV3hzJs6L8+58LFpzTjZzDH/gfP4Agd6RPA==</latexit> h(T2,)1 <latexit sha1_base64="EV47VmZDPBs9/1DEoVX6lpLCP3c=">AAAB9HicbVBNS8NAEJ3Ur1q/qh69LBahgpSkCPZY8OKxQr+gjWWz3bRLN5t0d1MoIb/DiwdFvPpjvPlv3LY5aOuDgcd7M8zM8yLOlLbtbyu3tb2zu5ffLxwcHh2fFE/P2iqMJaEtEvJQdj2sKGeCtjTTnHYjSXHgcdrxJvcLvzOjUrFQNPU8om6AR4L5jGBtJHc8SJo3TvqUlKvX6aBYsiv2EmiTOBkpQYbGoPjVH4YkDqjQhGOleo4daTfBUjPCaVrox4pGmEzwiPYMFTigyk2WR6foyihD5IfSlNBoqf6eSHCg1DzwTGeA9VitewvxP68Xa7/mJkxEsaaCrBb5MUc6RIsE0JBJSjSfG4KJZOZWRMZYYqJNTgUTgrP+8iZpVyuOXXEeb0v1WhZHHi7gEsrgwB3U4QEa0AICU3iGV3izZtaL9W59rFpzVjZzDn9gff4AgsqROw==</latexit>

xT +1 <latexit sha1_base64="qetRKztGPtBFR5l7XpLVxqsh6Nc=">AAAB7nicbVBNS8NAEJ3Ur1q/qh69LBZBEEoigj0WvHis0C9oQ9lsN+3SzSbsTsQS+iO8eFDEq7/Hm//GbZuDtj4YeLw3w8y8IJHCoOt+O4WNza3tneJuaW//4PCofHzSNnGqGW+xWMa6G1DDpVC8hQIl7yaa0yiQvBNM7uZ+55FrI2LVxGnC/YiOlAgFo2ilztMga155s0G54lbdBcg68XJSgRyNQfmrP4xZGnGFTFJjep6boJ9RjYJJPiv1U8MTyiZ0xHuWKhpx42eLc2fkwipDEsbalkKyUH9PZDQyZhoFtjOiODar3lz8z+ulGNb8TKgkRa7YclGYSoIxmf9OhkJzhnJqCWVa2FsJG1NNGdqESjYEb/XlddK+rnpu1Xu4qdRreRxFOINzuAQPbqEO99CAFjCYwDO8wpuTOC/Ou/OxbC04+cwp/IHz+QPbn481</latexit>

Layer <latexit sha1_base64="ZxQuDh5uth0sILyD6mLZELll5sw=">AAAB7nicbVA9SwNBEJ2LXzF+RS1tFoNgFe5SqGXAxsIigvmA5Ah7m7lkyd7esbsnHEd+hI2FIrb+Hjv/jZvkCk18MPB4b4aZeUEiuDau++2UNja3tnfKu5W9/YPDo+rxSUfHqWLYZrGIVS+gGgWX2DbcCOwlCmkUCOwG09u5331CpXksH02WoB/RseQhZ9RYqXtPM1TEG1Zrbt1dgKwTryA1KNAaVr8Go5ilEUrDBNW677mJ8XOqDGcCZ5VBqjGhbErH2LdU0gi1ny/OnZELq4xIGCtb0pCF+nsip5HWWRTYzoiaiV715uJ/Xj814Y2fc5mkBiVbLgpTQUxM5r+TEVfIjMgsoUxxeythE6ooMzahig3BW315nXQadc+tew+NWvOqiKMMZ3AOl+DBNTThDlrQBgZTeIZXeHMS58V5dz6WrSWnmDmFP3A+fwCPy48A</latexit>

1

(b) History repackaging in mixed group convolutions, where we write out zt explicitly by Eq. (6).

Figure 6: Using the equivalence established by Theorem 1, we can transfer the notion of history repackaging in recurrent networks to trellis networks.

In Section 4, we formally described the relationship between TrellisNets, RNNs, and temporal convolutional networks (TCN). On the one hand, TrellisNet is a special TCN (with weight-tying and injected inputs), while on the other hand it can also express any structured RNN via a sparse convolutional kernel. These relationships open clear paths for applying techniques developed for either recurrent or convolutional networks. We summarize below some of the techniques that can be applied in this way to TrellisNet, categorizing them as either inspired by RNNs or TCNs.
B.1 FROM RECURRENT NETWORKS
History repackaging. One theoretical advantage of RNNs is their ability to represent a history of infinite length. However, in many applications, sequence lengths are too long for infinite backpropagation during training. A typical solution is to partition the sequence into smaller subsequences and perform truncated backpropagation through time (BPTT) on each. At sequence boundaries, the hidden state ht is "repackaged" and passed onto the next RNN sequence. Thus gradient flow stops at sequence boundaries (see Figure 6a). Such repackaging is also sometimes used at test time.
We can now map this repackaging procedure to trellis networks. As shown in Figure 6, the notion of passing the compressed history vector ht in an RNN corresponds to specific non-zero padding in the mixed group convolution of the corresponding TrellisNet. The padding is simply the channels from the last step of the final layer applied on the previous sequence (see Figure 6b, where without the repackaging padding, at layer 2 we will have hT(1+) 1,T +1 instead of h(T1+) 1,1). We illustrate this in Figure 6b, where we have written out zt(i) in TrellisNet explicitly in the form of ht,t according to Eq. (6) . This suggests that instead of storing all effective history in memory, we can compress history in a feed-forward network to extend its history as well. For a general TrellisNet that employs a dense kernel, similarly, we can pass the hidden channels of the last step of the final layer in the previous sequence as the "history" padding for the next TrellisNet sequence (this works in both training and testing).
Gated activations. In general, the structured gates in RNN cells can be translated to gated activations in temporal convolutions, as we did in Appendix A in the case of an LSTM. While in the experiments we adopted the LSTM gating, other activations (e.g. GRUs (Cho et al., 2014) or activations found via architecture search (Zoph & Le, 2017)) can also be applied in trellis networks via the equivalence established in Theorem 1.
RNN variational dropout. Variational dropout (VD) for RNNs (Gal & Ghahramani, 2016) is a useful regularization scheme that applies the same mask at every time step within a layer (see Figure 7a). A direct translation of this technique from RNN to the group temporal convolution implies that we need to create a different mask for each diagonal of the network (i.e., each history starting point), as well as for each group of the mixed group convolution. We propose an alternative (and extremely simple) dropout scheme for TrellisNet, which is inspired by VD in RNNs as well as Theorem 1. In

14

. <latexit sha1_base64="RdkBBJ5iF/V21Ir3JttKZEnQ2EE=">AAAB7HicbVBNS8NAFHypX7V+VT16WSyCp5KIUPFU8OKxgmkLbSib7aZdutmE3RehhP4GLx4U8eoP8ua/cdvmoK0DC8PMG/a9CVMpDLrut1Pa2Nza3invVvb2Dw6PqscnbZNkmnGfJTLR3ZAaLoXiPgqUvJtqTuNQ8k44uZv7nSeujUjUI05THsR0pEQkGEUr+f1hgmZQrbl1dwGyTryC1KBAa1D9sjmWxVwhk9SYnuemGORUo2CSzyr9zPCUsgkd8Z6lisbcBPli2Rm5sMqQRIm2TyFZqL8TOY2NmcahnYwpjs2qNxf/83oZRjdBLlSaIVds+VGUSYIJmV9OhkJzhnJqCWVa2F0JG1NNGdp+KrYEb/XkddK+qntu3Xu4rjVvizrKcAbncAkeNKAJ99ACHxgIeIZXeHOU8+K8Ox/L0ZJTZE7hD5zPH+8Ajro=</latexit>

..

Under review as a conference paper at ICLR 2019

0 <latexit sha1_base64="+kYPchScNyik1n1ulpTEzE1fdeA=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lEqHgqePHYgv2ANpTNdtKu3WzC7kYoob/AiwdFvPqTvPlv3LY5aOuDgcd7M8zMCxLBtXHdb6ewsbm1vVPcLe3tHxwelY9P2jpOFcMWi0WsugHVKLjEluFGYDdRSKNAYCeY3M39zhMqzWP5YKYJ+hEdSR5yRo2Vmu6gXHGr7gJknXg5qUCOxqD81R/GLI1QGiao1j3PTYyfUWU4Ezgr9VONCWUTOsKepZJGqP1sceiMXFhlSMJY2ZKGLNTfExmNtJ5Gge2MqBnrVW8u/uf1UhPe+BmXSWpQsuWiMBXExGT+NRlyhcyIqSWUKW5vJWxMFWXGZlOyIXirL6+T9lXVc6te87pSv83jKMIZnMMleFCDOtxDA1rAAOEZXuHNeXRenHfnY9lacPKZU/gD5/MHdrGMrA==</latexit>
0 <latexit sha1_base64="+kYPchScNyik1n1ulpTEzE1fdeA=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lEqHgqePHYgv2ANpTNdtKu3WzC7kYoob/AiwdFvPqTvPlv3LY5aOuDgcd7M8zMCxLBtXHdb6ewsbm1vVPcLe3tHxwelY9P2jpOFcMWi0WsugHVKLjEluFGYDdRSKNAYCeY3M39zhMqzWP5YKYJ+hEdSR5yRo2Vmu6gXHGr7gJknXg5qUCOxqD81R/GLI1QGiao1j3PTYyfUWU4Ezgr9VONCWUTOsKepZJGqP1sceiMXFhlSMJY2ZKGLNTfExmNtJ5Gge2MqBnrVW8u/uf1UhPe+BmXSWpQsuWiMBXExGT+NRlyhcyIqSWUKW5vJWxMFWXGZlOyIXirL6+T9lXVc6te87pSv83jKMIZnMMleFCDOtxDA1rAAOEZXuHNeXRenHfnY9lacPKZU/gD5/MHdrGMrA==</latexit>

. <latexit sha1_base64="RdkBBJ5iF/V21Ir3JttKZEnQ2EE=">AAAB7HicbVBNS8NAFHypX7V+VT16WSyCp5KIUPFU8OKxgmkLbSib7aZdutmE3RehhP4GLx4U8eoP8ua/cdvmoK0DC8PMG/a9CVMpDLrut1Pa2Nza3invVvb2Dw6PqscnbZNkmnGfJTLR3ZAaLoXiPgqUvJtqTuNQ8k44uZv7nSeujUjUI05THsR0pEQkGEUr+f1hgmZQrbl1dwGyTryC1KBAa1D9sjmWxVwhk9SYnuemGORUo2CSzyr9zPCUsgkd8Z6lisbcBPli2Rm5sMqQRIm2TyFZqL8TOY2NmcahnYwpjs2qNxf/83oZRjdBLlSaIVds+VGUSYIJmV9OhkJzhnJqCWVa2F0JG1NNGdp+KrYEb/XkddK+qntu3Xu4rjVvizrKcAbncAkeNKAJ99ACHxgIeIZXeHOU8+K8Ox/L0ZJTZE7hD5zPH+8Ajro=</latexit>

..

. <latexit sha1_base64="RdkBBJ5iF/V21Ir3JttKZEnQ2EE=">AAAB7HicbVBNS8NAFHypX7V+VT16WSyCp5KIUPFU8OKxgmkLbSib7aZdutmE3RehhP4GLx4U8eoP8ua/cdvmoK0DC8PMG/a9CVMpDLrut1Pa2Nza3invVvb2Dw6PqscnbZNkmnGfJTLR3ZAaLoXiPgqUvJtqTuNQ8k44uZv7nSeujUjUI05THsR0pEQkGEUr+f1hgmZQrbl1dwGyTryC1KBAa1D9sjmWxVwhk9SYnuemGORUo2CSzyr9zPCUsgkd8Z6lisbcBPli2Rm5sMqQRIm2TyFZqL8TOY2NmcahnYwpjs2qNxf/83oZRjdBLlSaIVds+VGUSYIJmV9OhkJzhnJqCWVa2F0JG1NNGdp+KrYEb/XkddK+qntu3Xu4rjVvizrKcAbncAkeNKAJ99ACHxgIeIZXeHOU8+K8Ox/L0ZJTZE7hD5zPH+8Ajro=</latexit>

..

x1 <latexit sha1_base64="Gx1rB9K2pA68P1aOYP+U250HTz0=">AAAB6nicbVBNS8NAEJ3Ur1q/qh69LBbBU0lEqHgqePFY0X5AG8pmO2mXbjZhdyOW0J/gxYMiXv1F3vw3btsctPXBwOO9GWbmBYng2rjut1NYW9/Y3Cpul3Z29/YPyodHLR2nimGTxSJWnYBqFFxi03AjsJMopFEgsB2Mb2Z++xGV5rF8MJME/YgOJQ85o8ZK9099r1+uuFV3DrJKvJxUIEejX/7qDWKWRigNE1Trrucmxs+oMpwJnJZ6qcaEsjEdYtdSSSPUfjY/dUrOrDIgYaxsSUPm6u+JjEZaT6LAdkbUjPSyNxP/87qpCa/8jMskNSjZYlGYCmJiMvubDLhCZsTEEsoUt7cSNqKKMmPTKdkQvOWXV0nrouq5Ve/uslK/zuMowgmcwjl4UIM63EIDmsBgCM/wCm+OcF6cd+dj0Vpw8plj+APn8wcJSI2Y</latexit>

x2 <latexit sha1_base64="958LDsa5w4EedPbIxPcFgVddcGo=">AAAB6nicbVBNS8NAEJ3Ur1q/qh69LBbBU0mKoHgqePFY0X5AG8pmO2mXbjZhdyOW0J/gxYMiXv1F3vw3btsctPXBwOO9GWbmBYng2rjut1NYW9/Y3Cpul3Z29/YPyodHLR2nimGTxSJWnYBqFFxi03AjsJMopFEgsB2Mb2Z++xGV5rF8MJME/YgOJQ85o8ZK90/9Wr9ccavuHGSVeDmpQI5Gv/zVG8QsjVAaJqjWXc9NjJ9RZTgTOC31Uo0JZWM6xK6lkkao/Wx+6pScWWVAwljZkobM1d8TGY20nkSB7YyoGellbyb+53VTE175GZdJalCyxaIwFcTEZPY3GXCFzIiJJZQpbm8lbEQVZcamU7IheMsvr5JWreq5Ve/uolK/zuMowgmcwjl4cAl1uIUGNIHBEJ7hFd4c4bw4787HorXg5DPH8AfO5w8KzI2Z</latexit>

x3 <latexit sha1_base64="FsFp5SDaWDjvwmxpdt2PBoWBq3Q=">AAAB6nicbVBNS8NAEJ3Ur1q/qh69LBbBU0lUUDwVvHisaD+gDWWz3bRLN5uwOxFL6E/w4kERr/4ib/4bt20O2vpg4PHeDDPzgkQKg6777RRWVtfWN4qbpa3tnd298v5B08SpZrzBYhnrdkANl0LxBgqUvJ1oTqNA8lYwupn6rUeujYjVA44T7kd0oEQoGEUr3T/1znvlilt1ZyDLxMtJBXLUe+Wvbj9macQVMkmN6Xhugn5GNQom+aTUTQ1PKBvRAe9YqmjEjZ/NTp2QE6v0SRhrWwrJTP09kdHImHEU2M6I4tAselPxP6+TYnjlZ0IlKXLF5ovCVBKMyfRv0heaM5RjSyjTwt5K2JBqytCmU7IheIsvL5PmWdVzq97dRaV2ncdRhCM4hlPw4BJqcAt1aACDATzDK7w50nlx3p2PeWvByWcO4Q+czx8MUI2a</latexit>

. <latexit sha1_base64="RdkBBJ5iF/V21Ir3JttKZEnQ2EE=">AAAB7HicbVBNS8NAFHypX7V+VT16WSyCp5KIUPFU8OKxgmkLbSib7aZdutmE3RehhP4GLx4U8eoP8ua/cdvmoK0DC8PMG/a9CVMpDLrut1Pa2Nza3invVvb2Dw6PqscnbZNkmnGfJTLR3ZAaLoXiPgqUvJtqTuNQ8k44uZv7nSeujUjUI05THsR0pEQkGEUr+f1hgmZQrbl1dwGyTryC1KBAa1D9sjmWxVwhk9SYnuemGORUo2CSzyr9zPCUsgkd8Z6lisbcBPli2Rm5sMqQRIm2TyFZqL8TOY2NmcahnYwpjs2qNxf/83oZRjdBLlSaIVds+VGUSYIJmV9OhkJzhnJqCWVa2F0JG1NNGdp+KrYEb/XkddK+qntu3Xu4rjVvizrKcAbncAkeNKAJ99ACHxgIeIZXeHOU8+K8Ox/L0ZJTZE7hD5zPH+8Ajro=</latexit>

..

{ht,t <latexit sha1_base64="6w8lQX6t/2b6qRcXuEQdV3549jw=">AAACCnicbVC7SgNBFJ31GeNr1dJmNAgWMeyKEBshYGNhESEvyC7L7GQ2GTL7YOauENbUNv6KjYUitn6BnX/jJNlCEw9cOJxzDzP3+IngCizr21haXlldWy9sFDe3tnd2zb39lopTSVmTxiKWHZ8oJnjEmsBBsE4iGQl9wdr+8Hrit++ZVDyOGjBKmBuSfsQDTgloyTOPnGzgZVCGs9sxdvCDHriyy9jpxaDKuOGMPbNkVawp8CKxc1JCOeqe+aXDNA1ZBFQQpbq2lYCbEQmcCjYuOqliCaFD0mddTSMSMuVm01PG+EQrPRzEUk8EeKr+TmQkVGoU+nozJDBQ895E/M/rphBcuhmPkhRYRGcPBanAEONJL7jHJaMgRpoQKrn+K6YDIgkF3V5Rl2DPn7xIWucV26rYdxelWjWvo4AO0TE6RTaqohq6QXXURBQ9omf0it6MJ+PFeDc+ZqtLRp45QH9gfP4A/2+Yfg==</latexit>

L | t = 1, . . . , T }

0 <latexit sha1_base64="+kYPchScNyik1n1ulpTEzE1fdeA=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lEqHgqePHYgv2ANpTNdtKu3WzC7kYoob/AiwdFvPqTvPlv3LY5aOuDgcd7M8zMCxLBtXHdb6ewsbm1vVPcLe3tHxwelY9P2jpOFcMWi0WsugHVKLjEluFGYDdRSKNAYCeY3M39zhMqzWP5YKYJ+hEdSR5yRo2Vmu6gXHGr7gJknXg5qUCOxqD81R/GLI1QGiao1j3PTYyfUWU4Ezgr9VONCWUTOsKepZJGqP1sceiMXFhlSMJY2ZKGLNTfExmNtJ5Gge2MqBnrVW8u/uf1UhPe+BmXSWpQsuWiMBXExGT+NRlyhcyIqSWUKW5vJWxMFWXGZlOyIXirL6+T9lXVc6te87pSv83jKMIZnMMleFCDOtxDA1rAAOEZXuHNeXRenHfnY9lacPKZU/gD5/MHdrGMrA==</latexit>

0 <latexit sha1_base64="+kYPchScNyik1n1ulpTEzE1fdeA=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lEqHgqePHYgv2ANpTNdtKu3WzC7kYoob/AiwdFvPqTvPlv3LY5aOuDgcd7M8zMCxLBtXHdb6ewsbm1vVPcLe3tHxwelY9P2jpOFcMWi0WsugHVKLjEluFGYDdRSKNAYCeY3M39zhMqzWP5YKYJ+hEdSR5yRo2Vmu6gXHGr7gJknXg5qUCOxqD81R/GLI1QGiao1j3PTYyfUWU4Ezgr9VONCWUTOsKepZJGqP1sceiMXFhlSMJY2ZKGLNTfExmNtJ5Gge2MqBnrVW8u/uf1UhPe+BmXSWpQsuWiMBXExGT+NRlyhcyIqSWUKW5vJWxMFWXGZlOyIXirL6+T9lXVc6te87pSv83jKMIZnMMleFCDOtxDA1rAAOEZXuHNeXRenHfnY9lacPKZU/gD5/MHdrGMrA==</latexit>

x1 <latexit sha1_base64="Gx1rB9K2pA68P1aOYP+U250HTz0=">AAAB6nicbVBNS8NAEJ3Ur1q/qh69LBbBU0lEqHgqePFY0X5AG8pmO2mXbjZhdyOW0J/gxYMiXv1F3vw3btsctPXBwOO9GWbmBYng2rjut1NYW9/Y3Cpul3Z29/YPyodHLR2nimGTxSJWnYBqFFxi03AjsJMopFEgsB2Mb2Z++xGV5rF8MJME/YgOJQ85o8ZK9099r1+uuFV3DrJKvJxUIEejX/7qDWKWRigNE1Trrucmxs+oMpwJnJZ6qcaEsjEdYtdSSSPUfjY/dUrOrDIgYaxsSUPm6u+JjEZaT6LAdkbUjPSyNxP/87qpCa/8jMskNSjZYlGYCmJiMvubDLhCZsTEEsoUt7cSNqKKMmPTKdkQvOWXV0nrouq5Ve/uslK/zuMowgmcwjl4UIM63EIDmsBgCM/wCm+OcF6cd+dj0Vpw8plj+APn8wcJSI2Y</latexit>

x2 <latexit sha1_base64="958LDsa5w4EedPbIxPcFgVddcGo=">AAAB6nicbVBNS8NAEJ3Ur1q/qh69LBbBU0mKoHgqePFY0X5AG8pmO2mXbjZhdyOW0J/gxYMiXv1F3vw3btsctPXBwOO9GWbmBYng2rjut1NYW9/Y3Cpul3Z29/YPyodHLR2nimGTxSJWnYBqFFxi03AjsJMopFEgsB2Mb2Z++xGV5rF8MJME/YgOJQ85o8ZK90/9Wr9ccavuHGSVeDmpQI5Gv/zVG8QsjVAaJqjWXc9NjJ9RZTgTOC31Uo0JZWM6xK6lkkao/Wx+6pScWWVAwljZkobM1d8TGY20nkSB7YyoGellbyb+53VTE175GZdJalCyxaIwFcTEZPY3GXCFzIiJJZQpbm8lbEQVZcamU7IheMsvr5JWreq5Ve/uolK/zuMowgmcwjl4cAl1uIUGNIHBEJ7hFd4c4bw4787HorXg5DPH8AfO5w8KzI2Z</latexit>

x3 <latexit sha1_base64="FsFp5SDaWDjvwmxpdt2PBoWBq3Q=">AAAB6nicbVBNS8NAEJ3Ur1q/qh69LBbBU0lUUDwVvHisaD+gDWWz3bRLN5uwOxFL6E/w4kERr/4ib/4bt20O2vpg4PHeDDPzgkQKg6777RRWVtfWN4qbpa3tnd298v5B08SpZrzBYhnrdkANl0LxBgqUvJ1oTqNA8lYwupn6rUeujYjVA44T7kd0oEQoGEUr3T/1znvlilt1ZyDLxMtJBXLUe+Wvbj9macQVMkmN6Xhugn5GNQom+aTUTQ1PKBvRAe9YqmjEjZ/NTp2QE6v0SRhrWwrJTP09kdHImHEU2M6I4tAselPxP6+TYnjlZ0IlKXLF5ovCVBKMyfRv0heaM5RjSyjTwt5K2JBqytCmU7IheIsvL5PmWdVzq97dRaV2ncdRhCM4hlPw4BJqcAt1aACDATzDK7w50nlx3p2PeWvByWcO4Q+czx8MUI2a</latexit>

x4 <latexit sha1_base64="S4f+bq/VrUW0JYj64fGswbvelkw=">AAAB6nicbVBNS8NAEJ3Ur1q/qh69LBbBU0mkoHgqePFY0X5AG8pmO2mXbjZhdyOW0J/gxYMiXv1F3vw3btsctPXBwOO9GWbmBYng2rjut1NYW9/Y3Cpul3Z29/YPyodHLR2nimGTxSJWnYBqFFxi03AjsJMopFEgsB2Mb2Z++xGV5rF8MJME/YgOJQ85o8ZK90/9Wr9ccavuHGSVeDmpQI5Gv/zVG8QsjVAaJqjWXc9NjJ9RZTgTOC31Uo0JZWM6xK6lkkao/Wx+6pScWWVAwljZkobM1d8TGY20nkSB7YyoGellbyb+53VTE175GZdJalCyxaIwFcTEZPY3GXCFzIiJJZQpbm8lbEQVZcamU7IheMsvr5LWRdVzq95drVK/zuMowgmcwjl4cAl1uIUGNIHBEJ7hFd4c4bw4787HorXg5DPH8AfO5w8N1I2b</latexit>

x5 <latexit sha1_base64="dPT//6RAyPZ9Ol0ZAjwg7n/hgME=">AAAB6nicbVBNS8NAEJ3Ur1q/qh69LBbBU0lEUTwVvHisaD+gDWWz3bRLN5uwOxFL6E/w4kERr/4ib/4bt20O2vpg4PHeDDPzgkQKg6777RRWVtfWN4qbpa3tnd298v5B08SpZrzBYhnrdkANl0LxBgqUvJ1oTqNA8lYwupn6rUeujYjVA44T7kd0oEQoGEUr3T/1Lnrlilt1ZyDLxMtJBXLUe+Wvbj9macQVMkmN6Xhugn5GNQom+aTUTQ1PKBvRAe9YqmjEjZ/NTp2QE6v0SRhrWwrJTP09kdHImHEU2M6I4tAselPxP6+TYnjlZ0IlKXLF5ovCVBKMyfRv0heaM5RjSyjTwt5K2JBqytCmU7IheIsvL5PmWdVzq97deaV2ncdRhCM4hlPw4BJqcAt1aACDATzDK7w50nlx3p2PeWvByWcO4Q+czx8PWI2c</latexit>

{ht,t <latexit sha1_base64="9VCBo+1x7H1sq/4u1Lxpq28SfU4=">AAACCnicbVC7SgNBFJ2Nrxhfq5Y2o0GwiGFXhNgIARvLCHlBdllmJ5NkyOyDmbtCWLe28VdsLBSx9Qvs/BsnyRaaeODC4Zx7mLnHjwVXYFnfRmFldW19o7hZ2tre2d0z9w/aKkokZS0aiUh2faKY4CFrAQfBurFkJPAF6/jjm6nfuWdS8ShswiRmbkCGIR9wSkBLnnnspCMvhQqcjzPs4Ac9cG1XsNOPQFVw08k8s2xVrRnwMrFzUkY5Gp75pcM0CVgIVBClerYVg5sSCZwKlpWcRLGY0DEZsp6mIQmYctPZKRk+1UofDyKpJwQ8U38nUhIoNQl8vRkQGKlFbyr+5/USGFy5KQ/jBFhI5w8NEoEhwtNecJ9LRkFMNCFUcv1XTEdEEgq6vZIuwV48eZm0L6q2VbXvLsv1Wl5HER2hE3SGbFRDdXSLGqiFKHpEz+gVvRlPxovxbnzMVwtGnjlEf2B8/gAxQ5id</latexit>

k

Mask 1

<latexit sha1_base64="4ZHyCx04sxNSGjgTq8r6BUxDtgU=">AAAB7XicbVA9SwNBEJ2LXzF+RS1tFoNgFe7SmDJgYyNEMImQHGFvM5es2ds9dveEEPIfbCwUsfX/2Plv3CRXaOKDgcd7M8zMi1LBjfX9b6+wsbm1vVPcLe3tHxwelY9P2kZlmmGLKaH0Q0QNCi6xZbkV+JBqpEkksBONr+d+5wm14Ure20mKYUKHksecUeuk9i01YxL0yxW/6i9A1kmQkwrkaPbLX72BYlmC0jJBjekGfmrDKdWWM4GzUi8zmFI2pkPsOippgiacLq6dkQunDEistCtpyUL9PTGliTGTJHKdCbUjs+rNxf+8bmbjejjlMs0sSrZcFGeCWEXmr5MB18ismDhCmebuVsJGVFNmXUAlF0Kw+vI6adeqgV8N7mqVRj2PowhncA6XEMAVNOAGmtACBo/wDK/w5invxXv3PpatBS+fOYU/8D5/AL20joc=</latexit>
|

t = 1, . . . , T }

Mask <latexit sha1_base64="OPSDj8Q5IVTfQC+O+npTy+6yjVo=">AAAB7XicbVA9SwNBEJ2LXzF+RS1tFoNgFe7SmDJgYyNEMImQHGFvM5es2ds9dveEEPIfbCwUsfX/2Plv3CRXaOKDgcd7M8zMi1LBjfX9b6+wsbm1vVPcLe3tHxwelY9P2kZlmmGLKaH0Q0QNCi6xZbkV+JBqpEkksBONr+d+5wm14Ure20mKYUKHksecUeuk9i01Y1Lrlyt+1V+ArJMgJxXI0eyXv3oDxbIEpWWCGtMN/NSGU6otZwJnpV5mMKVsTIfYdVTSBE04XVw7IxdOGZBYaVfSkoX6e2JKE2MmSeQ6E2pHZtWbi/953czG9XDKZZpZlGy5KM4EsYrMXycDrpFZMXGEMs3drYSNqKbMuoBKLoRg9eV10q5VA78a3NUqjXoeRxHO4BwuIYAraMANNKEFDB7hGV7hzVPei/fufSxbC14+cwp/4H3+AL84jog=</latexit>

2

{ht,t <latexit sha1_base64="xOwikC/OvDelYgqWWxJ/0hJ1fJ0=">AAACDXicbVC7SgNBFJ2Nrxhfq5Y2g1GwiGFXhNgIARvLCHlBdllmJ7PJkNkHM3eFsOYHbPwVGwtFbO3t/BsnyRaaeODC4Zx7mLnHTwRXYFnfRmFldW19o7hZ2tre2d0z9w/aKk4lZS0ai1h2faKY4BFrAQfBuolkJPQF6/ijm6nfuWdS8ThqwjhhbkgGEQ84JaAlzzxxsqGXQQXOHSbEBDv4QQ9c2xXs9GNQFdx0Jp5ZtqrWDHiZ2DkpoxwNz/zSYZqGLAIqiFI920rAzYgETgWblJxUsYTQERmwnqYRCZlys9k1E3yqlT4OYqknAjxTfycyEio1Dn29GRIYqkVvKv7n9VIIrtyMR0kKLKLzh4JUYIjxtBrc55JREGNNCJVc/xXTIZGEgi6wpEuwF09eJu2Lqm1V7bvLcr2W11FER+gYnSEb1VAd3aIGaiGKHtEzekVvxpPxYrwbH/PVgpFnDtEfGJ8/kmKZ6Q==</latexit>

`

|Mt =as1k,<latexitsha1_base64="72z96cMDFAHDlZhDIp7SM9bZr0o=">AAAB7XicbVBNSwMxEJ3Ur1q/qh69BIvgqezWgz0WvHgRKtgPaJeSTbNtbDZZkqxQlv4HLx4U8er/8ea/MW33oK0PBh7vzTAzL0wEN9bzvlFhY3Nre6e4W9rbPzg8Kh+ftI1KNWUtqoTS3ZAYJrhkLcutYN1EMxKHgnXCyc3c7zwxbbiSD3aasCAmI8kjTol1UvuOmAm+GpQrXtVbAK8TPycVyNEclL/6Q0XTmElLBTGm53uJDTKiLaeCzUr91LCE0AkZsZ6jksTMBNni2hm+cMoQR0q7khYv1N8TGYmNmcah64yJHZtVby7+5/VSG9WDjMsktUzS5aIoFdgqPH8dD7lm1IqpI4Rq7m7FdEw0odYFVHIh+Ksvr5N2rep7Vf++VmnU8ziKcAbncAk+XEMDbqEJLaDwCM/wCm9IoRf0jj6WrQWUz5zCH6DPH8C8jok=</latexit>

.3. .

,

T}

Mask <latexit sha1_base64="DVx06ueRSIXHgwswu8ctZcXDE/k=">AAAB7XicbVBNSwMxEJ3Ur1q/qh69BIvgqewWwR4LXrwIFewHtEvJptk2NpssSVYoS/+DFw+KePX/ePPfmLZ70NYHA4/3ZpiZFyaCG+t536iwsbm1vVPcLe3tHxwelY9P2kalmrIWVULpbkgME1yyluVWsG6iGYlDwTrh5Gbud56YNlzJBztNWBCTkeQRp8Q6qX1HzARfDcoVr+otgNeJn5MK5GgOyl/9oaJpzKSlghjT873EBhnRllPBZqV+alhC6ISMWM9RSWJmgmxx7QxfOGWII6VdSYsX6u+JjMTGTOPQdcbEjs2qNxf/83qpjepBxmWSWibpclGUCmwVnr+Oh1wzasXUEUI1d7diOiaaUOsCKrkQ/NWX10m7VvW9qn9fqzTqeRxFOINzuAQfrqEBt9CEFlB4hGd4hTek0At6Rx/L1gLKZ07hD9DnD8JAjoo=</latexit>

4

Mx1a:Tsk<latexitsha1_base64="ocIS2tcIMuYbgr/+l7ECrgU4J+Y=">AAAB7XicbVBNSwMxEJ3Ur1q/qh69BIvgqewWxB4LXrwIFewHtEvJptk2NpssSVYoS/+DFw+KePX/ePPfmLZ70NYHA4/3ZpiZFyaCG+t536iwsbm1vVPcLe3tHxwelY9P2kalmrIWVULpbkgME1yyluVWsG6iGYlDwTrh5Gbud56YNlzJBztNWBCTkeQRp8Q6qX1HzARfDcoVr+otgNeJn5MK5GgOyl/9oaJpzKSlghjT873EBhnRllPBZqV+alhC6ISMWM9RSWJmgmxx7QxfOGWII6VdSYsX6u+JjMTGTOPQdcbEjs2qNxf/83qpjepBxmWSWibpclGUCmwVnr+Oh1wzasXUEUI1d7diOiaaUOsCKrkQ/NWX10m7VvW9qn9fqzTqeRxFOINzuAQfrqEBt9CEFlB4hGd4hTek0At6Rx/L1gLKZ07hD9DnD8PEjos=</latexit> <latexit sha1_base64="L7Pcqd/GJ1LwHaj9CnB7qcjLJvg=">AAAB7nicbVBNS8NAEJ3Ur1q/qh69LBbBU0lEqHgqePFYoV/QhrLZTtqlm03Y3Ygl9Ed48aCIV3+PN/+N2zYHbX0w8Hhvhpl5QSK4Nq777RQ2Nre2d4q7pb39g8Oj8vFJW8epYthisYhVN6AaBZfYMtwI7CYKaRQI7ASTu7nfeUSleSybZpqgH9GR5CFn1Fip8zTIvNvmbFCuuFV3AbJOvJxUIEdjUP7qD2OWRigNE1Trnucmxs+oMpwJnJX6qcaEsgkdYc9SSSPUfrY4d0YurDIkYaxsSUMW6u+JjEZaT6PAdkbUjPWqNxf/83qpCW/8jMskNSjZclGYCmJiMv+dDLlCZsTUEsoUt7cSNqaKMmMTKtkQvNWX10n7quq5Ve/hulKv5XEU4QzO4RI8qEEd7qEBLWAwgWd4hTcncV6cd+dj2Vpw8plT+APn8wfx5o9D</latexit>

5

Original
<latexit sha1_base64="r0j8rNB/sM95q79vYwH7pdQr+wU=">AAACEHicdVDLSgMxFM34rPU16tJNsBVdlZluFFcFNy4KVrAPaIeSSdM2NJMMyR2xDP0EN/6KGxeKuHXpzr8xfQj1dSBwOOdc7s0JY8ENeN6Hs7C4tLyymlnLrm9sbm27O7s1oxJNWZUqoXQjJIYJLlkVOAjWiDUjUShYPRycj/36DdOGK3kNw5gFEelJ3uWUgJXa7tGl5j0uicBlZQzOtyICfUpEWh61W8BuIVU2MMq33Zxf8CbA3i/yZeXQDJW2+97qKJpETAIVxJim78UQpEQDp4KNsq3EsJjQAemxpqWSRMwE6eRDI3xolQ7uKm2fBDxR5ydSEhkzjEKbHN9rfnpj8S+vmUD3NEi5jBNgkk4XdROBQeFxO7jDNaMghpYQqrm9FdM+0YSC7TA7X8L/pFYs+F7BvyrmSmezOjJoHx2gY+SjE1RCF6iCqoiiO/SAntCzc+88Oi/O6zS64Mxm9tA3OG+f/iudKQ==</latexit>

Loss

Lorig

. <latexit sha1_base64="RdkBBJ5iF/V21Ir3JttKZEnQ2EE=">AAAB7HicbVBNS8NAFHypX7V+VT16WSyCp5KIUPFU8OKxgmkLbSib7aZdutmE3RehhP4GLx4U8eoP8ua/cdvmoK0DC8PMG/a9CVMpDLrut1Pa2Nza3invVvb2Dw6PqscnbZNkmnGfJTLR3ZAaLoXiPgqUvJtqTuNQ8k44uZv7nSeujUjUI05THsR0pEQkGEUr+f1hgmZQrbl1dwGyTryC1KBAa1D9sjmWxVwhk9SYnuemGORUo2CSzyr9zPCUsgkd8Z6lisbcBPli2Rm5sMqQRIm2TyFZqL8TOY2NmcahnYwpjs2qNxf/83oZRjdBLlSaIVds+VGUSYIJmV9OhkJzhnJqCWVa2F0JG1NNGdp+KrYEb/XkddK+qntu3Xu4rjVvizrKcAbncAkeNKAJ99ACHxgIeIZXeHOU8+K8Ox/L0ZJTZE7hD5zPH+8Ajro=</latexit>

.

.

. <latexit sha1_base64="RdkBBJ5iF/V21Ir3JttKZEnQ2EE=">AAAB7HicbVBNS8NAFHypX7V+VT16WSyCp5KIUPFU8OKxgmkLbSib7aZdutmE3RehhP4GLx4U8eoP8ua/cdvmoK0DC8PMG/a9CVMpDLrut1Pa2Nza3invVvb2Dw6PqscnbZNkmnGfJTLR3ZAaLoXiPgqUvJtqTuNQ8k44uZv7nSeujUjUI05THsR0pEQkGEUr+f1hgmZQrbl1dwGyTryC1KBAa1D9sjmWxVwhk9SYnuemGORUo2CSzyr9zPCUsgkd8Z6lisbcBPli2Rm5sMqQRIm2TyFZqL8TOY2NmcahnYwpjs2qNxf/83oZRjdBLlSaIVds+VGUSYIJmV9OhkJzhnJqCWVa2F0JG1NNGdp+KrYEb/XkddK+qntu3Xu4rjVvizrKcAbncAkeNKAJ99ACHxgIeIZXeHOU8+K8Ox/L0ZJTZE7hD5zPH+8Ajro=</latexit>

.

.

. <latexit sha1_base64="RdkBBJ5iF/V21Ir3JttKZEnQ2EE=">AAAB7HicbVBNS8NAFHypX7V+VT16WSyCp5KIUPFU8OKxgmkLbSib7aZdutmE3RehhP4GLx4U8eoP8ua/cdvmoK0DC8PMG/a9CVMpDLrut1Pa2Nza3invVvb2Dw6PqscnbZNkmnGfJTLR3ZAaLoXiPgqUvJtqTuNQ8k44uZv7nSeujUjUI05THsR0pEQkGEUr+f1hgmZQrbl1dwGyTryC1KBAa1D9sjmWxVwhk9SYnuemGORUo2CSzyr9zPCUsgkd8Z6lisbcBPli2Rm5sMqQRIm2TyFZqL8TOY2NmcahnYwpjs2qNxf/83oZRjdBLlSaIVds+VGUSYIJmV9OhkJzhnJqCWVa2F0JG1NNGdp+KrYEb/XkddK+qntu3Xu4rjVvizrKcAbncAkeNKAJ99ACHxgIeIZXeHOU8+K8Ox/L0ZJTZE7hD5zPH+8Ajro=</latexit>

.

.

. <latexit sha1_base64="RdkBBJ5iF/V21Ir3JttKZEnQ2EE=">AAAB7HicbVBNS8NAFHypX7V+VT16WSyCp5KIUPFU8OKxgmkLbSib7aZdutmE3RehhP4GLx4U8eoP8ua/cdvmoK0DC8PMG/a9CVMpDLrut1Pa2Nza3invVvb2Dw6PqscnbZNkmnGfJTLR3ZAaLoXiPgqUvJtqTuNQ8k44uZv7nSeujUjUI05THsR0pEQkGEUr+f1hgmZQrbl1dwGyTryC1KBAa1D9sjmWxVwhk9SYnuemGORUo2CSzyr9zPCUsgkd8Z6lisbcBPli2Rm5sMqQRIm2TyFZqL8TOY2NmcahnYwpjs2qNxf/83oZRjdBLlSaIVds+VGUSYIJmV9OhkJzhnJqCWVa2F0JG1NNGdp+KrYEb/XkddK+qntu3Xu4rjVvizrKcAbncAkeNKAJ99ACHxgIeIZXeHOU8+K8Ox/L0ZJTZE7hD5zPH+8Ajro=</latexit>

..

Auxiliary <latexit sha1_base64="WreFjTnxmoUXC5AhPcp14C4Aeo4=">AAACEHicdVC7TgJBFJ3FF+ILtbSZCEYrskujscLYWFBgIo8ENpu7wwATZh+ZuWsgGz7Bxl+xsdAYW0s7/8ZdwARfJ7nJyTn35t573FAKjab5YWSWlldW17LruY3Nre2d/O5eQweRYrzOAhmolguaS+HzOgqUvBUqDp4redMdXqZ+85YrLQL/Bschtz3o+6InGGAiOfnji2gkpAA1ptVAa1rseIADBjKuTpwO8hHGEI0mRSdfsErmFNT8Rb6sApmj5uTfO92ARR73kUnQum2ZIdoxKBRM8kmuE2keAhtCn7cT6oPHtR1PH5rQo0Tp0l6gkvKRTtXFiRg8rceem3Sm5+qfXir+5bUj7J3ZsfDDCLnPZot6kaQY0DQd2hWKM5TjhABTIrmVsgEoYJhkmFsM4X/SKJcss2RdlwuV83kcWXJADskJscgpqZArUiN1wsgdeSBP5Nm4Nx6NF+N11pox5jP75BuMt08yfZ1J</latexit>

Loss

Laux

. <latexit sha1_base64="RdkBBJ5iF/V21Ir3JttKZEnQ2EE=">AAAB7HicbVBNS8NAFHypX7V+VT16WSyCp5KIUPFU8OKxgmkLbSib7aZdutmE3RehhP4GLx4U8eoP8ua/cdvmoK0DC8PMG/a9CVMpDLrut1Pa2Nza3invVvb2Dw6PqscnbZNkmnGfJTLR3ZAaLoXiPgqUvJtqTuNQ8k44uZv7nSeujUjUI05THsR0pEQkGEUr+f1hgmZQrbl1dwGyTryC1KBAa1D9sjmWxVwhk9SYnuemGORUo2CSzyr9zPCUsgkd8Z6lisbcBPli2Rm5sMqQRIm2TyFZqL8TOY2NmcahnYwpjs2qNxf/83oZRjdBLlSaIVds+VGUSYIJmV9OhkJzhnJqCWVa2F0JG1NNGdp+KrYEb/XkddK+qntu3Xu4rjVvizrKcAbncAkeNKAJ99ACHxgIeIZXeHOU8+K8Ox/L0ZJTZE7hD5zPH+8Ajro=</latexit>

..

. <latexit sha1_base64="RdkBBJ5iF/V21Ir3JttKZEnQ2EE=">AAAB7HicbVBNS8NAFHypX7V+VT16WSyCp5KIUPFU8OKxgmkLbSib7aZdutmE3RehhP4GLx4U8eoP8ua/cdvmoK0DC8PMG/a9CVMpDLrut1Pa2Nza3invVvb2Dw6PqscnbZNkmnGfJTLR3ZAaLoXiPgqUvJtqTuNQ8k44uZv7nSeujUjUI05THsR0pEQkGEUr+f1hgmZQrbl1dwGyTryC1KBAa1D9sjmWxVwhk9SYnuemGORUo2CSzyr9zPCUsgkd8Z6lisbcBPli2Rm5sMqQRIm2TyFZqL8TOY2NmcahnYwpjs2qNxf/83oZRjdBLlSaIVds+VGUSYIJmV9OhkJzhnJqCWVa2F0JG1NNGdp+KrYEb/XkddK+qntu3Xu4rjVvizrKcAbncAkeNKAJ99ACHxgIeIZXeHOU8+K8Ox/L0ZJTZE7hD5zPH+8Ajro=</latexit>

..

. <latexit sha1_base64="RdkBBJ5iF/V21Ir3JttKZEnQ2EE=">AAAB7HicbVBNS8NAFHypX7V+VT16WSyCp5KIUPFU8OKxgmkLbSib7aZdutmE3RehhP4GLx4U8eoP8ua/cdvmoK0DC8PMG/a9CVMpDLrut1Pa2Nza3invVvb2Dw6PqscnbZNkmnGfJTLR3ZAaLoXiPgqUvJtqTuNQ8k44uZv7nSeujUjUI05THsR0pEQkGEUr+f1hgmZQrbl1dwGyTryC1KBAa1D9sjmWxVwhk9SYnuemGORUo2CSzyr9zPCUsgkd8Z6lisbcBPli2Rm5sMqQRIm2TyFZqL8TOY2NmcahnYwpjs2qNxf/83oZRjdBLlSaIVds+VGUSYIJmV9OhkJzhnJqCWVa2F0JG1NNGdp+KrYEb/XkddK+qntu3Xu4rjVvizrKcAbncAkeNKAJ99ACHxgIeIZXeHOU8+K8Ox/L0ZJTZE7hD5zPH+8Ajro=</latexit>

..

(a) Left: variational dropout (VD) in an RNN. Right: VD in a (b) Auxiliary loss on intermediate layers

TrellisNet. Each color indicates a different dropout mask.

in a TrellisNet.

Figure 7: (a) RNN-inspired variational dropout. (b) ConvNet-inspired auxiliary losses.

each iteration, we apply the same mask on the post-activation outputs, at every time step in both the temporal dimension and depth dimension. That is, based on Eq. (6) in Theorem 1, we adapt VD to the TrellisNet setting by assuming ht,t ±  ht,t ; see Figure 7a. Empirically, we found this dropout to work significantly better than other dropout schemes (e.g. drop certain channels entirely).
Recurrent weight dropout/DropConnect. We apply DropConnect on the TrellisNet kernel. Merity et al. (2018b) showed that regularizing hidden-to-hidden weights Whh can be useful in optimizing LSTM language models, and we carry this scheme over to trellis networks.

B.2 FROM CONVOLUTIONAL NETWORKS

Dense convolutional kernel. Generalizing the convolution from a mixed group (sparse) one to a general (dense) convolution means the connections are no longer recurrent and we are computing directly on the hidden units with a large kernel, just like any temporal ConvNet.
Deep supervision. Recall that for sparse TrellisNet to recover truncated RNN, at each level the hidden units are of the form ht,t , representing the state at time t if we assume the history started at time t (Eq. (6)). We propose to inject the loss function at intermediate layers of the convolutional network (e.g., after every layers of transformations, where we call the auxiliary loss frequency). For example, during training, to predict an output at time t with a L-layer TrellisNet, besides zt(L) in the last layer, we can also apply the loss function on zt(L- ), zt(L-2 ), etc. ­ where hidden units will predict with a shorter history because they are at lower levels of the network. This had been introduced for convolutional models in computer vision (Lee et al., 2015; Xie & Tu, 2015). The eventual loss of the network will be:

Ltotal = Lorig +  · Laux

(16)

where  is a fixed scaling factor that controls the weight of the auxiliary loss.
Note that this technique is not directly transferrable (or applicable) to RNNs.
Larger kernel and dilations (Yu & Koltun, 2016). These techniques have been used in convolutional networks to more quickly increase the receptive field. They can be immediately applied to trellis networks. Note that the activation function f of TrellisNet may need to change if we change the kernel size or dilation settings (e.g., with dilation d and kernel size 2, the activation will be f (z^1(i:T) , z1(i:T) -d)).
Weight normalization (Salimans & Kingma, 2016). Weight normalization (WN) is a technique that learns the direction and the magnitude of the weight matrix independently. Applying WN on the convolutional kernel was used in some prior works on temporal convolutional architectures (Dauphin et al., 2017; Bai et al., 2018), and have been found useful in regularizing the convolutional filters and boosting convergence.
Parallelism. Because TrellisNet is convolutional in nature, it can easily leverage the parallel processing in the convolution operation (which slides the kernel across the input features). We note that when the input sequence is relatively long, the predictions of the first few time steps will have insufficient history context compared to the predictions later in the sequence. This can be addressed by either history padding (mentioned in Appendix B.1) or chopping off the loss incurred by the first few time steps.

15

Under review as a conference paper at ICLR 2019
C BENCHMARK TASKS
Word-level language modeling on Penn Treebank (PTB). The original Penn Treebank (PTB) dataset selected 2,499 stories from a collection of almost 100K stories published in Wall Street Journal (WSJ) (Marcus et al., 1993). After Mikolov et al. (2010) processed the corpus, the PTB dataset contains 888K words for training, 70K for validation and 79K for testing, where each sentence is marked with an <eos> tag at its end. All of the numbers (e.g. in financial news) were replaced with a ? symbol with many punctuations removed. Though small, PTB has been a highly studied dataset in the domain of language modeling (Miyamoto & Cho, 2016; Zilly et al., 2017; Merity et al., 2018b; Melis et al., 2018; Yang et al., 2018). Due to its relatively small size, many computational models can easily overfit on word-level PTB. Therefore, good regularization methods and optimization techniques designed for sequence models are especially important on this benchmark task (Merity et al., 2018b).
Word-level language modeling on WikiText-103. WikiText-103 (WT103) is 110 times larger than PTB, containing a training corpus from 28K lightly processed Wikipedia articles (Merity et al., 2017). In total, WT103 features a vocabulary size of about 268K1, with 103M words for training, 218K words for validation, and 246K words for testing/evaluation. The WT103 corpus also retains the original case, punctuation and numbers in the raw data, all of which were removed from the PTB corpus. Moreover, since WT103 is composed of full articles (whereas PTB is sentence-based), it is better suited for testing long-term context retention. For these reasons, WT103 is typically considered much more representative and realistic than PTB (Merity et al., 2018a).
Character-level language modeling on Penn Treebank (PTB). When used for character-level language modeling, PTB is a medium size dataset that contains 5M chracters for training, 396K for validation, and 446K for testing, with an alphabet size of 50 (note: the <eos> tag that marks the end of a sentence in word-level tasks is now considered one character). While the alphabet size of char-level PTB is much smaller compared to the word-level vocabulary size (10K), there is much longer sequential token dependency because a sentence contains many more characters than words.
Sequential and permuted MNIST classification. The MNIST handwritten digits dataset (LeCun et al., 1989) contains 60K normalized training images and 10K testing images, all of size 28 × 28. In the sequential MNIST task, MNIST images are presented to the sequence model as a flattened 784 × 1 sequence for digit classification. Accurate predictions therefore require good long-term memory of the flattened pixels ­ longer than in most language modeling tasks. In the setting of permuted MNIST (PMNIST), the order of the sequence is permuted at random, so the network can no longer rely on local pixel features for classification.
Sequential CIFAR-10 classification. The CIFAR-10 dataset (Krizhevsky & Hinton, 2009) contains 50K images for training and 10K for testing, all of size 32 × 32. In the sequential CIFAR-10 task, these images are passed into the model one at each time step, flattended as in the MNIST tasks. Compared to sequential MNIST, this task is more challenging. For instance, CIFAR-10 contains more complex image structures and intra-class variations, and there are 3 channels to the input. Moreover, as the images are larger, a sequence model needs to have even longer memory than in sequential MNIST or PMNIST (Trinh et al., 2018).
D HYPERPARAMETERS AND ABLATION STUDY
Table 4 specifies the trellis networks used for the various tasks. There are a few things to note while reading the table. First, in training, we decay the learning rate once the validation error plateaus for a while (or according to some fixed schedule, such as after 100 epochs). Second, for auxiliary loss (see Appendix B for more details), we insert the loss function after every fixed number of layers in the network. This "frequency" is included below under the "Auxiliary Frequency" entry. Finally, the hidden dropout in the Table refers to the variational dropout we translated from RNNs (see Appendix B), which is applied at all hidden layers of the TrellisNet. Due to the insight from Theorem 1, many techniques in TrellisNet were translated directly from RNNs or TCNs. Thus, most of the hyperparameters were based on the numbers reported in prior works (e.g., embedding size, embedding dropout, hidden dropout, output dropout, optimizer, weight-decay, etc.) with minor
1As a reference, Oxford English Dictionary only contains less than 220K unique English words.
16

Under review as a conference paper at ICLR 2019

adjustments (Merity et al., 2018b; Yang et al., 2018; Bradbury et al., 2017; Merity et al., 2018a; Trinh et al., 2018; Bai et al., 2018; Santoro et al., 2018). For factors such as auxiliary loss weight and frequency, we perform a basic grid search.

Table 4: Models and hyperparameters used in experiments. "-" means not applicable/used.

Word-PTB (w/o MoS) Word-PTB (w/ MoS) Word-WT103 Char-PTB (P)MNIST/CIFAR-10

Optimizer Initial Learning Rate Hidden Size (i.e. ht) Output Size (only for MoS) # of Experts (only for MoS) Embedding Size Embedding Dropout Hidden (VD-based) Dropout Output Dropout Weight Dropout # of Layers Auxiliary Loss  Auxiliary Frequency Weight Normalization Gradient Clip Weight Decay Model Size

SGD 20 1000 400 0.1 0.28 0.45 0.5 55 0.05 16 -
0.225 1e-6 24M

SGD 20 1000 480 15 280 0.05 0.28 0.4 0.45 55 0.05 16 0.2 1e-6 25M

Adam 1e-3 1600
512 0.0 0.1 0.1 0.0 70 0.08 25
!
0.1 0.0 180M

Adam 2e-3 1000
200 0.0 0.3 0.12 0.25 120 0.3 70
!
0.25 1e-6 13.4M

Adam 2e-3 100
0.2 0.2 0.1 16 -
!
0.5 1e-6 8M

We have also performed an ablation study on TrellisNet to study the influence of various ingredients and techniques on performance. We conduct the study on word-level PTB using a TrellisNet with 24M parameters. When we study one factor (e.g., removing hidden dropout), all hyperparameters and settings remain the same as in column 1 of Table 4 (except for the "Dense Kernel", where we adjust the number of hidden units so that the model size remains the same). The results are as follows:
Table 5: Ablation study on word-level PTB (w/o MoS)

Model Size Test ppl  SOTA

TrellisNet - Hidden (VD-based) Dropout - Weight Dropout - Auxiliary Losses - Long Seq. Parallelism - Dense Kernel (i.e. mixed group conv) - Injected Input (every 2 layers instead) - Injected Input (every 5 layers instead) - Injected Input (every 10 layers instead) - Injected Input (every 20 layers instead)

24.1M 24.1M 24.1M 24.1M 24.1M 24.1M 24.1M 24.1M 24.1M 24.1M

56.97 64.69 63.82 57.99 57.35 59.18 57.44 59.75 60.70 74.91

­  7.72  6.85  1.02  0.38  2.21  0.47  2.78  3.73  17.94

17


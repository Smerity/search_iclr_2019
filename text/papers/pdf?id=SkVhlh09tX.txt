Under review as a conference paper at ICLR 2019
PAY LESS ATTENTION WITH LIGHTWEIGHT AND DYNAMIC CONVOLUTIONS
Anonymous authors Paper under double-blind review
ABSTRACT
Self-attention is a useful mechanism to build generative models for language and images. It determines the importance of context elements by comparing each element to the current time step. In this paper, we show that a very lightweight convolution can perform competitively to the best reported self-attention results. Next, we introduce dynamic convolutions which are simpler and more efficient than self-attention. We predict separate convolution kernels based solely on the current time-step in order to determine the importance of context elements. The number of operations required by this approach scales linearly in the input length, whereas self-attention is quadratic. Experiments on large-scale machine translation, language modeling and abstractive summarization show that dynamic convolutions improve over strong self-attention models. On the WMT'14 English-German test set dynamic convolutions achieve a new state of the art of 29.7 BLEU.1
1 INTRODUCTION
There has been much recent progress in sequence modeling through recurrent neural networks (RNN; Sutskever et al. 2014; Bahdanau et al. 2015; Wu et al. 2016), convolutional networks (CNN; Kalchbrenner et al. 2016; Gehring et al. 2016; 2017; Kaiser et al. 2017) and self-attention models (Paulus et al., 2017; Vaswani et al., 2017). RNNs integrate context information by updating a hidden state at every time-step, CNNs summarize a fixed size context through multiple layers, while as self-attention directly summarizes all context. Attention assigns context elements attention weights which define a weighted sum over context representations (Bahdanau et al., 2015; Chorowski et al., 2015; Luong et al., 2015). Source-target attention summarizes information from another sequence such as in machine translation while as self-attention operates over the current sequence. Self-attention has been formulated as contentbased where attention weights are computed by comparing the current time-step to all elements in the context (Figure 1a). The ability to compute comparisons over such unrestricted context sizes are seen as a key characteristic of self-attention (Vaswani et al., 2017).

(a) Self-attention

(b) Dynamic convolution

Figure 1: Self-attention computes attention weights by comparing all pairs of elements to each other (a) while as dynamic convolutions predict separate kernels for each time-step (b).

However, the ability of self-attention to model long-range dependencies has recently come into question (Tang et al., 2018) and the unlimited context size is computationally very challenging due to the quadratic complexity in the input length. Furthermore, in practice long sequences require the introduction of hierarchies (Liu et al., 2018).
1Code and pre-trained models available at http://anonymized

1

Under review as a conference paper at ICLR 2019

In this paper, we introduce lightweight convolutions which are depth-wise separable (Chollet, 2017; Kaiser et al., 2017), softmax-normalized and share weights over the channel dimension. The result is a convolution with several orders of magnitude fewer weights than a standard non-separable convolution. Different to self-attention, lightweight convolutions reuse the same weights for context elements, regardless of the current time-step.
Dynamic convolutions build on lightweight convolutions by predicting a different convolution kernel at every time-step. The kernel is a function of the current time-step only as opposed to the entire context as in self-attention (Figure 1b). Dynamic convolutions are similar to locally connected layers in the sense that the weights change at every position, however, the difference is that weights are dynamically generated by the model rather than fixed after training (LeCun et al., 1998; Taigman et al., 2014; Chen et al., 2015). Our approach also bears similarity to location-based attention which does not access the context to determine attention weights, however, we do not directly take the attention weights from the previous time-step into account (Chorowski et al., 2015; Luong et al., 2015).
Our experiments show that lightweight convolutions perform competitively to strong self-attention results and that dynamic convolutions can perform even better. On WMT English-German translation dynamic convolutions achieve a new state of the art of 29.7 BLEU, on WMT English-French they match the best reported result in the literature, and on IWSLT German-English dynamic convolutions outperform self-attention by 0.8 BLEU. Dynamic convolutions achieve 20% faster runtime than a highly-optimized self-attention baseline. For language modeling on the Billion word benchmark dynamic convolutions perform as well as or better than self-attention and on CNN-DailyMail abstractive document summarization we outperform a strong self-attention model.

2 BACKGROUND

We first outline sequence to sequence learning and self-attention. Our work builds on non-separable convolutions as well as depthwise separable convolutions.

Sequence to sequence learning maps a source sequence to a target sequence via two separate networks such as in machine translation (Sutskever et al., 2014). The encoder network computes representations for the source sequence such as an English sentence and the decoder network autoregressively generates a target sequence based on the encoder output.

The self-attention module of Vaswani et al. (2017) applies three projections to the input X  Rn×d to obtain key (K), query (Q), and value (V) representations, where n is the number of time steps, d the input/output dimension (Figure 2a). It also defines a number of heads H where each head
can learn separate attention weights over dk features and attend to different positions. The module computes dot-products between key/query pairs, scales to stabilize training, and then softmax nor-
malizes the result. Finally, it computes a weighted sum using the output of the value projection
(V): Attention(Q, K, V ) = softmax( QKT )V dk

Depthwise convolutions perform a convolution independently over every channel. The number of parameters can be reduced from d2k to dk where k is the kernel width. The output O  Rn×d of a depthwise convolution with weight W  Rd×k for element i and output dimension c is defined as:

k

Oi,c = DepthwiseConv(X, W, i, c) =

Wc,j · X(i+j-

k+1 2

),c

j=1

3 LIGHTWEIGHT CONVOLUTIONS

In this section, we introduce LightConv, a depthwise convolution which shares certain output channels and whose weights are normalized across the temporal dimension using a softmax. Compared to self-attention, LightConv has a fixed context window and it determines the importance of context elements with a set of weights that do not change over time steps. We will show that models equipped

2

Under review as a conference paper at ICLR 2019

Linear

Mat Mul SoftMax

Scale

Mat Mul Q

K

Linear

Linear

V Linear

Linear LConv GLU Linear

Linear LConv GLU Linear

dynamic weights
Linear

input
(a) Self-attention

input
(b) Lightweight convolution

input
(c) Dynamic convolution

Figure 2: Illustration of self-attention, lightweight convolutions and dynamic convolutions.

with lightweight convolutions show better generalization compared to regular convolutions and that they can be competitive to state-of-the-art self-attention models (§6). This is surprising because the
common belief is that content-based self-attention mechanisms are crucial to obtaining state-of-the-
art results in natural language processing applications. Furthermore, the low computational profile of LightConv enables us to formulate efficient dynamic convolutions (§4).

LightConv computes the following for the i-th element in the sequence and output channel c:

cH

LightConv(X, W, i, c) = DepthwiseConv(X, softmax(W ), i,

)

d

Weight sharing.

We

tie

the

parameters

of

every

subsequent

number

of

d H

channels,

which

reduces

the number of parameters by a factor of H. As illustration, a regular convolution requires 7,340,032

(d2 × k) weights for d = 1024 and k = 7, a depthwise separable convolution has 7,168 weights

(d × k), and with weight sharing, H = 16, we have only 112 (H × k) weights. We will see that

this vast reduction in the number of parameters is crucial to make dynamic convolutions possible on

current hardware.

Softmax-normalization. We normalize the weights W  RH×k across the temporal dimension k using a softmax operation:

softmax(W )h,j =

exp Wh,j

k j

=1

exp

Wh,j

Module. Figure 2b shows the architecture of the module where we integrate LightConv. We first apply an input projection mapping from dimension d to 2d, followed by a gated linear unit (GLU; Dauphin et al. 2017), and the actual lightweight convolution. The GLU uses half of the inputs as gates by applying sigmoid units and then computes a pointwise product with the other inputs. We also apply an output projection of size W O  Rd×d to the output of LightConv.

Regularization. We found DropConnect to be a good regularizer for the LightConv module (Wan et al., 2013). Specifically, we drop every entry of the normalized weights sof tmax(W ) with probability p and divide it by 1 - p during training. This amounts to removing some of the temporal information within a channel.

Implementation. Existing CUDA primitives for convolutions did not perform very well to im-

plement LightConv and we found the following solution faster on short sequences: We copy and

expand the normalized weights W  RH×k to a matrix of size BH × n × n, where B is the batch

size.

We

then

reshape

and

transpose

the

inputs

to

size

BH

×n

×

d H

,

and

perform

a

batch

matrix

multiplication to get the outputs. We expect a dedicated CUDA kernel to be much more efficient.

3

Under review as a conference paper at ICLR 2019

4 DYNAMIC CONVOLUTIONS

A dynamic convolution has kernels that vary over time as a learned function of the individual time steps. A dynamic version of standard convolutions would be impractical for current GPUs due to their large memory requirements. We address this problem by building on LightConv which drastically reduces the number of parameters (§3).

DynamicConv takes the same form as LightConv but uses a time-step dependent kernel that is computed using a function f : Rd  RH×k:

DynamicConv(X, i, c) = LightConv(X, f (Xi), i, c)

we model f with a simple linear module with learned weights W Q  RH×k×d, i.e., f (Xi) =

d c=1

WhQ,j,c

Xi,c

.

Similar to self-attention, DynamicConv changes the weights assigned to context elements over time. However, the weights of DynamicConv do not depend on the entire context, they are a function of the current time-step only. Self-attention requires a quadratic number of operations in the sentence length to compute attention weights, while the computation of dynamic kernels for DynamicConv scales linearly in the sequence length.

Our experiments (§6) show that models using DynamicConv match or exceed the performance of state-of-the-art models that use context-based self-attention. This challenges the typical intuitions about the importance of content-based self-attention in natural language processing applications.

5 EXPERIMENTAL SETUP
5.1 MODEL ARCHITECTURE
We use an encoder-decoder architecture for sequence to sequence learning (Sutskever et al., 2014) and we closely follow the architectural choices presented in Vaswani et al. (2017). Our self-attention baseline is the fairseq re-implementation of the Transformer Big architecture (Ott et al., 2018).2
The encoder and decoder networks have N blocks each. Encoder blocks contain two sub-blocks: The first is a self-attention module (§2), a LightConv module (3), or a DynamicConv module (§4). The second sub-block is a feed-forward module: ReLU (W 1X + b1)W 2 + b2 where W 1  Rd×dff , W 2  Rdff ×d and d = 1024, dff = 4096 unless otherwise stated. Sub-blocks are surrounded by residual connections (He et al., 2015) and layer normalization (Ba et al., 2016).
Decoder blocks are identical except that they have an additional source-target attention sub-block between the self-attention and feed-forward module. The source-target attention is equivalent to the self-attention module, except that the values and keys are projections over the encoder output for each source word.
Words are fed to the encoder and decoder networks in d dimensional embeddings. We add sinusoidal position embeddings to encode the absolute position of each word in the sequence (Kaiser et al., 2017; Vaswani et al., 2017). The model computes a distribution over vocabulary V by transforming the decoder output via a linear layer with weights W V  Rd×V followed by softmax normalization.
LightConv and DynamicConv are identical to Transformer Big, except that self-attention modules are swapped with either fixed or dynamic convolutions. These models also use fewer parameters per block (cf. Figure 2b and Figure 2c) and we therefore increase the number of blocks to N = 7 for the encoder to roughly match the parameter count of Transformer Big. We generally set H = 16. Both LightConv and DynamicConv set the the encoder and decoder kernel sizes to 3, 7, 15, 31x4 for each block respectively; except for the decoder where we have only three top layers with kernel size 31.
5.2 DATASETS AND EVALUATION
To get a thorough understanding of the limitations of LightConv and DynamicConv we evaluate on three different tasks: machine translation, language modeling and abstractive summarization.
2https://github.com/pytorch/fairseq

4

Under review as a conference paper at ICLR 2019
Machine Translation. We report results on four benchmarks: For WMT English to German (EnDe) we replicate the setup of Vaswani et al. (2017), based on WMT'16 training data with 4.5M sentence pairs, we validate on newstest2013 and test on newstest2014.3 The vocabulary is a 32K joint source and target byte pair encoding (BPE; Sennrich et al. 2016). For WMT English to French (EnFr), we borrow the setup of Gehring et al. (2017) with 36M training sentence pairs from WMT'14, validate on newstest2012+2013 and test on newstest2014. The 40K vocabulary is based on a joint source and target BPE factorization.
For WMT English to Chinese (Zh-En), we pre-process the WMT'17 training data following Hassan et al. (2018) resulting in 20M sentence pairs. We develop on devtest2017 and test on newstest2017. For IWSLT'14 German-English (De-En) we replicate the setup of Edunov et al. (2018) for 160K training sentence pairs and 10K joint BPE vocabulary. For this benchmark only, data is lowercased.
For WMT En-De, WMT En-Fr, we measure case-sensitive tokenized BLEU.4 For WMT En-De only we apply compound splitting similar to Vaswani et al. (2017). For WMT Zh-En we measure detokenized BLEU to be comparable to Hassan et al. (2018).5
We train three random initializations of a each configuration and report test accuracy of the seed which resulted in the highest validation BLEU. Ablations are conducted on the validation set and we report the mean BLEU and standard deviation on this set. WMT En-De, WMT En-Fr are based on beam search with a beam width of 5, IWSLT uses beam 4, and WMT Zh-En beam 8 following Hassan et al. (2018). For all datasets, we tune a length penalty as well as the number of checkpoints to average on the validation set.
Language Modeling. We evaluate on the large-scale Billion word dataset (Chelba et al., 2013) which contains 768M tokens and has a vocabulary of nearly 800K types. Sentences in this dataset are shuffled and we batch sentences independently of each other. Models are evaluated in terms of perplexity on the valid and test portions.
Summarization. We test the model's ability to process long documents on the CNN-DailyMail summarization task (Hermann et al., 2015; Nallapati et al., 2016) comprising over 280K news articles paired with multi-sentence summaries. Articles are truncated to 400 tokens (See et al., 2017) and we use a BPE vocabulary of 30K types (Fan et al., 2017). We evaluate in terms of F1-Rouge, that is Rouge-1, Rouge-2 and Rouge-L (Lin, 2004).6 When generating summaries, we follow standard practice in tuning the maximum output length, disallowing repeating the same trigram, and we apply a stepwise length penalty (Paulus et al., 2017; Fan et al., 2017; Wu et al., 2016).
5.3 TRAINING AND HYPERPARAMETERS
Translation. We use a dropout rate of 0.3 for WMT En-De and IWSLT De-En, 0.1 for WMT EnFr, and 0.25 for WMT Zh-En. WMT models are optimized with Adam and a cosine learning rate schedule (Kingma & Ba, 2015; Loshchilov & Hutter, 2016) where the learning rate is first linearly warmed up for 10K steps from 10-7 to 10-3 and then annealed following a cosine rate with a single cycle. For IWSLT'14 De-En, we use a schedule based on the inverse square root of the current step (Vaswani et al., 2017). We train the WMT models on 8 NVIDIA V100 GPUs for a total of 30K steps on WMT En-De, 40K steps for WMT Zh-En and 80K steps for WMT En-Fr. For IWSLT De-En we train for 50K steps on a single GPU.
We use floating point 16 precision and accumulate the gradients for 16 batches before applying an update (Ott et al., 2018), except for IWSLT where we do not accumulate gradients. Batches contain up to 459K source tokens and the same number of target tokens for both WMT En-De and WMT Zh-En, 655K for En-Fr, and 4K for IWSLT De-En. We use label smoothing with 0.1 weight for the uniform prior distribution over the vocabulary (Szegedy et al., 2015; Pereyra et al., 2017).
Language Modeling. We follow the same setup as for translation but remove the encoder module. For the Billion word benchmark we use an adaptive softmax output layer to reduce the computational burden of the large vocabulary (Grave et al., 2016) and tie it with variable sized input word
3 https://github.com/tensorflow/tensor2tensor/blob/321bacaa3abcca5dbf341ed6fb3d4a1531e513ff/ tensor2tensor/data_generators/translate_ende.py#L60- L63
4 https://github.com/moses- smt/mosesdecoder/blob/master/scripts/generic/multi- bleu.perl
5SacreBLEU hash: BLEU+case.mixed+lang.zh-en+numrefs.1+smooth.exp+test.wmt17+tok.13a+version.1.2.11 6We use the following parameters for ROUGE-1.5.5.pl: -m -a -n 2
5

Under review as a conference paper at ICLR 2019

Model
Gehring et al. (2017) Vaswani et al. (2017) Ahmed et al. (2017) Chen et al. (2018) Shaw et al. (2018) Ott et al. (2018)
LightConv DynamicConv

Param (En-De)
216M 213M 213M 379M
210M
202M 213M

WMT En-De
25.2 28.4 28.9 28.5 29.2 29.3
28.9 29.7

WMT En-Fr
40.5 41.0 41.4 41.0 41.5 43.2
43.1 43.2

Table 1: Machine translation accuracy in terms of BLEU for WMT En-De and WMT En-Fr on newstest2014.

Model
Deng et al. (2018) Hassan et al. (2018)
Self-attention baseline LightConv DynamicConv

Param (Zh-En)
-
292M 285M 296M

IWSLT
33.1 -
34.4 34.8 35.2

WMT Zh-En
24.2
23.8 24.3 24.4

Table 2: Machine translation accuracy in terms of BLEU on IWSLT and WMT Zh-En.

embeddings (Anonymous et al., 2018). The head band has dimension 1024 for 60K types, the next band has dimension 256 for 100K types, and the last band has dimension 64 for 633K types.
We train on 32 GPUs with batches of 65K tokens for 975K updates. As optimizer we use Nesterov's accelerated gradient method (Sutskever et al., 2013) with a momentum value of 0.99 and we renormalize gradients if their norm exceeds 0.1 (Pascanu et al., 2013). The learning rate is linearly warmed up from 10-7 to 1 for 16K steps and then annealed using a cosine learning rate schedule (Loshchilov & Hutter, 2016) with one cycle.
Summarization. We train with Adam using the cosine learning rate schedule with a warmup of 10K steps and a period of 20K updates. We use weight decay 1e-3 and dropout 0.3.
6 RESULTS
6.1 MACHINE TRANSLATION
We first report results on WMT En-De and WMT En-Fr where we compare to the best results in the literature, most of which are based on self-attention. Table 1 shows that LightConv performs very competitively and only trails the state of the art result by 0.1 BLEU on WMT En-Fr; the state of the art is based on self-attention (Ott et al., 2018). This is despite the simplicity of LightConv which operates with a very small number of fixed weights over all time steps whereas self-attention computes dot-products with all context elements at every time-step.
DynamicConv outperforms the best known result on WMT En-De by 0.4 BLEU and achieves a new state of the art, whereas on WMT En-Fr it matches the state of the art. This shows that content-based self-attention is not necessary to achieve good accuracy on large translation benchmarks.
IWSLT is a much smaller benchmark and we therefore switch to a smaller architecture: dff = 1024, d = 512, and H = 4. The self-attention baseline on this dataset is the best reported result in the literature (Table 2).7 LightConv outperforms this baseline by 0.4 BLEU and DynamicConv improves by 0.8 BLEU. We further run experiments on WMT Zh-En translation to evaluate on a non-European language. LightConv outperforms the baseline by 0.5 BLEU and DynamicConv by 0.6 BLEU.
7We omit comparison to Elbayad et al. (2018) since their test set is not directly comparable.
6

Under review as a conference paper at ICLR 2019

Model
Vaswani et al. (2017) Self-attention baseline (k=inf, H=16) Self-attention baseline (k=3,7,15,31x3, H=16)
CNN (k=3) CNN Depthwise (k=3, H=1024) + Increasing kernel (k=3,7,15,31x4, H=1024) + DropConnect (H=1024) + Weight sharing (H=16) + Softmax-normalized weights [LightConv] (H=16) + Dynamic weights [DynamicConv] (H=16) Note: DynamicConv(H=16) w/o softmax-normalization
AAN decoder + self-attn encoder AAN decoder + AAN encoder

Param
213M 210M 210M
208M 195M 195M 195M 195M 195M 200M 200M
260M 310M

BLEU
26.4 26.9 ± 0.1 26.9 ± 0.3
25.9 ± 0.2 26.1 ± 0.2 26.4 ± 0.2 26.5 ± 0.2 26.5 ± 0.1 26.6 ± 0.2 26.9 ± 0.2
diverges
26.8 ± 0.1 22.5 ± 0.1

Sent/sec
52.1 ± 0.1 54.9 ± 0.2
68.1 ± 0.3 67.1 ± 1.0 63.3 ± 0.1 63.3 ± 0.1 63.7 ± 0.4 63.6 ± 0.1 62.6 ± 0.4
59.5 ± 0.1 59.2 ± 2.1

Table 3: Ablation on WMT English-German newstest2013. (+) indicates that a result includes all preceding features. Speed results based on beam size 4, batch size 256 on an NVIDIA P100 GPU.

6.2 MODEL ABLATION
In this section we evaluate the impact of the various choices we made for LightConv (§3) and DynamicConv (§4). We first show that limiting the maximum context size of self-attention has no impact on validation accuracy (Table 3). Note that our baseline is stronger than the original result of Vaswani et al. (2017). Next, we replace self-attention blocks with non-separable convolutions (CNN) with kernel size 3 and input/output dimension d = 1024. The CNN block has no input and output projections compared to the baseline and we add one more encoder layer to assimilate the parameter count. This CNN with a narrow kernel trails self-attention by 1 BLEU.
We improve this result by switching to a depthwise separable convolution (CNN Depthwise) with input and output projections of size d = 1024. When we progressively increase the kernel width from lower to higher layers then this further improves accuracy. This narrows the gap to self-attention to only 0.5 BLEU. DropConnect gives a slight performance improvement and weight sharing does not decrease performance. Adding softmax normalization to the weights is only 0.3 BLEU below the accuracy of the baseline. This corresponds to LightConv. Finally, dynamic convolutions (DynamicConv) achieve the same validation accuracy as self-attention with slightly fewer parameters and at 20% higher inference speed. Softmax-normalization is crucial for DynamicConv since training diverged in our experiments when removing it. To make the models more comparable, we do not introduce GLU after the input projection.
For comparison, we re-implemented averaged attention networks (AAN; Zhang et al. 2018) which compute a uniform average over past model states instead of a weighted average as in self-attention. Our re-implementation is efficient: we measure 129 sentences/sec for a base transformer-AAN on newstest2014 compared to 20 sentences/sec for Zhang et al. (2018). Table 3 shows that our models outperform this approach. Note that AANs still use self-attention in the encoder network while as our approach does away with self-attention both in the encoder and decoder.
6.3 LANGUAGE MODELING
As second task we consider language modeling on the Billion word benchmark. The self-attention baseline has N = 16 blocks, each with a self-attention module and a feed-forward module using dff = 4096 and d = 1024. DynamicConv uses N = 17 blocks to assimilate the parameter count and we use kernel sizes 15x2, 31x4 and 63x11. Table 4 shows that DynamicConv achieves slightly better perplexity than our self-attention baseline which is very competitive.
7

Under review as a conference paper at ICLR 2019

Model
2-layer LSTM-8192-1024 (Jo´zefowicz et al., 2016) Gated Convolutional Model (Dauphin et al., 2017) Mixture of Experts (Shazeer et al., 2017)
Self-attention baseline DynamicConv

Param
­ 428M 4371M 
331M 339M

Valid
­ ­ ­
26.67 26.60

Test
30.6 31.9 28.0
26.73 26.67

Table 4: Language modeling results on the Google Billion Word test set. does not include embedding and softmax layers

Model
LSTM (Paulus et al., 2017) CNN (Fan et al., 2017)
Self-attention baseline LightConv DynamicConv
Bottom-Up (Gehrmann et al., 2018) RL (Celikyilmaz et al., 2018)

Param
-
90M 86M 87M
-

Rouge-1
38.30 39.06
39.26 39.52 39.84
41.22 41.69

Rouge-2
14.81 15.38
15.98 15.97 16.25
18.68 19.47

Rouge-l
35.49 35.77
36.35 36.51 36.73
38.34 37.92

Table 5: Results on CNN-DailyMail summarization. We compare to likelihood trained approaches except for Celikyilmaz et al. (2018).

6.4 ABSTRACTIVE SUMMARIZATION
Finally, we evaluate on the CNN-DailyMail abstractive document summarization benchmark where we encode a document of up to 400 words and generate multi-sentence summaries. This tests the ability of our model to deal with longer sequences. We reduce model capacity by setting d = 1024, dff = 2048, H = 8, similar to the Transformer base setup of Vaswani et al. (2017).
Table 5 shows that LightConv outperforms the self-attention baseline as well as comparable previous work and DynamicConv performs even better. We also show results for the specialized summarization architecture of Gehrmann et al. (2018) and note that our model is a standard sequence to sequence architecture. Our model could also be trained with reinforcement learning (Celikyilmaz et al., 2018) to improve results.
7 CONCLUSION
We presented lightweight convolutions which perform competitively to the best reported results in the literature despite their simplicity. They have a very small parameter footprint and the kernel does not change over time-steps. This demonstrates that self-attention is not critical to achieve good accuracy on the language tasks we considered.
Dynamic convolutions build on lightweight convolutions by predicting a different kernel at every time-step, similar to the attention weights computed by self-attention. The dynamic weights are a function of the current time-step only rather than the entire context.
Our experiments show that lightweight convolutions can outperform a strong self-attention baseline on WMT'17 Chinese-English translation, IWSLT'14 German-English translation and CNNDailyMail summarization. Dynamic convolutions improve further and achieve a new state of the art on the test set of WMT'14 English-German. Both lightweight convolution and dynamic convolution are 20% faster at runtime than self-attention. On Billion word language modeling we achieve comparable results to self-attention.
We are excited about the future of dynamic convolutions and plan to apply them to other tasks such as question answering and computer vision where inputs are even larger than the tasks we considered in this paper.
8

Under review as a conference paper at ICLR 2019
REFERENCES
Karim Ahmed, Nitish Shirish Keskar, and Richard Socher. Weighted transformer network for machine translation. arxiv, 1711.02132, 2017.
Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016.
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. In Proc. of ICLR, 2015.
Asli Celikyilmaz, Antoine Bosselut, Xiaodong He, and Yejin Choi. Deep communicating agents for abstractive summarization. In Proc. of NAACL, 2018.
Ciprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge, Thorsten Brants, Phillipp Koehn, and Tony Robinson. One billion word benchmark for measuring progress in statistical language modeling. arXiv preprint arXiv:1312.3005, 2013.
Mia Xu Chen, Orhan Firat, Ankur Bapna, Melvin Johnson, Wolfgang Macherey, George Foster, Llion Jones, Niki Parmar, Mike Schuster, Zhifeng Chen, Yonghui Wu, and Macduff Hughes. The best of both worlds: Combining recent advances in neural machine translation. arxiv, 1804.09849, 2018.
Yu-hsin Chen, Ignacio Lopez-Moreno, Tara N Sainath, Mirko´ Visontai, Raziel Alvarez, and Carolina Parada. Locally-connected and convolutional neural networks for small footprint speaker recognition. In Proc. of Interspeech, 2015.
Franois Chollet. Xception: Deep learning with depthwise separable convolutions. 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1800­1807, 2017.
Jan Chorowski, Dzmitry Bahdanau, Dmitriy Serdyuk, KyungHyun Cho, and Yoshua Bengio. Attention-based models for speech recognition. arXiv, 1506.07503, 2015.
Yann N. Dauphin, Angela Fan, Michael Auli, and David Grangier. Language modeling with gated convolutional networks. In Proc. of ICML, 2017.
Yuntian Deng, Yoon Kim, Justin Chiu, Demi Guo, and Alexander M Rush. Latent alignment and variational attention. arXiv preprint arXiv:1807.03756, 2018.
Sergey Edunov, Myle Ott, Michael Auli, David Grangier, and Marc'Aurelio Ranzato. Classical structured prediction losses for sequence to sequence learning. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pp. 355­364. Association for Computational Linguistics, 2018. URL http://aclweb.org/anthology/N18-1033.
Maha Elbayad, Laurent Besacier, and Jakob Verbeek. Pervasive attention: 2d convolutional neural networks for sequence-to-sequence prediction. arXiv:1808.03867 [cs], Aug 2018. URL http: //arxiv.org/abs/1808.03867. arXiv: 1808.03867.
Angela Fan, David Grangier, and Michael Auli. Controllable abstractive summarization. arXiv preprint arXiv:1711.05217, 2017.
Jonas Gehring, Michael Auli, David Grangier, and Yann N Dauphin. A Convolutional Encoder Model for Neural Machine Translation. arXiv preprint arXiv:1611.02344, 2016.
Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N Dauphin. Convolutional Sequence to Sequence Learning. In Proc. of ICML, 2017.
Sebastian Gehrmann, Yuntian Deng, and Alexander M Rush. Bottom-up abstractive summarization. arXiv preprint arXiv:1808.10792, 2018.
Edouard Grave, Armand Joulin, Moustapha Cisse, David Grangier, and Herve Jegou. Efficient softmax approximation for gpus. arXiv, 1609.04309, 2016.
9

Under review as a conference paper at ICLR 2019
Hany Hassan, Anthony Aue, Chang Chen, Vishal Chowdhary, Jonathan Clark, Christian Federmann, Xuedong Huang, Marcin Junczys-Dowmunt, Will Lewis, Mu Li, Shujie Liu, Tie-Yan Liu, Renqian Luo, Arul Menezes, Tao Qin, Frank Seide, Xu Tan, Fei Tian, Lijun Wu, Shuangzhi Wu, Yingce Xia, Dongdong Zhang, Zhirui Zhang, and Ming Zhou. Achieving human parity on automatic chinese to english news translation. arXiv, 1803.05567, 2018.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep Residual Learning for Image Recognition. In Proc. of CVPR, 2015.
Karl Moritz Hermann, Toma´s Kocisky´, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman, and Phil Blunsom. Teaching machines to read and comprehend. In Neural Information Processing Systems (NIPS), 2015.
Rafal Jo´zefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring the limits of language modeling. CoRR, abs/1602.02410, 2016.
Lukasz Kaiser, Aidan N. Gomez, and Franc¸ois Chollet. Depthwise separable convolutions for neural machine translation. arXiv, 1706.03059, 2017.
Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Koray Kavukcuoglu. Neural Machine Translation in Linear Time. arXiv, 2016.
Diederik P. Kingma and Jimmy Ba. Adam: A Method for Stochastic Optimization. In Proc. of ICLR, 2015.
Yann LeCun, Leon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. Proc. of the IEEE, 86(11):2278­2324, 1998.
Chin-Yew Lin. Rouge: A package for automatic evaluation of summaries. In Workshop on Text Summarization Branches Out, 2004.
Peter J. Liu, Mohammad Saleh, Etienne Pot, Ben Goodrich, Ryan Sepassi, Lukasz Kaiser, and Noam Shazeer. Generating wikipedia by summarizing long sequences. arXiv, 1801.10198, 2018.
Ilya Loshchilov and Frank Hutter. SGDR: Stochastic Gradient Descent with Warm Restarts. arXiv, 1608.03983, 2016.
Minh-Thang Luong, Hieu Pham, and Christopher Manning. Effective approaches to attention-based neural machine translation. In Proc. of EMNLP, 2015.
Ramesh Nallapati, Bowen Zhou, Caglar Gulcehre, Bing Xiang, et al. Abstractive text summarization using sequence-to-sequence rnns and beyond. Conference of the Association for the Advancement of Artificial Intelligence (CONLL), 2016.
Myle Ott, Sergey Edunov, David Grangier, and Michael Auli. Scaling neural machine translation. In Proc. of WMT, 2018.
Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio. On the difficulty of training recurrent neural networks. In Proceedings of The 30th International Conference on Machine Learning, pp. 1310­ 1318, 2013.
Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive summarization. arXiv preprint arXiv:1705.04304, 2017.
Gabriel Pereyra, George Tucker, Jan Chorowski, Lukasz Kaiser, and Geoffrey E. Hinton. Regularizing neural networks by penalizing confident output distributions. In Proc. of ICLR Workshop, 2017.
Abigail See, Peter J Liu, and Christopher D Manning. Get to the point: Summarization with pointergenerator networks. Annual Meeting of the Association for Computational Linguistics (ACL), 2017.
Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words with subword units. In Proc. of ACL, 2016.
10

Under review as a conference paper at ICLR 2019
Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani. Self-attention with relative position representations. In Proc. of NAACL, 2018.
Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. In Proc. of ICLR, 2017.
Ilya Sutskever, James Martens, George E. Dahl, and Geoffrey E. Hinton. On the importance of initialization and momentum in deep learning. In ICML, 2013.
Ilya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to Sequence Learning with Neural Networks. In Proc. of NIPS, pp. 3104­3112, 2014.
Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna. Rethinking the Inception Architecture for Computer Vision. arXiv preprint arXiv:1512.00567, 2015.
Yaniv Taigman, Ming Yang, Marc'Aurelio Ranzato, and Lior Wolf. Deepface: Closing the gap to human-level performance in face verification. In Proc. of CVPR, 2014.
Gongbo Tang, Mathias Mller, Annette Rios, and Rico Sennrich. Why self-attention? a targeted evaluation of neural machine translation architectures. In Proc. of EMNLP, 2018.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention Is All You Need. In Proc. of NIPS, 2017.
Li Wan, Matthew Zeiler, Sixin Zhang, Yann Le Cun, and Rob Fergus. Regularization of neural networks using dropconnect. In International Conference on Machine Learning, pp. 1058­1066, 2013.
Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation. arXiv preprint arXiv:1609.08144, 2016.
Biao Zhang, Deyi Xiong, and Jinsong Su. Accelerating neural transformer via an average attention network. arXiv, 1805.00631, 2018.
11


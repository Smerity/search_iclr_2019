Under review as a conference paper at ICLR 2019
PURCHASE AS REWARD: SESSION-BASED RECOMMENDATION BY IMAGINATION RECONSTRUCTION
Anonymous authors Paper under double-blind review
ABSTRACT
One of the key challenges of session-based recommender systems is to enhance users' purchase intentions. In this paper, we formulate the sequential interactions between user sessions and a recommender agent as a Markov Decision Process (MDP). In practice, the purchase reward is delayed and sparse, and may be buried by clicks, making it an impoverished signal for policy learning. Inspired by the prediction error minimization (PEM) and embodied cognition, we propose a simple architecture to augment reward, namely Imagination Reconstruction Network (IRN). Specifically, IRN enables the agent to explore its environment and learn predictive representations via three key components. The imagination core generates predicted trajectories, i.e., imagined items that users may purchase. The trajectory manager controls the granularity of imagined trajectories using the planning strategies, which balances the long-term rewards and short-term rewards. To optimize the action policy, the imagination-augmented executor minimizes the intrinsic imagination error of simulated trajectories by self-supervised reconstruction, while maximizing the extrinsic reward using model-free algorithms. Empirically, IRN promotes quicker adaptation to user interest, and shows improved robustness to the cold-start scenario and ultimately higher purchase performance compared to several baselines. Somewhat surprisingly, IRN using only the purchase reward achieves excellent next-click prediction performance, demonstrating that the agent can "guess what you like" via internal planning.
1 INTRODUCTION
A good recommender system can enhance both satisfaction for users and profit for content providers (Gomez-Uribe & Hunt, 2016). In many real-world scenarios, the recommender systems make recommendations based only on the current browsing session, given the absence of user profiles (because the user is new or not tracked or not logged in, till the final purchase step). A session is a group of sequential interactions between a user and the system within a short period of time. To model this phenomenon, Recurrent Neural Networks (RNNs) were recently employed as session-based recommenders (Hidasi et al., 2016; Jannach & Ludewig, 2017). For instance, GRU4Rec (Hidasi et al., 2016) utilizes the session-parallel mini-batch training to handle the variable lengths of sessions, and predicts the next action given the sequence of items in the current session. However, these approaches primarily focus on next-click prediction and model the session data via sequential classification, and thus cannot distinguish the different effects of user clicks and purchases. In this paper, we consider the session-based recommendation as a Markov Decision Process (MDP), which can take into account both the click reward and the purchase reward (see Figure 1), and leverage Reinforcement Learning (RL) to learn the recommendation strategy. In practice, several challenges need to be addressed. First, the recommender systems involve large numbers of discrete actions (i.e., items), making current RL algorithms difficult to apply (Dulac-Arnold et al., 2015; Sunehag et al., 2015). This requires the agent to explore its environment for action feature learning and develop an ability to generalize over unseen actions. Second, we found it difficult to specify the click reward and the purchase reward; the policy may be biased by long sessions that contain many user clicks, as RL algorithms maximize the accumulated reward. Besides, real-world recommender systems require quick adaptation to user interest and robustness to the cold-start scenario (i.e., enhancing the purchase performance of short sessions). Therefore, we will be particularly interested in a case where only the purchase is used as reward (click sequences are used as inputs of the imagination core for
1

Under review as a conference paper at ICLR 2019
exploration).1 However, the purchase reward is delayed and sparse (one session may contain only one purchase), making it a difficult signal for policy learning. To augment reward and encourage exploration, we present the Imagination Reconstruction Network (IRN), which is inspired by the prediction error minimization (PEM) (Hohwy, 2016; Friston, 2010; Lotter et al., 2017) and embodied cognition (Clark, 2013; Burr & Jones, 2016; Seth, 2014; de Bruin & Michael, 2017) from the neuroscience literature. The PEM is an increasingly influential theory that stresses the importance of brain-body-world interactions in cognitive processes, involving perception, action and learning. In particular, IRN can be regarded as a proof-of-concept for the PEM from the recommendation perspective, following the ideas in Burr & Jones (2016) and Seth (2014) -- the brain utilizes active sensorimotor predictions (or counterfactual predictions) to represent states of affairs in the world in an action-oriented manner. Specifically, the imagination core of IRN that predicts the future trajectories (i.e., a set of imagined items that user may purchase) conditioned on actions sampled from the imagination policy, can be considered as the generative model of the brain that simulates sensorimotor predictions. To update the action policy, the imagination-augmented executor minimizes the intrinsic imagination error of predicted trajectories by self-supervised reconstruction, while maximizing the extrinsic reward using RL, with shared input state or output action representations for predictive learning. This simulates the active perception (a key aspect of embodied cognition) of the body under the PEM framework, which adapts the agent to possible changes that arise from the ongoing exploratory action. Note that the imagination policy imitates the action policy through distillation or a delayed target network, and thus IRN constructs a loop between brain and body, encouraging the agent to perform actions that can reduce the error in the agent's ability to predict the future events (Pathak et al., 2017). IRN equips the agent with a planning module, trajectory manager, that controls the granularity of imagined trajectories using the planning strategies (e.g., breadth-n and depth-m). Besides, IRN is a combination of model-based planning and self-supervised RL, as the imagined trajectories provide dense training signals for auxiliary task learning (see section 2). The key contributions of this paper are summarized as follows:
· We formulate the session-based recommendation as a MDP, and leverage deep RL to learn the optimal recommendation policy, and also discuss several challenges when RL is applied.
· We consider a special case where only the purchase is used as reward, and then propose the IRN architecture to optimize the sparser but more business-critical purchase signals, which draws inspiration from the theories of cognition science.
· We present a self-supervised reconstruction method for predictive learning, which minimizes the imagination error of simulated trajectories over time. IRN achieves excellent click and purchase performance even without any external reward (predictive perception (Seth, 2014)).
· We conduct a comprehensive set of experiments to demonstrate the effectiveness of IRN. Compared to several baselines, IRN improves data efficiency, promotes quicker adaptation to user interest, and shows improved robustness to the cold-start scenario and ultimately higher purchase performance. These are highly valuable properties in an industrial context.
2 RELATED WORK
Session-based Recommenders Classical latent factor models (e.g., matrix factorization) break down in the session-based setting, given the absence of user profiles. A natural solution is the neighborhood approach like item-to-item recommendation (Mirowski et al., 2016). In this setting, a item similarity matrix can be precomputed based on co-occurrences of clicked items in sessions. However, this method only considers the last clicked item of the browsing session for recommendations, ignoring the sequential information of the previous events. Recently, RNNs have been used with success in this area (Hidasi et al., 2016; Hidasi & Karatzoglou, 2017; Jannach & Ludewig, 2017). GRU4Rec (Hidasi et al., 2016) is the first application of RNNs to model the session data. The network is trained with a pairwise ranking loss on one-hot vectors of clicked items, and then used to provide recommendations after each click for new sessions. These models primarily focus on next-click prediction and model the click-streams via sequential classification, while here we aim at modeling the purchase behavior and enhancing users' purchase intentions.
1We also conduct experiments that take into account the click reward.
2

err

error

RcdnateigwNRoinnae1234 atrcNwdtoiAeo:monRtroneSwnpkmSryeAeRhmoAssafepuFtefihodlrsusloennoluedeirrtikwarscacrsoedststheAisoie.ooqsanquannRsnuute-heteesnboicrnetrtoetiwa(ianrsanlt)alsuarpredceoct.cdpitooIunimnlRosatmhrcnieelssanecspsNdssaoaiifiptoemieconrtan-,twbsmiwo,aneisote,efirdwosnrkhmimmdicouphadloaedtrtloteisaonnsueotsnqtetouecexelnpantlshiisacailinfitrccdeeaicfttofiheomernempnmuetrineactdthheaaottsidheosenwtcoaillislcmiMknogaDdnoePdfl,

5 wemhaeirle the purchase is used as the reward signal for recommender agent and user

Under review as a conference paper at ICLR 20196
7

3 - ADneonnoyiwssmeineosgsupisor:oAnpsu5otahsreSeorta(rses)aimtedp0laesaerxcth3eir/tnpeacltuenrev,i0rnoanmmeelny0t.ImToaghiannadtlieonthRe escpoarnssetrpuucrtciohnasNe reetww(Daorerdkn,sify)

(Densify)

-peoodumnletsaomcrcoelgs111ane10289dessdsensaiir2fi1eto.icron--aan-tDIbtsineo,aepnisPntte,urAisdwuatsiAfjhbm(ipeIuyrAemIRifneusscoRcficdepvtmrhNdtldrcNodhaiioPedrarharilweroo:)t:tadasaucali,isnsosinnestusrsiwenmotsssttceteti,re55heohlheellcinxawwfecal-ptaanIhsasslsihimurmtsecradiihpRoinsfio.taeclacddBlIireesa-0?nievmgfotytwsfliRihueospimeatrenntieaoeprdginwmpenaurtipdroirenadtamcerltt33ehichiafrei:coadroozdttyosidhniinecSnm:osestfwgtntecrSroRtoilttuhlsiehhlmcRmcie?0seseekntosieigicafodnaonicunoooecnetdotnfrltununiisnn-or--isessg44snbbcitntciaoarpreabisunolmsuseclcfeeiadocectrgdrviytiPRoaniarRntoaaceeinttodnciiNdeonoiocnsnceNmct.ieotoporwmmeomIrnmoltobireEwcarminaynrgknreo5diowdenarrianhmtttkMeiidawonoxinainnittsmhhitm0eaiizraorieieznnsaigttsthaitontehinonce3.

04

Target = SR

5 0 3 04

Rollout Session log

liIcFenmaiaegnradudngritideirnnaag1jatee:(tcRiiotTtoaenhcrm-iaeaensusai)aèggfrgmeteeenernetn-eteutraaseacledt.h,erd2Aui0nsbg1etyee7rnra;catPlsclieatciaskorcAnn(agselnlrduieanmepentpMo)radooDle.ar,lPc2pch:0uaet1rnhsc7ehib;naerScsecioeclarvo(ptermeoergrdmeoa)trt.eiiannzlegd.,deo2rif0nfa1-tgpo7eo;mnlSitocu(ydtRteoeAlxn-b,p) a1egsr9eei9end1ner)cre.eaiBtn(eeyfso.gura.cs,eilniimmsgteaaongnft-mm^Rawpoonnosdoomeocedhndoaacessmeoe,hhaeadsrRaveednnho2euoocltmnts.epiBmlctheuce-s...indobirrufdothcnafiaelt.ieeuIdnsj,scacmsrlenscrnopaeie-dNtnph-prceEn-nEetbneep-iarehspet-cepvewdismannlenHefouaanir+epBlgecrrreen.ic-nsitEEeneari.aooonyeIdtctwtaReeh,amultcm.pdfia-olneibtaaeru-tnyvnnseselnurispeEgoessrrasnigoeptvn-woeniuirftoNevurnaedqsupetnMacea[mnsruthisaonomayoeisiehCortptailoyoiltsdecfipenccrdusttonhmigglcsnlonsrdiersactacioisereaeeis(sirmyhsevtiigrtoftauelmowtersI.itdsaatauraietdnnarritbts)oimoeodwhpaitninorcNtortirain+cmicrpfstcsmpmveeineiin,aiwadronsrrdemhttegto,rtieiuorrnndehtiugav,tnleerrlaieoceseuafitl133223122212122211213eocyr{behmca,cihehmcshainouebsdotetopteureineodamnexlliptdsnIgietnericatehmiinidcr197890348632265451037pcsEvndaneEtubhe1one(brirei.coltpr-dkwcsnpscnhcMlaenetrueaPraaodnysrefirrtn.ioaintisitxpetenxaropptgmeeeescnnaawhnaaanhhttammctitnwiieooecybcseiusoyonameeinds(berrEepEewniirsdunhgpsltcsrttcSerhvsdslfiodntsrlohciitosynrdnfaMxedtpsstonmtdeeeih,co=ei,sihieshodrtrtdtivnmluynheacastieiiedsmesouefe}tmsormirexittooforaeairre.pthtnenipuseeilinhoatcdlhtcispeoywnd^^anthldstdTlr2iraeCidisa1esSIapdnttaiaa,sefr,eToreseeansorioonmlazcircpiyitntEhflrpatpovnnyoidn[ce^nahonairnitotryeeoaseta.fito,sprnuswtteraini,cdtuavpetn0trrsic.cccrdiieTruaifiirrlndntuuiirmoois1,eoTynnxahathmsaz,chNeyseeuuii+teiowesrtcrdug'selndtmaa,esn1211112222121111tndinicrmmyee^footnoaiccoira1humiatesdaashtbtoihiplwctsr,.crrsibtI9znekncrlnziotrrc0109373415256842219876543s,hMreaecuaVteentmrfotunftrmsmamwesitaceroycse.lufgilao-hcitftxp8sginetcsRmie1traeetnaatiicmcnetthticnwronhcwi,mipeoollrwae-ia,,zidscIomEhoeniioalueturdtcmonrrtster.stltnuhr+fuoscti)endieaer(y1psgtrqxetobe1tbRsutcdeishMehestItietpitklhahiNautdhtievera2uwrewncnhaat-tp3r..osihi-sahovesstloiagmeex^^ionrrhrohctfesovii1isaimtrnni,octtoiuheea(asmtrardutetnothreeartnmnn,naitoaftrntettgantshechfacrNae1etsroeuoohaattDebaiarm,marhnivtaslrehToepaaiontensoetTD[chhahcieredtitbhs,eteteenr+osdrotnnfewIrrmcidothigtstecnnqonnt,tel]ttyntehtrcnorleurrmh1etnsleiiptrotpsovchr-Noee,cVtoseeiioeeciesneuunn.teoaameabshtirehd[win1oaroeftiPpfnseaonrnconarhaimdpgktoyseAcduucpninaoeibtsine9erlrblzdflerrrnrwrforliacgaiooounmuct1iyngrrttmde1su,,aruo.ln.ieaIintdilfedcrisepneaepsrmewereognysfacreenmiaraeietomoeeict,naaeu,ariemr=l,nnazrcomeeenmgraitetoe)dnmunvias112222222211213121111nmyshgrrnrntohuscsxtbmctFilninternionraeq{dumihtnbuumcooojntnmMwt850904285463217019367278651943sinep.klxast,oaiwfwcnmibgaiwaiee.am3.ndiibluneosetetebdaoed)egire,hdseuoehtrtsdscnnpsltca-ttotoo^cduearssureupttuttiierucwtstlmo.adilgrrnri1ntotltreaieeha.e3ebd,iuosansemau.diuruseeirceatavgrminospwaiodyouneitucDgwwA(piner-knttirdcuptIespeuFspdiae^^stsa-ohtdenaiudsdepiaissC1tro-ilsrooLtdtteceasclonttesehimxi]nh2ItetiritooRlurmlhttedteihwfnglxtnnsehrisl+ni.rertrsoT3vhutrossugrosmrousinttmeosToyseucietetsgFe,inect.+tohaRtsmeagfionotenmhceadiinfoesoliPenocoIsltseondbdedekneeacgyipcocunoepe.ettCvrihitstVoerrpcsfobaNrthefeleremrcrpiao.luirrhtvsptto1ntmmoomsnrtoiotisancreroTrenlewc.,o,ole,tdnrc.ubiNmcsatttceridosaowoonrdtsInipkreohii1elgdiueeeeedanpmst1sorstiliehhsicIpsctspmprrrleaerecxipInfsorsaaaurteehtthmtch=nemegoeisntvosboayrTonrsaocostu,enrbi.nean}avetentaweiadeyorn)cihac.acmlndnponnaandupcnoypftlsyeCqaceespmturas)lolsssinteessauncstevneqc,erqinogolfrdrarfllqs)cyrsgtt-idtnoeceashsamopdta.ethiRas,,,e,lo,pieetstetucerGtctliopuroutsisbfthacetsorolruhn)ncgiieotoeshoouwte.f.iww(ammirvrdardiceseusptppFIuceavilceqrrarahgnvaieaoyeueteacmheaIae.'tosohpmrekItbrGcolRnetehooxreorratiltfnormePecunsuoesrsRaosoltt.egenhtoptnRsnikiuheRkrifewayyai-eictenerhhsscgraaerbdoersvnhloeeyteveeaFmriusrar'claNrenpnorktbencnhnnfoopruserieoIPteayldaemRdcuploNctimeclexearidstUehwnsiriipakrctenoedieaudytreinipemwsftxsaaRerlxooNsinitlredmwrohuschraniaethiscerrsamososannlpunrfhheinrpet-oayqdtao)olata.fctadadUntaictrdpcsap(tbaaqaceadeaseohed4rnrfloeftiprvte,sqisltt-naeeNoinermtaortcnatrrdessRnslll,hiRssulihlraeet.ssteuontnnoiahotpqetisRaaoecemtutuaicetci,rosrhrRbwcoandweeiemuvotelsnr4iecorecpiantdulsniedrrachasoanesednanticeedsralsactolcrnoaucfrhsNedpohLnwcotoieotdaocaaicewlreisii,etohaaRcaarrstonoscnhroeocetEenlrdcem.its.ltcneedthnthReecdfiua.ameeeemItnleacpnsrlelecetelnoeiepeirtrmhecntnue-nwpihaieioNwfeendkwR-pitewcarBlrinreac-eosemiac-rtEnanoCuon.natoI-deocestoctrRdapppimctma-dnaeefocnuceushedeisepvepmmeslaeafnNminorrtiaroIecaaermnnnlptfitoth,oy)otaafioanndrtltCemfiunaccsapeterooirlrslcaasdr-rndamoiisrtentoiprvervcralarealeasnsmdaaieafsauimtotrhnIstadpmwoccahpoetriinobitocdtooendlcsopsaelmmicconthasssnonfrcrpecemts.ihtidgspctEredEmrthbetoa.iegttt-accclbllleeandiaaieptiotehroeribdn.iendmcwheiennpBlbtrteeo-siirmNwiaiearioohnsndeamuIdseoneiolbeeurypna-ss-anEpcErnuerensebgtepvseeosfcpusnenelnxadnseaamanMonryn.sai,nmtniecslitiefihsnrrntblsellyswsraapehicamiroivorneeaeidsytnfitmrssoaiitpEoasuevierpgetssen(boiepotcisowhwdccssdtdmcmstaiennusasmereereessnmfihrtigtiriecherutlrpcvlbeienlhcsciuywhdaaiannrhtottitmiituia,erneiosesrievennreseicriindeioituafielwnfeiolsdysahiarhtnahmrara.fisopulehcserpineyncdtnxaprctitddipnrvr,imnne,oiceaceossanirden(bnatwzuaypsecrii.ttkavnndsccfithMroeniitryaeefieopacoroeeyaca.shahduitoehmiecdirotustpmegeneneidss,rtlaarilhAwtrleccaalhcshootcyanidradisdlaEtnihiinyiac,m,es.ltrirtncinnytcdiMnefnelwer-r,ossIirtoxasao.fiipasoeinuecdthopttitgtItufdctrriviareaaoenyoaetga,wsbazoRcnyisoatxndiEinhonnifwyeeswitolactnnr'snrtahaoiiclccrnthtannrnriseso,xeutta,oAbes(oohtoawspiltcwrmsoslatdIitoinicmnohhetttpNnunsteueucahriptflw-,hsIexeivssooefiyiinusnenttthchtutiifhasReeygrntertanbmrimRdnnersnttaoisrnin,herssohNewothcserimehheillharanrrpet,erionteer,hierita(utaeaoshsato(wiiroqfetthtNcteteeuyehrgttzareirAnohriavrscnahmNinseendehhcinwtrAsamcNeaooeoiimSedsinftpsresninne,ustamaaitehcirciwteifmioare,taurieaeosazedinaAebhaninnennrlrozfdnrtmcrriyhthsigntaerAcourntctingnrrthbdmmfwAaneltbiiooicmfMipsrenepcsulaunmaeaieiwbwhwctioeoafmia.rnoimfieraamoeoclnzaAtchoigomgtnndceaioenesrisrdmcucnttmnygetthsbctmftauisnteunrierumerncctctbnmoenrtMuts,ouaipnlkaahamfiaiee.be,umwmdwucnoistoadepe)aiionasoodoretrDuscuapmoginttia-ohstuensrunrcwtttooeto.desisilluntaieet.iuierdnuaoetrstyollegrsthtgrdnsreluapsenrnttnrihamovr-ohbesidyeermagenciintatdsaostlomeaeoiitDesad,rodnlosePtdntserncseonrtkmoysnhiycuioutegnetsasea)rnttAbeiiaensrlesleriroltntoruschtdeiscmlc.hitraovrmaa,,eiroAoeioie.rotliIsownttmentismee.glc.drsnfnmaPieaeso.pmoootonieIoecdkelaayecgeersimsurtPspTeucefraraunlrbinmutodmTennnrbi,tn,nntaandn.onlhssmpmatunanneolsspmtuceeetmotcsrtergosrtRmsysdauetaaoeerttwaetocdslclshahusufslflloo))niieeeeiriahrlqarnrlgqmiogeteeciheot.oetch.eipr.trGcsavomoerorcocdsrogoccdeunmcafgaiocoeroauvwtfytaaicee'rrarkvbticnyoniiPrRlkexbhochunsneokiaayPsFtienoa(laeexnhlnolnusnraraaaenn(rpaaysodlataltuasUnasaaocrettspntserpenayisdtoaomsas)mrtoctarrnrsslgslacinsyihtqstnsontstie(bitiwddeeed)4tsoeirssoslislunaicfrhtttdcRtaltour)oiniotiobiaacodeeiio,,rR++oe.nisilreitinnancaccmfied.rnasrprlconoenrNGthooaaacowiic,sf11arntcectEnl.eittcnRga,rmccdfietnneiselcnoatihoNe'ptrkwaboaCaraeoicocectrdi.RlechtcsRmmfdmokerIntyaerinett)SonnlioefcuspeipdmnhtoaoanllaoiceoayardmceisobeUocaeisesmReyrfmorpcEItEbrsrntcnnlondmanrrn.infscpseidemhahoamoqeUynssflapErtgteo(bemdsnbM4oflesnameeiiiehrsleroetupcEEiRtroueercseniootwcasdaraan.niean,rrcRiephviaumneanoerynuusrpEavnrcgieafieoymrdlsschmnuNterdnmtiaeArtcalhwidetehua.tkirceMteEneerroeiamiotwtotgdaaaanrwnocooarcEignisnplnwvrtneiNrtsxeutroaeSnrhaictdiivanChtyasohmexiduoftimeienndithcacnteat(cahtasdesotohrm.eahpMnicteh)reitresnwaNintteegrnaadmaantttwochNeceieoelsEaanaiiltoasitchetsixerlozdrroaaelhinrdsimIanlhisifnpsresepcEeibexiio-ielnnzcmilnaaePndtmanr.cttnaisresibmMdspkwaahhpwwncnoihbreiutooogsEwsttttoeSsutermtngtMcNeehaebsi,emaiaiihoaaootiDnlydsotnsasoonttinlezoorlerhrasirrovrnreeiealti2meiaefasrdenPeiondkycuiecselrzefrcvrmilaanemdmt,,n.tivn0rrbemfilMskayhwwcniergndmtto1esidutet.tgkedchAhaedaioaootDdot8nsneaooanttneoarihdcEirolrretteiett)dnrPhssondkeycteistnvfrartl.soix,,of.itoohoahntadoooDftpncar,eitrnrnashseioisiterractmaofnpeitiaamsrocnrbplci.otosceertrbd,maootentiollvrsectmeyutsberomrremierbndhmueddsatneo.avett.tihsFinooooorondsfrt,. orinoftnemmrnoeadnlet,ml-abonaddseeelxdopfalogthietenatwdsdouirtsliduo,anltalhlyetrsaaugifnefinenrtgscsafirngongmaelnsmetrooadilmiezlpeertroorvouernsdsareteeasnuelfstfitiacntigeesnf,crroye.mmHafoiuwnnevcvtaieloird,ntihaneptpphreeorxfroeimramlaeatninovcnie-.Nandrnc[ueItooda]PcesontelSoione,lroanNos2rspNncsP0euwrisfie1taoset,iut8witvxlotuer)ishcnia.oensnohlyDeretdrusIklareonty323222ss.cr068791wtf'nuaeobsocrrereodtmrpdaCesugqtedre.ctuirouagbeefnikshrrfstm.esrecttiiue,tieornnhrpposn^invtneatb333etaneee213inseuq(rsPiar-usetentubsftllirer.dsToaSIaweotoaa.tnirceuotnrscgwlwe;hebrmmeotemnimenciiedts^aatirhslovaioeshfintl)yktetdienrtmyescnrsw,esedceacrgtlscameeatconliotogairtSohtrcleemnoe3li-mkyksy-2iwnnfiosisnmmpnodatcadffeh(ra-nnefleCeolemRntdnddhrnnuotulsdNsnaoedyepcrfpma-e(enowuNaaprtNrrennircerlesionic)ntdIsceatocwhPnamihodclenstaS-oiirt,oe(owstnbeunehpten2MsnhysnroNis,0stereo,dlreit1veatueyrdnulhic8rnsdrowskuaeheb)tpullr.troidiyeirsshItrrsaD'ntvahebasttfchceetenotootcuaqbtrritictrmrshuscneuroipnteereeaacoyoneentn,tpiiat)russotrtnew.ifldini.riarosdcnfailePr,ousertalerermrtrsoomelrmrnccieciuuabeneobsaoassnrttngemuyrssweccidhtmrnamepeeGossgra.aeo'eer.voRSknyntisynindhsUFeosfmfleotoqf(oer4ruuoRmusdoserRrue.rsnwNrreEnce(iNNeattrCaihdleIa-t)PawtbulSceolysob-ln2rewraet0rtlhMsdih1oheos8aidirian)rsvfila.rtdhtioDtkcaoiorssooadritectvoatntfiahsrotoolic.atecrnrhsdaeiieralmscpettiborscnriepr.obosdhrub,msaot.veolv.meiuFoemorremsrnm.eddneoattthsinoooondft, I2As (Racanière et al., 2017) were proposed to address this issue. I2As augment model-free agentsn(cseotto+;n1N)e.urBaly3I2nifomIrnmimtaataitoninynPrgeraotlc-hewseosirnldg aSpypsltiecmatsio(nNsI,PuSse2r0s1'8c)u.rDreontnionttedriestsrtisbaurtee.influenced by their historical behaviors.

{item 8} S1
candidate items

RA

v1

not item 5

v2 ... vn

{item 8, item 5} S2
not reward

{item 8, item 5, item 1} S3 click
RA

{item 8, item 5, item 1, item 2}
S4
purchase RA

item 1 v1 v2 ...
vn

v1 item 2 v2
... vn

s in the rea33l eTnovimroodneml theins tp;henomenon, previous sequential recommenders with user historical records. For
with imagination and usein turn should allow the

an

interpretation

module

to

handle

imperfect

predictions.

The

imagined

ot+2 trajectories of I2As are providedSubmitted to 32nd Conference on Neural Information Processing Systems (NIPS 2018). Do not distribute. as additional context (i.e., input features) to a policy network, while

rewards of different actions

the proposed IRN uses the trajectories as additional training signals for self-supervised reconstruction.

ot+k

Self-supervised Reinforcement Learning In many real-world scenarios, reward is extremely ent sparse and delayed, and the agent updates its policy only if it reaches a pre-defined goal state. To model
v1 this phenomenon, self-supervised reinforcement learning have often been used, which accelerates the acquisition of a useful representation with auxiliary task learning (Jaderberg et al., 2016; Pathak et al., 2017; Mirowski et al., 2016; Shelhamer et al., 2016). Specifically, auxiliary tasks provide additional losses for feature learning, and can be trained instantaneously using the self-supervision from the environment. For instance, UNREAL (Jaderberg et al., 2016) maximizes many other pseudo-reward functions simultaneously, e.g., pixel change control, with a common representation shared by all tasks. In contrast, the proposed IRN do not require the external supervision from the environment, i.e., self-supervised reconstruction is performed on internal imagined trajectories.

3 PRELIMINARIES

We interpret the sequential recommendation task based on the standard reinforcement learning setting: An recommender agent (RA) interacts with an environment E (or user sessions) by sequentially choosing a list of recommendation items over a number of discrete time steps, so as to maximize its cumulative reward. As shown in Figure 1, we model this problem as a Markov Decision Process (MDP), which consists of a tuple of five elements (S, A, P, R, ):

State space S: A state st  S is defined as the previous items that a user clicked/purchased in one session. Specifically, the initial state s1 contains the first item i0 of one session. The items in st = {i0, i1, ..., it-1} are sorted in chronological order.

Action space A: An action at  A is to recommend items to a user at time t according to its policy , where  is a mapping from st to at. We assume that the RA only recommends one item to the user each time, since we use the observed click/purchase sequences for off-policy training. During off-policy evaluation, we can recommend a list of K candidates to the user.

Reward R: After the RA takes an action at at the state st , i.e., recommending an item to a user, the user browses this item and provides her feedback (click or purchase). The agent receives a scalar

reward r(st, at) according to the user's feedback. We also define the k-step return starting from state

st as Gt,t+k(st) =

t+k j=t

j-tr(sj ,

aj ),

where





[0,

1]

is

a

discounting

factor.

3

Under review as a conference paper at ICLR 2019

Transition probability P : Transition probability p(st+1|st, at) defines the probability of state transition from st to st+1 when the RA takes action at. In this case, the state transition is deterministic after taking the ground-true action at = it, i.e., p(st+1|st, it) = 1 and st+1 = st  {it}. The goal of the RA is to find an optimal policy , such that V  (s1)  V (s1) for all policies  and start state s1, where V (st) is the expected return for a state st when following a policy , i.e., V (st) = Esj>tS,aj>t[Gt,(st)].

Asynchronous Advantage Actor-Critic. This paper builds upon the A3C algorithm, an actor-critic approach that constructs a policy network (a|s; ) and a value function network V (s; v), with all non-output layers shared (Mnih et al., 2016). The policy and the value function are adjusted towards the bootstrapped k-step return Gt,t+k(st) + k+1V (st+k+1; v), using an entropy regularization penalty to encourage exploration, H((st, ·; ). To increase the probability of rewarding actions, A3C applies an update g() to the parameters  using an unbiased estimation (Sutton et al., 2000):

g() =  log (at|st; )A(st, at),

(1)

where the advantage function A(st, at) (Baird III, 1993) is computed as the difference of the bootstrapped k-step return and the current state value estimate:

A(st, at) = Gt,t+k(st) + k+1V (st+k+1; v) - V (st; v).

(2)

The value function V (s; v) is updated following the recursive definition of the Bellman Equation,

V (st; v squared

e)r=rorEbsej>twteeSn

Gt,t+k(st) + k+1V the target return and

(st+k+1; v) . Then g(v) the current value estimate:

is

obtained

by

minimizing

a

g(v )

=

-A(st,

at)

 v

V

(st;

v ).

(3)

In A3C multiple agents interact in parallel, with multiple instances of the environment. The asynchronous execution accelerates and stabilizes learning. In practice, we combine A3C with the session-parallel mini-batches proposed in (Hidasi et al., 2016). Each instance of the agent interacts with multiple sessions simultaneously, gathering M samples from different sessions at a time step. After k steps, the agent updates its policy and value network according to Eq. (1)(3), using k  M samples. This decorrelates updates between samples of one session in the instance level. Besides, to build the A3C agent, we employ an LSTM that jointly approximates both policy  and value function V , given the one-hot vectors of previous items clicked/purchased as inputs.

4 IRN ARCHITECTURE
In this section we incorporate the imagination reconstruction module into the model-free agents (e.g., A3C) in order to enhance data efficiency, promote more robust learning and ultimately higher performance under the sparse extrinsic reward. Our IRN implements an imagination-augmented policy via three key components (Figure 2). The imagination core (IC) predicts the next time steps conditioned on actions sampled from the imagination policy ^. At a time step t, the trajectory manager (TM) determines how to roll out the IC under the planning strategy, and then produces imagined trajectories T^1, . . . , T^n of an observable world state st. Each trajectory T^j is a sequence of items {^ij,t,^ij,t+1, . . .}, that users may purchase (or click) from the current time t. The Imaginationaugmented Executor (IAE) aggregates the internal data resulting from imagination and external rewarding data to update its action policy . Specifically, the IAE optimizes the policy  by maximizing the extrinsic reward while minimizing the intrinsic imagination error, L = LA3C +LIRN . In principle, IRN encourages exploration and learns predictive representations via imagination rollouts, and therefore promotes quick adaptation to user interest.
Imagination Core In order to simulate imagined trajectories, we rely on environment models that, given the present state and a candidate action, make predictions about the future states. In general, we can employ an environment model that build on action-conditional next-step predictors (Oh et al., 2015), and train it in an unsupervised fashion from agent experiences. However, the predictors usually suffer from model errors, resulting in poor agent performance, and require extra computational cost (e.g., pre-training). Besides, the predictors may learn a trivial identical function, since the state

4

Rollout Rollout Session log

==mwwmmxgnuagdncenamngcyuoerns+oarttgtrcdIaeaononngtaQnrwmaigkiehiruahoDrlpaateaeIuril.cenp(ngRrehtovDtaaaelaaii,enoherccl=eoctlhl)ihlsndrdvossetrtegnBeaieeietnstecusteng(creadiitrnkay^gsepcoanasoPiaitrteardletgrcniNrerLtpiQkeiNctedpoihtepunxsnvee[iI.anirhitoaaalarn.oaoistrioucaineliachissithrrdtsoornxg,ltsiemetitsoseallime2,na^rseccnro,oon^t{dtinbrttngsrtnretekeepcNnciino(Ipe+gdtiitlrorttfanw,rswii.reietueeitgbtndehiwhsnsptnre,eeclotit[^fntatE(pomeae,nTdwnriis[,o,iefcnrdothrwaaRshpeyrtpmiaiasoelpe+rtatrplmfaridpsii]daerniomaossyiesiot,h]tecoooa[feIntgnpraanwdl[e(rfgpnnrspoenrlrdtsmegiMits)o,rparsnoeeoelhyagp]oNRttso.,ecyaeyp"]lgtreeraoronnehrt,lrtetdnirtperjosgdgil-e,see;oefohnsoistalteo.t]^iiesue^pnagrlalgrirnrureorieitasecberrz(srsCNo-tsiceereup-msloaaincaioT[etsaaevcoiftene)dpn.nenn(olssC,sclttuodpneevl-nyaooctTpealxbgstc,ocfItnidit,bhtnnistr.ljntygspceyeisonsif+i,l=tIysaatf,tend,wrnteireailetnraoesradhe2nifIesoeidauaarcnodefrmetiosoperdhg2r|ymohin-iditutaoomltreianamusatctlnin{gotc)lttnnaknrrruovetiacatsnti1oshrgflrEbetry)aa^erfnshrrdErenmihrefiwiolsmairrtmdi,Inhipthtmiasisdchcyninni^isuyettcen^snnetdesodaa-esnoa-soc,etnscreoafooooeenvevnalslnnuf]e2eptrr=e,rwireewoecttsemeeitfttstualefgdsjtti)gsd.egu".p^cltatye..trhrptlrndkraeangtwdieaod^meesirofumtwigiraEaeip^eurioipfureumdeltaihil,teeheiiaonaui)=alhyehavomtto.hriBnnetrSite=ciuyteadedannmeoraa.nappPtiortsacinsrwtpmtirotneesrioa.npttgnoogmceetea.ifoai+alnmstnrmst.iinooahecnevohtsringsacfeIsrleteooteotsvDrimar1grorrcrblt}troter+c)aiwpsendIlmosglceretifenilnteTavnTDratometneee(rghwsgircorekaitsgeet}avion)aiioesendrieiose^tetntiiwhayeiotot,e^enursnviai[rmnridrsksigstoprgaoieicineoe1rinretianmjsidtorirhagpn,pmdpimioeitioseaouseloxsienvene[nrrno^dinmirhes(taoenhtlt(ndntnilyoliamgsite,orzrw,efot,soerwirsiteotiensuinistitdlonlnpleenedobenr((tdan^dtinah(;upcdawnaaiteaecmhhrnctpeodnpnseapn.aausellidrs,oprrstpoarwilxrbida,rmaisbhittdcnisponhagseieiomergtltnadgdaee(,11111111111111111111111111111sopandetn|rtrae,eicts.eshrra^cepeeeefoiosgpec|mhaoiataesluncsmeireerpsgoraoe43234432222324334423334244324ltai,dtrsndmaskrrtihs)todtloehfgliryt;onenIg.tltirio]rl^i.dieei(ctrasdrotItnnrirurrhd99258586216161474333094857270masiersupsrEnsingeigttieoeiroepocgtmlronaf+Rtis[lcafaefavcai2eote"reron,dreenl(o,sitg4,ctteneacillntaltiec)dd;lt"e^csyroh;nkrlnoennitie.elitjtydmce]^iisaeitistgEi,raSpritxhttnyurstidwtoTdlsiaotarai)askNee,nntoiccrthnnrmrlnhies[cedeee^doaepinvceonnie^srttnrrnrsn(oesmucttumeoogtei.gotfelnc)ptatmn1111111111111111111111111111111111tcupvi^idewyvnscieguIauicpLsmsoaeiictltitaIqfia)r^eemo)ikdrgna4334434424234242233422324131123312.,rijtyhrcelsostmi,iws,pm[mpytrcufeeginctTmhnhrhirmind.omt5634917723502996041774885680196328eienm(otnnttrassaieaomeitavoreseoscnatshont.lrootersusi]fnasisem]eok^o=en)warnniarw+etsanixgdrAeuceemnieraoromustusvmjpgddjitieohiag.otfpcem)utaitu.antuvtcuisripterdidsiaaaal^bmndtirhaeo.i)se^eerTt(tttiidtFUptasrsoeadyarnmtierhio^awzxg.esw(ircuppiIctieIiu4actagosaiIFtLpmqslaalraipniic,o.emihetini,tinallpen,lhtedndycdbf(3aivfnetthrthhmitmhnhhceim;BeceeawnnnhenatcttnuaetemnmuitroayccnnnaodhnoossspnsrydsusoiInfoeaotugPcliikogroercirorl+pnapnisiAeetrefip]eiarei.chevaitieeaai=sggeeanCn+wittwiugksaitelndisnagdcalwstbndrseounoaptoecgoiDt|mltemlcpastEresieahulatsangdj^eeiesotn(ncssgaleicatoimelasenaud.2ycons3prifiitumrrmheoerrtctlsga=aiiordbnaotrsmeuahennlyddIrittgcetc.s)elgdareegcisrntictIelooaetnnigtirmatxaCnNtwergkgrnueutti,raeesdeuiageDhucrIwrttesemlrendtmlaairei,(nuygRlhenemnainaignls+oigioitieiarceyllithp=aiteraht+mtlivnehletr+rfQsanairraBtoiiettimxseaaeAsggndtRtacsluhtceei(gmntaanpmdi,dlasrcynioeinos+lo-i.aenpt;atiehiadPtotmnNastlhroeaQmwoaihiieonciotenirhpespieocliegpnestare.ianipdhtitotmsaitcteanrdiao2ageeocoIeaieiairdilsvke,ynogs,cnrgnllne3tstacisenocaglaai^=snaceaohct^otsale1encehaeoaNtstsn^todntrletngrrletreLgrgsr,tkereNeLcmnrad:rtk-tesNefooncedido2misaolreleattaopatisn.ntihrilClnuibnnnetrhtisyhtarexebcailt(a.irittmotu)ime,gupaacanin,ceti^,oahiaai{elsttotcrtnnonsnnmxotleppIhsm[lnitititcoe(tpIp+eenegygbthrRws.fbslmiaem,deotescy,oiooio{wtranpa,u)ccoEboi[ts,^Rfnson.etlcrtrpiancTwsetbhatpi=]2[efAenrdiisiteaomaRowah(gsrhtIp+ymiwosraprturntgflrxaeadp]rioeai,eiotostmyaceetduit]dstaeioa,i.ene:uosaraeeaat,)lanircoepiceseiiael[pf^ef^sgotMoeti,Naap|mehhtaenTownmnahoSi.us[NtTt.fn,nrdeypagpnaiRlngyrkrt+ret:dotrextmml.tinisirp)oiejorrariIsrr-toesnilersteil,yosanocespa]ofblaioettor^ynanienrIrrtaeeohiymvutnetuz]iistCoyeniooahsdeieli-nrndonIhIlnraTmieishtslasofrnag)di^ncp,epua.lerenplssathgfsoonMpneeefaoaytss,on,otcstetttt.cIonhsafisuctf,nnosmltosNnIttorlrln.,,tefe2eelniyef+mparelgsc+ber2garl(rinaweewrereen4torhdht2rriyre[eIpiiridrlrruaa(eujiooadhmme)hutte-drrietus-ed^j,eetcytadtenrspltnRaaennkaeaotroactncpnnennsul^eagdkiddipmruowitvaere1tetoohiriEgiEtpileapeanre]lfgHxbgz,nimsCtdiee+tolebhatireenteead-soaEhr+ioriieisocytgtshn^aaTetekiohsrdaons,trolla-fisreneno,)ednreu.ead.eoldetaaee,nvnttlv.nuptpaeiietortnnaasyrlliwiosa-isnarearweustatsatfvNctcItiintfsioogeiotug,e.Nepefre..gtc|mem,y[nphcncrtltllraetholiutceideesaieooeni1nf+aewgisratiae^talfeofuraum,tlethoinweonirgcuegrrnnkdeyrarlrrltdhtroa2ail.pristtItegneioni=iidiywplaibomrydtoclmrfetitnitntt;Tisrro.itacm^mi)-ntatihi(tronttnlrao.nptIlanhmirenaiclaedearoansoIsvnoantRi.giaccnbohemtraasanacn+tninnygttieconcoios,siwrroorcmttordeat^=hc1eeioo}hoooroucrgtnrcraEeoaeeafIsourfliceegcCvTDntealadomeoltfer2ceertdugrhcenmj{tdseerae:uarhact)ainupmrshndeiosee^etniiuisnuceayoeettelennptn^Sie,ietenriaefiinrgkdr)moi+grithpsdeoa-enntie"^,eertna(1nnreytrht1oodrioenmkressreprnvoinogtennuursxstnenvprlrd[mrtroni)neitidtrrinyrsereEraoptdpiedxsipdmitlita^si,t,eotdrtstCa;lpft,oencfesnntawtjaisoeetoiassaetoetl).laginncsmyoadnirssea.n.^hrdttp,ninxaphhulesnaretedncuehroodnaeea.de.ts[iearur(iuhwws,vcgiprraes.ertwior^araui122233212223111121111322lo,giffitddotutnmwsptncetorhietmpgenetapo=iaptd|a(tuttootgdcnin21112311132311212322222tmte.ie,ifnl]sctH.hrro^metepet.ier+risni1901872364556473281090329ceaai}mois=erelamiiyevshismpsaierrseamidr^raofprtnsotgmaesaieoonsrnantti.shcettaotaaioshh72114239370604935882156evie)egmBIgr.sslryiindorn-ernt(lesowaoe.nnpTtrervntmeverwprpuoErealmn[eopsttfehgotritde.ieRTntuorsartoteecctdoonsmonl"inereer(ngog,tieoct,etopondttaigcrireoctfto+advoatdlurtmiferaaihse;ilneltiadilt}tlo;]h^iiiog.irierEgieraaanrxathcoturradiei^wIoTslcehea;ressareeitnNensrvTDataico+sruotestmlyoennmt,sgii,[,ewroicgrceredmeaen^vce^catineosi^rrrtuTttmnmjnd)ooirpCpsIee1dsSa.n(oesri,ctneuhn^aasndeldn^oasmci^rinnhezin{tkcd,rnrcaeuncwicldtnt=ee,u^nnnor.xjtylciueeiTmstri^rsdhsndSse1arsIppiCefii,keetenn(ghiy^tecpeswuesfrtttt(dvtnrxa.tnceiittlyiotrra1odtesn1eurnuimemroteotctptztarien)jrsait+pyioeyeoaoniitniingneittexttci.esiittoetauxsanrveuem+et)fs[recoemsunxud.scoap.a,ro(ietodsig.ostfc)turuiihnn+oonheou;vitscsdaaowxrm[riabpssefndegeaertmui)hh^o.+eeilioac.ctodsimgttdsMemrn,fiohrpnvwgshorumroaptir+,fmetd2i.cd,sms.piltoetaedtusscisoifnni]oe=dahtltlceetmisnnnmcffnmabacwnmrriasammn^rsrdtooaein,aih(}monelxmondrirretnmafsteoe]esnaergptemrgioDc=ennis..rwpnie|shemtiauaewaoeauBsco.mregmnspduceasaFgdjre^raaeomcees,hiaeartsIasu..enwirdsior+rmmctTmopearar,tstsllirdeenypsatadtnoamstdoonpxen.eersgepaioetma=aertcsfvtudlrivtri,uireItrdnaehsorl(aib)a,tenheehbdenalfipeegiiirlh,ihrihtianhlctttno.tvInkauahrettty+ntBrtepeesaerearttcru.ttes(rpldupatectonasayFltetmple,nsgfia1crPor^eatrlnorpcrio=prthpch.unernaie.tnvesorgiiniditrgndstiotmateadh[erensIipansicToaciraoprslhetli+ownngtslt,soteetohaqlnntaifacytaaihepatuntsoteseensrIgel.eltsieeeefimdnthasl^dcoiirt2rtge-e(rnte1sofnaotcnealrbcorsttmertiaheeeurrocetnr;predtaeig^eEdegittinsnieltotetdaoneeo=nrhtt2goeeteuneeiulotgrbihcrduwssei.Rgituhs+ibeqeeoeeiacdr:ynsreautpElfoSoi^etre"ottnerlwiernetsreuinitcnno,,weerhe=,yit,ttho-roaeuer+ed2erirge.nmttkstm,tiadadnsnlidrcppeueIsioitcdo)lttasaen,lefiesi^renoiwsmotaeteFeht;n;=ceeeellneltm^nnrtldgpnaiaqi{ltdeudeynnctmpigo]^iriibssr)otlemeaiosnroseganrria.rer+yxtriaohRrixuoaretcehaeimewioxT,sail[bnnn.hpatrtdlnecrn,yaundncro(rsipbolNereng.ncttasoicyrtnadtolrcdippeuseefo1neae,tmeoesltaistseesattnv^suin[ln,amcaInga)iwnpralthkmaeeaab,vc^cbere2(eea^lrgbiitlebakoadueptotrinhaiatniuesmhone(mtolevssmc+tptuotesegpnprrtcel.snNhiasltoadetpcxctp^ovmotyi)l[nretnl.otusatotnspaludxsbgtloo,ithnewrsucatg[nTc2iilcerrbhtmetnintsoisRnnt.bgslcnei3atisoapaus3ldcuio^.ejtatynceecmeid,dagaiwwsiaiegerot+-iuw^aa)wsf=ii,ir,ln2a-kaesoLoeoauar)nebuycdcedcreifuujrxitodhpetudegr|meehod,enituoeiseoottatraircersdstcxhf2ltetteur:iaete+paahoenktrtrts^noeowtlintcsstegur(to]htipsnfr=,ltblrtypsinoreanaSenosrngaaratiotieo.3oolstIrnir+Twrh3ddificInnupninhimsasttyniodttallutwliwieroceu;+raesux..c{ieies.i-knenomao.utiphdorcsipctcpo.penotdarfiuitoiagtonoCtfilexlac)rtsl,rft2ehnnediubvbfoaareerntdet(vpstvbiosotiexntoeeest2^int,sohitliernetneh,)aafuoodtnwn)woeyn^lm"^e]hstiystrcornanhnltht.nnkrht^drdlsnrnsgioseOhhrdimrwsglsaiieeretoliEstlpphotm[fiprmefipthinr,dht^slhtoipce[ltaaicetsotcac)raduleu(sauhahlrifanitwnurtIachdeanpcstrmecpcnoteosmtertnantrcnaoiea=acdoeaoanstlEeppimeanltoemauvrtseigwsrs-iwottrrorcvpiauslooLnn]etHo}coogoote.orfiiAee+urmanatytopCserisst]elietvc)ertpofnmnieantsuctf=rdde+ryristeytktttstiereoseoeehonweeoilietrnsegflrelernistsrtctnelwlssovtaae[cttarmowspellmopdtinuueroafe{oi-nniiengidTijrweusleaormo)tvewirhee;(aioiiuiocdeaitule..aoe;[voeatrotrhloomactoihirnt,,,t,h,tea,eeeoeordomivtlaoldtint^oetprt+ortcanaronntoorrctctseessaawhdornt^gaasecmnugeiaramntrC,talitrmjtIdsorlgnlhauearIlIpemiatntclr;leotieehtilnlaihee^grohntor,lrmenrhetnfntemirhaleaosreesla(iIeasttil(ahetiatw+lyfortenhCk=ttmopcdttRRevanl{rzyie-tiiwroarerrrBniteotrcienrtieiunwtc,y}eucetrtemrll(tdo(adidteoocnlh;cnlawa)reoponnsreoha1naseae+{f-mPshooyprktcuteordceeisteotppnulmm,pcieitssurcpmcoeaulrhson^teefiuaiialllmcta)adiaaINantlhtipemelnratIna;ns1odailgatNlrp,,,,t|,atniirdnaestmrmtcsonteaai^eesaa)ocItsnc^poyciteeoiafhatsiotshonncwsxnsofmitkeltehtRsgniaatcopsya-iesemysnmetrs)bomruohiaa2ngiwrtertoymorxiobtttsnrcr.bj-ncttedgrtuottcohenrteed.+trlf-=tcgopeninvoiegorihgltiegpmaeitirerd+nmcuasltit}dttafntaaityetr=rieiiotdttehtldeceingmdue)gbnpahcwMlw,spemeiae:trios;atanne}meNoscoondhemaldnouenakidtEcoeariceropeteteaionnaharirietssSnittptnenoheftiohwBrchecys+s.ei,Tk,a,nnnegcetidnteerrfahnceeuerpa^emSdteyinenrsdepsaasrrctiidnrIocnonelertd-nemeieonsisatneptferiphodeupdyrnrreanhseadeIlmaiiiastaneomaier)ngpnk^pa,}daBleandeca-oeamaifsotrsnnim[lat)toseenaegtIsIrop.eCt.uoMtwetmotyymis,smsnsonrr^oag-,tCCecrotsnd.o-llaeenlnmma.kt]tdec2eemsdnthahm^)wepsvlrrnaanncnor[exfojdrwiuerultc(iuchoss.eg,n(mth.ubeatuatj,sifdactooetfoclpyoeiisoaaeenttttpa...o,etono.ssuooanTttelficnocrynatetktedoianx.lmi.adssnipo]RincH.arberiame(inlohi,+eenndneatbfueairevteuepsirfeEa.cLhiaaiotnh+euothpyaBdsnsecaurrn-frltsetrnnpsunlc.oenupelrfipaairghogrhversolaaaspendopmd2cTcgntmvtmeelsac-nootose-ntaptatElnlixbefgt,sveersetdmeslaeutbhpitegvoitAl^mNnrec[nrieftotoergsumeturtdii.eott.edoee(blnactealaeiterwhnaincenyuyietvoor,tuonuiaegcmNiatiRredirsl,dioeaauatlnifinaniasdrmietkausary+recosttnodfeRlaarvraiotiep;etaan,lo.iiIogro|mohsnee(birtltgvutercratlgarriilogtkerstali,siasaiNsracsysalnp+pltotntpn;seooi,yncrk,llrwrtIoieaoncirdlisntncgaepooot1nitaecnscmtotttueimo[liebv,nry,siennnae(bcraltlwmstaaareatuhplniciCohl{iItgeutaeirNucedtreiInfifacer^iaoiMehtomiasunenstonuounlhrlnitcprbydeRdbnfiscwhaignyhsediiot)vtrtcnceito2imNicocehttcanis=csoermoaf1oahein^oeelrCruiurtetlnn=l,hfrts2eotoce)enddueh,ge)e:taertatlnnyoioeerlseehcteiecynnnx)etintnnttrtaSaeetpoiler,+ea,enen,sn{esnaN)ieardtnasaerin)c"i^aeesyiniyxinrexhrttakrb0erairendndnedg(rutohbioIomemnsiiad.nms.t^,vtaueeraiefpsotivnn(bcdtEsncptpwti.naplfosmddsls-^csrr,tildttitenutlo2reeaoi=tteoafitr[)aeszirnium(uhyautrsuoatojt,cthrntelnnyudlsdrocn}mpenirenwunmchtesyetamsnet1121232111312212322322dioii1mcaiensh]eHrrpaatieisinep+emgetoeesiaIEeuiichegr-iwnwBntotsidseresg,nuc.rlohtaaevTreetehoogogtrlltehc9623165075784332821094.to-fnyn)wwsfttmcvpttt'snt+ueiettsco[at112132111123212312123222oinc,thtofd,otes^vaeseoislnaufdtfcdtareronihetbgcerdop,lwaac9suisinIetarithswigsy;rrorlitiaoten;a.io.iluae395626107504174382821093879.tittrufaneefiwtearophasdtelmproayn+gefetetinitcetoilT,rtiwrornroRtrdilrurtceoeogiotmiryowuenoomer,(steai,toeem^ooggrainozerah{yvmoreru.ccethteueeninhyodnllhp^mnhtrnnzfindgcorrettiersiatwIoualntoasd1odad^tirotrnnar(dtaqltnIorsceatr)r(astnyeesritefihtnxigit^^htnt,}hpcNtmniTeeestnnuaeraa1CisIeSdpsirtttaeimnjcdy)hpareonxnheawisssttneyhkttsmd'esnueehiioeesm.mtilnthrinccsvoeniisfetdsItutuutolim=s.ndmttitbhr+hnmeboueeep^^ale(wnm;dsnnersIT.ptmct(e}memhideIsSa1paCsiTt,,c,,o,lyouru...iteroaerrptTcmeiuoneeeeiehzefoh,deBeroatrtds+rd2oeeitemTeygiCgn.tictipatebnthReflesaetoonrdrottoe^doetam(isdceetrthhifgort,.clo;mnurcrcrTawhsobruuiieeoaiaTredgtrmhirhancVarlod,msmmfeiapn+orrtuunteermpycgixmc,esai.uam^onontiruamusa(fo.n1rdaitqddrsltznmcceoiroatfchrpawialtinmitd(eantebrmh,NteomsiiereruteaefvhanxtVt)ninttIgrmftiecspab|etnmteinhiiis.meeiim1lutknoehrosctytsgt^t11teuetmoteis.tlhshs+eooIntmuccpdnoiTpaesfpdaiernmesebcnmeep,ecenotennnhtroasgtgd2naio.mohac-t,ossi-taterh[sifaavhmsi)inriohnplmner1egtpttegteree)pau1orpimashatnrrfreaInuscratcearebn.helreaacrtttrrdfottaepaalraiooesceatnurlthuirgtreotewctvdenoninrtMg.Iirrgtivebiieefpiaetorilaierrt.krd+tgniutleseipasfesaapaiercn^tonoiqaerleaPnuyentiunnur.deour=r,tlsntntteehi[agiiroaademsoinanrocudnlgmprttoin-teppauant;detlsrnrsoefnsistagaootFihuerlderjonenndeniettnecneq{eieeaesragiyqoojrduerteertSyitiegtbtm^noeinlsxortuyrenrguer=,tgtvtonieretbsgatsteni-ndin(bkoenm,nnnnestslFtsecnlteoor.e,eq{)naeedelecridsppeoojihsetcieneeeti,osxhnp^ltirdcm^abeynnaepsnddu(sbortnbnespainaarriie,ursehdnoime)mnnpeammltedias^iirtrocsptusctoiitcutlphemnmlcausadxrt.iloswya(oiwtuoottcdlrdcippIeeusuelusoamearion^taiwucsetgmniwntmcon)nctniact.kirtia,eittssoeianfpageoa-iwgaoeor.-iowgaLiwaotiucseomeLmlllesIcetegsoutRetbodaterrfeg.tiPnheanh+outtsxtomottutilnrnnibistetsteifurootaarcstuoorudt+etertsepttoion.ttltlttwi;tiaaRssnts.eosllereoeematiatgsaipmwtcnhoctmi^eo3)wbehrnnttoateotodtcniieuotcienbrmdcelwxtoiwdlhg;rewrieoretittisvmyoniss.-rks+tsosrucgsvdoeeihuNnerhet^thuesactptptchrswacteliaeroIcaeparlttlno=tlesnrlxitonaoauo2nattirnfrieteahee3oc.wslno}lwtn)TdcheiuiisyhNeirtdidy)mdpoennlst,wioyoyktayeeixeen.ieipr-kao.elsssitrssphir.dfisseiuhnnirtmih^nonlhrlc)c,alcmcn.c;sbfalapyerencIts,,,,ae,xhp,ci,erGencmecap2ttriprthao=ohotnrosnwulncyhsiesoarsenrgtnvpolwinr}rhlgaoyfirpicrrfipatu-cor)h)elpovnncerdaysyayktshyetneioeaease'dpcltepeetntlssrtsktctaossbtnrosiortEeealiiuolr.etelvphsR)t,llcecheoaas;pooriAaclorktratey,sc,,rpm,,n,vvtnrdbemiesunSardchrtiaietlsemleicoeeemsustogtnhcsaCenyp)trnaslisnuseaciheutaeycweanoelroifsutiaralsaUceowetaiicuemevtaofpudtyiemcrlcaI)r+riisro.ittniaatldmCaatosyooaalrnh,olalrnsmsaotcoerreetnitononlrrtltfcagqmtc.eotsfh,nnpytseltiueehrGopsdtm(aibIoicoalr4eaclflgsetiietihrcgartmodo,yscrnrellhtaonoteoclgdaRwaroaslctk1naeurRsroty-nicoeohon,aeraaueam'emI,lrskallrRstabaahnellthtlnufeoyiirepogermounnemRamoltmchiewsacfip^knsiRmtsrklyns-ncsidoynlehNvnnpemictcrSyaaeaNmlbuawiiusaavyceaioatnncf-npocpurnnefchottEenen=.memacoaayhataruiusUtahdtrnarircpemeotysceesadu-n+seNno,newee:tapisatoaNnceanenqeltdpmnpatafeethnrnoardxssieaCgSttra+roeretmunwaqrtdiiriesfrrtnctrc,ndec-(nlnebeseodsn(eimnhartsuo4nofleeriralnIeiaiaciccredrsn.irrdtoImeio)et1sdnttN^oRtapurrwafeseoaonioisadoctnomeocdt.tndtoaalnesdsmserl,e,hhaelaadsriRa^veenhlo2ucctmns.epndBcheuce[t-...dobrufd(uhenltieeutnsj,scamrItlnsocroctnaei-odNnppceEn-netbee-aehspeet-dicapewdisanl]nH=efouaPai+epBlerrteec.ei-siEEeneairiiaen:aooyttwtaseh,oannutc.pfia-olnibtaaeru-tnyvnnbteunrseEgoeessrlsngoep+tmvdn-wnoSniuirfoNvuraqupetMacea[isrthosaontttnyoeitiehCot,aoilloyoltsefipencrcrdttonmiggcsnlonss++rdoircac^ioiserseeein(sirysviigrtaauelNowtr;o.idsaas2uraetdnatribnts)omerodwhpsininocNtsrt21raipen+cmNicrttsmveeienin,awadronrdtcttsto,rtieounetigPv,tnle0erlaieoeseuafitl33222322223222ehy{behpuca,cihehmwhainoesdteorteureixno+tmnNliliptstd(snfiIginrei1temitnidr20690311378425pcEavndanetbe1one(boi.cotprsr-tkwecntschclaenhstrueP)aa,onyefiert.iisiexplitutenerop8eescnwawinaaahnhhettatmctintwiioecybveiusoyoaxmeind(blerrEEewnidatunhpslotcsr.ttSeuhervsdfion)irlohceiioynrfMxdu.stcsstntdee,co=ei,hishhttetvcnnmlyineasteaeii.dsmsroee}+tmoermgrnxnisoforaeairr.pthteiunrseiliohadlhtcipheywn^^thBdstdTlsru2ilaepae1IsiiasCdSdttaia,ef,TyreeseasooopDnmzciacepiyrtE{hferatpovtndytoid[c^rnahnarntoeaset.t.o,rnunstteaii,cdtuvpetn0rhsrI1o.cckrdleTruaifiirlrlnuuiiroi1,eoTryeannxarthahms,z,heyseeiio+owesrtug'sntenyydma,sn133111211212231122232122nidinicrmye^fosnoir1sumiaesd.oashitbaiiIplwccutsr.crbtI9znekcorrnuziorrc221179025856348693370041724591638,whreltacuaVtenmfoftrsmtmwestacoypc'sne.fgnohctfnp8intcsRmie=1treetnaaiicumcetatteicornhbi,nmipeolwaresia,idsIEhoenioaluteuur}tmoitster.sfnuhr+ofueosci)endaer(y1pgrqxcte1tbRsuutcdgreirheeestIpitlhhrNeoattevorpad2remcaatp3r..osiisahoedssttloigmxm^^orrhrctfesrTviliCISisdsaaep1im(untrocttiuheea(asgtrrqdeothretertnmrnnnaitatrnttanthehfrpcrNa1etroeueoonhaattDbaadrm,ardmuehn.tslcsrIehtTepaaiuoui^soomtT[chrhacerebsh,etetveenr+odootnueagIrmcidcthibgtseefcnqonnnt,teilti]ttyksehtrcnohrlrh1etArnsblenpurtpsvhr-ecVtoseiotmfesecitsnetutmunn.toa.mebsthhtirehe[sir1oaoetipneseaainrncmcontrhihpgttysiimcduucpaunioee,bttsne9errrbz+lerrrfoiaacioindmuceot1riygrrttrmnde1tuEaruo.nhnaIinatdlfeneaepearreognsfeppareosneiaaraietom^oieeictl,naieu,anarivr=,nnitaroooeenecgaiette)nmunavoitasb212132211312223211321211nysngrerntrntouscsttcFinalnerncnneaq{dmeihtnnbumcooojntnewtt908127034563154673218902693485271ine)tpii.lxsdt,onifsicibgawaes.a3e.undiqi(nbolyneohneterstedeoed)gre,Phiasuetrrs-duescnnnpsltscatto^dueasureetuetttiieerutwtstnmte.addlurnrbisf1ntottreaitee.o3ebdl,iuosamli.iirwuserrcetavremminrgspaaiPoournigtu.cwwA(ier-dntlridcuppIeFpessutae^^st-ostdenaToudCsiSs1eiadapItgroa-iwleotoLotoatteceaslosntesaehiemi].h2IttirtiooRlutmhnttedteihwflxtsehpil+ni=.rcreretrsT3vhutrossugrosmrusuiotmosTsytseucotsgnteFe,inerct+itohaRstsmecgfionotenmhceagaiwfloolieno)osltswoe;bhedeeneeacyihnbcounoee.etCvriheitrsVormrmcfobeNrtheel{eemcrpio.otuireretsptot1nmloomtsnrtootisnamcreronrenewi3c.omole,tcdnrcueinNmcscattceriidsaowioonrdsIiip-kroie1edidiueeedthapmssnt1hsr^iiehhicIsctspmspa^arrrlseaerexpInfso2rstaaurteie/httihmrtch=wnmegehsisnltovvaisboayoroenrmeaocsu,oseanrbi.nhen}afietntanweideyto)cihl.acmn)cponnnaansdupcoyptlyktyeetCaqcteeeespmtras)loldisinteessnauncrtstevpnemqc,eqynoglfrdarle(so)rcyrsstt-idcsnn;rcdfasssmoda.th+iRwas,,,,et,l,pieeetssteetucerGttlliopduouisbfthcetscrohlrun)ncegeiiaiieotoeshracoowter..igww(ammrvrtdlacirdteuspepsFupIceavicerscraign.aieaoeneeacmamaCae.'osolhpemrezkIeattbcrGcolReetehonoxlreoratiiltfnomeecunsuosrsRao.solttct.goenhttopdtRsnikihekrgaiewgyyai-irectnrhhsscgraherbtSdoeosvhnhnloeeyteveaoFgriusrar'cletIeNrernpnoktcbennhnflepruseeminoeoeIPte3aylaemRdckuploNc}imelleridstUehwisiriipa-krmtennoekdyaieoauytrniniemwsfSkxsaaRerslxyoo-sin2triedmwrohsussch.rwaniaehisensamosnosanpurfhenrfietf-oaypqao)lots.fctiadmasdUntam,ct,rdpcsa,p(tbaaaqceasyohed4nrfleftipremvpte,sqsltt-naeeNnionerdmtaorttccnaardessRnpsll,hiRfsulihreet.ssteuontoiatpqtisRaraoesfehemutuacetc,ros(rhrrRbacoadwei-nemuvotesnrn4ieeflorecpiandeunieedrlraCehsonoeltednanticeeemdraRlsctnotlcraufrhNedpoLnwcotioieodtanocaaiendewlnreidsi,etohhraaRcarstonosnnhroenocueoEnrcdcemts.ltuceedthntecdua.ameeeItnlacmpnsrlellseocctelneepdeNirrmscntnnuenawoepihaieidNwafeedkwRy-ietewpcarBlinrecac-ersemia-rtEnanoCuoen.atofI-dopetotnrRdampap-imcta-natocnuceueshdeiep(vepemesnslaafonNwuinrrtiaroNcaaaearmnnpliptfitoth,oy)raaotn,droNrltCemfiunarcaeteroorlslaasdor-rndeaoiisrntentoiprneirrcvecrrllareaeean(smdaieassauitotrnhnIsadpwoccahpetiriinonioocdtopendc)lcnstpdslmmcnonthaNsssnonIsrcrceecemts.htaidgspctEdmrtbettoa.ett-acclocbllleeandiwhPaineptiortherid.ameNidschwheennpBlobreedco-sirmNwiilariooehnsndeauIdseoeIiolbenusrpea-s-anEpctErnueesba,tepvsSe-sfocpusneelnxaneamaPniMoinyre.sai,tnmtni,eocslivtifihnernt(blsllyswoswraapshicatnuiroivboreeueaedsytnfirsoaitEoasueviurpngetss(oieeptScisohhdptccssdtmmsien2taieusasMmeeesreessnhrtigntirsicheruhtlryrcvlsobeielhcsiynwhdaaiannhtortotitmNiisitia,e,neoseriven0nrsicriiaendeoiufielwnfeiolsstdeysrah2iaerhtnoamra,ra.fisopueserpinencdtnxapctddilrnrevirltimn,oiec1eosrveaaniden(btawzuaypsecri.ttukvn0enyrddscfithnroeniitryaeuefilospacooeesycashhhdutohmiIieccdirosme8enreneiss,rntlaasdilhrrleoc1lhcwssotcyanidradi'sdnlaEtiiniac,m,es.ltrikrnucinaytcdeihMenfbe)tplw-sr,ossIirtoxaso.fiipuseinuecdtopttt8tfItufdctlrvirleaaoeyoaretga.ctwsbazRyrisooeaitdixnydEienhninoifrwyeeswiolsactr'nrsahhaoiIlccrh)nnrnriterrso,xuuta,soabe(ootoawqspDtcrms'osnlattivtrinicmnaettthp.Nunsteeuceabrapflw-s,hsIxeivsoftiiinustnemttthcrfhtcutiifhaseeygrnthuecrtnbmriRdersneetttaoirnienn,horsstohwoothcsDeriomrhheilhrnrrpet,eritocuter,ahieria(utaqeobhtseatriiriroqfetetahtNcteteeuethirgtzarecnohriavrscnthrminsenmdehrhciwsamshNeaoouseoiocmdifntpentsunnne,untmaaithcircirwtoieifpiarne,turiaeosaieinatebhanneenrrrozfeeanrtmcrrithsaigntcaeroAcournciotnygtrrthdmmwanontbiioeocmfenipenntepcsulunmneaieiibwcti,oetoafpia.rnmeraamoieoianzanttoigtnnce)aiensiraisrrmuscucnsotnygrotetthbctmannunrieerumerncctcwtbnnmo.einrtuts,oaifpnlamaiee.bd,mimwnlucistoe.pe)iinriaasooPdroretuscutpoginta-ohstuenosdrurcwtcnttteto.dessiflaaieet.iiierluotstyolletherPdnsreluprerernttnhamovr-ohbesd,odyerumgsecinertttdsaostlaomeaoiitles,looedentseerncrerertmoysnhyuiorutegmnerttssa)rnttbiiiraeleserrsoolntoomushdeeiclmlc.hirtrcvmrrenmcaairosoeiocirotIswntmentimee.glc.rscinfenmcaieopmoootonieIiekelaayueecguersimtssrtoaPpbTenecearaueonrSbibmstaoaodmTennnrrbitnnaandsn.onlhstmspmaunansneolnsrttspmtumceeemngotiemcsuryergosstrssyssdsauetwaaeoeertwaetocdlcblsausufsalfccllo))niiiieeedhtmiriarlqarnrlgqmirognetecaihemont.oetcah.epmr.uetrGcsavpetmoeerorGccdsorogocscdeuncfgsgaioracereauvtfytgaairce'rark.vtbticnyaoneiio'ePeRrlkexbhochunsne.oekiavaoyeePsFienoaRSlaeexnhlnolnusaksnraynaennSrpaayodltataltiuassUnsaa.ocnr

dNtdoospneoct ipfryedi4ct

the

rewards, sinib2cnuIetRtriiNtwnissedinocsotnetiolrultrsopuerrfseusedlaiiacntstdfrtohefpreoosrrreectwelefdas-rsitdnuhsp[e,]se.iirmna4vnciaedsgeitindnisRarnteSioc,otorneunwspseaotfrrulduilsccayotsifor^denpit.foofTertgreheedninstienamrc[at]ait.okeanenmssdotihnreeRaaScc,ctriueowrnaatpredosalioccftyidoinffsem.reonrteacrtoiobnusst areIhnaprdratcotiscpee,citfhye imagination policy ^ can be obtained from policy distillationot(+Rk acanière et al., 2017)

to or

k-step

a fixed target network like DQN (Mnih et al., 2015). The former distills the action policy (st; ) into

a smaller rollo4ut The latter uses a

network ^(st; ^), shared but slowly

using a cro4ss-entropy loss, l,^ (st) changing network ^(st; -), where

= -

araep(raev|siot)ulsogp^ar(aam|sett;e^r)s.

in (st; ). By imitating the action policy , the imagined trajecRtLoArgieenst will be similar to agent

experiences in the real environment; tnh-rioslloaulst o helps IAE learn predictive representations of rewarding

states, and in turn should allow the easy learning of the action policy undervt1he sparse reward signals.

Trajectory Manager The TM rolls out the IC over multiple time steps into the future, generating multiple imagined trajectories with the present information. Additionally, various planning strategies are supported for trajectory simulation: breadth-n, depth-m and their combination. For breadth-n imagination, the TM generates n trajectories, T^1, . . . , T^n, over one time step from the current state st, i.e., T^j = {^ij,t}. Empirically, the IAE using breadth-n imagination will motivate the agent to focus on short-term events and predict the next step more accurately (e.g., enhancing the next-click prediction performance even when we do not formalize the click event as reward). For depth-m imagination, the TM generates only one trajectory T^1 through m time steps, i.e., T^1 = {^i1,t, . . . ,^i1,t+m-1}. This enables the agent to learn to plan the long-term future, and thus recommend items that yield high rewards (purchases). Finally, we can also achieve the trade-off between breadth-n and depth-m to balance the long-term rewards and short-term rewards. Specifically, we generate n trajectories, and each has a depth m, i.e., {T^ } = {{^i1,t, . . . ,^i1,t+m-1}, . . . , {^in,t, . . . ,^in,t+m-1}}.

Imagination-augmented Executor As mentioned before, the IAE uses external rewarding data and internal imagined trajectories to update its action policy . For the j-th trajectory, T^j = {^ij,t, . . . ,^ij,t+m-1}, we define a multi-step reconstruction objective using the mean squared error:

m
Lj =  ||AE((T^j, )) - (T^j, )||2,

(4)

 =1

where Tj, is the  -th imagined item, (·) is the input encoder shared by  (for joint feature learning), AE is the autoencoder that reconstructs the input feature, and the discounting factor  is used to mimic Bellman type operations. In practice, we found that action representation learning (i.e., the

5

Under review as a conference paper at ICLR 2019

output weights of ) is crucial to the final performance due to the large size of candidate items.

Therefore, we use the one-hot transformation as (·) and replace AE with the policy  (excluding

the final softmax function), and only back-propagate errors of non-zero entries. Besides, the overall

imagination reconstruction loss is defined as: LIRN =

n j=1

Lj

.

There are several advantages associated with the imagination reconstruction. First, imagined trajectories provide auxiliary signals for reward augmentation. This speeds up policy learning when extrinsic reward is delayed and sparse. Second, by using a shared policy network, IAE enables exploration and exploitation, and thus improves feature learning when the number of actions is large. Third, compared with agents that predict the next observations for robust learning (Mirowski et al., 2016), our IAE reconstructs the imagined trajectories generated by the TM over time for predictive learning. This self-supervised reconstruction approach can achieve excellent click and purchase prediction performance even without any external reward (unsupervised learning in this case, where inputs and output targets used for training  are all counterfactual predictions, and the input states are transformed through actions in order to match predictions, i.e., predictive perception in Seth (2014)).

5 EXPERIMENTS
5.1 EXPERIMENTAL SETUP We evaluate the proposed model on the dataset of ACM RecSys 2015 Challenge2, which contains click-streams that sometimes end with purchase events. The purchase reward and the click reward (if used) are empirically set as 5 and 1, respectively. Focusing on the most recent events has shown to be effective (Jannach & Ludewig, 2017); therefore we collect the latest one month of data and keep sessions that contain purchases. We follow the preprocessing steps in Hidasi et al. (2016) and use the sessions of the last three day for testing (we also trained IRN and baselines on the full six month training set, with slightly poorer results; the relative improvements remained similar). The training set contains 72274 sessions of 683530 events, and the test set contains 7223 sessions of 63100 events, and the number of items is 9167. We also derive a separate validation set from the training set, with sessions of the last day in the training set. The evaluation is done by incrementally adding the previous observed event to the session and checking the rank of the next event. We adopt Recall and Mean Reciprocal Rank (MRR) for top-K evaluations, and take the averaged scores over all events in the test set. We repeat this procedure 5 times and report the average performance. Without special mention, we set K to 5 for both metrics. Besides, we build an environment using session-parallel mini-batches, where the agent interacts with multiple sessions simultaneously (see section 3).
Baselines We choose various baseline agents for comparison, including: (1) BPR (Rendle et al., 2009), a pairwise ranking approach, widely applied as a benchmark; (2) GRU4Rec (Hidasi et al., 2016), a RNN-based approach for session-based recommendations with a BPR-max loss function (note that original GRU4Rec gives much lower purchase performance, thus we only use the clicked items from the same mini-batch as negative examples); (3) CKNN (Jannach & Ludewig, 2017), a session-based KNN method, which incorporates heuristics to sample similar past sessions as neighbors; (4) A3C-F and A3C-P, the base agents without imagination, using the click and purchase reward (-F) or only the purchase reward (-P); (5) IRN-F and IRN-P, the proposed models that augment A3C with imagiantion; (6) PRN-P, an A3C agent that reconstructs the previous observed trajectories (i.e., click/purchase sequences), using the purchase reward.
Architecture We implemented IRN via Tensorflow3, which will be released publicly upon acceptance. We use grid search to tune hyperparameters of IRN and compared baselines on the validation set. Specifically, the input state st is passed through a LSTM with 256 units which takes in the one-hot representation of recent clicked/purchased items. The output of the LSTM layer is fed into two separate fully connected layers with linear projections, to predict the value function and the action. A softmax layer is added on top of the action output to generate the probability of 9167 actions. The discounting value  is 0.99. The imagination policy ^ is obtained from  using the fixed target network, and the weights of ^ are updated after every 500 iterations. Without special mentioned, TM employs the combination of breadth-2 and width-2 for internal planning. The imagination
2http://2015.recsyschallenge.com/ 3https://www.tensorflow.org

6

Under review as a conference paper at ICLR 2019

Table 1: Recommendation performance for purchase and click events. Purchase Recall@3 Recall@5 Recall@10 MRR@3 MRR@5 MRR@10

BPR CKNN GRU4Rec

0.471 0.519 0.500

0.679 0.685 0.675

0.820 0.805 0.788

0.372 0.420 0.404

0.434 0.470 0.457

0.458 0.490 0.475

A3C-F A3C-P IRN-F PRN-P IRN-P

0.505 0.512 0.525 0.515 0.537

0.684 0.704 0.734 0.718 0.752

0.813 0.828 0.879 0.867 0.908

0.405 0.411 0.419 0.409 0.427

0.458 0.469 0.482 0.471 0.490

0.479 0.489 0.506 0.492 0.514

Click BPR CKNN GRU4Rec A3C-F A3C-P IRN-F PRN-P IRN-P

Recall@3 0.198 0.206 0.201 0.207 0.197 0.210 0.185 0.212

Recall@5 0.249 0.292 0.297 0.313 0.277 0.310 0.255 0.306

Recall@10 0.276 0.383 0.413 0.437 0.367 0.422 0.328 0.406

MRR@3 0.168 0.167 0.162 0.168 0.160 0.171 0.154 0.173

MRR@5 0.182 0.190 0.192 0.200 0.184 0.198 0.172 0.200

MRR@10 0.187 0.207 0.209 0.218 0.198 0.216 0.184 0.215

reconstruction is performed every one environment step. The action policy is trained with immediate purchase reward (when found) or 3-step returns (when click reward is used). Besides, weights of IRN are initialized using Xavier-initializer (Glorot & Bengio, 2010) and trained via Adam optimizer (Kingma & Ba, 2014) with the learning rate and the batch size set to 0.001 and 128, respectively.
5.2 RESULTS We first evaluate the top-K recommendation performance. The experimental results are summarized in Table 1. From the purchase performance comparison, we get:
· A3C-P has already outperformed classical session-based recommenders (BPR, CKNN and GRU4Rec) on Recall metrics and achieved comparable results on MRR metrics. GRU4Rec gives poor purchase performance, as it focuses on next-click prediction.
· Comparing IRN-P with A3C-P, we can see that the purchase (and click) performance can be significantly improved with imagination reconstruction, demonstrating that IRN-P can guess what you like via internal planning and learn predictive representations.
· IRN-P consistently outperforms IRN-F, and A3C-P also outperforms A3C-F for purchase prediction. This demonstrates that purchase events can better characterize user interest, and the agents may be biased if clicks are used as reward.
· Comparing PRN-P with A3C-P and IRN-P, we found that reconstructing the previous trajectories (i.e., click-streams) also improves the purchase performance (compared to A3CP). This is because that PRN-P can learn better representations for clicked items, and user purchases are sometimes contained in the click-streams. Besides, IRN-P outperforms PRN-P, since imagination reconstruction (without any real trajectories) promotes more robust policy learning and the TM enables the agent to learn predictive representations.
From the click performance comparison, we get: · GRU4Rec achieves excellent next-click performance (e.g., top-5 and top-10) compared to BPR and CKNN, as it models the session data via sequential classification. · A3C-F performs much better than A3C-P and GRU4Rec. This indicates that RL-based recommenders trained on clicks can generate actions that better preserve the sequential property, possibly due to the accumulated click reward (of longer sessions).
7

Under review as a conference paper at ICLR 2019

Recall@5 MRR@5

validation Recall@5
0.8

0.7

0.6

0.5 IRN-P-0.0

IRN-P-0.05

0.4

IRN-P-0.1 IRN-P-1.0

0.3

0.2

0.1

0.0 0 1 2 3 4 5 6 7 8Num9ber1o0f It1e1rs (1x2200130) 14 15 16 17 18 19 20 21
(a) Purchase Recall@5 on the validation set

validation MRR@5
0.5

0.4

0.3

IRN-P-0.0 IRN-P-0.05

IRN-P-0.1

IRN-P-1.0

0.2

0.1

0.0 0 1 2 3 4 5 6 7 8Num9ber1o0f It1e1rs (1x2200130) 14 15 16 17 18 19 20 21
(b) Purchase MRR@5 on the validation set

Figure 3: Possibility of being stuck in an non-optimal policy with varying reward sparsity for IRN.

Table 2: Purchase performance comparison with varying reward density d.

Recall@5

MRR@5

Algorithm d = 0.1 d = 0.05 d = 0.0 d = 0.1 d = 0.05 d = 0.0

A3C-F A3C-P IRN-F IRN-P

0.679 0.687 0.730 0.735

0.676 0.578 0.726 0.725

0.674 ­
0.720 0.653

0.460 0.464 0.480 0.482

0.459 0.398 0.478 0.475

0.459 ­
0.473 0.432

· Somewhat interesting, IRN-P significantly outperforms A3C-P, and gets comparable results like IRN-F and A3C-F. This demonstrates that the IRN-P agent may learn to plan and reconstruct the previous clicked trajectories even when only the purchase reward is provided.
Varying the degree of purchase reward sparsity We now explore the robustness of four RLbased recommenders to different purchase reward density. We randomly sample a d proportion of purchase events from the training set. The click events remain unchanged. As shown in Table 2, A3C-F and IRN-F are robust to different purchase sparsity, since purchases are sometimes contained in the click sequences. IRN using only the click reward for policy learning can also enhance the purchase prediction performance (see d = 0). While the performance of A3C-P degrades with sparser purchase reward, the proposed IRN-P achieves comparable performance; the imagination reconstruction promotes predictive learning of rewarding states. To our surprise, we have found that IRN-P performs well even without any external reward from the environment (i.e., predictive perception, see A3C-F and IRN-P with d = 0). Minimizing the imagination error of predictive trajectories over time enables the agent to learn sequential patterns in an unsupervised fashion. Figure 3 compares the performance of IRN-P on different reward sparsity setting, where one epoch contains nearly 5000 iterations. We can observe that the performance of all models is gradually improved, and IRN-P with a larger d learns faster, indicating better exploration and exploitation. Note that IRN-P with d = 0 will adversely decrease the performance due to the local over-training. In extreme cases, a final purchase decision would be unknown, the imagination reconstruction may be applied without external reward, but we can use the click prediction performance for validation and early stopping. Effectiveness of the trajectory manager We then analyze the effectiveness of different planners of the TM. Table 3 shows the best results obtained with IRN-P when using alternative measurements. Note that the purchase event in one session is usually the last user interaction, and "First" means that the second event is evaluated separately (the first clicked item is used as the initial state). We can observe that, different planners equip the agent with different prediction capacity. For instance, IRN-P with a larger n performs better on First and Click metrics, indicating that the agent with breadth-n planning focuses more on short-term rewards. On the contrary, a larger m can improve the purchase performance at a cost of lower First and Click results, since depth-m planning enables the agent to imagine the longer future. The combination of breadth-n and depth-m can better balance the long-term rewards and short-term rewards. Besides, for IRN-P without any external reward (d = 0.0), the depth-2 planner gives better performance than depth-1 and breadth-2 on three measurements (by 2-5%), possibly due to the more predictive representations learned after unsupervised training.
8

Under review as a conference paper at ICLR 2019

Recall@5 MRR@5 Recall@5 MRR@5

0.75 cold-start recall@5
A3C-P 0.70 A3C-F 0.65 IRN-P 0.60 0.55 0.50 0.45 0.40 0.35 0.30
1234 C
(a) cold-start -- Recall

5

0.500 cold-start MRR@5

A3C-P

0.475

A3C-F

IRN-P

0.450

0.425

0.400

0.375

0.350

0.325

0.300 1234 C
(b) cold-start -- MRR

5

online Recall@5

0.7

A3C-P-tr A3C-F-tr

0.6

IRN-P-tr A3C-P-te

0.5

A3C-F-te IRN-P-te

0.4

0.3

0.2

0.1

0.0 0 5 10 15Num2b0er of2I5ters 3(x0100)35 40 45 50
(c) online -- Recall

online MRR@5

0.5 A3C-P-tr

A3C-F-tr

0.4

IRN-P-tr A3C-P-te

A3C-F-te

0.3 IRN-P-te

0.2

0.1

0.0 0 5 10 15Num2b0er of2I5ters 3(x0100)35 40 45 50
(d) online -- MRR

Figure 4: Purchase performance comparison on Recall@5 and MRR@5 metrics. (a, b) Results under the cold-start scenarios. (c, d) Results in online learning.

Table 3: Performance of IRN-P with different planning strategies (breadth-n and depth-m).

Recall@5

MRR@5

n, m First Click Purchase First Click Purchase

1, 1 0.397 0.307 2, 1 0.413 0.316 1, 2 0.344 0.294 2, 2 0.372 0.306

0.733 0.736 0.755 0.752

0.299 0.308 0.280 0.290

0.199 0.205 0.190 0.200

0.477 0.479 0.488 0.490

Robustness to the cold-start scenario We simulate a cold-start scenario using the test set. Specifically, we use a parameter c to control the number of items in the input state, i.e., new events will not be added to the input state if the number of items exceeds c, but are still used for evaluations. Figure 4 (a,b) shows the purchase performance w.r.t. the cold-start parameter c. We can see that IRN-P outperforms A3C-P and A3C-F over all ranges of c, verifying the effectiveness of imagination reconstruction. In other words, IRN-P can guess what you like (or learn predictive representations) and obtain a better user (or session) profile. Besides, A3C-F achieves slightly better results than A3C-P, which is different from that in Table 1. A3C-F that trained with the click reward can preserve the sequential property of sessions, and thus provide auxiliary (implicit) information under the cold-start setting (in the warm-start setting, the agent using more clicked items as input may be biased and thus focuses on next-click prediction). Adaptation to user interest To demonstrate that IRN can improve data efficiency and promote quick adaptation to user interest, we create a more realistic scenario for online learning. Specifically, the training set is sorted in chronological order, and each event is used only once for training. The test set remains unchanged. Figure 4 (c,d) shows the purchase performance that is evaluated on both the training set ("-tr", averaged over a batch of training purchases) and the test set ("-te", averaged over all test purchases); for a given purchase event in the training set, the model first checks its rank and then uses it for an incremental update (measuring the short-term interest). We can see that, IRN-P promotes quick adaptation to user interest (after 2000 iterations) compared to A3C-P and A3C-F on the two datasets, and IRN-P-te shows improved data efficiency and purchase performance compared to A3C-F-te and A3C-P-te after online learning. Different from IRN-P, A3C-F and A3C-P perform poorly on the test set (compared to that of the last training batch); this highlights the importance of most recent events and also demonstrates that IRN-P can capture user's long-term interest.
6 CONCLUSION
In this paper, we propose the IRN architecture for session-based recommendation, which is inspired by the theories of cognition science. IRN can be regarded as a combination of model-based planning and self-supervised reinforcement learning, which employs a self-supervised reconstruction method for predictive learning, using the imagined trajectories generated by the internal model. We conducted experiments to study the impacts of difference components under different scenarios, verifying the effectiveness of our IRN architecture. We believe this kind of approaches has the potential to make a shift in the way we use recommender systems.
9

Under review as a conference paper at ICLR 2019
REFERENCES
Leemon C Baird III. Advantage updating. Technical report, WRIGHT LAB WRIGHT-PATTERSON AFB OH, 1993.
Christopher Burr and Max Jones. The body as laboratory: Prediction-error minimization, embodiment, and representation. Philosophical Psychology, 29(4):586­600, 2016.
Andy Clark. Whatever next? predictive brains, situated agents, and the future of cognitive science. Behavioral and brain sciences, 36(3):181­204, 2013.
Leon de Bruin and John Michael. Prediction error minimization: Implications for embodied cognition and the extended mind hypothesis. Brain and cognition, 112:58­63, 2017.
Gabriel Dulac-Arnold, Richard Evans, Hado van Hasselt, Peter Sunehag, Timothy Lillicrap, Jonathan Hunt, Timothy Mann, Theophane Weber, Thomas Degris, and Ben Coppin. Deep reinforcement learning in large discrete action spaces. arXiv preprint arXiv:1512.07679, 2015.
Karl Friston. The free-energy principle: a unified brain theory? Nature Reviews Neuroscience, 11(2): 127, 2010.
Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural networks. In AISTATS, 2010.
Carlos A Gomez-Uribe and Neil Hunt. The netflix recommender system: Algorithms, business value, and innovation. TMIS, 6(4):13, 2016.
Balázs Hidasi and Alexandros Karatzoglou. Recurrent neural networks with top-k gains for sessionbased recommendations. arXiv preprint arXiv:1706.03847, 2017.
Balázs Hidasi, Alexandros Karatzoglou, Linas Baltrunas, and Domonkos Tikk. Session-based recommendations with recurrent neural networks. ICLR, 2016.
Jakob Hohwy. The self-evidencing brain. Noûs, 50(2):259­285, 2016. Max Jaderberg, Volodymyr Mnih, Wojciech Marian Czarnecki, Tom Schaul, Joel Z Leibo, David
Silver, and Koray Kavukcuoglu. Reinforcement learning with unsupervised auxiliary tasks. arXiv preprint arXiv:1611.05397, 2016. Dietmar Jannach and Malte Ludewig. When recurrent neural networks meet the neighborhood for session-based recommendation. In RecSys, pp. 306­310. ACM, 2017. Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. William Lotter, Gabriel Kreiman, and David Cox. Deep predictive coding networks for video prediction and unsupervised learning. In ICLR, 2017. Piotr Mirowski, Razvan Pascanu, Fabio Viola, Hubert Soyer, Andrew J Ballard, Andrea Banino, Misha Denil, Ross Goroshin, Laurent Sifre, Koray Kavukcuoglu, et al. Learning to navigate in complex environments. arXiv preprint arXiv:1611.03673, 2016. Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control through deep reinforcement learning. Nature, 518(7540):529, 2015. Volodymyr Mnih, Adrià Puigdomènech Badia, Mehdi Mirza, Alex Graves, Timothy P. Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement learning. In ICML, pp. 1928­1937, 2016. Junhyuk Oh, Xiaoxiao Guo, Honglak Lee, Richard L Lewis, and Satinder Singh. Action-conditional video prediction using deep networks in atari games. In NIPS, pp. 2863­2871, 2015. Razvan Pascanu, Yujia Li, Oriol Vinyals, Nicolas Heess, Lars Buesing, Sebastien Racanière, David Reichert, Théophane Weber, Daan Wierstra, and Peter Battaglia. Learning model-based planning from scratch. arXiv preprint arXiv:1707.06170, 2017.
10

Under review as a conference paper at ICLR 2019 Deepak Pathak, Pulkit Agrawal, Alexei A Efros, and Trevor Darrell. Curiosity-driven exploration by
self-supervised prediction. In ICML, volume 2017, 2017. Sébastien Racanière, Théophane Weber, David Reichert, Lars Buesing, Arthur Guez, Danilo Jimenez
Rezende, Adrià Puigdomènech Badia, Oriol Vinyals, Nicolas Heess, Yujia Li, et al. Imaginationaugmented agents for deep reinforcement learning. In NIPS, pp. 5694­5705, 2017. Steffen Rendle, Christoph Freudenthaler, Zeno Gantner, and Lars Schmidt-Thieme. Bpr: Bayesian personalized ranking from implicit feedback. In UAI, pp. 452­461, 2009. Anil K. Seth. A predictive processing theory of sensorimotor contingencies: Explaining the puzzle of perceptual presence and its absence in synesthesia. Cognitive Neuroscience, 5(2):97­118, 2014. Evan Shelhamer, Parsa Mahmoudieh, Max Argus, and Trevor Darrell. Loss is its own reward: Self-supervision for reinforcement learning. arXiv preprint arXiv:1612.07307, 2016. David Silver, Hado van Hasselt, Matteo Hessel, Tom Schaul, Arthur Guez, Tim Harley, Gabriel Dulac-Arnold, David Reichert, Neil Rabinowitz, Andre Barreto, et al. The predictron: End-to-end learning and planning. pp. 3191­3199, 2017. Peter Sunehag, Richard Evans, Gabriel Dulac-Arnold, Yori Zwols, Daniel Visentin, and Ben Coppin. Deep reinforcement learning with attention for slate markov decision processes with highdimensional states and actions. arXiv preprint arXiv:1512.01124, 2015. Richard S Sutton. Dyna, an integrated architecture for learning, planning, and reacting. ACM SIGART Bulletin, 2(4):160­163, 1991. Richard S Sutton, David A McAllester, Satinder P Singh, and Yishay Mansour. Policy gradient methods for reinforcement learning with function approximation. In NIPS, pp. 1057­1063, 2000.
11


Under review as a conference paper at ICLR 2019

OPTIMIZATION ON MULTIPLE MANIFOLDS
Anonymous authors Paper under double-blind review

ABSTRACT
Optimization on manifold has been widely used in machine learning, to handle optimization problems with constraint. Most previous works focus on the case with a single manifold. However, in practice it is quite common that the optimization problem involves more than one constraints, (each constraint corresponding to one manifold). It is not clear in general how to optimize on multiple manifolds effectively and provably especially when the intersection of multiple manifolds is not a manifold or cannot be easily calculated. We propose a unified algorithm framework to handle the optimization on multiple manifolds. Specifically, we integrate information from multiple manifolds and move along an ensemble direction by viewing the information from each manifold as a drift and adding them together. We prove the convergence properties of the proposed algorithms. We also apply the algorithms into training neural network with batch normalization layers and achieve preferable empirical results.

1 INTRODUCTION

Machine learning problem is often formulated as optimization problem. It is common that the optimization problem comes with multiple constraints due to practical scenarios or human prior knowledge that adding some of them help model achieve a better result. One way to handle these constraints is adding regularization terms to the objective, such as the 1 and 2 regularization. However, it is hard to adjust the hyper-parameters of the regularization terms to guarantee that the original constraints get satisfied.

Another way to deal with the constraints is to optimize on manifolds determined by the constraints. Then the optimization problem becomes unconstrained on the manifold, which could be easy to solve technically. Furthermore, optimization on manifold indicates optimizing on a more compact space, and may bring performance gain when training neural networks, e.g., (Meng et al., 2018; Cho & Lee, 2017).

Most previous works on manifold optimization focus on a single manifold (Zhang & Sra, 2016; Wei et al., 2016). However, in practice, we often face more than one constraints, each of them corresponding to one manifold. If we still solve the optimization problem with multiple constraints by method on manifold, we need to handle it on the intersection of multiple manifolds, which may no longer be a manifold (Pathak et al., 2015). Due to this, traditional optimization methods on manifold does not work in this case.

In this paper, we consider the problem of optimization on multiple manifolds. Specifically, the problem is written as

arg min f (x),

x

n i=1

Mi

(1)

where each Mi is a manifold. We propose a method solving this problem by choosing the moving direction as -f (x)(on manifold is -gradf (x)) with several drifts which are derived from the

descent information on other manifolds. By this method, we get sequence that has information from

all manifolds.

1.1 RELATED WORK
There are several articles discussing the problem of optimization on manifold. Most of them focus on a single manifold. Readers can find a good summary about this topic and the advantages of op-

1

Under review as a conference paper at ICLR 2019

timization on manifold in (Absil et al., 2009). Recently, popular first order algorithms in Euclidean space are studied in the manifold setting, e.g., the convergence of gradient descent (Zhang & Sra, 2016; Boumal et al., 2017), sub-gradient method (Hosseini & Uschmajew, 2017), stochastic variance reduction gradient (SVRG) (Zhang et al., 2016) and the gradient descent with momentum (Liu et al., 2017).
Riemann approaches (Cho & Lee, 2017; Huang et al., 2017) have also been applied to train deep neural network by noticing that the parameters of the neural network with batch normalization live on Grassmann manifold and Oblique manifold, respectively.

1.2 CONTRIBUTION
(1) This paper introduces an algorithm to deal with optimization with multiple manifolds. The algorithm adds drifts obtained from other manifolds to the moving direction, in order to incorporate the information from multiple manifolds during the optimization process.
(2) We prove the convergence of this algorithm under a very general framework. The proof is also applicable to the convergence of many other algorithms including gradient descent with momentum and gradient descent with regularization. Moreover, our proof does not depend on the choices of Retrx on the manifold.

2 OPTIMIZATION WITH MULTIPLE MANIFOLDS

2.1 BASIC DEFINITION AND LEMMA
The specific definition of manifold M can be found in any topology book. For better understanding, we introduce several properties of manifold here. A manifold is a subspace of Rn. For a given point x  M, it has a tangent space TxM which is a linear space but M may not. For gradient descent method, the iterates are generated via
xk+1 = xk - f (xk),

 is step length. However, the iterate generated by gradient descent may not on manifold anymore
because manifold is not a linear space. To fix this, we introduce a retraction function Retrx() : TxM  M to determine how point moves on manifold. Specifically, if M is Rn, the Retrx becomes x + . We can consider  in Retrx as the moving direction of the iterating point. Then, the gradient descent on manifold (Boumal et al., 2017; Zhang & Sra, 2016) is given by

1 xk+1 = Retrx(- L gradf (xk)),

(2)

where gradf (x) is Riemannian gradient. Riemannian gradient is the orthogonal projection of gra-
dient f (x) to tangent space TxM as f (x) may not in tangent space TxM and the moving direction on manifold is only decided by the vector in TxM. All of notations related to manifold
can be referred to (Absil et al., 2009).

We next use a lemma to describe a property of the minimum point of the problem arg minxM f (x), which is a special case of Yang et al., 2014, Corollary 4.2 and Boumal et al., 2017, Proposition 1.

Lemma 2.1 Let x be a local optimum for the optimization problem arg minxM f (x), which means there exists a neighborhood Ux of x satisfy f (x)  f (y) for y  Ux. If f (x) is differential in x, then gradf (x) = 0.

We see that gradf (x) plays a role of f (x) on manifold. Similar as Boumal et al. (2017) discussed, we assume function has the property of Lipschtiz gradient. The definition of Lipschtiz gradient is

Definition 2.1 (Lipschtiz gradient) For any two points x, y in the manifold M, f (x) satisfy:

f (y)  f (x) +

gradf (x), Retrx-1(y)

L +
2

Retr-x 1(y)

2

Then we say that f satisfies the Lipschtiz gradient condition.

2

Under review as a conference paper at ICLR 2019

We next introduce a condition that guarantees the convergence of iterative algorithms.
Definition 2.2 (Descent condition) For a sequence {xk} and ak > 0, if f (xk) - f (xk+1)  ak gradf (xk) 2,
then we say the sequence satisfies the descent condition.

2.2 GRADIENT DESCENT ON TWO MANIFOLDS
First, we introduce a theorem to describe the convergence when the object function f is lower finite, i.e., there exists a f  such that f (x)  f  > - for all x, and the iterates satisfy descent condition. This theorem plays a key role in proof of the rest theorems.

Theorem 2.1 If f is lower finite, and the iteration sequence {xk} satisfies the descent condition for any given {ak}, where each ak > 0. Then lim infk ak gradf (xk) = 0

Proof 1 The proof is available in Supplemental.

For better presentation, we first describe the algorithm under the circumstance of two manifolds. Considering the objective function f constrained on two manifolds M1, M2, we aim to find the minimum point on M1 M2. Since M1 M2 may not be a manifold, previous methods on manifold optimization cannot apply directly. We propose a method that integrates information from
two manifolds over the optimization process.

Specifically, we construct two sequences {xk}, {yk}, each on one manifold respectively. We add a drift which contains information from the other manifold to the original gradient descent on manifold
(equation 2). The updating rules are

xk+1

=

Retrxk

[-

1 L

(a(k1)

gradf

(xk

)

+

bk(1)hk(1))],

yk+1

=

Retryk

[-

1 L

(a(k2)

gradf

(yk

)

+

b(k2)hk(2))]

(3) (4)

If bk = 0 in (equation 3) and (equation 4), the updating rules reduce to normal gradient descent on manifold equation 2. The drift hk is in the tangent space TxM of each manifold, which represents information from the other manifold. We call this algorithm gradient descent on manifold with
drifting, whose procedure is described in Algorithm 1.

Algorithm 1 Gradient descent with drift on manifold

Input  > 0, x0  M1, y0  M2, Retrx,  > 0 k0

while gradf (xk) >  or gradf (yk) >  do

  a(k1)  2

  ak(2)  2

Calculating gradf (xk) = Px(1k)f (xk), gradf (yk) = Py(k2)f (yk).

Px(1k) and Py(k2) are respectively projection matrix of tangent space Txk , Tyk .

Obtain h(k1)  Txk M1 and hk(2)  Tyk M2.

b =(1) 2(1-ak(1)) gradf (xk),hk(1)
k hk(1) 2

b =(2) 2(1-ak(2)) gradf (yk),hk(2)
k hk(2) 2

xk+1

=

Retrxk

[-

1 L

(a(k1)

gradf

(xk

)

+

b(k1)hk(1))]

update step

yk+1

=

Retryk

[-

1 L

(ak(2)

gradf

(yk

)

+

b(k2)hk(2))]

k  k+1

update step

end while

return xk, yk

3

Under review as a conference paper at ICLR 2019

We next present the convergence theorem of this algorithm, which illustrates how we set ak and bk in the algorithm.

Theorem 2.2 For function f (x) is lower finite, and Lipschtiz gradient. If we construct the sequence {xk} like equation (3), and for any 0 <  < 2, we control   ak  2. Setting

bk

=

2(1

-

ak )

gradf (xk), hk hk 2

,

(5)

then xk convergence to a local minimizer.

Proof 2 The proof is based on construction of the descent condition (equation 12) and is available in Supplemental.

From the construction of bk, we can see that the smaller the correlation between gradf (xk)

and hk is, the smaller effect the information from M2 brings. In fact, we set h(k1) :=

gradf (xk)

Px(1k)gradf (yk) Px(1k)gradf (yk)

, where Px(1k)

is the projection matrix to tangent space Txk M1.

Sim-

ilarly we set hk(2) which exchanges xk and Px(1k) with yk and Py(k2)(projection matrix of tangent space

Tyk M2). The drift intuitively gives xk a force moving towards the minimizer on the other manifold. If the two manifolds are Rn, then xk and yk are symmetry with each other. We have

xk+1

=

xk

-

1 L

(ak(1)

f

(xk

)

+

bk(1)hk(1))

yk+1

=

yk

-

1 L

(ak(2)f

(yk

)

+

bk(2)hk(2)).

(6)

If the equation system is stable and x0, y0 are mutually close, the distance between xk and yk will be small when k  . By Schwarz inequality, we see bk  2(1 - ak). Since hk = gradf (xk) , the scale of the drift is the same as the original Riemannian gradient. Hence, information from another manifold will not affect much, when the points xk and yk are close to a minimizer. We can control the contribution of the information from the other manifold by adjusting ak. For instance, ak = 1 indicates we do not integrate information from the other manifold.
We can also prove the convergence rate of this algorithm.

Theorem 2.3

If f

is lower finite satisfy f (x)  f 

>

-,

we

choose

ak

as

max{

1 k+1

,

},

0

<



<

2. Then, for any  > 0, making sure gradf (xk) <  needs at most

exp

+f (x0)-f 
2

2 12

-

1



2(f (x0)-f ) 2 (2 - 2 )

iterations.

Proof 3 The proof is delegated to Supplemental.

Theorem 2.3 states the number of iterations we need to achieve a specific accuracy. Here we can adjust ak as long as  < ak < 2.

2.3 GRADIENT DESCENT ON n MANIFOLDS

In this subsection, we describe our algorithm for the case with multiple (more than 2) manifolds.
Suppose we have n manifolds, M1, · · · , Mn, and sequence on manifold Mi is denoted as {xk(i)}. In the following, we use sequence {xk(1)} on M1 as an example, and other sequences on other manifolds can be derived accordingly. Let gk(i)  TxM1 denote the drift from manifold Mi (gk(1) is gradf (xk(1)). Then let the updating rule be

x(k1+)1 = Retrxk

-1 L

n
ak(i)gk(i)

.

i=1

(7)

4

Under review as a conference paper at ICLR 2019

Since f satisfies Lipschtiz gradient condition(2.1), we have

f (x(k1)) - f (x(k1+)1)



1 L

n

gk(1),

ak(i)gk(i)

i=1

-1 2L

n2
ak(i)gk(i)
i=1

=

1 L

{(ak(1)

-

ak(1)2 ) 2

gk(1), gk(1)

+

n
[a(ki) gk(1), gk(i)

-

1 2

a(k1)a(ki)

gk(1), gk(i)

i=2

-1 2

n

a(ki)a(kj) gk(i), gk(j) ]}.

j=1

We choose ak(1) such that 0 <  < a(k1) < 2. For j = 2 · · · n, we choose ak(j) such that

n
a(kj) gk(i), gk(j) = 2(1 - ak(1)) gk(1), gk(i)
j=2

for i = 2 · · · n

(8)

then

a(ki) gk(1), gk(i)

-1 2

n

a(ki)ak(j) gk(i), gk(j)

-

1 2

a(k1)

a(ki)

gk(1), gk(i)

=0

j=1

for i = 2 · · · n.

The way of choosing a(kj) is to obtain the descent condition. Specifically, when n = 2, the solution of (equation 8) becomes bk in (equation 5). If ak(j) solve the linear equation system (8), we get the descent condition (equation 12). From theorem 2.1, we prove the convergence of the updating rule
(equation 7) for the case with n manifolds. Writing (equation 8) in the matrix form, we have

Gkk = k,
where Gk = ( gk(i), gk(j) )ij, i, j from 2 to n, and k = (ak(2), · · · , ak(n))T , k = (2(1 - a(k1)) gk(1), gk(2) , · · · , 2(1 - ak(1)) gk(1), gk(n) )T . If Gk is invertible, the linear equation system has an unique solution.

3 APPLY OUR ALGORITHM TO TRAIN NEURAL NETWORK

3.1 NEURAL NETWORK WITH BATCH NORMALIZATION

Batch normalization has been widely used since its proposition (Ioffe & Szegedy, 2015). It transforms the input value to a neuron from z = wT x to

z - E(z) wT (x - E(x))

BN (w) =

=

.

V ar(z)

wT Vxw

We can calculate the derivative as follows

BN (w) w

=

x - E(x) wT Vxw

-

wT

(x - E(x))Vxw

(wT

Vx

w)

3 2

.

For any a = 0, a

 R, we see that BN (w)

=

BN (aw)

and

BN (aw) aw

=

1 a

BN (w) w

.

These

equations mean that after a batch normalization, the scale of parameter has no relationship with the

output value, but scale of gradient is opposite with the scale of parameter. Cho & Lee (2017) have

discussed that batch normalization could have an adverse effect in terms of optimization since there

can be an infinite number of networks, with the same forward path but different scaling, which may

converge to different local optima owing to different gradients.

To avoid this phenomenon, we can eliminate the effect of scale by considering the weight w on
the Grassmann manifold or Oblique manifold. On these two manifolds, we can ignore the scale of
parameter. Cho & Lee (2017); Huang et al. (2017) respectively discuss that BN (w) has same image space on G(1, n) and St(n, 1) as well as Rn, where G(1, n) is a Grassmann manifold and St(n, 1) is an Oblique manifold. Due to these, we can consider applying optimization on manifold to batch

5

Under review as a conference paper at ICLR 2019

normalization problem. However, the property of these two manifold implies that we can actually
optimize on the intersection of two manifolds. Since optimization on a manifold rely on Riemannian gradient gradf (x) and Retrx, for a specific Retrx (9) of Grassmann manifold G(1, n), we get a unit point x when  = -gradf (x) = 0 in formula (9). The condition gradf (x) = 0 means we obtain
a unit critical point on Grassmann manifold which is also on Oblique manifold.

The specific discussion of Grassmann manifold and Oblique manifold can be found in (Absil et al., 2009). G(1, n) is a quotient manifold defined on a vector space, it regards vector with same direction
as same element. For example (1, 1, 1) and (10, 10, 10) correspond to same element. We represent elements on G(1, n) with same direction by choosing one of them as representation element. Oblique manifold is given by St(n, p) = {X  Rn×p : ddiag(XT X) = Ip}, where ddiag(·) is diagonal matrix of a matrix.

We have discussed above that iteration point on G(1, n) would be a unit point when it's a local minimizer. Due to this, the local minimizer we find is actually live on the intersection of St(n, 1) and G(1, n). Hence, training neural network with batch normalized weights can be converted to the problem
arg min f (x).
xG(1,n) St(n,1)

Let Riemannian gradient be projection of f (x) to tangent space of x. On G(1, n), we have

Px(1)() =  - xT 

x x2

gradf (x) = Px(1)(f (x)) = f (x) - (xT f (x))

x x2

Retrx(1)() =

x x

cos 

+

 

sin 

(9)

On St(n, 1), we have

Px(2)() =  - xddiag(xT )

gradf (x) = Px(2)(f (x)) = f (x) - xddiag(xT f (x))

Retrx(2)() =

x+ x+

the Px is the projection matrix onto the tangent space at x. These results can be derived from the general formulas from (Absil & Gallivan, 2006) and (Edelman et al., 1999).

In backward process of training neural network, weight parameter of each layer is a matrix. Hence, we get gradient to a matrix in every layer. To make calculation easier, we treat the gradient matrix and parameters matrix as vector. For example a m × n gradient matrix can be viewed as a m × n dimensional vector. Then we apply Algorithm 1 to update parameters, which means we optimize on a product manifold

G(1, k1) × · · · G(1, kn), St(k1, 1) × · · · St(kn, 1)

ki is number of parameters for the i-th hidden layer, and n is number of hidden layers. We need to operate algorithm for parameter vector on each hidden layer. In other words, we update parameters
layer by layer.

3.2 EXPERIMENT
In this section, we use data set CIFAR-10 and CIFAR-100 (Krizhevsky & Hinton, 2009) to test our algorithm. These two data sets are color images respectively have 10 and 100 classes, each of them has 50,000 training images and 10,000 test images. The deep neural network we used is WideResNet (Zagoruyko & Komodakis, 2016), it output a vector which describe the probability of a data divided into each class.
In every hidden layer of neural network, we apply batch normalization to weight parameters and treat them as a vector. We have already discussed that minimizers of a neural network with batch normalized weights live on the intersection of Grassmann manifolds and Oblique manifold. Hence, we can train neural network with batch normalized weights by our algorithm(1). The biases of every

6

Under review as a conference paper at ICLR 2019

hidden layer is unrelated to batch normalization and are updated by SGD. For every training step,

we

calculate

mean

loss

1 S

xiS l(f (xi, ), yi) of a mini batch to substitute the real loss function

Ex[l(f (x, ), y)], where S is batch size.

The process of algorithm on two manifolds follows Algorithm 1, where the two manifolds are G(1, n) and St(n, 1), respectively. In Algorithm 1, we choose

h(k1) =

gradf (xk)

Px(1k)gradf (yk)) , Px(1k)gradf (yk)

hk(2) =

gradf (yk)

Py(k2)gradf (xk)) Py(k2)gradf (xk)

and

ak(1)

=

ak(2)

=

max{

1 k+1

,

}.

In

the

updating

rules

of

xk

and

yk ,

we

add

a

norm-clip

to

vectors

(ak(1)gradf (xk) + b(k1)h(k1)) and (ak(2)gradf (yk) + b(k2)hk(2)). Then we times  to the two vectors,

where  is the learning rate.

In the experiments, we compare three methods: 1) stochastic gradient descent on manifold with drifting (Drift-SGDM), 2) stochastic gradient descent on manifold Boumal et al. (2017) (SGDM), and 3) stochastic gradient descent (SGD). In Algorithm 1, we can get two sequences each corresponding to a model on a manifold. We predict output class by adding two output vectors of two models and choosing the biggest as prediction class.

For Drift-SGDM (Algorithm 1), we set  = 0.9 and initial learning rate m = 0.4 for weights parameters which is multiplied by 0.4 at 60, 120, and 160 epochs. Initial learning rate  for biases is 0.01 which is multiplied by 0.4 at 60, 120, and 160 epochs. Norm clip is 0.1. Training batch size is 128. The number of training epochs is 200.

For SGDM, we choose a = 1 in Algorithm 1. The other settings are the same as Drift-SGDM. That a = 1 in Algorithm 1 means that SGDM optimizes on each manifold individually

We set SGD as baseline. The learning rate is 0.2 which is multiplied by 0.2 at epoch 60,120 and 160. Weight decay is set as 0.0005, but we do not apply weight decay for algorithms on manifold. All other settings are the same as the above two algorithms.

(a) Training loss for WRN-28-10 (b) Accuracy for WRN-28-10 on (c) Accuracy for WRN-16-4 on

on CIFAR-100

CIFAR-100

CIFAR-100

Figure 1: Some results based on Wide-ResNet. WRN-d-k denotes a wide residual network that has d convolutional layers and a widening factor k

About Drift-SGDM and SGDM, the loss is achieved from the average of two model. The parameter scale of the two model can be different, because they respectively live on Grassmann manifold and Oblique manifold. Due to this, the comparing between Drift-SGDM and SGDM is more reasonable. We also give the accuracy curve and a tubular of accuracy rate on test sets to validate our algorithms.

Dataset Model WRN-52-1 WRN-16-4 WRN-28-10

SGD 93.3 94.46 95.59

CIFAR-10 Drift-SGDM
92.88 94.16 95.07

SGDM 93.11 94.16 94.79

SGD 71.07 74.74 78.88

CIFAR-100 Drift-SGDM
69.61 76.4 78.92

SGDM 69.58 76.04 79.37

Table 1: Accuracy rate on test sets of data.

7

Under review as a conference paper at ICLR 2019
We see that our algorithm perform better on larger neural network. Our algorithm does not have regularization term, and it does not perform well in the aspect of generalization. We can actually add a regularization term like in (Cho & Lee, 2017) to achieve better generalization. We choose  in Algorithm 1 as 0.9. Since b(ki)  2(1 - ak(i)) where i = 1, 2 as we have discussed in section 2, we see drift term bk(i)hk(i) in Algorithm 1 doesn't affect much to iteration point. We can actually set a smaller  to enhance the influence of drift term bk(i)hk(i).
4 CONCLUSION
In this paper, we derive an intuitively method to approach optimization problem with multiple constraints which corresponds to optimizing on the intersection of multiple manifolds. Specifically, the method is integrating information among all manifolds to determine minimum points on each manifold. We don't add extra conditions to constraints of optimization problem, as long as each constraint can be converted to a manifold. In the future, we may add some conditions to manifolds which derive a conclusion that minimum points on each manifold achieved by our algorithm are close with other. If this conclusion is established, the problem of optimization on intersection of multiple manifolds is solved. According to the updating rule (equation 3), we can derive many other algorithms, because the drift hk in (equation 3) is flexible. On the other hand, Retrx on our algorithm does not limit to a specific one. Since there are some results for Retrx = Expx, for example Corollary 8 in (Zhang & Sra, 2016), we may get more elegant results by using Expx as retraction function in our algorithm. The manifolds we encounter in optimization are mainly embedded sub-manifold and quotient manifold (Absil et al., 2009). Embedded sub-manifold is F -1(y) for a smooth function F : M1  M2, where M1, M2 are two manifolds and y  M2. Quotient manifold is a quotient topology space generalized by a specific equivalence relationship . In this paper, we use Oblique manifold and Grassmann manifold which are embedded sub-manifold and quotient manifold respectively. The difficulty we faced in optimization on manifold is calculating tangent space TxM and Riemannian gradient gradf (x). Giving a exact formula of a tangent space TxM is not a easy problem. On the other hand, since Riemannian gradient is f (x) projected to a tangent space TxM, finding projection matrix to a specific space TxM is nontrivial.
8

Under review as a conference paper at ICLR 2019
ACKNOWLEDGMENTS
Use unnumbered third level headings for the acknowledgments. All acknowledgments, including those to funding agencies, go at the end of the paper.
REFERENCES
P. A. Absil and K. A. Gallivan. Joint diagonalization on the oblique manifold for independent component analysis. ICASSP, 2006.
P.-A. Absil, Robert. Mahony, and Rodolphe Sepulchre. Optimization algorithms on matrix manifolds. Princeton Press, 2009.
Nicolas Boumal, P.-A. Absil, and Coralia Cartis. Global rates of convergence for nonconvex optimization on manifolds. arxiv preprint arxiv:1605.08101, 2017.
Minhyung Cho and Jaehyung Lee. Riemannian approach to batch normalization. Advances in Neural Information Processing Systems 30 (NIPS 2017), 2017.
Alan Edelman, T. A. Arias, and Steven.T. Smith. The geometry of algorithms with orthogonality constraints. Society for Industrial and Applied Mathematics, 1999.
Seyedehsomayeh Hosseini and Andr Uschmajew. A riemannian gradient sampling algorithm for nonsmooth optimization on manifolds. Siam Journal on Optimization, 27, 2017.
Lei Huang, Xianglong Liu, Bo Lang, and Bo Li. Projection based weight normalization for deep neural networks. arxiv preprint arXiv:1710.02338, 2017.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In Proceedings of The 32nd International Conference on Machine Learning,ICML, pp. 448­456, 2015.
Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. Department of Computer Science, University of Toronto, 2009.
Yuanyuan Liu, Fanhua Shang, James Cheng, Hong Cheng, and Licheng Jiao. Accelerated first-order methods for geodesically convex optimization on riemannian manifolds. Neural Information Processing Systems (NIPS), pp. 4875­4884, 2017.
Qi Meng, Wei Chen, Shuxin Zheng, Qiwei Ye, and Tie-Yan Liu. Optimizing neural networks in the equivalent class space. arXiv preprint arXiv:1802.03713, 2018.
Deepak Pathak, Philipp Krahenbuhl, and Trevor Darrell. Constrained convolutional neural networks for weakly supervised segmentation. In Proceedings of the IEEE International Conference on Computer Vision, pp. 1796­1804, 2015.
Songui Wang, Jianhong Shi, Suju Yin, and Mixia Wu. Introduction to linear model. Science press Beijing, 2003.
Ke Wei, Jian-Feng Cai, Tony F Chan, and Shingyu Leung. Guarantees of riemannian optimization for low rank matrix recovery. SIAM Journal on Matrix Analysis and Applications, 37(3):1198­ 1222, 2016.
Weihong Yang, Leihong Zhang, and Ruyi Song. Optimality conditions for the nonlinear programming problems on riemannian manifolds. Pacific Journal of Optimization, 10:415­434, 2014.
Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. arxiv preprint arXiv:1605.07146v4, 2016.
Hongyi Zhang and Suvrit Sra. First-order methods for geodesically convex optimization. In Conference on Learning Theory, pp. 1617­1638, 2016.
Hongyi Zhang, Sashank.J. Reddi, and Sra Suvrit. Riemannian svrg:fast stochastic optimization on riemannian manifolds. Advances in Neural Information Processing Systems 29 (NIPS 2016), 2016.
Vladimir. A. Zorich. Mathematical Analysis I. Spring-Verlag, 2002.
9

Under review as a conference paper at ICLR 2019

A DISCUSSION ABOUT THE FRAMEWORK OF GRADIENT DESCENT WITH
DRIFT

A.1 CONNECTION WITH CLASSICAL ALGORITHMS

In this section, we study the frame work of gradient descent with drift. In a special case, we regard Rn as a manifold. Then, Rienmann gradient gradf (x) = f (x), tangent space TxM = Rn and
Retrx() = x + . In Algorithm (1), we set

hk+1

=

-1 L

(akf (xk)

+

bk hk ),

where

 < ak < 2,

bk =

2(1 - ak) f (xk), hk hk 2

.

Then we have

xk+1

=

xk

-

1 L

(ak

f

(xk

)

+

bk hk )

=

xk

-

1 L (akf (xk)

-

bk L

(ak-1f (xk-1)

+

bk-1hk-1)),

which is exactly a kind of gradient descent with momentum. And this algorithm is convergence as

we proved. On the other hand, if choosing hk as gradient of a regularization term R(x) on xk. For example, hk becomes 2xk when R(x) = x 2. The iteration point in Algorithm (1) is achieved by

gradient descent with regularization term.

A.2 A STOCHASTIC DRIFT

The drift in (equation 3) we have discussed is non-stochastic. But actually, we can change the drift

as a stochastic term to construct a non-descent algorithm. Meanwhile, stochastic drift gives iteration

sequence ability of jumping from local minimizer. The update rule is

1 xk+1 = Retrxk [- L (akgradf (xk) + bkPxk k), ]

(10)

where k is a random vector with mean vector µ, covariance matrix . The process of this algorithm is Algorithm 2.

Algorithm 2 Non-descent method with stochastic noise

Input 0 <  < 2, x0  M, Retrx,  > 0 k1

while gradf (x) >  do

Sample k with mean vector µ and covariance matrix 

bk

=

1 k2

ak =

1-

k

1 gradf (xk)



gradf (xk)



1 k

gradf (xk)

<

1 k

xk+1

=

Retrxk

[-

1 L

(ak

gradf

(xk

)

+

bkPxk k)]

Pxk is projection matrix to tangent space TxM

k  k+1

update step

end while

return xk

We give convergence theorem of Algorithm 2. The proof implies that this algorithm is non-descent, it also shows how we set ak and bk.

Theorem A.1 For function f (x)  f  > -, and Lipschtiz gradient. If we construct the sequence

{xk} like (equation 10), choosing {ak}, {bk} satisfy bk <  and
k=1

ak =

1-

k

1 gradf (xk)



gradf (xk)



1 k

gradf (xk)

<

1 k

(11)

10

Under review as a conference paper at ICLR 2019

where 0 <  < 2, we have lim infk gradf (xk) 2 = 0.
In this theorem, bk control the speed of back fire. The noise k in (equation 10) has small effect to iteration process when k is large, because sequence is about to be stable after enough iterations. But in beginning of iteration procedure, noise k effects much which give iteration sequence ability of jumping from local minimizer.

B PROOF OF THEOREMS

In this section, we give proof of theorems in this paper. The proof of Theorem 2.1 is

Proof 4 (proof of Theorem 2.1) According to definition 2.2 of descent condition, we have
k-1
f (x0) - f (xk)  ai gradf (xi) 2,
i=0
for any k. Since f is lower finite, we have

ai gradf (xi) 2  f (x0) - f  < ,
i=0
where f (x)  f  > -, it means lim infk ak gradf (xk) = 0.

The proof of Theorem 2.2 is

Proof 5 (proof of Theorem 2.2) Since f satisfy Lischtiz gradient(2.1), we have

f (xk)

-

f (xk+1)



1 L (ak

-

ak2 2

)

- b2k 2L

hk

2

gradf (xk)

2+

1 L bk(1 - ak)

gradf (xk), hk

.

By the definition of bk, we got

f (xk) - f (xk+1)  (ak -

ak2 ) 2

gradf (xk)

2.

Since ak

-

ak2 2

>

0, by Theorem 2.1, we have

lim inf (ak -
k

ak2 ) 2

gradf (xk)

2

= 0.

The definition of ak implies that gradf (x) 2  0.

(12)

The proof of Theorem 2.3 gives convergence rate of gradient descent on manifold with drift.

Proof 6 (proof of Theorem 2.3) For any  > 0, assuming gradf (xi)  , i  N until i = k.

Then

f

(x0)

-

f

(xk )



k-1
[(

i

1 +

1

-

2(i

1 +

1)2

)



(

-

2 )]
2

gradf (xi)

2

i=0

 [(log (k + 1) -



2(i

1 +

1)2

)



k(

-

2 2

)

gradf (xi)

2,

i=0

here we use the relationship log (1 + x)



x when x



0.

Since



1 2(i+1)2

=

2 12

<

(Zorich,

i=0

2002), for i  k - 1, we have

log (k + 1) 

f (x0) - f  gradf (xi) 2

+

2 12

and Since

gradf (xi)

k



( -

f (x0) - f 

2 2

)

gradf (xi)

.
2

  when i  k - 1, k 

exp

+f (x0)-f 
2

2 12

-1



2(f (x0)-f ) 2 (2 - 2 )

.

11

Under review as a conference paper at ICLR 2019

Before proof Theorem A.1, we need two lemmas.
Lemma B.1 A random vector with Ex = µ and Covx = . Then for any symmetric matrix A, we have
E(xT Ax) = µT Aµ + tr(A).
This lemma can be derived from Wang et al. (2003)'s theorem 3.2.1 of Page 57. The other lemma is

Lemma B.2 If A and  are n × n symmetric matrix, then tr(A)  tr(A)max, where max is the largest engine value of 

Proof 7 (proof of Lemma B.2) According to spectral decomposition of symmetric matrix A, A can
n
be written as iiiT , where i is engine value of A and i is engine vector corresponding to i.
i=1
They satisfy iT j = ij ,
ij is Kronecker delta. Hence

nn

n

tr(A) = tr(iiiT ) = tr(iiT i)  max tr(i) = maxtr(A).

i=1 i=1

i=1

Here we use Rayleigh theorem of theorem 2.4.(Wang et al., 2003)

Proof 8 (proof of Theorem A.1) Since Pxk is a projection matrix, which is a symmetric idempotent matrix. Because f satisfies Lipschtiz gradient(2.1), we have

f (x1)

-

f (xk)



1 L

k-1
(ai -

a2i ) 2

gradf (xi)

2 + bi(1 - ai)i -

bi2 2

i

,

i=1

where i = (Pxi gradf (xi))T i = gradf (xi))T i, i = iT PxTi Pxii = iT Pxi i. Due to the two random variables, algorithm (2) is not necessary descent.  is a symmetric positive definite matrix. By Schwarz equality and definition of ai, we have

k-1

k-1

E [bi(1 - ai)i]  E bi(1 - ai) gradf (xi) i

i=1 i=1

k-1
11
 bi E(1 - ai)2 gradf (xi) 2 2 E i 2 2

i=1

k-1

=

bi

E(1 - ai)2

gradf (xi)

2

1
2[

µ

2

+

tr()]

1 2

i=1

[

µ

2+



1
tr()] 2

k-1

bi

.

i

i=1

By Fatou's lemma, we have

k-1

k-1

E lim inf[ bi(1 - ai)i]  lim inf E[ bi(1 - ai)i]

k

k

i=1 i=1



lim [

µ

2+



tr()]

1 2

k-1 bi

=[

µ

2+



tr()]

1 2



bi < ,

k

i

i

i=1 i=1

which implies

k-1
lim inf bi(1 - ai)i < 
k i=1

a.s.

12

Under review as a conference paper at ICLR 2019

Since tr(Pxi ) = rank(Pxi ) and the largest engine value of Pxk is 1, we have E(i) = E(iT Pxi i) = µT Pxi µ + tr(Pxi )  µ 2 + maxd,

where d is the dimension of x and max is the largest engine value of . By Levy's theorem, we

have

k-1
E[ lim
k

b2i 2

i]

=

lim
k

k-1
E[

bi2 2

i]



lim
k

k-1
(

µ

2

+

maxd)

bi2 2

< ,

i=1 i=1 i=1

which implies



bi2 2

i

is

finite

almost

surely.

Hence

i=1



(ai

-

ai2 2

)

gradf (ik)

k-1

2



L(f (x1)

-

f)

-

lim inf
k

bi(1 - ai)i +



b2i 2

i

<



i=1 i=1 i=1

a.s.

then we have

lim inf
k

(ak

-

a2k ) 2

gradf (xk)

2=0

a.s.

(13)

by the definition of ak(equation 11), equation (13) is equivalent to lim infk gradf (xk) 2 = 0 almost surely.

13


Under review as a conference paper at ICLR 2019

ADASHIFT: DECORRELATION AND CONVERGENCE OF ADAPTIVE LEARNING RATE METHODS
Anonymous authors Paper under double-blind review

ABSTRACT
Adam is shown not being able to converge to the optimal solution in certain cases. Researchers recently propose several algorithms to avoid the issue of nonconvergence of Adam, but their efficiency turns out to be unsatisfactory in practice. In this paper, we provide a new insight into the non-convergence issue of Adam as well as other adaptive learning rate methods. We argue that there exists an inappropriate correlation between gradient gt and the second moment term vt in Adam (t is the timestep), which results in that a large gradient is likely to have small step size while a small gradient may have a large step size. We demonstrate that such unbalanced step sizes are the fundamental cause of non-convergence of Adam, and we further prove that decorrelating vt and gt will lead to unbiased step size for each gradient, thus solving the non-convergence problem of Adam. Finally, we propose AdaShift, a novel adaptive learning rate method that decorrelates vt and gt by temporal shifting, i.e., using temporally shifted gradient gt-n to calculate vt. The experiment results demonstrate that AdaShift is able to address the non-convergence issue of Adam, while still maintaining a competitive performance with Adam in terms of both training speed and generalization.

1 INTRODUCTION

First-order optimization algorithms with adaptive learning rate play an important role in deep learn-

ing due to their efficiency in solving large-scale optimization problems. Denote gt  Rn as the gradient of loss function f with respect to its parameters   Rn at timestep t, then the general

updating rule of these algorithms can be written as follows (Reddi et al., 2018):

t+1

=

t

-

t vt

mt.

(1)

In the above equation, mt (g1, . . . , gt)  Rn is a function of the historical gradients; vt

(g1, . . rate for

. , gt) the n

 R+n is an elements in

n-dimension vector gt respectively; t

with non-negative elements, which adapts the learning

is

the

base

learning

rate;

and

t vt

is

the

adaptive

step

size for mt.

One common choice of (g1, . . . , gt) is the exponential moving average of the gradients used in Momentum (Qian, 1999) and Adam (Kingma & Ba, 2014), which helps alleviate gradient oscillations. The commonly-used (g1, . . . , gt) in deep learning community is the exponential moving average of squared gradients, such as Adadelta (Zeiler, 2012), RMSProp (Tieleman & Hinton, 2012), Adam
(Kingma & Ba, 2014) and Nadam (Dozat, 2016).

Adam (Kingma & Ba, 2014) is a typical adaptive learning rate method, which assembles the idea

of using exponential moving average of first and second moments and bias correction. In general,

Adam is robust and efficient in both dense and sparse gradient cases, and is popular in deep learning

research. However, Adam is shown not being able to converge to optimal solution in certain cases.

Reddi

et

al.

(2018)

point

out

that

the

key t

issuein the vt - t

covntv-e1rge,nce t-1

proof

of

Adam

lies

in

the

quantity (2)

which is assumed to be positive, but unfortunately, such an assumption does not always hold in

Adam. They provide a set of counterexamples and demonstrate that the violation of positiveness of

t will lead to undesirable convergence behavior in Adam.

1

Under review as a conference paper at ICLR 2019

tReddi et al. (2018) then propose two variants, AMSGrad and AdamNC, to address the issue by

keeping t positive. Specifically, AMSGrad defines v^t as the historical maximum of vt, i.e., v^t = max {vi}ti=1, and replaces vt with v^t to keep vt non-decreasing and therefore forces t to be positive; while AdamNC forces vt to have "long-term memory" of past gradients and calculates vt

as their average to make it stable. Though these two algorithms solve the non-convergence problem

of Adam to a certain extent, they turn out to be inefficient in practice: they have to maintain a very

large

vt

once

a

large

gradient

appears,

and

a

large

vt

decreases

the

adaptive

learning

rate

t vt

and

slows down the training process.

In this paper, we provide a new insight into adaptive learning rate methods, which brings a new perspective on solving the non-convergence issue of Adam. Specifically, in Section 3, we study the counterexamples provided by Reddi et al. (2018) via analyzing the accumulated step size of each gradient gt. We observe that in the common adaptive learning rate methods, a large gradient tends to have a relatively small step size, while a small gradient is likely to have a relatively large step size. We show that the unbalanced step sizes stem from the inappropriate positive correlation between vt and gt, and we argue that this is the fundamental cause of the non-convergence issue of Adam.

In Section 4, we further prove that decorrelating vt and gt leads to equal and unbiased expected step size for each gradient, thus solving the non-convergence issue of Adam. We subsequently propose AdaShift, a decorrelated variant of adaptive learning rate methods, which achieves decorrelation between vt and gt by calculating vt using temporally shifted gradients. Finally, in Section 5, we study the performance of our proposed AdaShift, and demonstrate that it solves the non-convergence issue of Adam, while still maintaining a decent performance compared with Adam in terms of both training speed and generalization.

2 PRELIMINARIES

Adam. In Adam, mt and vt are defined as the exponential moving average of gt and gt2: mt = 1mt-1 + (1 - 1)gt and vt = 2vt-1 + (1 - 2)gt2,

(3)

where 1  [0, 1) and 2  [0, 1) are the exponential decay rates for mt and vt, respectively, with m0 = 0 and v0 = 0. They can also be written as:

tt
mt = (1 - 1) 1t-igi and vt = (1 - 2) 2t-igi2.
i=1 i=1

(4)

To avoid the bias in the estimation of the expected value at the initial timesteps, Kingma & Ba (2014) propose to apply bias correction to mt and vt. Using mt as instance, it works as follows:

mt

=

(1 - 1) (1 - 1)

t i=1

1t-i

gi

t i=1

1t-i

=

t i=1

1t-i

gi

t i=1

1t-i

=

(1

-

1) 1

-

t
i=1
1t

1t-igi

.

(5)

Online optimization problem. An online optimization problem consists of a sequence of cost

functions f1(), . . . , ft(), . . . , fT (), where the optimizer predicts the parameter t at each

timestep t and evaluate it on an unknown cost function ft(). The performance of the optimizer

is usually evaluated by regret between the online prediction

R(T ) ft(t)

and

T t=1

[ft

(t)

-

ft()],

which

the best fixed-point parameter

is the sum prediction

of the ft()

difference for all the

previous steps, where  = arg min

T t=1

ft

()

is

the

best

fixed-point

parameter

from

a

feasible

set .

Counterexamples. Reddi et al. (2018) highlight that for any fixed 1 and 2, there exists an online optimization problem where Adam has non-zero average regret, i.e., Adam does not converge to
optimal solution . The counterexamples in the sequential version are given as follows:

ft() =

C, -,

if t mod d = 1; otherwise,

(6)

where C is a relatively large constant and d is the length of an epoch. In Equation 6, most gradients of ft() with respect to  are -1, but the large positive gradient C at the beginning of each epoch

2

Under review as a conference paper at ICLR 2019

makes the overall gradient of each epoch positive, which means that one should decrease t to

minimize the loss. However, according to (Reddi et al., 2018), the accumulated update of  in Adam

under some circumstance is opposite (i.e., t is increased), thus Adam cannot converge in such case.

Reddi et al. (2018) assumption of t

a(rguvet/thatt-the vret-as1o/not-f 1t)hedoneosnn-cootnavlweragyesncheolodfiAn dAadmamli.es

in

that

the

positive

Basic Solutions Reddi et al. (2018) propose maintaining the strict positiveness of t as solution, for example, keeping vt non-decreasing or using increasing 2. In fact, keeping t positive is not the only way to guarantee the convergence of Adam. Another important observation is that for
any fixed sequential online optimization problem with infinitely repeating epochs (e.g., Equation 6), Adam will converge as long as 1 (or 2) is large enough. Formally, we have the following theorem:

Theorem 1 (The influence of 1). For any fixed sequential online convex optimization problem

with infinitely repeating of finite length epochs, let the length of an epoch be d, if G  R such that

ft()   G and T  N,  2 >

1 > 0 such that

1

<

d

t vt

G2

<

2 holds for all t > T ,

then, for any fixed 2  [0, 1), there exists a 1  [0, 1) such that Adam has average regret  2;

The intuition behind Theorem 1 is that, if 1  1, then mt 

d i=1

gi/d,

i.e.,

mt

approaches

the average gradient of an epoch, according to Equation 5. Therefore, no matter what the adaptive

learning rate t/ vt is, Adam will always converge along the correct direction.

3 THE CAUSE OF NON-CONVERGENCE: UNBALANCED STEP SIZE
In this section, we study the non-convergence issue by analyzing the counterexamples provided by Reddi et al. (2018). We show that the fundamental problem of common adaptive learning rate methodsis that: vt is positively correlated to the scale of gradient gt, which results in a small step size t/ vt for a large gradient, and a large step size for a small gradient. We argue that such an unbalanced step size is the cause of non-convergence.
We will first define net update factor for the analysis of the accumulated influence of each gradient gt, then apply the net update factor to study the behaviors of Adam using Equation 6 as an example. The argument will be extended to the stochastic online optimization problem and general cases.

3.1 NET UPDATE FACTOR

When 1 = 0, due to the exponential moving effect of mt, the influence of gt exists in all of its following timesteps. For timestep i (i  t), the weight of gt is (1 - 1)1i-t. We accordingly define a new tool for our analysis: the net update net(gt) of each gradient gt, which is its accumulated
influence on the entire optimization process:

net(gt)

 i=t

i [(1 - vi

1)1i-tgt]

=

k(gt) ·

gt,

where

k(gt)

=

 i=t

i vi

(1

-

1)1i-t,

(7)

and we call k(gt) the net update factor of gt, which is the equivalent accumulated step size for

gradient gt. {vi}i=t are

Note related

that k(gt) depends on to gt. Therefore, k(gt)

{vi}i=t, and in Adam, is a function of gt.

if

1

=

0,

then

all

elements

in

It is worth noticing that in Momentum method, vt is equivalently set as 1. Therefore, we have k(gt) = t and net(gt) = tgt, which means that the accumulated influence of each gradient gt in Momentum is the same as vanilla SGD (Stochastic Gradient Decent). Hence, the convergence
of Momentum is similar to vanilla SGD. However, in adaptive learning rate methods, vt is function over the past gradients, which makes its convergence nontrivial.

3.2 ANALYSIS ON SEQUENTIAL ONLINE OPTIMIZATION COUNTEREXAMPLES
Note that vt exists in the definition of net update factor (Equation 7). Before further analyzing the convergence of Adam using the net update factor, we first study the pattern of vt in the sequential online optimization problem in Equation 6. Since Equation 6 is deterministic, we can derive the formula of vt as follows:

3

Under review as a conference paper at ICLR 2019

Theorem 2 (Limit of vt). In the sequential online optimization problem in Equation 6, denote 1, 2  [0, 1) as the decay rates, d  N as the length of an epoch, n  N as the index of epoch, and i  {1, 2, ..., d} as the index of timestep in one epoch. Then the limit of vnd+i when n   is:

lim
n

vnd+i

=

1 1

- -

2 2d

(C

2

- 1)2i-1

+1

.

(8)

Given the formula of vt in Equation 8, we now study the net update factor of each gradient. We start

with a simple case where 1 = 0. In this case we have

lim
n

k(gnd+i)

=

lim
n

vntd+i

.

(9)

Since the limit of vnd+i in each epoch monotonically decreases with the increase of index i according to Equation 8, the limit of k(gnd+i) monotonically increases in each epoch. Specifically, the first gradient gnd+1 = C in epoch n represents the correct updating direction, but its influence is the smallest in this epoch. In contrast, the net update factor of the subsequent gradients -1 are relatively
larger, though they indicate a wrong updating direction.

The above problem stems from the inappropriate correlation between vt and gt. Recall that vt = 2vt-1 + (1 - 2)gt2, and we assume vt-1 is independent of gt. When a new gradient gt arrives, if gt is large, vt is likely to be larger; and if gt is small, vt is also likely to be smaller. As a result, a large gradient is likely to have a small net update factor, while a small gradient is likely to have a
large net update factor in Adam.

We further consider the general case where 1 = 0. The result is presented in the following theorem:

Theorem 3 (Unbalanced net update factor). In the sequential online optimization problem in Equation 6, when n  , the limit of net update factor k(gnd+i) of epoch n is:



lim
n

k(gnd+i)

=

t=nd+i

(1 - 1)1t-nd-i

.

1-2 1-2d

(C 2

-

1)2(t-1)

mod

d

+

1

(10)

And there exists 1  j  d such that

lim
n

k(C )

=

lim
n

k(gnd+1)

<

lim
n

k(gnd+2)

<

·

·

·

<

lim
n

k(gnd+j

),

and

lim
n

k(gnd+j )

>

lim
n

k(gnd+j+1)

>

·

·

·

>

lim
n

k(gnd+d+1)

=

lim
n

k(C ),

where k(C) denotes the net update factor for gradient gi = C.

(11) (12)

Theorem 3 tells us that, in sequential online optimization problem in Equation 6, the net update factors are unbalanced. Specifically, the net update factor for the large gradient C is the smallest in the entire epoch, while all gradients -1 have larger net update factors. Such unbalanced net update factors will possibly lead Adam to a wrong accumulated update direction.

3.3 ANALYSIS ON STOCHASTIC ONLINE OPTIMIZATION COUNTEREXAMPLES

The counterexamples are also extended to stochastic cases in Reddi et al. (2018), where a finite set of cost functions appear in a stochastic order. Compared with sequential online optimization counterexample, the stochastic version is more general and closer to the practical situation. For the simplest one dimensional case, at each timestep t, the function ft() is chosen as i.i.d.:

ft() =

C, -,

with

probability

p

=

1+ C +1

;

with

probability

1

-

p

=

C - C +1

,

(13)

where  is a small positive constant that is smaller than C. The expected cost function of the above

problem

is

F ()

=

1+ C +1

C

-

C - C+1



=

,

therefore,

one

should

decrease



to

minimize

the

loss.

Reddi et al. (2018) prove that when C is large enough, the expectation of accumulated parameter

update in Adam is positive and results in increasing .

To conduct more rigorous study on the stochastic online optimization problem in Equation 13, we derive the expectation of the net update factor for each gradient in the following theorem:

4

Under review as a conference paper at ICLR 2019

Theorem 4 (Unbalanced net update factor in stochastic online optimization problem). In the stochastic online optimization problem in Equation 13, assuming t = 1, the expectation of net update factors are as follows:

k(C) =

 t=0

(1

-

1)1t

 +1
(1-2)2t C2+(1+2t+1-2k)E[gi2]

Dt

8[(1-2

)2t

C

2

+(1+2t+1

-2t

)E[gi2

]]

5 2

,

and

(14)

k(-1) =

t=0(1 - 1)1t

 +1
(1-2)2t +(1+2t+1-2k)E[gi2]

Dt

8[(1-2

)2t

+(1+2t+1

-2t

)E[gi2

]]

5 2

,

(15)

where k(C) denotes the net update factor for gi = C and k(-1) denotes the net update factor for gi = -1. Dt is a positive value.

Though the formulas of net update factors in the stochastic case are more complicated than those in deterministic case, the analysis is actually more easier: the gradients with the same scale share the same expected net update factor, so we only need to analyze k(C) and k(-1). We can see that each term in the infinite series of k(C) is smaller than the corresponding one in k(-1), therefore, the accumulated influence of gradient C is smaller than gradient -1.
The above observation can also be interpreted as a direct consequence of the inappropriate correlatkbiu(ogtnta)blse=otwtheeeine=nvtttiraein/idnfigvnti:i(teg1is-veeqnu1ve)ntc1=ie-{tvn2iev}gti-a=t1tiv+peolys(i1ctio-vrerleyl2ac)tgoet2rsr,ewnlaiotttheosenawlcyihthvvtgi tpi.noSs{iitvniivc}eeil=tyhtec, ointreratellsuaoptednsaetgweaifttaihvcetgoltyr, correlates with gt. That is, k(gt) for a large gradient is likely to be smaller, while k(gt) for a small gradient is likely to be larger.
The unbalanced net update factor causes the non-convergence problem of Adam as well as all other adaptive learning rate methods where vt correlates with gt. All these counterexamples follow the same pattern: the large gradient is along the "correct" direction, while the small gradient is along the opposite direction. Due to the fact that the accumulated influence of a large gradient is small while the accumulated influence of a small gradient is large, Adam may update parameters along the wrong direction. Even if Adam updates parameters along the right direction in general, the unbalanced net update factors are still unfavorable since they slow down the convergence.

4 THE PROPOSED METHOD: DECORRELATION VIA TEMPORAL SHIFTING
According to the previous discussion, we conclude that the main cause of the non-convergence of Adam is the inappropriate correlation between vt and gt. Currently we have two possible solutions: (1) making vt act like a constant, which declines the correlation, e.g., using a large 2 or keep vt nondecreasing (Reddi et al., 2018); (2) using a large 1 (Theorem 1), where the aggressive momentum term helps to mitigate the impact of unbalanced net update factors. However, neither of them solves the problem fundamentally.
The dilemma caused by vt enforces us to rethink its role. In adaptive learning rate methods, vt plays the role of estimating the second moments of gradients, which reflects the scale of gradient on average. With the adaptive learning rate t/ vt, the update step of gt is scaled down by vt and achieves rescaling invariance with respect to the scale of gt, which is practically useful to make the training process easy to control and the training system robust. However, the current scheme of vt, i.e., vt = 2vt-1 + (1 - 2)gt2, brings a positive correlation between vt and gt, which results in reducing the effect of large gradients and increasing the effect of small gradients, and finally causes the non-convergence problem. Therefore, the key is to let vt be a quantity that reflects the scale of the gradients, while at the same time, be decorrelated with current gradient gt. Formally, we have the following theorem:
Theorem 5 (Decorrelation leads to convergence). For any fixed online optimization problem with infinitely repeating of a finite set of cost functions {f1(), . . . , ft(), . . . fn()}, assuming 1 = 0 and t is fixed, we have, if vt follows a fixed distribution and is independent of the current gradient gt, then the expected net update factor for each gradient is identical.
5

Under review as a conference paper at ICLR 2019

Let Pv denote the distribution of vt. In the infinitely repeating online optimization scheme, the expectation of net update factor for each gradient gt is

E[k(gt)]

=

 i=t

Evi

Pv

[

i vi

(1

-

1

)1i-t

].

(16)

Given Pv is independent of gt, the expectation of the net update factor E[k(gt)] is independent of gt and remains the same for different gradients. With the expected net update factor being a fixed constant, the convergence of the adaptive learning rate method reduces to vanilla SGD.

Momentum (Qian, 1999) can be viewed as setting vt as a constant, which makes vt and gt independent. Furthermore, in our view, using an increasing 2 (AdamNC) or keeping v^t as the largest vt (AMSGrad) is also to make vt almost fixed. However, fixing vt is not a desirable solution, because it damages the adaptability of Adam with respect to the adapting of step size.

We will next introduce the proposed solution to make vt independent of gt, which is based on temporal independent assumption among gradients. The proposed solution decorrelates vt and gt with a simple temporal shifting.

4.1 TEMPORAL DECORRELATION

In practical setting, ft() usually involves different mini-batches xt, i.e., ft() = f (; xt). Given the randomness of mini-batch, we assume that the mini-batch xt is independent of each other and further assume that f (; x) keeps unchanged over time, then the gradient gt = f (; xt) of each mini-batch is independent of each other. Then we present the temporal decorrelation algorithm as
follows.

Algorithm 1 Temporal Decorrelation

Input: 0, g0, {ft()}Tt=1, {t}Tt=1 and 2 1: set v0 = 0
2: for t = 1 to T do

3: gt = ft(t)

4: 5:

vt t

= =

2vt-1 t-1 -

+ t

(1- / vt

2)gt2-1 · gt

6: end for

7: // we ignore the bias-correction and epsilon for the sake of clarity

The key change is that the update rule for vt now involves gt-1 instead of gt (line 4), which makes vt and gt temporally shifted and hence decorrelated. Note that in the sequential online optimization problem, the assumption "gt is independent of each other" does not hold. However, in the stochastic online optimization problem and practical neural network settings, our assumption generally holds.
4.2 TEMPORAL-SPATIAL DECORRELATION
In most optimization schemes, there exists many parameters, i.e., the dimension of  is high, thus gt and vt are also of high dimension. However, in Algorithm 1, vt is element-wisely computed; that is, we only use the i-th dimension of gt-1 to calculate i-th dimension of vt. In other words, it only makes use of the independence between gt-1[i] and gt[i], where gt[i] denotes the i-th element of gt.
Actually, in the case of high-dimensional gt and vt, we can further assume that all elements of gradient gt-1 at previous timestep are independent of the i-th dimension of gt. Thus all elements in gt-1 can be used to compute vt without introducing correlation. We propose introducing a function  over all elements of gt-1 to achieve this goal, i.e., vt = 2vt-1 + (1 - 2)[(gt-1)]2. For easy of reference, we name the elements of gt-1 other than gt-1[i] as the spatial elements of gt-1 and name  the spatial function or spatial operation.
There is no restriction on the choice of , and we use (x) = maxi x[i] for most of our experiments, which is shown to be a good choice. The maxi x[i] operation has a side effect that turns the adaptive learning rate vt into a shared scalar. An important thing here is that, we no longer interpret vt as the

6

Under review as a conference paper at ICLR 2019

second moment of gt. It is merely a random variable that is independent of gt, while at the same time, reflects the overall gradient scale. We leave further investigations on  as future work.
Algorithm 2 Block-wise Temporal-Spatial Decorrelation
Input: 0, g0, {ft()}tT=1, {t}Tt=1 and 2 1: set v0 = 0 2: for t = 1 to T do 3: gt = ft(t) 4: for i = 1 to M do 5: vt[i] = 2vt-1[i] + (1 - 2)[(gt-1[i])]2 6: t[i] = t-1[i] - t/ vt[i] · gt[i] 7: end for 8: end for 9: // if vt[i] outputs a scalar, it turns out to be "block-wise adaptive learning rate SGD"

4.3 BLOCK-WISE TEMPORAL-SPATIAL DECORRELATION
In practical setting, e.g., deep neural network,  usually consists of many parameter blocks, e.g., the weight and bias for each layer. In deep neural network, the gradient scales (i.e., the variance) for different layers tend to be different (Glorot & Bengio, 2010; He et al., 2015). Different gradient scales make it hard to find a learning rate that is suitable for all layers, when using SGD and Momentum methods. In traditional adaptive learning rate methods, they apply element-wise rescaling for each gradient dimension, which achieves rescaling-invariance and somehow solves the above problem. However, Adam sometimes does not generalize better than SGD (Wilson et al., 2017; Keskar & Socher, 2017), which might relate to the excessive learning rate adaptation in Adam.
In our temporal-spatial decorrelation scheme, we can solve the "different gradient scales" issue more naturally, by applying  block-wisely and outputs a shared adaptive learning rate scalar vt[i] for each block. It makes the algorithm work like an adaptive learning rate SGD, where each block has an adaptive learning rate t/ vt[i] while the relative gradient scale among in-block elements keep unchanged. The corresponding algorithm is illustrated in Algorithm 2, where the parameters t including the related gt and vt are divided into M blocks. Every block contains the parameters of the same type or same layer in neural network.

4.4 INCORPORATING FIRST MOMENT: MOVING AVERAGING WINDOWS

First moment estimation, i.e., defining mt as a moving average of gt, is an important technique of modern first order optimization algorithms, which alleviates mini-batch oscillations. In this section,
we extend our algorithm to incorporate first moment estimation.

We have argued that vt needs to be decorrelated with gt. Similarly, when introducing the first moment estimation, we need to make vt and mt independent to make the expected net update factor unbiased. Based on our assumption of temporal and spatial independence, we further keep out the latest n gradients {gt-i}ni=-01, and update vt and mt via

vt = 2vt-1 + (1 - 2)[(gt-n)]2 and mt =

n-1 i=0

1i

gt-i

N -1 i=0

1i

.

(17)

In Equation 17, 1  [0, 1] plays the role of decay rate for temporal elements. It can be viewed as a truncated version of exponential moving average that only applied to the latest few elements. Since
we use truncating, it is feasible to use large 1 without taking the risk of using too old gradients. In the extreme case where 1 = 1, it becomes vanilla averaging.

The pseudo code of the algorithm that unifies all proposed techniques is presented in the Appendix with the following parameters: spatial operation , n  N+, 1  [0, 1], 2  [0, 1) and t.

Summary The key difference between Adam and the proposed method is that the latter temporally shifts the gradient gt for n-step, i.e., using gt-n for calculating vt and using the kept-out n gradients

7

Under review as a conference paper at ICLR 2019

to evaluate mt (Equation 17), which makes vt and mt decorrelated and consequently solves the non-convergence issue. In addition, based on our new perspective on adaptive learning rate methods, vt is not necessarily the second moment and it is valid to further involve the calculation of vt with the spatial elements of previous gradients. And we found that when the spatial operation  outputs a shared scalar for each block, the resulting algorithm turns out to be closely related to
SGD, where each block has an overall adaptive learning rate and the relative gradient scale in each
block is maintained. This is what we call "block-wise adaptive learning rate SGD". We name the proposed method that makes use of temporal-shifting to decorrelated vt and mt AdaShift, which means "ADAptive learning rate method with temporal SHIFTing".

5 EXPERIMENTS

In this section, we empirically study the proposed method and compare them with Adam, AMSGrad and SGD, on various tasks in terms of training performance and generalization. Without additional declaration, the reported result for each algorithm is the best we have found via parameter grid search. The anonymous code is provided at http://bit.ly/2NDXX6x.

5.1 ONLINE OPTIMIZATION COUNTEREXAMPLES

Firstly, we verify our analysis on the stochastic online optimization problem in Equation 13, where we set C = 101 and  = 0.02. We compare Adam, AMSGrad and AdaShift in this experiment. For fair comparison, we set  = 0.001, 1 = 0 and 2 = 0.999 for all these methods. The results are shown in Figure 1a. We can see that Adam tends to increase , that is, the accumulate update of  in Adam is along the wrong direction, while AMSGrad and AdaShift update  in the correct direction. Furthermore, given the same learning rate, AdaShift decreases  faster than AMSGrad, which validates our argument that AMSGrad has a relatively higher vt that slows down the training. In this experiment, we also verify Theorem 1. As shown in Figure 1b, Adam is also able to converge to the correct direction with a sufficiently large 1 and 2. Note that (1) AdaShift still converges with the fastest speed; (2) a small 1 (e.g., 1 = 0.9, the light-blue line in Figure 1b) does not make Adam converge to the correct direction. We do not conduct the experiments on the sequential online
optimization problem in Equation 6, because it does not fit our temporal independence assumption. To make it converge, one can use a large 1 or 2, or set vt as a constant.

40

Adam beta1:0.0 AMSGrad beta1:0.0 AdaShift n:1 beta1:0.0

20

40 20

Adam beta1:0.9 beta2:0.999 Adam beta1:0.9 beta2:0.9999 Adam beta1:0.9999 beta2:0.999

t t

00

20 0.0

0.2 0.4 0.6 0.8 iterations

1.0 1e7

20 0.0

0.2 0.4 0.6 0.8 iterations

1.0 1e7

(a) Comparison of Adam, AMSGrad and AdaShift.

(b) Adam and Adam with large 1 and 2.

Figure 1: Experiments on stochastic counterexample.

5.2 LOGISTIC REGRESSION AND MULTILAYER PERCEPTRON ON MNIST

We further compare the proposed method with Adam, AMSGrad and SGD by using Logistic Regression and Multilayer Perceptron on MNIST, where the Multilayer Perceptron has two hidden layers and each has 256 hidden units with no internal activation. The results are shown in Figure 2 and Figure 3, respectively. We find that in Logistic Regression, these learning algorithms achieve very similar final results in terms of both training speed and generalization. In Multilayer Perceptron, we compare Adam, AMSGrad and AdaShift with reduce-max spatial operation (max-AdaShift) and without spatial operation (non-AdaShift). We observe that max-AdaShift achieves the lowest training loss, while non-AdaShift has mild training loss oscillation and at the same time achieves better

8

Under review as a conference paper at ICLR 2019

generalization. The worse generalization of max-AdaShift may be due to overfitting in this task, and the better generalization of non-AdaShift may stem from the regularization effect of its relatively unstable step size.

train loss test accuracy training loss test accuracy

2.0

SGD

1.5 Adam AMSGrad

1.0

max-AdaShift non-AdaShift

0.5

0.0 0

50000 100000 150000 iterations

(a) Training loss

0.9

0.8

0.7 0.6 0.5 0.4 0

SGD Adam AMSGrad max-AdaShift non-AdaShift
2000 4000 6000 8000 iterations

(b) Test accuracy

Figure 2: Logistic Regression on MNIST.

500 SGD
400 Adam
AMSGrad 300 max-AdaShift
200 non-AdaShift

100

00

20000 40000 60000

iteration

(a) Training loss

0.9

0.8

0.7 0.6 0.5 0

SGD Adam AMSGrad max-AdaShift non-AdaShift

20000 40000 iteration

60000

(b) Test accuracy

Figure 3: Multilayer Perceptron on MNIST.

5.3 DENSENET AND RESNET ON CIFAR-10

ResNet (He et al., 2016) and DenseNet (Huang et al., 2017) are two typical modern neural networks, which are efficient and widely-used. We test our algorithm with ResNet and DenseNet on CIFAR10 datasets. We use a 18-layer ResNet and 100-layer DenseNet in our experiments. We plot the best results of Adam, AMSGrad and AdaShift in Figure 4 and Figure 5 for ResNet and DenseNet, respectively. We can see that AMSGrad is relatively worse in terms of both training speed and generalization. Adam and AdaShift share competitive results, while AdaShift is generally slightly better, especially the test accuracy of ResNet and the training loss of DenseNet.

training loss

0.8 0.6 0.4 0.2 0.00

test accuarcy

Adam AMSGrad AdaShift

0.90 0.85

5000 10000 15000 20000 iterations

0.80 0.750

Adam AMSGrad AdaShift
5000 10000 15000 20000 iterations

Figure 4: ResNet on Cifar-10. Left: Training Loss. Right: Test Accuracy.

training loss

0.8

Adam

0.90

0.6

AMSGrad AdaShift

0.85

test accuarcy

0.4 0.2 0.00

0.80 0.75 50 100 150 0.700 iterations

Adam AMSGrad AdaShift

50 100 iterations

150

Figure 5: DenseNet on Cifar-10. Left: Training Loss. Right: Test Accuracy.

5.4 DENSENET WITH TINY-IMAGENET
We further increase the complexity of dataset, switching from CIFAR-10 to Tiny-ImageNet, and compare the performance of Adam, AMSGrad and AdaShift with DenseNet. The results are shown in Figure 6, from which we can see that the training curves of Adam and AdaShift are basically overlapped, but AdaShift achieves higher test accuracy than Adam. AMSGrad has relatively higher training loss, and its test accuracy is relatively lower at the initial stage.

9

Under review as a conference paper at ICLR 2019

training loss test accuarcy

3.0 0.5 Adam

2.5 AdaShift

AMSGrad

0.4

2.0

1.5 1.0 0

50 100 150 200 250 epoch

0.3 0.2 0

Adam AdaShift AMSGrad
50 100 150 200 250 epoch

Figure 6: DenseNet on Tiny-ImageNet. Left: Training Loss. Right: Test Accuracy.

5.5 GENERATIVE MODEL AND RECURRENT MODEL

We also test our algorithm on the training of generative model and recurrent model. We choose WGAN-GP (Gulrajani et al., 2017) that involves Lipschitz continuity condition (which is hard to optimize), and Neural Machine Translation (NMT) (Luong et al., 2017) that involves typical recurrent unit LSTM, respectively. In Figure 7a, we compare the performance of Adam, AMSGrad and AdaShift in the training of WGAN-GP discriminator, given a fixed generator. We notice that AdaShift is significantly better than Adam, while the performance of AMSGrad is relatively unsatisfactory. The test performance in terms of BLEU of NMT is shown in Figure 7b, where AdaShift achieves a higher BLEU than Adam and AMSGrad.

training loss bleu

2750 3000 3250

Adam AMSGrad AdaShift

20 15

3500

10 Adam AdaShift

3750 0

50000

100000

iteration

5 AMSGrad

0

5000

10000

15000

iterations

(a) Training WGAN Discriminator.

(b) Neural Machine Translation BLEU.

Figure 7: Generative and Recurrent model.

6 CONCLUSION

In this paper, we study the non-convergence issue of adaptive learning rate methods from the perspective of the equivalent accumulated step size of each gradient, i.e., the net update factor defined in this paper. We show that there exists an inappropriate correlation between vt and gt, which leads to unbalanced net update factor for each gradient. We demonstrate that such unbalanced step sizes are the fundamental cause of non-convergence of Adam, and we further prove that decorrelating vt and gt will lead to unbiased expected step size for each gradient, thus solving the non-convergence problem of Adam. Finally, we propose AdaShift, a novel adaptive learning rate method that decorrelates vt and gt via calculating vt using temporally shifted gradient gt-n.
In addition, based on our new perspective on adaptive learning rate methods, vt is no longer necessarily the second moment of gt, but a random variable that is independent of gt and reflects the overall gradient scale. Thus, it is valid to calculate vt with the spatial elements of previous gradients. We further found that when the spatial operation  outputs a shared scalar for each block, the resulting algorithm turns out to be closely related to SGD, where each block has an overall adaptive learning rate and the relative gradient scale in each block is maintained. The experiment results demonstrate that AdaShift is able to solve the non-convergence issue of Adam. In the meantime, AdaShift achieves competitive and even better training and testing performance when compared with Adam.

10

Under review as a conference paper at ICLR 2019
REFERENCES
Timothy Dozat. Incorporating nesterov momentum into adam. 2016. Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural
networks. In Proceedings of the thirteenth international conference on artificial intelligence and statistics, pp. 249­256, 2010. Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron C Courville. Improved training of wasserstein gans. In Advances in Neural Information Processing Systems, pp. 5767­5777, 2017. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. In Proceedings of the IEEE international conference on computer vision, pp. 1026­1034, 2015. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770­778, 2016. Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely connected convolutional networks. 2017. Nitish Shirish Keskar and Richard Socher. Improving generalization performance by switching from adam to sgd. arXiv preprint arXiv:1712.07628, 2017. Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. Minh-Thang Luong, Eugene Brevdo, and Rui Zhao. Neural machine translation (seq2seq) tutorial. https://github.com/tensorflow/nmt, 2017. Ning Qian. On the momentum term in gradient descent learning algorithms. Neural networks, 12 (1):145­151, 1999. Sashank J. Reddi, Satyen Kale, and Sanjiv Kumar. On the convergence of adam and beyond. In International Conference on Learning Representations, 2018. URL https://openreview. net/forum?id=ryQu7f-RZ. Tijmen Tieleman and Geoffrey Hinton. Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude. COURSERA: Neural networks for machine learning, 4(2):26­ 31, 2012. Ashia C Wilson, Rebecca Roelofs, Mitchell Stern, Nati Srebro, and Benjamin Recht. The marginal value of adaptive gradient methods in machine learning. In Advances in Neural Information Processing Systems, pp. 4148­4158, 2017. Matthew D Zeiler. Adadelta: an adaptive learning rate method. arXiv preprint arXiv:1212.5701, 2012.
11

Under review as a conference paper at ICLR 2019

33000
22550 200
C 20 150 11050
1500
50 50 0
22 110..000.000..82 00..642 00..604.040..82 10.0.0.60

50

40
C30
20

10

0

1.0 0.8

11 200..800.8

0.20.6 0.040..2400..4600..620.80.0 0.0

(a) Final result of  for sequential problem after 2000 updates, varied with 1 and 2.

30 25
C 20 15

50
40
C30
20

10
5
0 1.0
2 10.00.8 00..62 0.04.40.2 00..60

00..80

10
0 1.0 0.8 0.60.8
1 20.2 0.04.2 00..46
0.0

(b) Critical value of C with varying 1 and 2 under the sequential optimization setting.

750 500 250
0 250 500 750 1000 1250

0.0

2 10.2 0.4 0.6 0.8 1.0

1.0

0.8

0.6

0.4 0.2 0.0

(c) Final result of  for stochastic problem after 2000 updates, varied with 1 and 2.

47050
35500
32050
C 250 22505000 17550 110000 15250
0 0.0
2 21.0 00..280.04.06.600..48 0.2 1.0

10..00

00..80

1.0
1 100..26 0.04.4 0.06.20.80.0

(d) Critical value of C with varying 1 and 2 under the stochastic optimization setting.

Figure 8: Both 1 and 2 influence the direction and speed of optimization in Adam. Critical value of Ct, at which Adam gets into non-convergence, increases as 1 and 2 getting large. Leftmost two for the sequential online optimization problem and rightmost two for stochastic online problem.

A THE RELATION AMONG 1, 2 AND C

To provide an intuitive impression on the relation among C, d, 1, 2 and the convergence of Adam, we let C = d = 6, initialize 1 = 0, vary 1 and 2 among [0, 1) and let Adam go through 2000 timesteps (iterations). The final result of  is shown in Figure 8a. It suggests that for a fixed
sequential online optimization problem, both of 1 and 2 determine the direction and speed of Adam optimization process. Furthermore, we also study the threshold point of C and d, under
which Adam will change to the incorrect direction, for each fixed 1 and 2 that vary among [0, 1). To simplify the experiments, we keep d = C such that the overall gradient of each epoch being +1.
The result is shown in Figure 8b, which suggests, at the condition of larger 1 or larger 2, it needs a larger C to make Adam stride on the opposite direction. In other words, large 1 and 2 will make the non-convergence rare to happen.

We also conduct the experiment in the stochastic problem to analyze the relation among C, 1, 2 and the convergence behavior of Adam. Results are shown in the Figure 8c and Figure 8d and the observations are similar to the previous: larger C will cause non-convergence more easily and a larger 1 or 2 somehow help to resolve non-convergence issue. In this experiment, we set  = 1.

Theorem 6 (Critical condition). In the sequential online optimization problem Equation 6, let t being fixed, define S(1, 2, C, d) to be the sum of the limits of step updates in a d-step epoch:

S(1, 2, C)

d
lim
nd i=1

mvnndd++ii

.

(18)

Let S(1, 2, C) = 0, assuming 2 and C are large enough such that vt



C

+

1

=

(1

-

1d

)( 

2d

-

1d)(1

-

2) .

(1 - 1)( 2 - 1)(1 - 2d)

1, we get the equation: (19)

Equation 19, though being quite complex, tells that both 1 and 2 are closely related to the counterexamples, and there exists a critical condition among these parameters.

12

Under review as a conference paper at ICLR 2019

B THE ADASHIFT PSEUDO CODE

Algorithm 3 AdaShift: This algorithm incorporate the moving average window of gradients to the algorithm 2 and unify all proposed techniques. We use a queue Q to denote the average window with the length of n. P ush(Q, gt) denotes pushing vector gt to the tail of Q, while P op(Q) pops and returns the head vector of Q. And W is the weight vector calculated via 1

Input: 0, {ft()}tT=1, {t}tT=1, 1, 2, n and 

1: set v0 = 0

2: W = [1n-1, 1n-2, . . . , 1, 1]/

n-1 i=0

1n

3: for t = 1 to T do

4: gt = ft(t)

5: P ush(Q, gt)

6: if t > n then

7: gt-n = P op(Q) 8: mt = W · Q

9: for i = 1to M do

10: vt[i] = 2vt-1[i] + (1 - 2)[(gt-n[i])]2

11: t[i] = t-1[i] - t/ vt[i] · mt[i]

12: end for

13: end if

14: end for

C CORRELATION BETWEEN gt AND vt

In order to verify the correlation between gt and vt in Adam and AdaShift, we conduct experiments to calculate the correlation coefficient between gt and vt. We train the Multilayer Perceptron on MNIST until converge and gather the gradient of the second hidden layer of each step. Based on
these data, we calculate vt and the correlation coefficient between gt[i] and gt-n[i], between gt[i] and gt-n[j] and between gt[i] and vt[i] of the last 10 epochs using the Pearson correlation coefficient, which is formulated as follows:

=

n i=1

(Xi

-

X¯ )(Yi

-

Y¯

)

.

n i=1

(Xi

-

X¯ )2

n i=1

(Yi

-

Y¯

)2

To verify the temporal correlation between gt[i] and gt-n[i], we range n from 1 to 10 and calculate the average temporal correlation coefficient of all variables i. Results are shown in Table 1.

Table 1: Temporal correlation coefficient between gt[i] and gt-n[i].
n1 2 3 4 5  -0.000368929 -0.000989286 -0.001540511 -0.00116966 -0.001613395 n 6 7 8 9 10  -0.001211721 0.000357474 -0.00082293 -0.001755237 -0.001267641

To verify the spatial correlation between gt[i] and gt-n[j], we again range n from 1 to 10 and randomly sample some pairs of i and j and calculate the average spatial correlation coefficient of all the selected pairs. Results are shown in Table 2.
Table 2: Spatial correlation coefficient between gt[i] and gt-n[j].
n1 2 3 4 5  -0.000609471 -0.001948853 -0.001426661 0.000904615 0.000329359 n 6 7 8 9 10  0.000971337 -0.000644563 -0.00137805 -0.001147973 -0.000592037

13

Under review as a conference paper at ICLR 2019

To verify the correlation between gt[i] and vt[i] within Adam, we calculate vt and the average correlation coefficient between gt2 and vt of all variables i. The result is 0.435885276. To verify the correlation between gt-n[i] and vt[i] within non-AdaShift and between gt-n[i] and vt within max-AdaShift, we range the keep number n from 1 to 10 to calculate vt and the average correlation coefficient of all variables i. The result is shown in Table 3 and Table 4.
Table 3: Correlation coefficient between gt2-n[i] and vt[i] in non-AdaShift.
n1 2 3 4 5  -0.010897023 -0.010952548 -0.010890854 -0.010853069 -0.010810747 n 6 7 8 9 10  -0.010777789 -0.01075946 -0.010739279 -0.010728553 -0.010720019
Table 4: Correlation coefficient between gt2-n[i] and vt in max-AdaShift.
n1 2 3 4 5  -0.000706289 -0.000794959 -0.00076306 -0.000712474 -0.000668459 n 6 7 8 9 10  -0.000623162 -0.000566573 -0.000542046 -0.000598015 -0.000592707

D PROOF OF THEOREM 1
Proof.

With bias correction, the formulation of mt is written as follows

mt

=

(1 - 1) (1 - 1)

t i=1

1t-i

gi

t i=1

1t-i

=

t i=1

1t-i

gi

t i=1

1t-i

.

According to L'Hospitals rule, we can draw the following:

(20)

t

lim
1 1

i=1

1t-i

=

lim
1 1

1 - 1t 1 - 1

= t.

Thus,

lim mt =
1 1

t i=1

gi

.

t

According to the definition of limitation, let g =

t i=1
t

gi

,

we

have,



> 0, 1  (0, 1), such that

mt - g  < .

We set

to

be

|

g 2

|,

then

for

each

dimension

of

mt,

i.e.

mt[i],

g[i] 2



mt[i]



3g[i] 2

So, mt shares the same sign with g in every dimension.

Given it is a convex optimization problem, let the optimal parameter be , and the maximum step

size

is

t vt

G

that

holds

1/dG

<

t vt

G

<

2/dG, we have,

lim
t

t - 

<

2/dG.

(21)

Given ft()   G, we have ft() - ft() < 2/d < 2, which implies the average regret

T
R(T )/T = [ft(t) - ft()]/T < 2.
t=1

(22)

14

Under review as a conference paper at ICLR 2019

E PROOF OF THEOREM 2
Proof. Let 1, 2  [0, 1) , d  N , 1  i  d and i  N.

nd+i

mnd+i =(1 - 1)

1nd+i-j gj

j=1


n


nd+i-1

=(1 - 1) (C + 1) 1jd+i-1 -

1j 

j=0

j=0

=(1 - 1)

1

- 1(n+1)d 1 - 1d

1i-1(C

+

1)

-

1

- 1nd+i 1 - 1

1 =

- 1

1(n+1)d - 1d

(1

-

1)1i-1(C

+

1)

-

(1

-

1nd+i)

For a fixed d, as n approach infinity, we get the limit of mnd+i as:

Similarly, for vnd+i:

lim
nd

mnd+i

=

1 1

- -

1 1d

(C

+ 1)1i-1

-1

nd+i

vnd+i =(1 - 2)

2nd+i-j gj2

j=1


n


nd+i-1

=(1 - 2) (C2 - 1) 2jd+i-1 +

2j 

j=0

j=0

=(1 - 2)

1

- 1

2(n+1)d - 2d

2i-1(C

2

-

1)

+

1

- 2nd+i 1 - 2

1 =

- 1

2(n+1)d - 2d

(1

-

2)2i-1(C

+

1)

-

(1

-

2nd+i)

For a fixed d, as n approach infinity, we get the limit of vnd+i as:

lim
nd

vnd+i

=

1 1

- -

2 2d

(C

2

- 1)2i-1

+

1

.

F PROOF OF THEOREM 6

Proof. From theorem 2, we can get:

lim
nd

mvnndd++ii

=

1-1 1-1d

(C

+

1)1i-1

-

1

1-2 1-2d

(C 2

-

1)2i-1

+

1

We sum up all updates in an epoch, and define the summation as S(1, 2, C).

S(1, 2, C)

=

d i=1

lim
nd

mvnndd++ii

15

Under review as a conference paper at ICLR 2019

Assume 2 and C are large enough such that vt 1, we get the approximation of limit of vnd+i as:

lim vnd+i
nd



1 1

- -

2 2d

(C

2

- 1)2i-1

Then we can draw the expression of S(1, 2, C) as:

d
S(1, 2, C) =
i=1

1-1 1-2d

(C

+

1)1i-1

-

1

1-2 1-2d

(C 2

-

1)2i-1

d
=
i=1

1-1 1-1d

(C

+

1)1i-1

d
-

1-2 1-2d

(C 2

-

1)2i-1

i=1

1

1-2 1-2d

(C 2

-

1)2i-1

=

1 - 2d (1 - 2)2d-1

C C

+1 -1

·

1 - 1 1 - 1d

·

2d - 1d 2 - 1

-

(1

1 -

- 2d 2)2d-1

·

1 C2 -

1

2d - 1 2 - 1



=

1 - 2d (1 - 2)2d-1(C - 1)

(1

-(1-1)(1d2)d(-21d)-

C+ 1)

1

-

 C

+

2d- 1 1( 2 -

1)

Let S(1, 2, C) = 0, we get the equation about critical condition:

C

+1

=

(1

- 1d)( 2d

-

1d)(1

 - 2)

(1 - 1)( 2 - 1)(1 - 2d)

G PROOF OF THEOREM 3

Proof. First, we define Vi as:

1

Vi

= lim  nd vnd+i

=

1

1-2 1-2d

(C 2

-

1)2(i-1)

+

1

where 1  i  d and i  N. And Vi has a period of d. Let t = t - nd, then we can draw: 16

Under review as a conference paper at ICLR 2019


lim k(gnd+i) =
nd t=nd+i

(1 - 1)1t-nd-i

1-2 1-2d

(C 2

-

1)2(t-1)

%

d

+

1


=

(1 - 1)1t -i

t =i

1-2 1-2d

(C 2

-

1)2(t

-1) % d

+

1


=

ld+i-1
(1 - 1)1j -i · Vj

l=1 j =(l-1)d+i

 i+d-1

= 1(l-1)d

(1 - 1)1j -i · Vj

l=1 j =i

 d-1
= 1(l-1)d (1 - 1)1j · Vj+i
l=1 j=0


 d-1



= 1(l-1)d 1 (1 - 1)1j · Vj+i+1 + (1 - 1)(1 - 1d) · Vi

l=1 j=0



=1

·

lim
nd

k(gnd+i+1)

+

1(l-1)d(1 - 1)(1 - 1d) · Vi

l=1

Thus, we can get the forward difference of k(gnd+i) as:









lim k(gnd+i+1) - lim k(gnd+i) =

nd

nd

1(l-1)d (1 - 1)2

1j · Vj+i+1 + (1 - 1)2

1j · Vi

l=1 j=0

j=0

 d-1

=(1 - 1)2

1(l-1)d

1j · Vj+i+1 - Vi

l=1 j=0

Vnd+1i monotonically increases within one period, when 1  i  d and i  N. And the weigh 1j for every difference term Vj+i+1 - Vi is fixed when i varies. Thus, the weighted summa-

tion

d-1 j=0

1j

·

Vj+i+1 - Vi

is monotonically decreasing from positive to negative.

In other

words, the forward difference is monotonically decreasing, such that there exists j, 1  j  d and

lim k(gnd+1) is the maximum among all net updates. Moreover, it is obvious that lim k(gnd+1)

nd

nd

is the minimum.

Hence, we can draw the conclusion: 1  j  d, such that

lim k(C) = lim k(gnd+1) < lim k(gnd+2) < · · · < lim k(gnd+j)

nd

nd

nd

nd

and

lim k(gnd+j) > lim k(gnd+j+1) > · · · > lim k(gnd+d+1) = lim k(C),

nd

nd

nd

nd

where K(C) is the net update factor for gradient gi = C.

17

Under review as a conference paper at ICLR 2019

H PROOF OF THEOREM 4

Lemma 7. 1 For a bounded random variable X and a differentiable function f (x), the expectation of f (X) is as follows:

f E[f (X)] = f (E[X]) +

(E[X 2

])

D(X

)

+

R3

where D(X) is variance of X, and R3 is as follows:

(23)

R3

=

f [3]() 3

E(X

-

E[X ])3 +

+ f (E[X]) + f (E[X])(x - E[X])2 + f (X) dF (x)
|x-E[X ]|>c

(24) (25)

F (x) is the distribution function of X. R3 is a small quantity under some condition. And c is large enough, such that: for any > 0,

P (X  [E[X] - c, E[X] + c]) = P (|X - E[X]|  c)  1 -

(26)

Proof. (Proof of Theorem 4 ) In the stochastic online optimization problem equation 13, the gradient subjects the distribution as:

gi =

C, -1,

with

probability

p

:=

1+ C +1

;

,

with

probability

1-p

:=

C - C +1

.

(27)

Then we can get the expectation of gi :

E[gi] = 

E[gi2]

=

C2

·

1+ C +1

+

C C

- +1

=

C

+ (C

+ 1)

D[gi] = C + (C + 1) - 2

E[gi4] = C(C2 - C + 1) + (C - 1)(C2 + 1)

D[gi2] = C3 - 2C2 + C + (C3 - 3C2 - C - 1) - 2(C + 1)2

(28)
(29)
(30) (31) (32)

Meanwhile, under the assumption that gradients are i.i.d., the expectation and variance of vi are as following when nd  :

i

E[vi]

=

lim (1
i

-

2)

2i-j E[gj2]

=

lim (1
i

-

2i )E[gj2]

=

C

+

(C

+

1)

j=1

(33)

i

D[vi]

=

lim (1
i

-

2)

2i-j D[gj2]

=

lim (1
i

-

2i )D[gj2]

=

D[gj2]

j=1

(34)

Then, for the gradient gi, the net update factor is as follows:


k(gi) =
t=0

(1 - 1)1t 2t+1vi-1 + (1 - 2)2t · gi2 + (1 - 2)

t j=1

2t-j

gi2+j

It should to be clarified that we define

t j=1

2t-j gi2+j

equal

to

zero

when

t

=

0.

Then

we

define

Xt as:

t

Xt = 2t+1vi-1 + (1 - 2)2t · gi2 + (1 - 2) 2t-j gi2+j

j=1

1See detial in: https://stats.stackexchange.com/questions/5782/variance-of-a-function-of-one-randomvariable

18

Under review as a conference paper at ICLR 2019

t
E[Xt] =2t+1E[vi-1] + (1 - 2)2t · gi2 + (1 - 2) 2t-j E[gi2+j ]
j=1
=2t+1E[g2] + (1 - 2)2t · gi2 + (1 - 2t )E[g2] =(1 + 2t+1 - 2t )E[g2] + (1 - 2)2t · gi2

(35)
(36) (37)

D[Xt]

=22(t+1)D[vi-1]

+

(1

-

2)2(1 - 1 - 22

22t) D[gi2+j ]

=

22(t+1)

+

(1

-

2)2(1 - 1 - 22

22t)

D[g2]

(38) (39)

For

the

function

f (x)

=

1 x

:

3 · x-5/2 f (x) =
8

According to lemma 7, we can the expectation of f (Xt) as follows:

E[f (Xt)]

=

(E[Xt])-1/2

+

3 8

(E[Xt

])-5/2

·

D[Xt]

(40)

E[Xt] and D[Xt] are expressed by equation 35 and equation 38. Then we can obtain the expectation expression of net update factor as follows:

k(gi) =

(1 -  )  +
t=0

t 11

1 (1-2)2t gi2+(1+2t+1-2k)E[gi2]

3Dt

8[(1-2

)2t

gi2

+(1+2t+1

-2t

)E[gi2

]]

5 2

where Dt = D[Xk]. Then for gradient C and -1, the net update factor is as follows:

k(C) = and

(1 -  )  +
t=0

t 11

1 (1-2)2t C2+(1+2t+1-2k)E[gi2]

3Dt

8[(1-2

)2t

C

2

+(1+2t+1

-2t

)E[gi2

]]

5 2

k(-1) =

 t=0

(1

-

1)1t

 +1
(1-2)2t +(1+2t+1-2k)E[gi2]

3Dt

8[(1-2

)2t

+(1+2t+1

-2t

)E[gi2

]]

5 2

(41) (42) (43)

19


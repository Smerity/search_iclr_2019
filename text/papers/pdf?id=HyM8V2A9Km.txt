Under review as a conference paper at ICLR 2019

ACTRCE: AUGMENTING TEACHER'S ADVICE
Anonymous authors Paper under double-blind review

EXPERIENCE

VIA

ABSTRACT
Sparse reward is one of the most challenging problems in reinforcement learning (RL). Hindsight Experience Replay (HER) attempts to address this issue by converting a failure experience to a successful one by relabeling the goals. Despite its effectiveness, HER has limited applicability because it lacks a compact and universal goal representation. We present Augmenting experienCe via TeacheR's adviCE (ACTRCE), an efficient reinforcement learning technique that extends the HER framework using natural language as the goal representation. We first analyze the differences among goal representation, and show that ACTRCE can efficiently solve difficult reinforcement learning problems in challenging 3D navigation tasks, whereas HER with non-language goal representation failed to learn. We also show that with language goal representations, the agent can generalize to unseen instructions, and even generalize to instructions with unseen lexicons. We further demonstrate it is crucial to use hindsight advice to solve challenging tasks, but we also found that little amount of hindsight advice is sufficient for the learning to take off, showing the practical aspect of the method.

1 INTRODUCTION
Many impressive deep reinforcement learning (deep RL) applications rely on carefully-crafted reward functions to encourage the desired behavior. However, designing a good reward function is nontrivial (Ng et al., 1999), and requires a significant engineering effort. For example, even for the seemingly simple task of stacking Lego blocks, Popov et al. (2017) needed 5 complicated reward terms with different importance weights. Moreover, handcrafted reward shaping (Gullapalli & Barto, 1992) can lead to biased learning, which may cause the agent to learn unexpected and undesired behaviors (Clark & Amodei, 2016).
One approach to avoid defining a complicated reward function is to use a sparse and binary reward function, i.e., give only a positive or negative reward at the terminal state, depending on the success of the task. However, the sparse reward makes learning difficult.
Hindsight Experience Replay (HER) (Andrychowicz et al., 2017) attempts to address this issue. The main idea of HER is to utilize failed experiences by substituting with a fake goal in order to convert them to successful experiences. For their algorithm to work, Andrychowicz et al. made the non-trivial assumption that for every state in the environment, there exists a goal which is achieved in that state. As the authors pointed out, this assumption can be trivially satisfied by choosing the goal space to be the state space. However, representing the goal using the enormous state space is very inefficient and contains much redundant information. For example, if we want to ask an agent to avoid collisions while driving where the state is the raw pixel value from the camera, then there can be many states (i.e. frames) that achieve this goal. It is redundant to represent the goal using each state.
Therefore, we need a goal representation that is (1) expressive and flexible enough to satisfy the assumption in HER, while also being (2) compact and informative where similar goals are represented using similar features. Natural language representation of goals satisfies both requirements. First, language can flexibly describe goals across tasks and environments. Second, language representation is abstract, hence able to compress any redundant information in the states. Recall the previous driving example, for which we can simply describe "avoid collisions" to represent all states that satisfy this goal. Moreover, the compositional nature of language provides transferable features for generalizing across goals.
1

Under review as a conference paper at ICLR 2019

In this paper, we combine the HER framework with natural language goal representation, and propose an efficient technique called Augmenting experienCe via TeacheR's adviCE (ACTRCE; pronounced "actress") to a broad range of reinforcement learning problems. Our method works as follows. Whenever an agent finishes an episode, a teacher gives advice in natural language to the agent based on the episode. The agent takes the advice to form a new experience with a corresponding reward, alleviating the sparse reward problem. For example, a teacher can describe what the agent has achieved in the episode, and the agent can replace the original goal with the advice and a reward of 1. We show many benefits brought by language goal representation when combining with hindsight advice. The agent can efficiently solve reinforcement learning problems in challenging 2D and 3D environments; it can generalize to unseen instructions, and even generalize to instruction with unseen lexicons. We further demonstrate it is crucial to use hindsight advice to solve challenging tasks, but we also found that little amount of hindsight advice is sufficient for the learning to take off, showing the practical aspect of the method.
We note that our work is also interesting from a language learning perspective. Learning to achieve goals described in natural language is part of a class of problem called language grounding (Harnad, 1990), which has recently received increasing interest, as grounding is believed to be necessary for more general understanding of natural language. Early attempts to ground language in a simulated physical world (Winograd, 1972; Siskind, 1994) consisted of hard coded rules which could not scale beyond their original domain. Recent work has been using reinforcement learning techniques to address this problem (Hermann et al., 2017; Chaplot et al., 2017). Our work combines reinforcement learning and rich language advice, providing an efficient technique for language grounding.

2 BACKGROUND

2.1 REINFORCEMENT LEARNING AND DEEP Q-NETWORKS

We first review the traditional reinforcement learning setting, where an agent interacts with an

infinite-horizon, discounted Markov Decision Process (S, A, , P, r). Here, S is the state space,

A is the action space,  the discount factor, P the transition model and r is a reward function

r : S × A  R. We consider a policy (a|s) parameterized by . At time t, the agent chooses an

action at  A according to its policy (a|st) where st  S is the current state. The environment

in turn produces a reward r(st, at) and transitions to the next state st+1 according to the transition

probability return E[
denoted as

PQt(=s(t0s+t1,t|arst(t)s, at=,ta)E.t)T][hweigtih=oatrleosifp-tethcret(astogi,etanhite)i|spstoo=limcsyatx,piaamr=aizmeaettht]e,erwsexhpi.ceThctheised

-discounted action-value
the expected

cumulative function is
discounted

sum of rewards obtained by performing an action a at state s and following the policy  thereafter.

We denote  as the optimal policy such that Q (s, a)  Q(s, a) for every s  S, a  A and

policy . The optimal Q-function Q = Q satisfies the Bellman equation:

Q(s, a) = Es p(·|s,a)

r(s, a) +  max Q(s , a )
a A

(1)

Q-learning (Sutton & Barto, 2018) is an off-policy, model-free RL algorithm that is based on
the Bellman equation. The algorithm uses semi-gradient descent (Sutton & Barto, 2018) to minimize the squared Temporal Difference (TD) error: L = E(Q(st, at) - yt)2, where the yt = rt +  maxa A Q(st+1, a ) is the TD target. Deep Q-Network (Mnih et al., 2013) builds on the Q-learning algorithm and uses a neural network to approximate Q. It also uses a replay buffer as
well as a target network to improve training stability.

2.2 GOAL ORIENTED REINFORCEMENT LEARNING

We review the goal-oriented reinforcement learning framework following Schaul et al. (2015). We

augment the previously defined infinite-horizon, discounted Markov Decision Process with a goal

space G. A goal g is chosen from G and stays fixed for each episode. The goal induces a reward

function rg : S × A  R, that assigns reward to a state conditioned on the given goal. At time step t, the agent (at|st, g) chooses an action conditioned on the current state st, as well as the current goal

g. The agent's objective is to maximize the expected discounted cumulative return given the goal, i.e.,

E [

 t=0



tr(st,

at,

g)].

2

Under review as a conference paper at ICLR 2019

State St

Text
Vision Processing

Image Representation

Gated Attention

Value Network

"Go to the blue torch"
Natural Language Goal G

Language Processing

Goal  Embedding

Attention

Figure 1: The diagram illustrates our model architecture.

Value for
actions

2.3 HINDSIGHT EXPERIENCE REPLAY (HER)
We follow Andrychowicz et al. (2017) and consider a specific family of goal conditioned reward functions, where the reward is either 0 or 1, depending on the resulting state, rg(st, at) = fg(st+1), where fg : S  {0, 1}. In many scenarios, the function fg acts as a predicate that determines whether the agent is successful or not according to g, and only assigns positive reward at the terminal state. Hence the reward function is very sparse and makes it difficult for an agent to learn.
HER proposes a solution to this problem. The agent first collects an episode of experiences s0, a0, r1, ..., sT -1, aT -1, rT , sT under the goal g, where rT = rg(sT -1, aT -1) is the terminal reward. If rT is zero, one can replace g with g  G such that rg (sT -1, aT -1) = 1. With an off-policy algorithm, one is able to learn from the goal transformed experience. In order to flexibly relabel with a desirable goal, the authors assume that there exists a representation map from the state space to the goal space T : S  G, so that for each s  S, the corresponding goal representation satisfies the predicate fg (T(s)) = 1. However, such mapping is not simple to construct. Although the author pointed out a trivial construction can be done by taking G = S and fg(s) = [g = s], such goal representation is redundant and uninformative, and largely limits the applicability of the algorithm. Consider an example where the state is the first-person raw pixel observation in a 3-D environment, and the goal is to walk towards a particular object. There are many possible directions to approach the object from, which results in many different possible states that satisfy the goal. However, no subset of the raw pixel observation can abstractly represent the goal.

3 AUGMENTING EXPERIENCE VIA TEACHER'S ADVICE (ACTRCE)

For a goal oriented MDP, {S, G, A, , P, rg}, and a parameterized agent (at|st, g), our objective is

to maximize the expected discounted cumulative return, i.e., E[

 t=0

 t rg (st ,

at)]

for

each

g



G.

Following HER, we assume there exists one or more more goal representation mappings T : S  G,

which describe what goal is satisfied at each state. For example, in the original HER paper, the

mapping T is simply a subspace projection from the state. Such representation can work well for their

robotic tasks, where subset of the state (i.e., the coordinates of objects and body parts) can reasonably

well represent the goal. However, in general, such mapping can not abstract meaningful information

about the goal, and will cause redundancy.

Language provides an abstract representation for a goal and hence reduces redundancy in representation. This motivates our proposal to use natural language for representing the goal space. Concretely, for each state s  S, we define T as a teacher that gives an advice T(s), a natural language description of the goal that is achieved in the state s. To implement such a teacher, one can ask a human to look at a state and describe the goal in words, or generate a goal description automatically, as part of a narration or accessibility feature in a game. Given T, we can convert a failure trajectory to a successful one by relabeling the original goal with the teacher advice. To illustrate, say the original goal for the episode is "Go to the armor", but at this particular time step, the agent has reached the blue torch instead. The teacher then tells the agent that the goal that reflects the current state is "Go to blue torch". The agent can hence take the advice and use it as a positive experience.

Besides positive reward signals, we also find that negative experience is necessary for training. Hence, in addition to a teacher who describes the achieved goal with a positive reward, we also introduce a teacher who gives negative reward with a description of a goal that has not been achieved. We further consider the scenario where there is more than one goal that can be satisfied in a state, e.g.,

3

Under review as a conference paper at ICLR 2019

Algorithm 1 Augmenting Experience via Teacher's Advice (ACTRCE)

Given:

· an off-policy RL algorithm A, and replay buffer R.

e.g. DQN, DDPG, NAF, SDQN

· A language goal space G and a desired goal space Gd.

· A group of teachers {T}

for episode = 1, M do Sample a desirable goal description g  Gd and an initial state s0. for t = 0, T - 1 do

Sample an action at using the behavioral policy from A: at  b(st||g)
end for

|| denotes concatenation

for every teacher T do
Compute advice g = T(sT ). for t = 0, T - 1 do

Teacher's advice in natural language

r = rg (st, at) Store transitions {(st||g , at, r, st+1||g )} in R end for

HER

end for

Perform training with A and a minibatch B from the replay buffer. end for

the state that is described as "Reach a blue torch" may also be described as "Reach the largest blue object". Each different description of the goal corresponds to a different teacher (see more discussions and experiments on variations of teachers in Appendix E). Therefore, in general, there is a group of teachers of different kinds, each giving different advice. We then relabel the original goal with each advice and corresponding positive or negative reward, and augment the replay buffer with these trajectories. Algorithm 1 describes our proposed approach formally.
Note that in the above formulation, we assume a MDP setting, and let the teacher give advice based solely on the terminal state g = T(sT ). By contrast, in the POMDP setting, we ask the teacher to give advice based on the history of states and actions during the episode, i.e., g = T({s0, a0, . . . , sT }).
3.1 LANGUAGE REPRESENTATION
There remains a question of how we deal with the natural language goal representation, a sequence of discrete symbols. In this paper, we explore two standard ways to convert a language goal into a continuous vector representation for neural network training. One way is to represent each word as a one-hot vector, and use a recurrent neural network (RNN) to sequentially embed each word into its hidden state. We can then obtain some function of its hidden states (e.g., last hidden state, an attention over all hidden states) as the representation of the goal. The other way is to use a pre-trained language component obtained by other approaches and dataset (e.g. Mikolov et al. (2013)) to represent the given language instructions. Since a pre-trained sentence embedding defines a reasonable similarity metric among sentences, one can expect the agent to understand unseen lexicons that are closely related to the language goals in training. This allows the agent to gain better natural language understanding, and potentially be more robust to the language advice it gets from teachers (advice from humans can be quite noisy).
To integrate language representation of goals into the model, we design the architecture as follows. The architecture consists of 3 modules. One language component that takes an instruction and convert it into a continuous vector, and we apply an attention vector. The second component processes the observation to obtain a image representation using convolution neural networks. We then use gated attention from Chaplot et al. (2017) to fuse goal information and the observation. The third component then take the result of the fused representation and compute the output value. Figure.(1) shows the computational graph.
4 EXPERIMENTS
In the following experiments, we demonstrate the effectiveness of the proposed method and discuss qualitative differences from the baseline. We first describe the experimental set up with our 2 environments, KrazyGrid World and ViZDoom. In the subsequent sections, we investigate:
4

Under review as a conference paper at ICLR 2019

Success Rate Success Rate

1.0 Representation: ViZDoom (Single)

0.8

0.6

0.4

0.2

ACTRCE GRU ACTRCE OneHot

ACTRCE InferLite

0.00M 2M Fram4Mes 6M

(a)

Re1p.0resentation: ViZDoom (Composition)

0.8

0.6

0.4

0.2

ACTRCE GRU ACTRCE OneHot

ACTRCE InferLite

0.00M 16M Fra32mMes 48M 64M

(b)

Figure 2: Performance in average success rate during training, comparing between different sentence embedding methods for the single target and composition task on ViZDoom.

1. A comprehensive comparison between goal representations in hindsight advice, generalization, and semantic similarities. We compare different goal representations: a naive one hot vector for each instruction, a Gated Recurrent Unit (GRU) (Chung et al., 2014) that embeds the discrete language tokens, and a pre-trained word embedding. We show that as the number of instructions increases, the one hot approach does not scale as well as the GRU and pre-trained embedding. From visualizing the representation, we can see that structures emerge from GRU and pre-trained embeddings. We also demonstrate that pre-trained embedding representation can generalize to goals containing out of training vocabulary words.
2. Does the hindsight language advice help with learning? How much advice do we need? We show significant improvement in sample efficiency by using advice from teachers. In many of the challenging tasks, the agent cannot learn at all without teachers' advice. We also show that significant improvement can be achieved even if we provide limited amount of teachers' advice, showing low burden of the method in practice.
4.1 ENVIRONMENTS AND TRAINING
We tested our method in a 2D grid world (KrazyGrid World (Stadie et al., 2018)), as well as a 3D environment (ViZDoom (Chaplot et al., 2017)). We describe the environment and our modifications below.
KrazyGrid World (KGW) (Stadie et al., 2018): KrazyGrid World is a 2D grid environment. For our experiments we chose to use 4 functionality of tiles: Goal, Lava, Normal, and Agent. We added an additional colour attribute in: Red, Blue, Green. The desired goals in this environment is to reach goals of different colours. Appendix A lists all language goals and additional environment details.
ViZDoom(Kempka et al., 2016; Chaplot et al., 2017): The 3D learning environment is based on the first person shooter game Doom. The state is a raw pixel image from first person perspective of a room containing several random doom objects of various types, colours, and sizes. The goal for the episode is a natural language instruction in the form "Go to the [target attribute] [object]", such as "Go to the green torch". See Appendix B for list of instructions and more description of ViZDoom.
Compositions of goals. We refer language goals introduced in KGW and ViZDoom sections as singleton tasks. We further consider a set of more challenging tasks ­ tasks that are composed of singleton tasks. Given two singleton tasks A, B, we take composition function "and" and "or" to form two new tasks "A and B", and "A or B". The task "A and B" is considered as complete when the agent completes both A and B within an episode, and "A or B" is considered as complete when one of the task is achieved. In our experiments, we consider all combinations of singleton tasks with both compositions "and" and "or" for KGW, and "and" for ViZDoom.
Training details. We used the DQN algorithm as the base reinforcement learning algorithm in both environments. A detailed architecture description for all of our experienments can be found in Appendix C. We carried out multi environment training as follows. First we sample 16 different environments. We collected data from all 16 environments. We updated the agents with an average gradient. We then resampled 16 environments. Further training details can be found in Appendix D.
5

Under review as a conference paper at ICLR 2019

(a) GRU Embedding

(b) InferLite (c)One-hot Embedding

Figure 3: Comparison among different sentence embedding methods showing the pairwise correlation between the sentence embedding vectors for each of the singleton instructions. The darker the colour, the higher the correlation.

4.2 INVESTIGATING DIFFERENT TYPES OF GOAL REPRESENTATION
In this section, we investigate the hypothesis that using language for goal representation helps learning in various aspects. We compare language-based goal representations and a non-language goal representation. In summary, we show that when the difficulty of the tasks increases, language goal representations became more effective in providing learning signals. We also show how one can use language goal representation to generalize to unseen goals in training, whereas non-language goal representation could not. With a pre-trained sentence embedding, we can even generalize to instructions consisting of unseen lexicons, showing the robustness of the language goal representation. Three goal representations are described as follows,
Language Sentence Representation with GRUs: For each instruction, we represent each word as a one hot vector, and sequentially embed words into a GRU. We use take the last hidden state representation of the GRU for representing the instruction. We use the same GRU architecture from Chaplot et al. (2017), with hidden size 256.
Pre-trained Language Sentence Representation: We also consider models where the language component is pre-trained. We use InferLite (Kiros & Chan, 2018) as the pre-trained sentence representation and hold the parameters fixed during training. InferLite is a lightweight form of generic sentence encoder trained to perform natural language inference (Bowman et al., 2015; Williams et al., 2018) resembling InferSent (Conneau et al., 2017) but without the use of RNNs. The original sentence embedding vector is of dimension 4096, and we use a learned linear layer to project it down to 256 for keeping every other part of the model same as the other two.
One-Hot Representation: The non-language baseline represents each instruction as a one hot vector, which does not contain any language structure. We embed this one-hot goal representation to a vector with the same dimension as the GRU representation that is learned independently.
4.2.1 HOW DOES EACH GOAL REPRESENTATION PERFORM?
We use all three goal representations with hindsight advice for learning both singleton tasks and compositional tasks of ViZDoom environment. The singleton tasks consists of 7 objects, and the compositional tasks consists of 5 objects. The result of comparisons is plotted in Figure 2. We see that in an easier benchmark--singleton--tasks, agents using one-hot goal representations are able to learn as quickly as agents using the other two goal representations. However, in a much more challenging benchmark-- compositional­tasks, the one-hot representations can only achieve a 24% success rates, whereas ACTRCE with language representations can achieve 97%. Since one hot representations represent each goal independently, the agent is unable to generalize to any new unseen instructions. With language representation, we can easily generalize to unseen instructions that consists of seen lexicons. We reported the generalization results in Table 1, under "ZSL" (zero shot learning). Remarkably, with GRU language goal representation, the agent is able to generalize to unseen instructions 83% of the time, showing great generalization ability of language goal representations.
4.2.2 VISUALIZATION OF LEARNED EMBEDDINGS
We carried out a visualization analysis to see the statistical relations between the learned embeddings of goals. We took the model trained in the singleton VizDoom benchmark, where the agent was able

6

Under review as a conference paper at ICLR 2019

Method
DQN (GRU) DQN (InferLite) DQN (OneHot)
ACTRCE (GRU) ACTRCE (InferLite) ACTRCE (One Hot)

Single (7 objects) MT ZSL

Composition (5 objects) MT ZSL

0.08 ± 0.01 0.065 ± 0.007 0.115 ± 0.007 0.07 ± 0.04

0.05 ± 0.07 0.04 ± 0.06 0.10 ± 0.02 0.13 ± 0.07

0.035 ± 0.007

-

0.02 ± 0.03

-

0.80 ± 0.06 0.80 ± 0.01 0.80 ± 0.03

0.76 ± 0.03 0.76 ± 0.02
-

0.97 ± 0.04 0.83 ± 0.09

0.88 ± 0.05 0.62 ± 0.01

0.24 ± 0.05

-

Table 1: The table shows the averaged success rates, calculated over 100 episodes, in multitask and zero-shot scenario for the model trained with DQN versus DQN with ACTRCE. The standard deviations are calculated across 2 seeds. In the Multitask (MT) scenario, the goal for the episode is sampled from the training set of instructions but with a random environment initialization. For Zero-Shot (ZSL), the instruction is sampled from the held out test instruction set and with a random environment initialization.
to learn with all three goal representations. However, we find there are huge differences in learned embeddings. For each goal instruction, we extract the corresponding learned embedding for all three goal representation, which is the continuous vector before the attention layer (recall Fig.(1), all of size 256. We then calculated the correlation matrix for each goal representations. We plot all three correlation matrices in Figure.(3). The rows and columns are grouped by object type, then colour and size. We found that GRU and InferLite embeddings have very similar block-like structure, due to clustering of colours and shapes, while each one-hot goal embeddings share almost no correlation between each other. We further performed t-SNE (Maaten & Hinton, 2008) embeddings and observe meaningful clustering with language goal representations and sporadic embeddings for one-hot goals. See more details in Appendix H.1.

4.2.3 GENERALIZING TO UNSEEN LEXICONS WITH PRE-TRAINED WORD EMBEDDING
We now show how to use pre-trained embeddings to allow our model to generalize to unseen lexicons at test time through representation transfer, similar in spirit to DeViSE (Frome et al., 2013) that was used for image classification with unseen classes. We took the agent trained in singleton tasks with InferLite goal representations, and replaced one word in each original instruction with its synonym1. We evaluate the performance of the agent on these new instructions that contain unseen lexicons. The results are shown in Tab.(3). We find the agent is able to achieve tasks above 66% of time. The ability of understanding sentences of similar meanings become useful when one implements the method with advice comes from humans. Since humans can describe the same meaning in many different ways, understanding synonyms can improve the robustness of learning in general noisy settings.

Method

MT + Synonym ZSL + Synonym

ACTRCE (InferLite) 0.66 ± 0.05 0.62 ± 0.02

Table 2: Multitask evaluation with synonym and Zero-shot with synonym using InferLite.

4.3 DO WE NEED HINDSIGHT ADVICE?
In the previous section, we demonstrate the effectiveness of language goal representation from various perspectives. In this section, we show how hindsight advice plays an important role in learning. We compared our method (denoted as "ACTRCE") to the algorithm DQN, the base algorithm without any hindsight language advice. We show that without hindsight advice, DQN is not able to learn many challenging tasks. However, we also found that little advice (1%) is sufficient for learning to take off, showing the practicality of our method. For language representation, we chose to use recurrent neural networks for embedding language goals, for both our method and the baseline method.
4.3.1 COMPARISON OF ACTRCE TO DQN
Singleton tasks. For experiments on KGW, we tried two different kind of grids, one with 3 lavas and the other with 6 lavas, and both with 3 goals of different colours. Fig. 4 (a) and (b) show the
1See Appendix B.4 for details on the synonyms used.

7

Under review as a conference paper at ICLR 2019

Success Rate

KrazyGrid World 3 goals 3 lavas
0.8

0.6

0.4

0.2 DQN

0.00M

ACTRCE 8M Fr1a6mMes 24M 32M

(a)

1.0 ViZDoom Single (7 objects)

Success Rate

KrazyGrid World 3 goals 6 lavas
0.8

0.6

0.4

0.2 DQN

0.00M

ACTRCE 8M Fr1a6mMes 24M 32M

(b)

1.0 ViZDoom Composition (5 objects)

Success Rate

KrazyGrid World compositional goals

0.8

0.6

0.4

0.2 DQN

0.00M

ACTRCE 5M Fr1a0mMes 15M 20M

(c)

1.0Limit Advice: ViZDoom (Single) GRU

0.8 0.8 0.8

Success Rate

Success Rate

0.6 0.6 0.6

0.4 0.4 0.4

DQN

0.2

DQN 0.2

DQN 0.2

ACTRCE GRU 1.0 ACTRCE GRU 0.1

ACTRCE

ACTRCE

ACTRCE GRU 0.01

0.00M

8M Fra16mMes 24M

32M 0.00M

16M Fra32mMes 48M

64M 0.00M

8M Fra16mMes 24M 32M

(e) (f) (g)

Success Rate

Figure 4: Performance comparisons on KGW and ViZDoom environments. The success rates are calculated over all desired goals and 16 different environments. Shaded area represents standard deviation 2 random seeds.

average success rate over all goals on 16-environments training. The shaded area represents the standard deviation over 2 random seeds. The baseline DQN is shown in blue curve, which failed to learn anything at all. Our method (shown in green) quickly learned and achieved good results on both environments (around 80% success rate). In ViZDoom experiments, ACTRCE helped the agent learn in a 7-objects environment in the hard mode, while the baseline DQN is unable to learn at all. Figure 4 (a) illustrates the training instruction average success rate (over 100 episode chunks) vs. the number of frames set, when using ACTRCE versus DQN baseline. Table 1 summarizes the agent's Multitask and Zero-Shot generalization performance.

Compositional tasks. In KGW, we carried out the experiments in a grid with 3 goals and 3 lavas. A comparison of average success rates over all goals of ACTRCE, and baseline DQN is shown in in Fig. 4 (C). The shaded area represents the standard deviation over 2 random seeds. We observed that the baseline still could not learn the task while ACTRCE learned efficiently and achieved good performance in this highly challenging environment. In ViZDoom, we chose to use 5 objects in easy mode for the compositional task, where all 5 objects appeared in front of the agent at the beginning of the episode. Figure 4 (b) shows the average success rate when using ACTRCE vs. baseline DQN. Likewise, our agent was able to learn very well with hindsight advice, whereas the baseline DQN failed to learn.

4.3.2 HOW MUCH ADVICE DO WE NEED?
We consider a variant of ACTRCE where the teacher provides hindsight advice only in the first {10%, 1%} of frames during training, and stops giving any advice for the remaining portion of the training. We perform this experiment in the ViZDoom environment single target mode with 7 objects, using the GRU as the sentence embedding. Figure 4 illustrates the training average success rate over the frames, and Table 4 lists the final MT and ZSL performance on the trained agents. We observe that the agent is still able to learn comparably well even given very little (1%) advice, indicating of the method's robustness in practical settings.
8

Under review as a conference paper at ICLR 2019
5 RELATED WORK
Several approaches have used natural language in the context of reinforcement learning. In pioneering work, Maclin & Shavlik (1994) translated language advice into a short program, implemented as a neural network. This advice network encouraged the policy to take the suggested action. Similarly, Kuhlmann et al. (2004) exploited natural language advice for a RL agent learning to play a (simulated) soccer game. In Ling & Fidler (2017), human feedback in the form of natural language was exploited to shape the reward of an image captioning RL agent. Kaplan et al. (2017) introduced an agent that learns to use English instructions to self-monitor its progress to beat Atari games. This was accomplished by implementing a multi-modal embedding of the pairs of game frames and instructions to determine if the instruction is satisfied, then provide additional reward to the agent. Andreas et al. (2017) proposed to use language as the latent parameter space for few-shot learning problems, including policy search.
The reverse has also been studied: using reinforcement learning to learn grounded language, referred to as task-oriented language grounding. Hermann et al. (2017) presented an agent that learns to execute written instructions in a 3D environment through reinforcement learning and unsupervised learning. Our work builds on Chaplot et al. (2017) who proposed a gated-attention architecture for combining the language and image features to learn a policy that execute written instruction in a 3D environment.
Our work also relates to augmenting the experience using a experience replay buffer. Lin (1992) introduced experience replay (ER) to speed up the credit assignment process. The technique was later popularized by DQN (Mnih et al., 2013) for Atari games. Further works focus on sampling techniques from the replay buffer to accelerate learning. Hindsight Experience Replay (Andrychowicz et al., 2017) builds on ER by providing additional experiences in a goal conditional task. Our work extends this technique to use a natural language goal representation.
6 CONCLUSION
In this work, we propose the ACTRCE method that uses natural language as a goal representation for hindsight advice. The main point of the paper is to show that using language as goal representations can bring many benefits, when combined with hindsight advice. We analyzed the differences among goal representation, and show that ACTRCE can efficiently solve difficult reinforcement learning problems in challenging 3D navigation tasks, whereas HER with non-language goal representation failed to learn. We also show that with language goal representations, the agent can generalize to unseen instructions. With pre-trained language component, the agent can even generalize to instructions with unseen lexicons, demonstrating its potential to deal with noisy natural language advice from humans. Although ACTRCE algorithm crucially relies on hindsight advice, we showed that little amount of advice is sufficient for the learning to take off, showing its great practicality.
REFERENCES
J. Andreas, D. Klein, and S. Levine. Learning with Latent Language. ArXiv e-prints, November 2017.
Marcin Andrychowicz, Dwight Crow, Alex K Ray, Jonas Schneider, Rachel Fong, Peter Welinder, Bob McGrew, Josh Tobin, Pieter Abbeel, and Wojciech Zaremba. Hindsight experience replay. In NIPS, 2017.
Samuel R Bowman, Gabor Angeli, Christopher Potts, and Christopher D Manning. A large annotated corpus for learning natural language inference. In EMNLP, 2015.
Devendra Singh Chaplot. Gated-attention architectures for task-oriented language grounding. https://github.com/devendrachaplot/DeepRL-Grounding, 2017.
Devendra Singh Chaplot, Kanthashree Mysore Sathyendra, Rama Kumar Pasumarthi, Dheeraj Rajagopal, and Ruslan Salakhutdinov. Gated-attention architectures for task-oriented language grounding. arXiv preprint arXiv:1706.07230, 2017.
J. Chung, C. Gulcehre, K. Cho, and Y. Bengio. Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling. ArXiv e-prints, December 2014.
9

Under review as a conference paper at ICLR 2019
Jack Clark and Dario Amodei. Faulty reward functions in the wild, 2016. URL https://blog. openai.com/faulty-reward-functions/.
Alexis Conneau, Douwe Kiela, Holger Schwenk, Loïc Barrault, and Antoine Bordes. Supervised learning of universal sentence representations from natural language inference data. In EMNLP, 2017.
Andrea Frome, Greg S Corrado, Jon Shlens, Samy Bengio, Jeff Dean, Tomas Mikolov, et al. Devise: A deep visual-semantic embedding model. In NIPS, 2013.
V. Gullapalli and A. G. Barto. Shaping as a method for accelerating reinforcement learning. In Proceedings of the 1992 IEEE International Symposium on Intelligent Control, pp. 554­559, Aug 1992. doi: 10.1109/ISIC.1992.225046.
Stevan Harnad. The symbol grounding problem. Physica D: Nonlinear Phenomena, 42(1):335 ­ 346, 1990. ISSN 0167-2789. doi: https://doi.org/10.1016/0167-2789(90)90087-6. URL http: //www.sciencedirect.com/science/article/pii/0167278990900876.
Hado van Hasselt, Arthur Guez, and David Silver. Deep reinforcement learning with double q-learning. In Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence, AAAI'16, pp. 2094­ 2100. AAAI Press, 2016. URL http://dl.acm.org/citation.cfm?id=3016100. 3016191.
K. M. Hermann, F. Hill, S. Green, F. Wang, R. Faulkner, H. Soyer, D. Szepesvari, W. M. Czarnecki, M. Jaderberg, D. Teplyashin, M. Wainwright, C. Apps, D. Hassabis, and P. Blunsom. Grounded Language Learning in a Simulated 3D World. ArXiv e-prints, June 2017.
Russell Kaplan, Christopher Sauer, and Alexander Sosa. Beating atari with natural language guided reinforcement learning. ArXiv e-prints, 04 2017.
Michal Kempka, Marek Wydmuch, Grzegorz Runc, Jakub Toczek, and Wojciech Jas´kowski. ViZDoom: A Doom-based AI research platform for visual reinforcement learning. In IEEE Conference on Computational Intelligence and Games, pp. 341­348, Santorini, Greece, Sep 2016. IEEE. URL http://arxiv.org/abs/1605.02097. The best paper award.
Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.
Jamie Ryan Kiros and William Chan. Inferlite: Simple universal sentence representations from natural language inference data. In EMNLP, 2018.
G. Kuhlmann, P. Stone, R. Mooney, and J. Shavlik. Guiding a reinforcement learner with natural language advice: Initial results in robocup soccer. In AAAI Workshop on Supervisory Control of Learning and Adaptive Systems, 2004.
Guillaume Lample. Arnold. https://github.com/glample/Arnold, 2017.
Guillaume Lample and Devendra Singh Chaplot. Playing fps games with deep reinforcement learning. In Proceedings of AAAI, 2017.
Long-Ji Lin. Self-improving reactive agents based on reinforcement learning, planning and teaching. Machine Learning, 8(3):293­321, May 1992. ISSN 1573-0565. doi: 10.1007/BF00992699. URL https://doi.org/10.1007/BF00992699.
Huan Ling and Sanja Fidler. Teaching machines to describe images via natural language feedback. In NIPS, 2017.
Thang Luong, Hieu Pham, and Christopher D. Manning. Effective approaches to attention-based neural machine translation. In EMNLP, 2015.
Laurens van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. In JMLR, 2008.
Richard Maclin and Jude W. Shavlik. Incorporating advice into agents that learn from reinforcements. In National Conference on Artificial Intelligence, pp. 694­699, 1994.
10

Under review as a conference paper at ICLR 2019
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. Distributed representations of words and phrases and their compositionality. In C. J. C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K. Q. Weinberger (eds.), Advances in Neural Information Processing Systems 26, pp. 3111­3119. Curran Associates, Inc., 2013. URL http://papers.nips.cc/paper/ 5021-distributed-representations-of-words-and-phrases-and-their-compositionality. pdf.
V. Mnih, A. Puigdomènech Badia, M. Mirza, A. Graves, T. P. Lillicrap, T. Harley, D. Silver, and K. Kavukcuoglu. Asynchronous Methods for Deep Reinforcement Learning. ArXiv e-prints, February 2016.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin A. Riedmiller. Playing atari with deep reinforcement learning. CoRR, abs/1312.5602, 2013.
Andrew Y. Ng, Daishi Harada, and Stuart J. Russell. Policy invariance under reward transformations: Theory and application to reward shaping. In Proceedings of the Sixteenth International Conference on Machine Learning, ICML '99, pp. 278­287, San Francisco, CA, USA, 1999. Morgan Kaufmann Publishers Inc. ISBN 1-55860-612-2. URL http://dl.acm.org/citation.cfm?id= 645528.657613.
Ivaylo Popov, Nicolas Heess, Timothy P. Lillicrap, Roland Hafner, Gabriel Barth-Maron, Matej Vecerik, Thomas Lampe, Yuval Tassa, Tom Erez, and Martin A. Riedmiller. Data-efficient deep reinforcement learning for dexterous manipulation. CoRR, abs/1704.03073, 2017.
Tom Schaul, Dan Horgan, Karol Gregor, and David Silver. Universal value function approximators. In Proceedings of the 32Nd International Conference on International Conference on Machine Learning - Volume 37, ICML'15, pp. 1312­1320. JMLR.org, 2015. URL http://dl.acm. org/citation.cfm?id=3045118.3045258.
Jeffrey Mark Siskind. Grounding language in perception. Artificial Intelligence Review, 8(5):371­391, Sep 1994. ISSN 1573-7462. doi: 10.1007/BF00849726. URL https://doi.org/10.1007/ BF00849726.
Bradly Stadie, Ge Yang, Rein Houthooft, Xi Chen, Yan Duan, Yuhuai Wu, Pieter Abbeel, and Ilya Sutskever. Some considerations on learning to explore via meta-reinforcement learning. ICLR, 2018.
Richard S. Sutton and Andrew G. Barto. Reinforcement Learning: An Introduction. MIT Press, Cambridge, MA, USA, 2nd edition, 2018. ISBN 9780262193986.
Adina Williams, Nikita Nangia, and Samuel Bowman. A broad-coverage challenge corpus for sentence understanding through inference. In NAACL, 2018.
Terry Winograd. Understanding natural language. Cognitive Psychology, 3(1):1 ­ 191, 1972. ISSN 0010-0285. doi: https://doi.org/10.1016/0010-0285(72)90002-3. URL http://www. sciencedirect.com/science/article/pii/0010028572900023.
11

Under review as a conference paper at ICLR 2019
A KRAZYGRID WORLD LANGUAGE GOALS
A.1 MORE DETIALS ON THE ENVIRONMENT
KrazyGrid World is a 2D grid environment. For our experiments we chose to use 4 functionality of tiles: Goal, Lava, Normal, and Agent. We added an additional colour attribute ranging in 3 colours: Red, Blue, Green. There are 4 discrete actions, "up", "down", "right", "left". The desired goals in this environment is to reach goals of different colours. We use a global view of the grid state as the observation to the agent. Each tile in the grid is represented by a concatenation of its functionality and colour attribute, both in one hot vectors. The grid state also includes a one hot vector that represents the agent's position. In our experiments, we use an environment of grid size 9 × 9, with 3 goals in the environment, each has a distinct colour. We tried various number of lavas in the environment, and made sure there is at least 1 lava of each colour. In the simple task setting, the episode terminates when the agent reaches a lava or a goal, or the episode runs over a maximum time step T = 25. We automatically generates descriptions give any states of the environment as the teacher.
Compositional We enlarged the goal space by considering the task of compositions of goals. As the goal space changed, we also had to make several modifications to the environment. We let the episode terminate when the agent reaches a lava or reaches 2 different goals, or runs over the maximum time step of 50. Since we still included simple goals (e.g., "Reach blue goal") in the goal space, we added an extra action called "flag". This action can be used for the agent to terminate the episode when it thinks the given goal is accomplished. The environment will be terminated if the agent indeed has accomplished the given task, and will stay unchanged otherwise.
A.2 LIST OF LANGUAGE INSTURCTIONS
Single goal setting The entire goal space G consists of 8 goals: {Reach red goal, Reach blue goal, Reach green goal, Reach red lava, Reach blue lava, Reach green lava, Avoid any lava, Avoid any goal}. The desired goal space Gd consists of 3 goals {Reach red goal, Reach blue goal, Reach green goal}.
Compositional goal setting The desired goal space Gd consists of 21 goals {Reach red goal, Reach blue goal, Reach green goal, Reach red goal and Reach blue goal, Reach blue goal and Reach red goal, Reach red goal or Reach blue goal, Reach blue goal or Reach red goal, Reach red goal and Reach green goal, Reach green goal and Reach red goal, Reach red goal or Reach green goal, Reach green goal or Reach red goal, Reach blue goal and Reach green goal, Reach green goal and Reach blue goal, Reach blue goal or Reach green goal, Reach green goal or Reach blue goal, Reach red goal and Avoid any lava, Avoid any lava and Reach red goal, Reach blue goal and Avoid any lava, Avoid any lava and Reach blue goal, Reach green goal and Avoid any lava, Avoid any lava and Reach green goal}.
The entire goal space G has another of 17 goals: {Reach red lava or Reach blue lava, Reach blue lava or Reach red lava, Reach red lava or Reach green lava, Reach green lava or Reach red lava, Reach blue lava or Reach green lava, Reach green lava or Reach blue lava, Reach red lava, Reach red lava and Avoid any goal, Avoid any goal and Reach red lava, Reach blue lava, Reach blue lava and Avoid any goal, Avoid any goal and Reach blue lava, Reach green lava, Reach green lava and Avoid any goal, Avoid any goal and Reach green lava, Avoid any lava and Avoid any goal, Avoid any goal and Avoid any lava}.
B VIZDOOM ENVIRONMENT DETAIL
B.1 ENVIRONMENT DESCRIPTION
ViZDoom(Kempka et al., 2016; Chaplot et al., 2017): The 3D learning environment is based on the first person shooter game Doom. At each time step, the state is a raw pixel image from first person perspective of the 3D environment. The environment consists of a room containing several random doom objects of various types, colours, and sizes. The agent can navigate the environment via 3 possible actions: turn left, turn right, and move forward. The goal for the episode is a natural language instruction in the form "Go to the [target attribute] [object]", such as "Go to the green torch". Only one target object is present in each episode. The episode terminates either when the
12

Under review as a conference paper at ICLR 2019

agent reaches an object (regardless of whether it is correct or not), or the maximum time step is reached (T = 30). A reward of 1 is given if the correct object is reached, otherwise a reward of 0 is given. The environment has several difficulty modes, depending on how the objects and the agent are distributed at the beginning of the episode. In the easy mode, the agent is spawned at a fixed location facing forward. The objects are placed evenly spaced apart along a horizontal row in front of the agent. In hard, both the objects and the agent are randomly spawned, and the objects are not necessarily in view. We focus on the easy and hard mode for our experiments.

B.2 VIZDOOM COMPOSITION INSTRUCTIONS
The compositional instructions consist of two single object instructions from the original training set joined by the word "and", such as "Go to the red torch and Go to the pillar".
We ensured that the desirable instructions were unambiguous­given two instructions, the set of objects satisfying the first instruction is mutually exclusive from the set of objects satisfied in the second instruction. For example, "Go to the torch and Go to the pillar" have mutually exclusive set of valid objects. An ambiguous compositional instruction may have objects that satisfy both instructions. For example, "Go to the blue object and Go to the torch" is ambiguous because a blue torch satisfies both instructions. This is illustrated in Figure 5.

Go to the red torch 

Go to the pillar

Go to the blue object 

Go to the torch

(a) (b)
Figure 5: Example of (a) unambiguous composition of instructions and (b) ambiguous composition of instructions
B.3 VIZDOOM COMPOSITION: MODIFICATION FOR STATE
For the ViZDoom composition task, we modify the raw pixel image of the environment to include a basic head-up display (HUD) consisting of black rectangle in the bottom left of the screen. When the agent reaches an object, a small thumbnail image of the reached object will appear inside the HUD. The HUD can show up to 2 objects reached, and the episode terminates once the again has reached a second object. Figure 6 illustrates what the agent sees as the input image throughout a composition task episode.

(a) (b) (c)
Figure 6: Example state input screens seen by the agent in a ViZDoom composition task episode. The composition instruction for this episode is "Go to the green short torch and Go to the green short pillar". (a) Shows the starting state. (b) is the state when the agent has reached the green short pillar. (c) is the final state once the agent reached the short green torch.
B.4 VIZDOOM SYNONYM INSTRUCTIONS
We generate the synonym instructions by replacing an original word in the instruction with one of the synonyms listed in Table 3.
13

Under review as a conference paper at ICLR 2019

Original Word
Go to largest smallest small large short tall red blue green yellow torch pillar skull key card armor object

Synonym
Reach, Approach, Get to biggest littlest tiny big, huge low, little big, towering crimson indigo, teal olive, lime amber, gold, sunny lamp, beacon, lantern, flaming object column, pedestal cranium, head opener, unlocker badge, pass shield thing, item

Table 3: Original vocabulary and corresponding synonyms

C ARCHITECTURES
KrazyGrid World
· Singleton We use series of convolution layers follows with ReLU activation functions for prepossessing the grid observation: 32 of 3x3 kernel with stride 1, followed by 64 of 3x3 kernel size with stride of 2 and then 64 of 3x3 kernel with a stride of 1 and 128 of 3x3 kernel with a stride of 2. We input the language sentences as word level one hot vectors to a LSTM of hidden size 128. The LSTM's last hidden vector is passed into a fully connected layer with 128 output units with sigmoid activation. This acts as the attention vector that is multiplied channel-wise with the 64 feature maps from the convolutional later. The gated feature maps are then flattened and passed through a fully connected layer with ReLU activation with 256 units. We then pass into a linear layer to predict the 4 action values.
· Compositional We use series of convolution layers follows with ReLU activation functions for prepossessing the grid observation: 32 of 1x1 kernel with stride 1, followed by 32 of 3x1 kernel size with stride of 2. We input the language sentences as word level one hot vectors to a bidirection LSTM, of hidden size 40. After obtaining the preprocessed observation, we augment with the history vector provided by the environment as a query to attend to all the hidden states in the Bi-direction LSTMs via Luong attention Luong et al. (2015). We then keep processing the observation using two more convolution layers follows with ReLU activation functions: 64 of 3x3 kernel with stride 2, followed by 64 of 3x3 kernel size with stride of 2. At each of these two layers, we use the context vector formed by language attention to attend back the observation by gating on 64 feature maps in a channel-wise fashion. The final gated feature maps are then flattened and passed through a fully connected layer with ReLU activation with 256 units. We then pass into a linear layer to predict the 5 action values.
ViZDooms Our architecture is almost identical to Chaplot et al. (2017), except that we uses a linear output layer for the action values, and we did not use dueling architecture. The state input to the network is RGB image of size 3 × 300 × 168. A series of convolution layers follows with ReLU activation functions: 128 of 8x8 kernel with stride 4, followed by 64 of 4x4 kernel size with stride of 2 and then 64 of 4x4 kernel with a stride of 2. To process the language input, we first use an embedding matrix to embed the voculary to a vector of size 32. We use the Gated Recurrent Unit (GRU) Chung et al. (2014) of size 256 as the recurrent cell to process word embeddings. The GRU's last hidden vector is passed into a fully connected layer with 64 output units with sigmoid activation. This acts as the attention vector that is multiplied channel-wise with the 64 feature maps from the convolutional later. The gated feature maps are then flattened and passed through a fully connected
14

Under review as a conference paper at ICLR 2019
layer with ReLU activation with 256 units. We then pass into the LSTM with 256 hidden units, and its hidden units go through a linear layer to predict the 3 action values.
D TRAINING DETAILS
KrazyGrid World For KrazyGrid World experiments, we tune the following hyperparameters within corresponding range: learning rate {0.0003, 0.001, 0.003}, replay buffer size of {5000, 10000}, training the network every {1, 2, 4} frames. We generate episodes with -greedy policy starting at = 1.0 then decaying linearly to 0.01 by 10000 frames and remain at 0.01. We use Double DQN Hasselt et al. (2016) to reduce the Q-value overestimation and Huber loss ( = 1) for stable gradients.
ViZDoom For ViZDoom environment, we use the same set of 56 training instructions to train the agent, and a set of 15 held out test instructions for zero-shot evaluation as in Chaplot et al. (2017). We also used their code Chaplot (2017) for reproducing training using Asynchronous Advantage Actor Critic (A3C) Mnih et al. (2016). We implemented our version of DQN on top of the implementation of Arnold Lample & Chaplot (2017); Lample (2017). The cyclic buffer replay buffer contains the last 10000 and 20000 most recent transitions for the easy and hard mode, respectively, chosen from the range {1000, 10000, 10000} and fine tuned. We generate episodes with -greedy policy starting at = 1.0 then decaying linearly to 0.01 by 10000 frames and remain at 0.01. We found that this performed better than using a larger epsilon = 0.1. Only after the first 1000 frames have been collected that the training begins, selected from a range of {1000, 10000, 10000}. We use Double DQN Hasselt et al. (2016) to reduce the Q-value overestimation and Huber loss ( = 1) for stable gradients. We use the Adam optimizer Kingma & Ba (2014) with a learning rate of 0.0001. The network is updated every 4 frames on easy and 16 frames on difficult mode. While updating less often reduces sample efficiency, in practice we found that this speeds up training wall clock time and converges to a better performance. When sampling from the replay buffer, we sample the start a random episode and select 32 consecutive frames from the replay buffer. This approach leads to more accurate estimates of the hidden state of the LSTM. In addition, the sequential mini-batch allows us to perform n-step Q learning as outlined in Mnih et al. (2016). The correlation between samples in the mini-batch is alleviated by running 16 parallel threads to send gradient updates to the shared network parameters. We synchronize the training thread model with the shared network each time before computing the training loss and gradient. The target network is synchronized with the current model once every 500 time steps. and use one additional thread to evaluate the multi-task success rate throughout the training process.
E DIFFERENT TYPES OF TEACHERS
In this section we described the details of teacher types. Firstly, note that not all of the language descriptions describe favorable behaviors. For example, in the KGW, a desirable goal is "reach a goal", but there are also undesirable states that corresponds to goal "reach a lava". Hence we consider a subset Gd  G to denote all desired goals that the agent is expected to perform. At each episode, we first sample a goal g  Gd from the set of all desired language goal spaces, and ask the agent to explore the environments conditioned on the goal. When the episode ends, we obtained an advice from a teacher T by observing the terminal state, g = T(sT ). Depending on the kind of teacher, we obtain a different kind of advice or no advice. We consider three kinds of teacher as follows:
· Optimistic A teacher who gives advice only when the agent achieved a desirable goal, i.e., when g  Gd. When the agent performs poorly, there is no advice from this teacher.
· Knowledgeable A teacher who describes what the agent has achieved in all scenarios, including the undesirable behaviors, as advice to the agent.
· Discouraging A teacher who describes a desired goal g  Gd that the agent has not achieved as advice to the agent. In this case, the trajectory with goal replaced by the teacher's advice will receive a reward of 0.
E.1 HOW DOES EACH TEACHER PERFORM?
In this section, we investigated how different teachers help in giving advice. We compared our method to the DQN algorithm. We denote the method using only optimistic and discouraging teachers as
15

Under review as a conference paper at ICLR 2019

"ACTRCE-". We denote the method using knowledgeable teachers as well as discouraging teachers as ACTRCE (this is the version we use in the main paper). We evaluated three methods in KrazyGrid World and the results are as follows.
We tried two different kind of grids, one with 3 lavas and the other with 6 lavas, and both with 3 goals of different colours. Fig. 4 (a) and (b) show the average success rate over all goals on 16-environments training. The shaded area represents the standard deviation over 2 random seeds. The baseline DQN is shown in blue curve, which failed to learn anything at all. "ACTRCE-" (shown in orange) quickly learned and achieved good results on both environments. However, we observed that when the number of lavas increased, the task became harder, and ACTRCE- performed worse after 32 millions frames of training (the performance dropped from 83% to 63%). On the other hand, we observed that having knowledgeable teachers always helped speed up learning. In particular, when the number of lavas increased, the task became harder for ACTRCE-. However, with knowledgeable teachers, language advice was provided even when the agent reached lava, and hence the amount of advice per step was kept the same in the more difficult setting. Therefore, we observed ACTRCE learning at a similar rate in the more difficult setting, leaving a big gap from ACTRCE- after 32 millions frames of training (80% vs 63%).

Success Rate Success Rate Success Rate

KrazyGrid World 3 goals 3 lavas
0.8

0.6

0.4

0.2

DQN ACTRCE

0.00M

ACTRCE 8M Fr1a6mMes 24M 32M

(a)

KrazyGrid World 3 goals 6 lavas
0.8

0.6

0.4

0.2

DQN ACTRCE

0.00M

ACTRCE 8M Fr1a6mMes 24M 32M

(b)

KrazyGrid World compositional goals

0.8

0.6

0.4

0.2

DQN ACTRCE

0.00M

ACTRCE 5M Fr1a0mMes 15M 20M

(c)

Figure 7: Performance comparisons on KrazyGrid World environments. The success rates are calculated over all desired goals and 16 different environments. Shaded area represents standard deviation 2 random seeds.

F CAN LEARNING EASY GOALS HELP LEARNING DIFFICULT GOALS?
As we mentioned, there are desirable tasks as well as undesirable tasks. One would worry what if the desirable tasks are very difficult, would the agent only learns to accomplish easier tasks which are undesirable? Hence, we would like to ask whether learning the easier tasks from hindsight can provide any learning signal for more difficult tasks. We hypothesize that it is possible in the KGW setting as learning to "reach lava" can aid learning controllers, which is used to "reach goals".
We designed the following transfer learning experiment to further our hypothesis. First, we artificially constructed a pessimistic teacher who only gives advice when the agent achieved undesirable goals, i.e., when g  G\Gd. We pretrained the agents with only the pessimistic teacher for 5 million frames. Now, we carried out training using ACTRCE- (with optimistic and discouraging teachers) with the pretrained agent and compared to unpretrained ones. Results are shown in Figure 8. We found that by pretraining the agent using pessimistic agents, even though the language goals in the pretraining phase have no overlapping with the actual training goals, the agent learned much faster than the unpretrained ones. In particular, in the environment with 3 lavas, the pretrained agent learned the fastest. In the environment with 6 lavas, pretrained agents learned as fast as ACTRCE, leaving a big gap from ACTRCE-, even though it was given the same amount of advice during training as ACTRCE-. This provides an evidence that learning easier goals can sometimes provide learning signals for harder goals, especially when both tasks require similar modules.
16

Under review as a conference paper at ICLR 2019

Success Rate Success Rate

3 goals 3 lavas Transfer Exp

0.8

0.6

0.4 DQN

0.2

ACTRCE ACTRCE

0.00M

Pretrained 8M Fr1a6mMes 24M 32M

3 goals 6 lavas Transfer Exp
0.8
0.6
0.4 DQN
0.2 ACTRCE ACTRCE
0.0 Pretrained 0M 8M Fr1a6mMes 24M 32M

Figure 8: Transfer experiments that demonstrate pre-training with easier goals can aid learning more difficult goals.
G ADDITIONAL DETAILS ON HINDSIGHT ADVICE ANNEALING

We provide the hindsight advice annealing on

Method ACTRCE (GRU) 1.0 ACTRCE (GRU) 0.1 ACTRCE (GRU) 0.01 ACTRCE (InferLite) 1.0 ACTRCE (InferLite) 0.1 ACTRCE (InferLite) 0.01 ACTRCE (OneHot) 1.0 ACTRCE (OneHot) 0.1 ACTRCE (OneHot) 0.01

MT 0.80 ± 0.06 0.75 ± 0.01 0.73 ± 0.04
0.80 ± 0.01 0.78 ± 0.04 0.76 ± 0.01
0.80 ± 0.03 0.74 ± 0.06 0.70 ± 0.06

ZSL 0.76 ± 0.03 0.74 ± 0.06 0.66 ± 0.05
0.76 ± 0.02 0.68 ± 0.01 0.68 ± 0.01
- - -

Table 4: Comparing the Multitask and Zero-shot Generalization when the agent has limited hindsight advice to only the beginning of training.

Li1m.0it Advice: ViZDoom (Single) InferLite

Li1m.0it Advice: ViZDoom (Single) One-Hot

0.8 0.8

0.6 0.6

0.4

DQN

0.2

ACTRCE InferLite 1.0 ACTRCE InferLite 0.1

ACTRCE InferLite 0.01

0.00M 8M Fra16mMes 24M 32M

(a)

0.4

DQN

0.2

ACTRCE OneHot 1.0 ACTRCE OneHot 0.1

ACTRCE OneHot 0.01

0.00M 8M Fra16mMes 24M 32M

(b)

Figure 9: Performance comparisons on VizDoom environment.

Success Rate Success Rate

17

Under review as a conference paper at ICLR 2019

GGGoGGooGtGooGGtoGooGGtGGoGGoottoGGtoooGGGGootootGGhootGGGGGttoGGGGoohGGtoooootootettthGGGGoottGooooeottooohoohooGooGtoottGGGooGGetGttGttoottoohsoooGeettGGhttttoooosGottohhttttGottoottoomttooGoeooooGohttolGoooomhheooottttoohhttteealsGGohhGGotttttootooaeohhtottotteettatosGttmtteehhthhrotGGotttteeoGlootattohhottooorgsoohhhhoeetghaGmhhhhltGohhtteetteeoolttgotlsglooohteesgoaoarteeehhethhrbsoeeelthtteeoeetttteeottottohgtheeehhraoghhhtGrbslorsroththheeoheetosslheattttglteGthgtsyGeorsgsueehhetyeGsrloohtbtrheeeheeosrottlattoheetoeoalleehmrueehattttteeorbttreehhrtdnteeeleeeoolrhoolsoseloreehhyyylaahbeattueehebtrdnltttleeltrdntllgeoggggtlsstttgorrlhlaarueehhtleeeeerssrsstalteehleldnoeetlgoggtttttogggrrrrrlrluehrrobbbgbbuehesssslolhhhhdneeelllsssoldneteeeeewlerrrtlwlllrrrbbblrrrrrdeerthhhhttttdlellelelehhhoooooooeeeeeeueeueeueeueeueeneehrrraaaatttttrrrtllltttoooossooowwwrrrrueeueeueehaaaasssseeeeeehdndndndndneeeeeokkkkaaandllllettttttrrrrllllrrrkkkkdndnndeeeelllldndndneeeeellltttttttllllolllooooooooooooooooooooooooooooaaauuuuyyyyttttttttttttttttttpppppppppppppbbbbbbbbbbbbbbbbbbbbbbbbbbbbbrrrllllcoooooooocooooocooocoollllmmmiiiiiiiiiiiiijjjjjjjjjjjjjjjjjjjjjjjjjjjjjkkkkaaaarrrrrrrrrrrrrrrrrrllllllllllllleeeeeeeeeeeeeeeeeeeeeeeeeeeeellllllllllllleeeeccccccccccccccccccrrrraaaaaaaaaaoaaoaoccccccccccccccccccccccccccccchhhhdhhhhhhhdhhhhhdhdhyyyyrrrrrrrrrrrrrrrrttttttttttttttttttttttttttttt

1.0

0.8 0.6 0.4 0.2

GGGoGGooGtGooGGtoGooGGtGGoGGoottoGGtoooGGGGootootGGhootGGGGGttoGGGGoohGGtoooootootettthGGGGoottGooooeottooohoohooGooGtoottGGGooGGetGttGttoottoohsoooGeettGGhttttoooosGottohhttttGottoottoomttooGoeooooGohttolGoooomhheooottttoohhttteealsGGohhGGotttttootooaeohhtottotteettatosGttmtteehhthhrotGGotttteeoGlootattohhottooorgsoohhhhoeetghaGmhhhhltGohhtteetteeoolttgotlgslooohteegsoaoarteeehhethhrsboeeelthtteeoeetttteeottottohgtheeehhraoghhhtGrbslorsroththheeoheetosslheattttglteGthtgsyGeorsgsueehhetyeGsrloohtbtrheeeheeorsottlattoheetoeoalleehmrueehattttteeortbtreehhtrdnteeeleeeoolrhoolsoseloreehhyyylaahbeattueehebtrdnltttleeltrdntllgeoggggtlsstttgorrlhlaarueehhtleeeeerssrsstalteehleldnoeetlgoggtttttogggrrrrrlrluehrrobbbgbbuehesssslolhhhhdneeelllsssoldneteeeeewlerrrtlwlllrrrbbblrrrrrdeerthhhhttttdlellelelehhhoooooooeeeeeeueeueeueeueeueeneehrrraaaatttttrrrtllltttoooossooowwwrrrrueeueeueehaaaasssseeeeeehdndndndnndeeeeeokkkkaaadnllllettttttrrrrllllrrrkkkkdndnndeeeelllldndnndeeeeellltttttttllllolllooooooooooooooooooooooooooooaaauuuuyyyyttttttttttttttttttpppppppppppppbbbbbbbbbbbbbbbbbbbbbbbbbbbbbrrrllllcoooooooocooooocooocoollllmmmiiiiiiiiiiiiijjjjjjjjjjjjjjjjjjjjjjjjjjjjjkkkkaaaarrrrrrrrrrrrrrrrrrllllllllllllleeeeeeeeeeeeeeeeeeeeeeeeeeeeellllllllllllleeeeccccccccccccccccccrrrraaaaaaaaaaoaaoaoccccccccccccccccccccccccccccchhhhdhhhhhhhdhhhhhdhdhyyyyrrrrrrrrrrrrrrrrttttttttttttttttttttttttttttt

0.0

1.0 0.8 0.6 0.4 0.2 0.0

GGoGoGGtoGtooGGGotooGttoGGGGootGGotoGGtooootooGGGGttoohtGGoGGGGoohtGGGtGGtottGGoooottooetthooooooGGGGetoottGoooohhooootoottttGGttooeGGGttsGGooooGhttttoooGettettttstthGttoooooGGttohhloooGttooooGoeooooottttlohGtttooooGeohhoomGooshhtteettattohhtttmoGttGttooooGatGttetotothhortlseettttaoteetthhhhottorGeeoooltaGGsoohhootoomGlgohhhottheettatlghlthhthhohhGeeteeooGsotlgrtosgmoohrteettogotaatseetteehhtthhtoeeebttheeoeeleeorotorhgeerertlthhtahsoslghthhttoobtsshthheeoteehoGtlaehrtsgerlttsrsyhtgoertseetytelgluhhGteooshGbeheeeehooaGoeerhottttletoalettrtrerltheearluheeoeeblhheeeeendeosrsohoooltletlmoterlttletlhyyyhataertahbeetuhtttleblendersrslondegggglglrltgotrrlthtaasssseeuhhtleeeetttlteaeeehorenderrrrrolrgggrlolllgggleuhssssoeblbbgbubhtleossslllhhhhendetlerrronderrrerlerlerlerlerlertttttbbbwedehhhheweedehhhoooooooerlerlerlttttterererteeeeeeeeeeuuuuunttteeaaaahrrsrrsoooollllooosssseeeeeeuuuhaaaaeeeeeehkkkkwenwetdttntwedentllndeltdnldoaaanderrrrrrrlllllllkkkkettntdetllneldnldetntdtlnldnldeeeeeoooooooooooooooooooooooooooooaaauuuuytttttttttttyttttytyttllllrrrpppppppppppppbbbbbbbbbbbbbbbbbbbbbbbbbbbbbllllccccooooooooooooooooooiiiiiiiiiiiiijjjjjjjjjjjjjjjjjjjjjjjjjjjjjlllllllllllllkkkkrrrrrrrrrrrrrrrrrraaaalllllllllmllmlmleeeeeeeeeeeeeeeeeeeeeeeeeeeeeccccrcccccccrccccrccrceeeeaaaaaaaoaaaoaoaaccccccccccccccccccccccccccccctytttttttthhhhtyttththrrtytttttdhhhhhyrrtttthhrrtrthhrthdrrthrrtrhdrrdr GGoGoGGtoGtooGGGotooGttoGGGGootGGotoGGtooootooGGGGttoohtGGoGGGGoohtGGGtGGtottGGoooottooetthooooooGGGGetoottGoooohhooootoottttGGttooeGGGttsGGooooGhttttoooGettettttstthGttoooooGGttohhloooGttooooGoeooooottttlohGtttooooGeohhoomGooshhtteettattohhtttmoGttGttooooGatGttetotothhortlseettttaoteetthhhhottorGeeoooltaGGsoohhootoomGlgohhhottheettatlghlthhthhohhGeeteeooGsotlgrtosgmoohrteettogotaatseetteehhtthhtoeeebttheeoeeleeorotorhgeerertlthhtahsoslghthhttoobtsshthheeoteehoGtlaehrtsgerlttsrsyhtgoertseetytelgluhhGteooshGbeheeeehooaGoeerhottttletoalettrtrerltheearluheeoeeblhheeeeendeosrsohoooltletlmoterlttletlhyyyhataertahbeetuhtttleblendersrslondegggglglrltgotrrlthtaasssseeuhhtleeeetttlteaeeehorenderrrrrolrgggrlolllgggleuhssssoeblbbgbubhtleossslllhhhhendetlerrronderrrerlerlerlerlerlertttttbbbwedehhhheweedehhhoooooooerlerlerlttttterererteeeeeeeeeeuuuuunttteeaaaahrrsrrsoooollllooosssseeeeeeuuuhaaaaeeeeeehkkkkwenwetdttntwedentllndeltdnldoaaanderrrrrrrlllllllkkkkettntdetllneldnldetntdtlnldnldeeeeeoooooooooooooooooooooooooooooaaauuuuytttttttttttyttttytyttllllrrrpppppppppppppbbbbbbbbbbbbbbbbbbbbbbbbbbbbbllllccccooooooooooooooooooiiiiiiiiiiiiijjjjjjjjjjjjjjjjjjjjjjjjjjjjjlllllllllllllkkkkrrrrrrrrrrrrrrrrrraaaalllllllllmllmlmleeeeeeeeeeeeeeeeeeeeeeeeeeeeeccccrcccccccrccccrccrceeeeaaaaaaaoaaaoaoaaccccccccccccccccccccccccccccctytttttttthhhhtyttththrrtytttttdhhhhhyrrtttthhrrtrthhrthdrrthrrtrhdrrdr

Figure 10: Correlation matrix for GRU (left) and InferLite (right). Best seen in electronic form.

H VISUALIZATION

For each single-target instruction in the training and test set (70 instructions), we obtain an embedding vector of dimension 256 using either GRU, InferLite, or One Hot (storing entire embedding). From this embedding matrix, we can visualize the learned instruction embedding in several ways.

H.1 EMBEDDING CORRELATION COMPARISON

For each pair of instructions i and j, we obtain their embeddings vectors vi and vj, and compute their correlation distance, cdist(vi, vj). We define the correlation distance between two vectors u and v as:

cdist(u,

v)

=

1

-

(u - u¯) · (v - v¯) ||(u - u¯)||2||(v - v¯)||

(2)

We divide each matrix by the maximum entry as to scale the results to [0, 1].

H.2 T-SNE PLOT COMPARISON
We use t-SNE to visualize the space of instruction embeddings. In the following figure, the shape of the datapoint indicates the type of object, such as triangle for armor, diamond for pillar, etc., and the colour indicate the object's colour, with black being used for when no colour is specified. Finally the size of the data point corresponds to the size indicated in the object, from 'smallest', 'small', none specified, 'large', and 'largest'. For the instructions with synonym word replacement, we simply leave the colour as black and the default size.

18

Under review as a conference paper at ICLR 2019

400 300 200 100
0 -100 -200 -300 -400 -500
-600

-400

-200

0

armor pillar torch keycard skullkey object
200 400 600

600 armor pillar torch keycard
400 skullkey object
200

0

-200

-400

-600 -600

-400

-200

0

200 400 600

500 400 300 200 100
0 -100 -200 -300
-600

-400

-200

0

armor pillar torch keycard skullkey object
200 400

150 armor pillar torch keycard skullkey
100 object

50

0

-50

-100

-150

-200 -150

-100

-50

0

50 100 150 200

Figure 11: t-SNE plots of instruction embeddings for GRU (top left), InferLite (top right), One-hot (bottom left) and Inferlite synonyms (bottom right). Best seen in electronic form.

19


Under review as a conference paper at ICLR 2019
A MORE GLOBALLY ACCURATE DIMENSIONALITY REDUCTION METHOD USING TRIPLETS
Anonymous authors Paper under double-blind review
ABSTRACT
We first show that the commonly used dimensionality reduction (DR) methods such as t-SNE and LargeVis poorly capture the global structure of the data in the low dimensional embedding. We show this via a number of tests for the DR methods that can be easily applied by any practitioner to the dataset at hand. Surprisingly enough, t-SNE performs the best w.r.t. the commonly used measures that reward the local neighborhood accuracy such as precision-recall while having the worst performance in our tests for global structure. We then contrast the performance of these two DR method against our new method called TriMap. The main idea behind TriMap is to capture higher orders of structure with triplet information (instead of pairwise information used by t-SNE and LargeVis), and to minimize a robust loss function for satisfying the chosen triplets. We provide compelling experimental evidence on large natural datasets for the clear advantage of the TriMap DR results. As LargeVis, TriMap is fast and scales linearly with the number of data points.
1 INTRODUCTION
Information visualization using dimensionality reduction (DR) is a fundamental step for gaining insight about a dataset. Motivated by the fact that the humans essentially think in two or three dimensions, DR has been studied extensively (Tenenbaum et al., 2000; Kohonen, 1998; Roweis & Saul, 2000; Hinton & Roweis, 2003; Maaten & Hinton, 2008). However, how to choose the best DR method for visualizing a given dataset is yet unresolved. Despite the plethora of quantitative measures such as trustworthiness-continuity (Kaski et al., 2003), mean (smoothed) precision-recall (Venna et al., 2010), and nearest-neighbor accuracy (Van Der Maaten et al., 2009), there is no standard procedure to assess the quality of a low-dimensional embedding in general. Also these measures only focus on the local neighborhood of each point in the original space and low-dimensional embedding and fail to reflect global aspects of the data. We will argue that selecting a DR method based on these local measures is misleading. In this paper, we mainly focus on the global properties of the data. These properties include the relative placements of the clusters in the dataset as well as revelation of outlier points that are located far away from the rest of the points in the high-dimensional space. It appears that quantifying the global properties is significantly more complicated and certainly local quality measures of DR (such as precision-recall1) do not capture these properties. Thus we resort to an evaluation based on visual clues that allows the practitioner to build confidence about the obtained DR results. To this end, we propose a number of transformations that happen naturally to real-world datasets. We argue that any DR method that claims to preserve the global structure of the data, should be able to handle these transformations. Ackerman et al. (2010) took a similar more theoretical approach for the task of clustering where certain natural properties are used to taxonomize different clustering algorithms. Next, we introduce a new DR method, called TriMap. The main idea behind TriMap is to capture higher orders of structure in the data by considering the relative similarities of a chosen set of triplets of points. The method minimizes a loss over triplets that measures how well a triplet is satisfied. This loss is made robust by capping it with a damping function. We provide compelling experimental
1We verified on small datasets that other common measures such as trustworthiness-continuity (Kaski et al., 2003) are also inadequate for evaluating the global structure. Also these measures are too expensive to calculate for moderately large datasets.
1

Under review as a conference paper at ICLR 2019

t-SNE

LargeVis

TriMap

PCA

? AUC = 0.130

AUC = 0.062

AUC = 0.036

AUC = 0.001

2.1.a AUC = 0.321

AUC = 0.233

AUC = 0.156

AUC = 0.001

2.1.b

AUC = 0.185

AUC = 0.074

AUC = 0.040

AUC = 0.002

Figure 1: DR tests: ?) full dataset, 2.1.a) a random %10 subset, 2.1.b) even digits only. The dotted line between the centers of the clusters `0' and `6' is drawn for comparing the distance between the cluster centers before and after removing the subsets from the dataset. Best viewed in color.

results on large natural datasets. We show that t-SNE and LargeVis, which are both based on preserving the pairwise (dis)similarities of the points, tend to form spurious clusters with clean boundaries and fail to properly reflect the global aspects of the data. On the other hand, TriMap's DR results preserve more of the global structure of the data as well as reveals outliers.

2 TESTING THE GLOBAL PERFORMANCE OF DR METHODS
Our proposed tests mimic the natural scenarios that happen in the real-world datasets. In each case we run the test on four DR methods: t-SNE, which is currently the most widely used DR method; LargeVis, a more recent method that is also based on pairwise (dis)similarities; TriMap, our new method based on triplet information; and PCA a global method which projects the data onto the two directions of largest variance. We defer the description of t-SNE & LargeVis as well as our new TriMap method to later sections. As a running example, we perform the tests on the MNIST dataset2 which contains 70,000 images of handwritten digits (represented by 784 dimensions)3. We normalized the data to have pixel values between [0, 1] and verified that the results are consistent over multiple runs of all methods. For each embedding, we calculated the area under the mean
2http://yann.lecun.com/exdb/mnist/ 3For ease of comparison, we use the same initial solution for t-SNE and TriMap, whenever possible. The current implementation of LargeVis does not support initial solutions.
2

Under review as a conference paper at ICLR 2019

t-SNE

LargeVis

TriMap

PCA

? AUC = 0.130

AUC = 0.062

AUC = 0.036

AUC = 0.001

2.2 AUC = 0.130

AUC = 0.062

AUC = 0.037

AUC = 0.001

2.3

AUC = 0.075

AUC = 0.050

AUC = 0.025

AUC = 0.001

Figure 1: (continued) DR tests: ?) full dataset, 2.2) outlier, 2.3) multiple scales. Best viewed in color. precision-recall curve (AUC) (Venna et al., 2010).4 Figure 1(?) shows the results on the full dataset by the four methods.
Partial observation test A DR tool should be invariant to removing a subset of points from the dataset. The placement of the remaining points (or clusters) after running the DR method on the reduced data set should remain relatively unchanged compared to the embedding created from the full dataset. In a first test, the subset of removed points are selected at random. This test mimics the fact that in reality we only see a sample from the data distribution and the DR method should be stable w.r.t. the sample size. Figure 1(2.1.a) shows the results after removing %90 of the dataset at random. t-SNE and TriMap both produce good results in that the clusters of the reduced datasets are located roughly in the same arrangement as in the full dataset. However, LargeVis moves the clusters around (e.g. cluster `1'). For labeled datasets, we might be interested in visualizing a particular subset of the classes, leading to our second test. Figure 1(2.1.b) gives the results on running the methods on just the subset of even digits of MNIST. As can be seen, only the TriMap method is able to preserve the relative distances between all the clusters after removing the odd digits, while t-SNE and LargeVis place the remaining clusters at arbitrary positions. Outlier test Natural datasets frequently contain outliers due to measurement or experimental errors. Detecting and removing the outliers is key step in statistical analysis that should be facilitated with the DR methods. We can check the ability of the DR methods to reveal outliers by adding artificial outlier
4More precisely, for each point we fix a neighborhood size of 20 in the high-dimension as the "relevant points" and vary the neighborhood size in the low-dimension between 1 and 100. This gives us a precision and recall curve for each point. The AUC measure is the area under the mean curve for all points.

3

Under review as a conference paper at ICLR 2019

points to the original dataset. Figure 1(2.2) shows the DR results after shifting point X (originally placed inside cluster `8' by all methods) far away in a random direction. The point X is clearly an outlier in the new dataset (as verified by the PCA result). Surprisingly, t-SNE and LargeVis both place the outlier inside the cluster `3', which happens to be the nearest cluster to this point in higher dimension. More disturbingly, adding a single outlier this way consistently rearranges the relative location of the clusters for t-SNE and LargeVis. TriMap shows the outlier and preserve the structure.

Multiple scales test A DR tool should be able to reflect the global structure of the data at different

scales. For instance, a dataset might consist of multiple clusters where each cluster itself may contain

multiple sub-clusters and so on. The practitioner can do a rudimentary test by duplicating and

shifting the natural dataset at hand. That is, for each point xn of the original dataset, we add a point

xn + c, where c guaranteed to be

is a far

fixed apart

random shift which is chosen large enough such that in the high-dimensional space5. When applied on the

the two copies are duplicated dataset,

the DR method should be able to show each copy separately. Figure 1(2.3) illustrates the results

on a duplicated MNIST dataset. We expect to see two identical copies of the same dataset in the

low-dimensional embedding, which can be verified by the PCA result. Curiously enough, both t-SNE

and LargeVis fail to provide any meaningful global structure6. In fact, both methods tend to split

some of the clusters into smaller sub-clusters. On the other hand, TriMap successfully recovers the

two identical copies, which look similar to the original dataset in Figure 1(?).

3 SKETCH OF THE T-SNE AND LARGEVIS
The t-SNE method (Maaten & Hinton, 2008) is perhaps the most commonly used DR method in practice. The main idea of t-SNE is to reflect the pairwise similarities of the points by minimizing a KL divergence between the normalized pairwise similarities in the high-dimensional space and the low-dimensional embedding. The t-SNE method has O(N 2) time complexity for N points. However, the complexity can be reduced to O(N log N ) by approximating the gradient using treebased algorithms (Van Der Maaten, 2014). t-SNE is easy to apply to general datasets and usually produces nicely separated clusters. Also it has been proven theoretically (Arora et al., 2018) that if the clusters are separated far enough in higher dimension, then t-SNE separates them in the embedding 7. However, as we showed in the previous section, the DR results produced by t-SNE are sometimes extremely misleading: the whole dataset is simply collapsed into an "orb" and the outliers are shown as "inliers". The latter can be explained by fact that for an outlier point, the pairwise similarities to the points that are closest to the outlier point dominate the rest, causing the method to pull back the outlier point to the closest cluster. LargeVis is a more recent method (Tang et al., 2016) that also aims to preserve the pairwise (dis)similarities of the points in the high-dimensional space in the low-dimensional embedding. To avoid the O(N 2) complexity of t-SNE, LargeVis uses a negative sampling approach (Mikolov et al., 2013) to randomly select a subset of the dissimilar pairs. However, the weights of all the dissimilar pairs are set to a positive constant. This causes the LargeVis to lose global information in the data, as we showed in the previous section. Overall, LargeVis forms well-separated clusters. However, the outlier points that are far away or in-between the clusters are pushed back into the closest clusters. This is a consequence of using a noisy distribution for negative sampling which tends to pick points from the denser regions with higher probability. For example, moving single points that lie between multiple large clusters inside the nearest cluster increases the likelihood of the model (because a single point has a very small probability of being selected as a dissimilar example by any of points that lie inside the large clusters).

4 THE NEW TRIMAP METHOD
The main idea in developing the TriMap method is to preserve a higher-order of structure in the data. In other words, we aim to reflect the relative (dis)similarities of triplets of points, rather than pairs of
5Similar results obtained by shifting the datasets in 4 directions. 6Verified with a large range of the perplexity parameter for t-SNE (from the default of 30 to 150). 7The proof requires that "early exaggeration" is used in all iterations of training.

4

Under review as a conference paper at ICLR 2019

points. Formally, a triplet is a constraint between three points i, j, and k, denoted as an ordered tuple (i, j, k), which indicates: "point i is more similar to point j than point k".

bgPoeraoalbiplseatmiorwfiFniosdremasliuomlwaitelairor-dintiymfeLunenstcio{tinxoannl}rbNnee=ptr1wesdeeeennntoaxtteiiotnahne{dyhnxig}jhnN-i=dn1imhfioegrnhsthidoeinsmealepnroesipinortesns(eainnntd2atDqi~oiojnro3fDN0).

points. Our Let p~ij 0 a similarity

function between yi and yj in low dimension. We denote by T be the set of all triplets (i, j, k) where

p~ij > p~ik, i.e., T = {(i, j, k) : p~ij > p~ik}. A low-dimensional embedding can be calculated by

minimizing the following objective

X min !ijk
{yn} (i,j,k)2T

q~ik q~ij

(1)

where

!ijk

is

the

weight

that

reflects

the

importance

of

the

triplet

(i,

j,

k)

and

the

ratio

q~ik q~ij

0 can be

seen as the loss associated with the triplet (i, j, k). Note that as q~ik becomes smaller and q~ij becomes

larger, the loss approaches zero.

Typically not all the triplets can be satisfied when mapping from high-dimension to lower-dimension,

i.e., we cannot have q~ij > q~ik, for all (i, j, k) 2 T . The issue arises because of the lower degree of freedom in the low-dimensional space. As a simple example, consider mapping uniformly distributed

points on a two-dimensional circle to a one-dimensional line; regardless of the choice of embedding,

the end points of line will always violate some triplet constraints. For a highly violated triplet (i, j, k)

with q~ik triplets.

Thuq~sijd,athmeptirnipgletht eloesfsfeqq~~ciikjt

will become too large and dominate the loss minimization over all of individual triplets is crucial whenever the triplets are sampled

from a high-dimensional space. We use properties of the generalized log and exp functions to define

robust versions of the weighted triplet loss: We propose the following objective

Loss transformation 3

Similarity function 1

X

min
{yn} (i,j,k)2T

!ijk

logt

1

+

q~i(kt0) ! q~i(jt0)

,

(2)

2 1

wlohgerte(xth) e=logtl(oxigs1(dxte)fine1d)/a(s1(Natu)dtsoi,ft2ht0e=0rw21)i:se.

00

024

024

`Ffo)irgaudnrideff(e2rr:ieg(nhltet)fvtsa)ilmLueoislsasorifttrytanafusnfndocrttm0io, anretiusopn!ec`ti!evxepllyto0.g(t

(1 + u2)

Nanodtegethnaetraloligztesisthceolnocgavfeunacntdionnownh-dicehcriesarseincogvered at t = 1. Thus Equation (2) includes the log-

transformation as a special case (see Figure 2(left, red)). with t > 1 grows slower than the log function and reaches

However, for x the value 1/(t

> 1, 1) in

the the

lliomgtit

function x ! 1.

Therefore, for t > 1, each triplet can contribute at most 1/(t 1) to the total loss and the objective

function of TriMap equation 2 is robust to violated triplets.

Also inspired by the good properties of the heavy-tailed distributions for low-dimensional embed-

ding (Maaten & Hinton, 2008; Yang et al., 2009), we define a parameterized similarity function

(

q~i(jt0) = expt0 ( kyi

yjk2), where expt0 (x) =

exp(x) [1 + (1

if t0 = 1 t0)x]1+/(1 t0) otherwise.

Here [ · ]+ := max(0, ·). Notice that the tail-heaviness of q~i(jt0) increases with the value of t0. That is, the Gaussian function is recovered at t0 = 1 and t0 = 2 amounts to the Student t-distribution with one degree of freedom. For t0 > 2, we recover distributions with even heavier tails. Figure 2(right) illustrates the q~i(jt0) function for several values of t0. In all our experiments, the default parameters for TriMap were set to t = t0 = 2, but we will briefly explore the other choices in Section 4.

Another crucial component of TriMap is the weight !ijk assigned to the triplet (i, j, k), quantifying the importance for satisfying (i, j, k):

!ijk

:=

p~ij p~ik

,

where p~ij = exp(

kxi xj k2 ).
ij

5

Under review as a conference paper at ICLR 2019

Figure 3: Effect of changing t and t0 parameters: top) results using fixed log-transformation (t = 1)

and different values of t0 for similarity function. Notice that having heavier-tail provides more

separability but cannot fix the clutter introduced by the unsatisfied triplets by itself, bottom) results

cfiuosxirenrdegs. pdNoifonftdeersetthnotantvohalaturvaeinsnsgofoflortmg(tai-.tetiro.anlnos(gEftoq-rlumoastasitoitnoranenqissufoactrrimuocnaita1ilo),tnot)

with t0 = 2 (i.e. Student t for similarities) obtain nice visualizations. Value of t = 0 = 1 recovers the log-transformation, t = 2

is used in our experiments. The boxes indicate the cases with acceptable choices of parameters by

our algorithm.

The scale factor i is set to the average distance of xi to its 10-th, 11-th, . . . 20-th nearest-neighbors. This choice of i adjusts the scale of p~ij to the density of the data (Zelnik-Manor & Perona, 2005).

Triplet sampling For N data points, the number of triplets is O(N 3). Thus the minimization of equation 2 with all the possible is too expensive. However, in practice, there exists a large amount of redundancy among the triplets. For instance, the two triplets (i, j, k) and (i, j, k0), in which k and k0 are located close together, convey the same information. We now develop a heuristic for sampling an O(N ) size subset of triplets, such that the mapping produced from the subset essentially reproduces the mapping derived from all triplets (evidence not shown for lack of space). We sample two types of triplets to form the embedding.

Nearest-neighbors triplets: For each point i, the closer point j is selected from m-nearest neighbors of the point and the distant points are sampled uniformly from the subset of points that are farther away from i than j. Note that this choice of triplets reflect the local as well as the global information of the data. For each point in the set of m-nearest neighbors, we sample m0 distant points which results in N  m  m0 triplets in total. In all our experiments, we use m = 50 and m0 = 10.

Random triplets: These triplets are sampled uniformly at random from the set of full triplet set T . Random triplets help preserve the global structure of the data. We add s randomly selected triplets for each points, which results in N  s triplets in total. In all our experiments, we set s = 5.

sInamppralecdtictrei,pdleivtsidainndgaedadcihngraaticoopnp~~iiskjtaantt

the end by the positive bias

maximum > 0 to all

ratio among the triplets in ratios improves the results.

the set of In all our

experiments, we set = 0.001.

Effect of changing t and t0 We show the effect of changing the parameters t and t0 in Figure 3. The experiments are performed on the USPS Digits dataset 8. In the first set of experiments, we fix t = 1 (i.e. use log-transformation) and increase the value of t0 = 2 (i.e. the tail-heaviness of the similarity function). It is clear from the results that having heavier-tail provides more separability but cannot fix the clutter introduced by the unsatisfied triplets. Next, we fix the similarity function to Student t-distribution (t0 = 2) and increase the value of t for the logt-transformation. Value of t = 0
8https://cs.nyu.edu/~roweis/data.html

6

Under review as a conference paper at ICLR 2019

t-SNE

LargeVis

i)

TriMap

ii) iii) iv)
v)
vi) Figure 4: Results of different methods: i) Epileptic Seizure Recognition, ii) Tabula Muris, iii) Fashion MNIST dataset, iv) TV News: commercial (yellow), non-commercial (blue), v) 380k+ lyrics, vi) DBLP dataset: collaboration network of authors. Best viewed in colors.
7

Under review as a conference paper at ICLR 2019
corresponds to no transformation (Equation equation 1) and t = 1 recovers the log-transformation. Note that larger values of t provides denser clusters and more separate clusters and reduces the clutter introduced by unsatisfied triplets. We use t = t0 = 2 in our experiments.
5 EXPERIMENTS
We evaluate the performance of TriMap for DR on a number of real-world datasets and compare the results to t-SNE and LargeVis9. We also compared our results to triplet embedding methods such as STE (van der Maaten & Weinberger, 2012). However, the results were clearly inferior and are omitted due to space constraints. The code for TriMap as well as the datasets for the experiments are available online10. In all our experiments, we use the default values t = t0 = 2. All experiments are performed on a single machine with 2.6 GHz Intel Core i5 processor and 16 GB RAM. We provide DR results on six datasets:
i) Epileptic Seizure Recognition11: contains 11,500 EEG recordings of patients under 5 different conditions. Each data point is one second of measurements with 178 samples / dimensions. ii) Tabula Muris12: contains 53,760 samples from 18 different mouse tissues in 23,433 dimensions. iii) Fashion MNIST13: consists of 70,000 examples. Each example is a 28  28 gray scale image (784 dimensions) from 10 classes of clothing items. iv) TV News14: contains audio-visual features extracted from 150 hours of TV broadcast from 5 different TV channels. The dataset contains 129,685 instances, labeled as commercial/non-commercial, and 4120 features. v) 380k+ lyrics15: the dataset contains lyrics of 380k+ different songs, gathered from http: //www.metrolyrics.com. We first form a weighted co-occurrence graph for the words based on the frequency of the pairs of words which occur together in a sliding window of length 5 words. Next, we compute the representation of each word using the LINE method Tang et al. (2015). We use number of negative samples equal to 5 and a learning rate of 0.025 with number of training samples equal to 1000M. We map each word to a 256 dimensional vector and then, calculate the representation of each song by a weighted average of the words that occur in the song. After removing the stop words and the songs with no available lyrics, we get 266,557 instances. vi) DBLP collaboration network16: the collaboration network of 317,080 authors. Two authors are connected in the network if they publish at least one paper together. We use LINE to find a 256 dimensional representation of the network before applying the methods. We set number of negative samples to 5, the learning rate to 0.025, and the number of training samples to 3000M. In our experiments, we use the sklearn implementation of fast t-SNE with the default parameters and learning rates. For LargeVis, we use the official implementation17 with the default setting of the learning equal to 1.0 and number of negative samples equal to 5. For TriMap, we reduce the dimensionality of data to 100 if necessary, using PCA and apply ANNOY18 (a random projection based approximate nearest neighbor search method) to calculate the nearest neighbors. However, in some cases the random projection based nearest neighbor search fails to correctly estimate the nearest neighbors of outlier points. This can be resolved by finding a small number (typically 5) of nearest neighbors using the Balltree algorithm and combining the results with ANNOY. Further details and
9Recently, a new DR method called UMAP was proposed by McInnes & Healy (2018). Although the running time of UMAP is comparable with TriMap, we did not include the UMAP results for two reasons: 1) the method is only described in an arXiv paper, 2) despite the different motivation, the results are strikingly similar to the LargeVis reported here.
10https://github.com/ANONYMOUS/trimap 11https://archive.ics.uci.edu/ml/datasets/Epileptic+Seizure+Recognition 12https://github.com/czi-hca-comp-tools/easy-data/blob/master/datasets/ tabula_muris.md 13https://github.com/zalandoresearch/fashion-mnist 14http://archive.ics.uci.edu/ml/datasets/tv+news+channel+commercial+ detection+dataset 15https://www.kaggle.com/gyani95/380000-lyrics-from-metrolyrics/data 16https://snap.stanford.edu/data/com-DBLP.html 17https://github.com/lferry007/LargeVis 18https://github.com/spotify/annoy
8

Under review as a conference paper at ICLR 2019

Dataset (size, dimension)
Epileptic Seizure (11,500, 178) Tabula Muris (53,760, 23,433) Fashion MNIST (70,000, 784) TV News (129,685, 4120) 380k lyrics (266,557, 256) DBLP (317,080,256)

t-SNE
0-00:07:34 1-11:25:22 0-03:49:33 1-23:12:49 0-17:12:40 1-02:46:08

LargeVis
0-00:06:55 0-1:59:53 0-00:15:15 0-02:04:04 0-01:03:02 0-01:18:15

TriMap
0-00:03:13 0-00:15:14 0-00:19:44 0-00:37:11 0-02:12:38 0-02:38:07

Table 1: Runtime of the methods in d-hh:mm:ss format. hints are provided in our implementation available online. We use m = 50, m0 = 5, and s = 5 for all datasets. The optimization is done with batch gradient descent and adaptive learning rate. Figure 4-i) illustrates the results on the Epileptic Seizure Recognition dataset. TriMap shows a smooth transition between measurements under different conditions while both t-SNE and LargeVis fail to preserve the global structure. Figure 4-ii) shows the results on the Tabula Muris dataset. As can be seen, TriMap reveals multiple outlier points which are not recovered using t-SNE and LargeVis. These outlier points can be verified by considering the ratio of the average distance of the top 5% farthest points from the center of the dataset to the average distance to the center of the remaining points. The actual ratio in high-dimensions equals to 5.32 where in low-dimension, equals to 1.50, 1.02, and 1.95 for t-SNE, LargeVis, and TriMap, respectively. The results on the Fashion MNIST dataset are shown in Figure 4-iii). t-SNE and LargeVis tend to create many spurious clusters. For instance, the cluster of "Bags" is divided into multiple smaller sub-clusters. On the other hand, TriMap represents a smooth embedding where each class in concentrated around the same region and there appears to be a smooth transitions among different classes. Figure 4-iv) shows the results on the TV News dataset. Bothe t-SNE and LargeVis tend to produce several patches of points that do not have a clear structure. TriMap on the other hand shows a smooth transition between the two classes with a few outliers that are present around the main cluster. Figure 4-v) shows the results on the 380k+ lyrics dataset. We use the 50 clusters in the high-dimensional space found by the k-means++ algorithm as labels. Unlike the other two methods, TriMap reveals multiple layers of structure with several outliers in the dataset. The main cluster that is shown both by t-SNE and LargeVis is also shown in the center of the map with TriMap. Finally, Figure 4-vi) shows the results on the DBLP dataset. As labels, we use the 50 clusters in the high-dimensional space found by the k-means++ algorithm. The results of LargeVis and TriMap are comparable. However, LargeVis tends to create more small clusters by grouping the outlier points that are close together. Finally, the runtime of the methods is shown in Table 5. TriMap is consistently faster than t-SNE and in some cases, outperforms LargeVis in terms of runtime. The main advantage of TriMap is evident on dataset with very large number of dimensions.

6 CONCLUSION
Dimensionality reduction will remain a core part of Machine Learning because humans need to visualize their data in 2 or 3 dimensions. We formulated a number of transformations and outlier tests that any practitioner can use to test the visualization method at hand. The main existing methods dramatically fail our tests. We also present a new DR method that performs significantly better. This new method called TriMap is based on triplet embedding but otherwise incorporates a number of redeeming features employed by the predecessors such as the use of the heavy tailed distributions and sub sampling for obtaining linear time complexity. Our results on wide range of datasets are quite compelling and suggest that further advances can be made in this area. Our next goal is to apply TriMap to data domains where we can test the quality of the obtained visualization and outlier detection by further physical experiments.

9

Under review as a conference paper at ICLR 2019
REFERENCES
Margareta Ackerman, Shai Ben-David, and David Loker. Towards property-based classification of clustering paradigms. In Advances in Neural Information Processing Systems, pp. 10­18, 2010.
Sanjeev Arora, Wei Hu, and Pravesh K. Kothari. An analysis of the t-SNE algorithm for data visualization. In Conference On Learning Theory, 2018.
Geoffrey E Hinton and Sam T Roweis. Stochastic neighbor embedding. In Advances in Neural Information Processing Systems, pp. 857­864, 2003.
Samuel Kaski, Janne Nikkil, Merja Oja, Jarkko Venna, Petri Trnen, and Eero Castrn. Trustworthiness and metrics in visualizing similarity of gene expression. BMC Bioinformatics, 4:48, 11 2003.
Teuvo Kohonen. The self-organizing map. Neurocomputing, 21(1):1­6, 1998. Laurens van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of Machine
Learning Research, 9(Nov):2579­2605, 2008. Leland McInnes and John Healy. UMAP: Uniform manifold approximation and projection for
dimension reduction. arXiv preprint arXiv:1802.03426, 2018. Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. Distributed representations
of words and phrases and their compositionality. In Advances in Neural Information Processing Systems, pp. 3111­3119, 2013. Jan Naudts. Deformed exponentials and logarithms in generalized thermostatistics. Physica A, 316: 323­334, 2002. URL http://arxiv.org/pdf/cond-mat/0203489. Sam T Roweis and Lawrence K Saul. Nonlinear dimensionality reduction by locally linear embedding. Science, 290(5500):2323­2326, 2000. Jian Tang, Meng Qu, Mingzhe Wang, Ming Zhang, Jun Yan, and Qiaozhu Mei. Line: Large-scale information network embedding. In Proceedings of the 24th International Conference on World Wide Web, pp. 1067­1077. International World Wide Web Conferences Steering Committee, 2015. Jian Tang, Jingzhou Liu, Ming Zhang, and Qiaozhu Mei. Visualizing large-scale and high-dimensional data. In Proceedings of the 25th International Conference on World Wide Web, pp. 287­297. International World Wide Web Conferences Steering Committee, 2016. Joshua B Tenenbaum, Vin De Silva, and John C Langford. A global geometric framework for nonlinear dimensionality reduction. Science, 290(5500):2319­2323, 2000. L. van der Maaten and K. Weinberger. Stochastic triplet embedding. In 2012 IEEE International Workshop on Machine Learning for Signal Processing, pp. 1­6, Sept 2012. doi: 10.1109/MLSP. 2012.6349720. Laurens Van Der Maaten. Accelerating t-sne using tree-based algorithms. Journal of Machine Learning Research, 15(1):3221­3245, 2014. Laurens Van Der Maaten, Eric Postma, and Jaap Van den Herik. Dimensionality reduction: a comparative. 2009. Jarkko Venna, Jaakko Peltonen, Kristian Nybo, Helena Aidos, and Samuel Kaski. Information retrieval perspective to nonlinear dimensionality reduction for data visualization. Journal of Machine Learning Research, 11(Feb):451­490, 2010. Zhirong Yang, Irwin King, Zenglin Xu, and Erkki Oja. Heavy-tailed symmetric stochastic neighbor embedding. In Advances in Neural Information Processing Systems, pp. 2169­2177, 2009. Lihi Zelnik-Manor and Pietro Perona. Self-tuning spectral clustering. In Advances in Neural Information Processing Systems, pp. 1601­1608, 2005.
10


Under review as a conference paper at ICLR 2019
DATASET DISTILLATION
Anonymous authors Paper under double-blind review
ABSTRACT
Model distillation aims to distill the knowledge of a complex model into a simpler one. In this paper, we consider an alternative formulation called dataset distillation: we keep the model fixed and instead attempt to distill the knowledge from a large training dataset into a small one. The idea is to synthesize a small number of data points that do not need to come from the correct data distribution, but will, when given to the learning algorithm as training data, approximate the model trained on the original data. For example, we show that it is possible to compress 60, 000 MNIST training images into just 10 synthetic distilled images (one per class) and achieve close to original performance with only a few steps of gradient descent, given a particular fixed network initialization. Apart from being an interesting new way to think about distillation, this approach could potentially open up several applications, such as fast domain adaptation and effective data poisoning attacks.
1 INTRODUCTION
Hinton et al. (2015) proposed network distillation as a way to transfer the knowledge from an ensemble of many separately-trained networks into a single, typically compact network, performing a type of model compression. In this paper, we are considering a related but orthogonal task: rather than distilling the model, we propose to distill the dataset. Unlike network distillation, we keep the model fixed but encapsulate the knowledge of the entire training dataset, which typically contains thousands to millions of images, into a small number of synthetic training images. In fact, we show that we can go as low as one synthetic image per category, training the same model to reach surprisingly good performance on these synthetic images. For example in Fig. 1a, we compress 60, 000 training images of MNIST digit dataset into only 10 synthetic images (one per class), given a fixed network initialization. Training the standard LENET (LeCun et al., 1998) architecture on these 10 images yields test-time MNIST recognition performance of 94%, compared to 99% for the original task. For networks with unknown random weights, 100 synthetic images train to 77% with a few gradient descent steps. We name our method Dataset Distillation and these images distilled images.
But why is dataset distillation useful? First, there is the purely scientific question of how much data is really encoded in a given training set and how compressible it is? Second, we wish to know whether it is possible to "load up" a given network with an entire dataset-worth of knowledge by a few gradient descent steps on a handful of images. Third, on the practical side, if dataset distillation is successful, it may enable new applications such as fast domain adaptation (Fig. 1b). Finally, this technique could have repercussions for computer security, as it might allow for effective data poisoning attacks, e.g., presenting a few training images that will force the network to "forget" a category (Fig. 1c).
A key question is whether it is even possible to compress a dataset into a small set of synthetic data samples. For example, is it possible to train an image classification model on synthetic images that are not on the natural image manifold? Conventional wisdom would suggest that the answer is no, as the synthetic training data may not follow the same distribution as the real test data. Yet, in this work, we show that this is indeed possible. We present a new optimization algorithm for synthesizing a small number of synthetic data samples that not only capture much of the original training data but also are tailored explicitly to fast model training. To achieve our goal, we first derive the network weights as a differentiable function of our synthetic training data. Given this connection, instead of optimizing the network weights for a specific training objective, we can optimize the pixel values of our distilled images. However, this formulation requires access to the initial network weights. To relax this assumption, we develop a method for generating distilled images effective for networks with random initializations from a certain distribution. To further boost the performance, we propose
1

Under review as a conference paper at ICLR 2019

distill 60K images

10 images

Fixed init

train

13% accuracy

94% accuracy

distill 50K images

train

Fixed init

Fixed init

100 images

9% accuracy

(a) Dataset distillation on MNIST and CIFAR10

54% accuracy

distill 73K images

Trained for SVHN

train

60K images

100 images encoding domain difference

52% accuracy

96% accuracy

(b) Application: fast domain adaptation for SVHN  MNIST

distill

Trained for train CIFAR10

Attacked Model

50K images

300 attack images

82% accuracy on class "plane"

7% accuracy on class "plane"

(c) Application: fast data poisoning attack for CIFAR10

Figure 1: Dataset Distillation: we can distill the knowledge of tens of thousands of images into a few synthetic training images called distilled images. (a): On MNIST, 10 distilled images can train a standard LENET with fixed initialization to 94% test accuracy (compared to 99% when fully trained). On CIFAR10, 100 distilled images can train a deep network with fixed initialization to 54% test accuracy (compared to 80% when fully trained). (b): Utilizing pre-trained networks for SVHN, we can distill the domain difference between two SVHN and MNIST into 100 distilled images. These images can be used to quickly adapt a network trained for SVHN to achieve high accuracy on MNIST. (c): Our formulation can be used to create adversarial attack images. If a well-optimized network is trained with these images for one single gradient step, it will catastrophically misclassify a particular targeted class.

an iterative version, where we obtain a sequence of distilled images to train a model and each distilled image can be trained with multiple passes. Finally, we study the case of a simple linear model, deriving a lower bound on the size of distilled data required to achieve the same performance as training on full dataset.
Experiments on multiple datasets demonstrate that a handful of distilled images can be used to train a model with a fixed initialization to achieve surprisingly high performance. For a network with random initializations (e.g., different weights pre-trained on the same task), we can still find distilled images for fast model fine-tuning with our iterative method. Built on this main result, we show two practical applications including fast domain adaptation and fast data poisoning attack. We systematically compare our method with existing baselines and alternative approaches. Our code and models will be available upon publication.

2 RELATED WORK
Knowledge Distillation The main inspiration for this paper is network distillation (Hinton et al., 2015), a widely used technique in ensemble learning (Radosavovic et al., 2018) and model compression (Ba & Caruana, 2014; Romero et al., 2015; Howard et al., 2017). While network distillation

2

Under review as a conference paper at ICLR 2019

aims to distill the knowledge of multiple networks into a single model, our goal is to compress the knowledge of an entire dataset into a few synthetic training images. Another way to distill knowledge is to summarize the entire dataset by a small subset, either by only using the "valuable" data for model training (Angelova et al., 2005; Lapedriza et al., 2013; Felzenszwalb et al., 2010) or by only labeling the "valuable" data via active learning (Cohn et al., 1996; Tong & Koller, 2001). However, these algorithms require many more training examples per category than we do, in part because their "valuable" images have to be real, whereas our distilled images are exempt from this constraint.
Gradient-based Hyperparameter Optimization Our work bears similarity with the gradient-based hyperparameter optimization techniques, which compute the gradient of hyperparameter w.r.t. the final validation loss by reversing the entire training procedure (Bengio, 2000; Domke, 2012; Pedregosa, 2016; Maclaurin et al., 2015). We also backpropagate errors through optimization steps. However, we use only training set data and focus much more heavily on learning synthetic training data rather than training hyperparameters. To our knowledge, this direction has only been slightly touched on previously (Maclaurin et al., 2015). We explore it in much greater depth and demonstrate the idea of dataset distillation through various applications. More crucially, our distilled images can work well across random initialization weights, which cannot be achieved by any previous work.
Understanding Datasets Researchers have presented various approaches for understanding and visualzing learned models (Zeiler & Fergus, 2014; Zhou et al., 2015; Mahendran & Vedaldi, 2015; Bau et al., 2017; Koh & Liang, 2017). Unlike these approaches, we are interested in understanding the intrinsic properties of the training data rather than a specific trained model. Analyzing training datasets has, in the past, been mainly focused on the investigation of bias in datasets (Ponce et al., 2006; Torralba & Efros, 2011). For example, Torralba & Efros (2011) proposed to quantify the "value" of dataset samples using cross-dataset generalization. Our method offers a new perspective for understanding datasets. We often find that the number of distilled images required to achieve good performance is an informative indicator of the dataset diversity.

3 APPROACH

Given a model and a dataset, we aim to obtain a new, much-reduced synthetic dataset which performs almost as well as the original dataset. We first present our main optimization algorithm for training a network with a fixed initialization with one gradient descent (GD) step (Sec. 3.1). In Sec. 3.2, we derive the resolution to a more challenging case, where the initial weight is random rather than fixed. We also discuss the initial weights distribution where our method can work well. Furthermore, we study a linear network case to help the readers understand both the solution and limits of our method in Sec. 3.3. In Sec. 3.4, we extend our approach to more than one gradient descent steps and more than one passes. Finally, Sec. 3.5 demonstrates two applications enabled by our method.

Consider a training dataset x = {xi}Ni=1, we parameterize our neural network as  and denote (xi, ) as the loss function that represents the loss of this network on a data point xi. Our task is to find the
minimizer of the empirical error over the entire training data:

 = arg min 1 N



N
i=1

(xi, ) = arg min


(x, ),

(1)

where for notation simplicity we overload the (·) notation so that (x, ) represents the average error of  over the entire dataset x = {xi}iN=1. We make the mild assumptions that is twice-differentiable, which holds for majority of modern machine learning models (e.g., most neural networks) and tasks.

3.1 OPTIMIZING DISTILLED DATA

Standard training usually applies minibatch stochastic gradient descent (SGD) or its variants. At each step t, we sample a minibatch of training data xt = {xt,j}jn=1 and update the current parameters as

t+1 = t - t (xt, t),

where  is the learning rate. Such a training process often takes tens of thousands or even millions of
above update steps to converge. Instead, we aim to learn a tiny set of synthetic distilled training data x~ = {x~i}iM=1 with M N and a corresponding learning rate ~ so that a single GD step like

1 = 0 - ~0 (x~, 0)

(2)

3

Under review as a conference paper at ICLR 2019

Algorithm 1 Dataset Distillation
Input: p(0): distribution of initial parameters
Input: : step size; n: batch size; ~0: initial value for ~
Input: T : the number of optimization iterations; M : the number of distilled data 1: Initialize x~ = {x~i}iM=1 randomly, ~  ~0 2: for each training step t = 1 to T do 3: Get a minibatch of real data xt = {xt,j }nj=1 4: Sample a batch of initial parameters 0(j)  p(0) 5: for each sampled 0(j) do 6: Compute updated parameter with GD: 1(j) = 0(j) - ~0(j) (x~, 0(j)) 7: Evaluate on real data: compute (xt, 1(j)) 8: end for 9: Update x~  x~ - x~ j (xt, 1(j)), and ~  ~ - ~ j (xt, 1(j)) 10: end for
Output: distilled data x~ and the optimized learning rate ~

using these learned synthetic data x~ greatly boosts performance on the real training dataset.
Given an initialization 0, we obtain these synthetic data and ~ that minimize the below objective L: x~, ~ = arg min L(x~, ~; 0) = arg min (x, 1) = arg min (x, 0 - ~0 (x~, 0)), (3)
x~,~ x~,~ x~,~
where we derive the new weights 1 as a function of distilled images x~ and learning rate ~ using Eqn. 2 and then evaluate the new weights over all the training images x. Note that the loss L(x~, ~; 0) is differentiable w.r.t. to x~ and ~, and can thus be optimized using standard gradient-based algorithms. In many classification tasks, the data x may contain discrete parts, e.g., the class labels in data-label pairs. For such cases, we fix the discrete part rather than learn them.

3.2 DISTILLED DATA FOR RANDOM INITIALIZATIONS

Unfortunately, the above distilled data optimized for a given initialization do not generalize well

to other initialization weights. The distilled data often look like random noise (e.g., in Fig. 2a) as

it encodes the information of both training dataset x and a particular network initialization 0. To address the above issue, we turn to calculate a small number of distilled data that can work for

networks with random initializations from a specific distribution. We formulate the optimization

problem as follows:

x~, ~ = arg min E0p(0)L(x~, ~; 0),
x~,~

(4)

where 0 is a randomly sampled network initialization from the distribution p(0): e.g., different networks pre-trained on the same task. Algorithm 1 illustrates our main method. During optimization,

the distilled data are optimized to work well for multiple networks whose initial weights are sampled

from p(0). In practice, we observe that the final distilled data generalize well to the unseen initializations. Besides, these distilled images usually look quite informative, encoding discriminative

features of each category (Fig. 3).

For distilled data to be properly learned, it turns out to be crucial for (x, ·) to share similar local conditions (e.g., output values, gradient magnitudes) over 0 sampled from p(0). In the next section, we derive a lower bound on the amount of distilled data needed for a simple model with arbitrary initial 0, and discuss its implications on choosing p(0).

3.3 ANALYSIS OF A SIMPLE LINEAR CASE WITH QUADRATIC LOSS

This section studies our formulation in a simple linear regression case. We derive the lower bound of
the number of distilled images needed to achieve the same performance as training on full dataset
for arbitrary initialization with one GD step. Consider a dataset x containing N data-target pairs {(di, ti)}iN=1, where di  RD and ti  R, which we represent as two matrices: an N × D data matrix d and an N × 1 target matrix t. Given the mean squared error and a D × 1 weight matrix , we have

1 (x, ) = ((d, t), ) =

d - t 2.

2N

(5)

4

Under review as a conference paper at ICLR 2019

We aim to learn M synthetic data-target pairs x~ = (d~, ~t), where d~ is an M × D matrix, ~t an M × 1
matrix (M N ), and ~ the learning rate, to minimize (x, 0 - ~0 (x~, 0)). The updated weight matrix after one GD step with these distilled data is

1 = 0 - ~0

(x~,

0)

=

0

-

~ M

d~ T

(d~ 0

-

~t)

=

(I

-

~ M

d~ T

d~ )0

+

~ d~T ~t. M

(6)

Suppose some learned distilled data x~ allows us to achieve the same performance as training on full dataset x (i.e., attaining the global minimum) for any initialization 0. For such models, global minimum is attained at any  satisfying dT d = dT t. Substituting Eqn. (6) in, we have

dT

d(I

-

~ M

d~T d~)0

+

~ M

d~T ~t

=

dT

t.

(7)

Here we make the mild assumption that the feature columns of the data matrix d are independent (i.e., dT d has full rank). For a x~ = (d~, ~t) to satisfy the above equation for any 0, we must have

·

I-

~ M

d~ T

d~

=

0,

which

implies

that

d~ T

d~

has

full

rank

and

M

 D, and

·

~ M

d~ T

~t

=

dT

t,

where

~t

projected

onto

d~

is

equal

to

t

projected

onto

d,

up

to

a

factor.

Discussion The above analysis considers only a simple case, but it suggests that the distilled data
seems to try to cover a (potentially transformed) subspace spanned by the training data. Moreover, we also showed that in this case, a small number of distilled data fails to generalize to arbitrary starting 0. This is intuitively expected as the optimization target (x, 1) = (x, 0 - ~0 (x~, 0)) depends on the local behavior of (x, ·) around 0, which can be drastically different across various 0 values. We note that the lower bound M  D is a quite restricting one, considering that real datasets often have
thousands to even hundreds of thousands of dimensions (e.g., image classification). This motivates us to focus on p() distributions that yield similar local conditions over the support, e.g., network
weights initialized using methods that attempts to ensure gradient flow of constant magnitude (He
et al., 2015; Glorot & Bengio, 2010), and network weights pre-trained on other tasks. Sec. 4.1 and
Sec. 4.2 explore both options. Additionally, to address the limitation of using single GD step, we
extend our method to multiple GD steps in the next section. In Sec. 4.1, we empirically verify that
using multiple steps is much more effective than using a single step on deep convolutional networks,
with total amount of distilled data fixed.

3.4 MULTIPLE GRADIENT DESCENT STEPS AND MULTIPLE EPOCHS

We can extend Algorithm 1 to more than one gradient descent steps by changing Line 6 to multiple sequential GD steps each on a different batch of distilled data and learning rate, i.e., each step i is

i+1 = i - ~ii (x~i, i),

(8)

and changing Line 9 to backpropagate through all steps. However, naively computing gradients is both memory-intensive and computationally-expensive. Therefore, we exploit a recent technique called back-gradient optimization, which allows for significantly faster gradient calculation of such updates in reverse-mode differentiation (i.e., backpropagation). Specifically, back-gradient optimization formulates the necesary second order terms into efficient Hessian-vector products (Pearlmutter, 1994), which can be easily calculated with modern automatic differentiation systems such as PyTorch (Paszke et al., 2017). For further algorithm details in this aspect, we refer readers to prior work (Domke, 2012; Maclaurin et al., 2015).
Multiple Epochs To further improve the performance, we can train the network with the same distilled images for multiple passes (epochs). For examples, for image data, we simply tie the image pixels for the same distilled images used in different epochs. Note that we do not tie the trained learning rates across epochs, since later epochs often use smaller learning rates.

3.5 APPLICATIONS
Here we demonstrate two applications enabled by our main algorithm. We present different objective functions tailored to individual applications.

5

Under review as a conference paper at ICLR 2019

Fast Domain Adaptation Domain mismatch and dataset bias represent a challenging problem in machine learning today (Torralba & Efros, 2011). Extensive prior work has been proposed to adapt models to new tasks and datasets (Daume III, 2007; Saenko et al., 2010). Why does a model trained on one dataset perform poorly on the other dataset? To answer this question, we characterize the domain mismatch via distilled data. In particular, we optimize distilled data for models trained on one dataset so that the distilled data that can largely improve the model performance on the new dataset. Formally, given a source dataset and a target dataset, we optimize the following objective:

x~, ~ = arg min Esp(s)Lt(x~, ~; s),
x~,~

(9)

where p(s) is the distribution of pre-trained models on the source dataset and Lt denotes the training loss for the target dataset.

Fast Data Poisoning Attack Moreover, our approach can be used to construct data poisoning attack. To illustrate this idea, we consider the following scenario. When a single GD step is applied with our synthetic adversarial data, a well-behaved image classifier catastrophically forgets a category but still maintains high performance on other categories. Formally, given an attacked category K and a target category T , we want the classifier to misclassify images from category K to category T . To achieve this, we optimize the following objective:

x~, ~ = arg min Ep()LKT (x~, ~; ),
x~,~

(10)

where p() is the distribution of well-trained classifiers. The classification loss LKT encourages the model to classify category K images mistakenly as category T while correctly predicting other
images, e.g., a cross entropy loss with target labels of K modified to T .

Compared to prior data poisoning attacks (Biggio et al., 2012; Li et al., 2016; Muñoz-González et al., 2017; Koh & Liang, 2017), our approach crucially does not require the poisoned training data to be stored and trained on repeatedly. Instead, our method attacks the model training just in one iteration and with only a few data. This makes our method effective for many online training algorithms and useful for the case where malicious users hijack the data feeding pipeline for only one gradient step (e.g., one network transmission). In Sec. 4.2, we show that a single batch of distilled data applied in one step can successfully attack well-optimized neural network models.

Finally, we note that this application can be viewed as distilling dataset knowledge of a specific category into data.

4 EXPERIMENTS
We present image classification results on MNIST (LeCun, 1998) and CIFAR10 (Krizhevsky & Hinton, 2009). For MNIST, distilled images are trained with LENET (LeCun et al., 1998), which can achieve about 99% test accuracy if fully trained. For CIFAR10, we use a network architecture following Krizhevsky (2012) (with top locally connected layers replaced by fully connected ones), which would achieve around 80% test accuracy if fully trained. For random initializations and baselines with randomness (e.g., randomly selecting real training images), we report means and standard deviations over 200 runs.
4.1 DATASET DISTILLATION
Fixed Initialization With access to initial network weights, distilled images can directly train this particular network to high performance. For example, 10 learned distilled images can boost the test accuracy of a neural network from 12.9% to 93.76% on MNIST (Fig. 2a) and 100 images can train a network from 8.82% to 54.03% test accuracy on CIFAR10 (Fig. 2b). This indicates that a few distilled images have enough capacity to include at least part of dataset knowledge.
Random Initialization Trained with randomly sampled initializations using an approach proposed by Glorot & Bengio (2010) (known as "Xavier"), the learned distilled images do not need to encode information tailored for a particular starting point and thus can represent meaningful content independent of network initializations. In Fig. 3, we see that such distilled images reveal discriminative features of the corresponding categories: e.g., the ship image in Fig. 3b. These 100 images can

6

Under review as a conference paper at ICLR 2019

0.L0R3sS9:t6e0,p.70:3.001097,0 Plane Car Bird Cat Deer Dog Monkey Horse Ship Truck

0.L0R1sS0:t6e1,p.60:9.016164,1 Label 0 Label 1 Label 2 Label 3 Label 4 Label 5 Label 6 Label 7 Label 8 Label 9

0.L1R1sS1:t7e0,p.00:2.518466,3 0.L0R1sS3:t5e0,p.00:2.908098,6

(a) MNIST. These images train networks with a partic- (b) CIFAR10. These images train networks with a particular initialization from 12.9% to 93.76% test accuracy. ular initialization from 8.82% to 54.03% test accuracy.

Figure 2: Learned distilled images given fixed initializations. Both experiments use 3 epochs and 10 images per GD step (one per category). MNIST distilled images use 1 GD step with 10 images in total. CIFAR10 distilled images use 10 GD steps with 100 images in total (only images for selected steps are shown). At left, we report the learning rates for 3 epochs.

0.L1R0sS3:t9e0,p.10:0.004658,1 Label 0 Label 1 Label 2 Label 3 Label 4 Label 5 Label 6 Label 7 Label 8 Label 9 0.L0R3sS6:t1e0,p.00:5.009180,9 Plane Car Bird Cat Deer Dog Monkey Horse Ship Truck

0.L1R4sS0:t1e0,p.10:0.513289,6

0.L0R3sS3:t0e0,p.00:8.500146,3

0.L0R8sS8:t5e0,p.10:3.900146,9

0.L0R8sS2:t9e0,p.00:5.906110,7

(a) MNIST. These images train networks with unknown (b) CIFAR10. These images train networks with un-

initialization to 77.28% ± 4.45% test accuracy.

known initialization to 36.79% ± 1.18% test accuracy.

Figure 3: Distilled images trained for 10 GD steps and 3 epochs with unknown random initializations. We show images from selected GD steps and corresponding trained learning rates (for all three epochs).

0.75 0.50 0.25
0

MNIST
10 20 Number of steps

0.35 0.30 0.25
30 0

CIFAR10
10 20 30 Number of steps

0.8 0.7 0.6 1

MNIST
234 Number of epochs

0.38 0.36 0.34 51

CIFAR10
234 Number of epochs

5

(a) Ablation study on number of steps

(b) Ablation study on number of epochs

Figure 4: Ablation studies: (a) average test accuracy w.r.t. the number of gradient descent steps. Each step contains 10 images (one per category). The number of epochs is fixed to be 2. (b) average test time accuracy with respect to the number of epochs. Each epoch uses the same set of distilled images, which has 10 GD steps each containing 10 images (one per category).

MNIST CIFAR10

N (#images per category)
1 5 10 20 30 1 5 10 20 30

Total #images (N × #categories)
10 50 100 200 300 10 50 100 200 300

Applied in 1 GD step
19.81% ± 5.83% 21.19% ± 5.47% 21.45% ± 5.81% 21.49% ± 5.96% 21.57% ± 5.91% 17.95% ± 2.41% 18.25% ± 2.51% 18.46% ± 2.58% 18.58% ± 2.58% 18.59% ± 2.56%

Applied in N GD steps
19.81% ± 5.83% 53.94% ± 9.45% 69.32% ± 8.86% 72.68% ± 12.44% 83.58% ± 5.08% 17.95% ± 2.41% 32.49% ± 1.28% 34.54% ± 1.55% 37.84% ± 1.63% 38.20% ± 1.47%

Table 1: Comparison between applying the same number of images in one GD step versus multiple steps, with the number of epochs fixed to 1. N denotes the total number of images per category. For multiple steps runs, each of the N steps applies one image per category. Using multiple steps drastically improves the performance.

Test Accuracy Test Accuracy

train randomly initialized networks to 36.79% average test accuracy on CIFAR10. Similarly, for MNIST, the 100 distilled images shown in Fig. 3a can train randomly initialized networks to 77.28% test accuracy.
More Than One Gradient Descent Steps To improve the accuracy, we can train a model with a sequence of distilled images and multiple GD steps. In Fig. 3, we learn distilled images for 10 GD steps, leading to a total of 100 images (with each step containing one image per category). The early steps tend to look noisier, likely regularizing random weights to points easier for optimization and fine-tuning. In later steps, the images become increasingly resembling real data, which summarizes the discriminative features for these categories. Fig. 4a shows that using more steps significantly improves the results. Alternatively, we can train the model with one GD step but a big batch size. Sec. 3.3 showed some limitation of using only one step in a simple linear case. In Table 1, we empirically verify that with deep convolutional networks, using multiple steps drastically outperforms using only a single step, with the same number of distilled images.
7

Under review as a conference paper at ICLR 2019

MNLIaSsTt StUepSPS USLPaSst SMteNpIST SVHLaNst SMteNpIST

Figure 5: We train 100 distilled images applied in 10 steps and 3 epochs for each fast domain adaptation task. Images in last steps are visualized. From top to bottom: MNIST  USPS, USPS  MNIST, SVHN  MNIST.

MNIST  USPS USPS  MNIST SVHN  MNIST

Ours with fixed initializations
97.90% 93.19% 96.15%

Ours with random initializations
94.62% ± 1.47% 92.74% ± 1.38% 85.21% ± 4.73%

Random real images
94.35% ± 1.49% 84.40% ± 3.27% 81.68% ± 3.78%

No adaptation
67.54% ± 3.91% 90.43% ± 2.97% 51.64% ± 2.77%

Directly training with much more real images
97.32% ± 0.27% 98.60% ± 0.53% 98.60% ± 0.53%

Table 2: Domain adaptation performance for our method and baselines. Distilled images are trained for 10 GD steps (100 images in total). Baselines use the same numbers of randomly sampled real images from target dataset and also apply in 10 GD steps. We evaluate these real images using learned learning rate, and each of the fixed learning rate in {0.001, 0.003, 0.01, 0.03, 0.1, 0.3}, and report the best result.

More Than One Epochs To further boost the model performance, we can train the model on the same distilled images but with multiple passes (epochs). Fig. 4b shows that using the same number of distilled images but with more than 1 epochs improves the performance. However, the improvement slows down as we increase the number of epochs. We believe that longer training (i.e., more epochs) can help the model learn all the knowledge from the distilled images, but can make the model training less stable and is eventually limited by the capacity of the images (i.e., number of total images).
4.2 APPLICATIONS
Next, we show two applications built on our main algorithm. Both cases assume that the initial weights are random but pre-trained on the same dataset, which is different from the current dataset.
Fast Domain Adaptation We can optimize distilled images to quickly fine-tune a pretrained model for a new dataset. Additionally, the learned distilled images can visualize the domain gap between two datasets. Fig. 5 visualizes distilled images trained to increase the performance of pre-trained models on various domain adaptation tasks. Table 2 shows that our method is more effective compared to using the same number of random real images.
Fast Data Poisoning Attack Finally, our method can construct a new type of data poisoning, where the attacker can apply just one GD step with a few malicious data to manipulate a well-trained model. As shown in Fig. 6, we train distilled images to make trained neural networks to misclassify a particular attacked category to another target category with just one GD step. On MNIST with the attacked category 0 and the target category 1, we can see that the distilled images for many categories other than 0 show the digit 0 shape, with the effect most evident in distilled image for target category 1, i.e., the category we want the attacked model to predict 0 digits as. Table 3 shows that our method is able to attack well-optimized models with a single GD step on a small batch of images, and is much more effective than the attack baseline with randomly sampled real images with incorrect labels.
5 DISCUSSION
In this paper, we present dataset distillation for compressing the knowledge of entire training data into a few synthetic training images. We can train a network to reach high performance with small number of distilled images and several gradient descent steps. Finally, we demonstrate two applications including fast domain adaptation and effective data poisoning attack. In the future, we plan to extend our method to compress large-scale visual datasets such as ImageNet (Deng et al., 2009) and other types of data (e.g., audio and text). Also, our current method works well for random pre-trained weights, but performs worse for randomly initialized networks. We would like to investigate various initialization strategies, with which distilled images can work well.
8

Under review as a conference paper at ICLR 2019

MNIST: 01 CIFAR10: planecar

#images
10 50 100 200 300 10 50 100 200 300

Attacked class classified correctly
4.71% ± 12.31% 2.88% ± 8.35% 2.55% ± 8.90% 1.74% ± 7.28% 1.61% ± 7.66% 26.63% ± 9.94% 12.41% ± 8.15% 9.23% ± 6.69% 7.96% ± 6.58% 7.15% ± 6.26%

Ours
Attacked class classified as target class
65.13% ± 30.44% 70.32% ± 31.16% 71.45% ± 29.68% 71.43% ± 30.74% 72.98% ± 30.78% 33.23% ± 14.37% 42.39% ± 17.13% 45.92% ± 18.11% 45.58% ± 19.05% 47.77% ± 18.43%

Overall accuracy w.r.t. incorrect labels
91.88% ± 4.25% 90.71% ± 5.59% 90.87% ± 5.28% 89.62% ± 6.88% 89.15% ± 7.71% 69.59% ± 2.03% 70.36% ± 2.12% 70.39% ± 2.39% 69.90% ± 3.20% 70.09% ± 2.76%

Attacked class classified correctly
98.74% ± 0.38% 98.80% ± 0.33% 98.81% ± 0.33% 98.80% ± 0.33% 98.80% ± 0.34% 77.81% ± 4.45% 78.93% ± 2.23% 79.31% ± 1.59% 79.57% ± 1.47% 79.69% ± 1.43%

Random real images
Attacked class classified as target class
0.02% ± 0.05% 0.02% ± 0.05% 0.02% ± 0.04% 0.02% ± 0.05% 0.02% ± .05% 3.05% ± 0.86% 3.01% ± 0.60% 3.01% ± 0.56% 3.03% ± 0.51% 3.02% ± 0.49%

Overall accuracy w.r.t. incorrect labels
88.84% ± 0.52% 88.87% ± 0.50% 88.88% ± 0.50% 88.88% ± 0.50% 88.88% ± 0.50% 68.71% ± 1.67% 69.69% ± 1.09% 69.96% ± 1.03% 70.08% ± 1.01% 70.11% ± 1.00%

Table 3: Fast data poisoning attack performance for our method and baseline. Distilled images are trained for one GD step. Baselines uses the same numbers of randomly sampled real images with incorrect labels and also apply one GD step. We evaluate these real images using learned learning rate, and each of the fixed learning rate in {0.001, 0.003, 0.01, 0.03, 0.1, 0.3}, and report the result that achieves the highest accuracy w.r.t. the incorrect labels. While the overall accuracies w.r.t. the incorrect labels are similar, our attack images are always better at changing the prediction for images of the attacked class.

Label 0 Label 1 Label 2 Label 3 Label 4 Label 5 Label 6 Label 7 Label 8 Label 9

DLisRt:ill0e.d00Im35a8g2es

Acc. Before Acc. After

±±94190.2.7.2.21285%8%%%

±±999903....54327264%%%%

±980..6643%% ±9100.9.233%%

±980..8665%% ±956..4245%%

±980..9618%% ±965..6209%%

±980..4681%% ±945..6342%%

±980..3462%% ±948..2303%%

±980..2975%% ±946..6761%%

±980..0799%% ±938..6706%%

±970..5812%% ±928..9876%%

(a) MNIST. Attacked category: 0. Target category: 1.
Plane Car Bird Cat Deer Dog Monkey Horse Ship Truck

DLisRt:ill0e.d00Im18a5g7es

Acc. Before Acc. After

±±281619....68294371%%%%

±±899403....19905738%%%%

±672..4279%% ±569..7150%%

±572..6042%% ±527..6783%%

±731..1985%% ±687..3599%%

±701..5595%% ±697..8000%%

±861..6041%% ±846..6941%%

±821..4229%% ±777..7394%%

±871..7021%% ±798..9328%%

±851..3063%% ±776..7963%%

(b) CIFAR10. Attacked category: plane. Target category: car.

Figure 6: Malicious distilled images can manipulate well-trained neural network models to misclassify attacked category images as target category with one GD step and 10 images. These images are trained with 2000 well-optimized neural network models and evaluated on 200 held-out models. Better results with larger batch size can be found at Table 3.

9

Under review as a conference paper at ICLR 2019
REFERENCES
Anelia Angelova, Yaser Abu-Mostafam, and Pietro Perona. Pruning training sets for learning of object categories. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), volume 1, pp. 494­501. IEEE, 2005.
Jimmy Ba and Rich Caruana. Do deep nets really need to be deep? In Advances in Neural Information Processing Systems (NIPS), pp. 2654­2662, 2014.
David Bau, Bolei Zhou, Aditya Khosla, Aude Oliva, and Antonio Torralba. Network dissection: Quantifying interpretability of deep visual representations. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 3319­3327. IEEE, 2017.
Yoshua Bengio. Gradient-based optimization of hyperparameters. Neural computation, 12(8):1889­1900, 2000.
Battista Biggio, Blaine Nelson, and Pavel Laskov. Poisoning attacks against support vector machines. arXiv preprint arXiv:1206.6389, 2012.
David A Cohn, Zoubin Ghahramani, and Michael I Jordan. Active learning with statistical models. Journal of artificial intelligence research, 4:129­145, 1996.
Hal Daume III. Frustratingly easy domain adaptation. In Annual Meeting of the Association for Computational Linguistics, 2007.
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on, pp. 248­255. IEEE, 2009.
Justin Domke. Generic methods for optimization-based modeling. In Artificial Intelligence and Statistics, pp. 318­326, 2012.
Pedro F Felzenszwalb, Ross B Girshick, David McAllester, and Deva Ramanan. Object detection with discriminatively trained part-based models. IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 32(9):1627­1645, 2010.
Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural networks. In Proceedings of the thirteenth international conference on artificial intelligence and statistics, pp. 249­256, 2010.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. In Proceedings of the IEEE international conference on computer vision, pp. 1026­1034, 2015.
Geoffrey Hinton, Oriol Vinyals, and Jeffrey Dean. Distilling the knowledge in a neural network. In NIPS Deep Learning and Representation Learning Workshop, 2015.
Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto, and Hartwig Adam. Mobilenets: Efficient convolutional neural networks for mobile vision applications. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017.
Pang Wei Koh and Percy Liang. Understanding black-box predictions via influence functions. In Doina Precup and Yee Whye Teh (eds.), International Conference on Machine Learning (ICML), volume 70. PMLR, 06­11 Aug 2017.
Alex Krizhevsky. cuda-convnet: High-performance c++/cuda implementation of convolutional neural networks. Source code available at https://github. com/akrizhevsky/cuda-convnet2 [March, 2017], 2012.
Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. Technical report, Citeseer, 2009.
Agata Lapedriza, Hamed Pirsiavash, Zoya Bylinskii, and Antonio Torralba. Are all training examples equally valuable? arXiv preprint arXiv:1311.6510, 2013.
Yann LeCun. The mnist database of handwritten digits. http://yann. lecun. com/exdb/mnist/, 1998.
Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278­2324, 1998.
10

Under review as a conference paper at ICLR 2019
Bo Li, Yining Wang, Aarti Singh, and Yevgeniy Vorobeychik. Data poisoning attacks on factorization-based collaborative filtering. In Advances in Neural Information Processing Systems (NIPS), 2016.
Dougal Maclaurin, David Duvenaud, and Ryan Adams. Gradient-based hyperparameter optimization through reversible learning. In International Conference on Machine Learning (ICML), 2015.
Aravindh Mahendran and Andrea Vedaldi. Understanding deep image representations by inverting them. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2015.
Luis Muñoz-González, Battista Biggio, Ambra Demontis, Andrea Paudice, Vasin Wongrassamee, Emil C Lupu, and Fabio Roli. Towards poisoning of deep learning algorithms with back-gradient optimization. In Proceedings of the 10th ACM Workshop on Artificial Intelligence and Security, pp. 27­38. ACM, 2017.
Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in pytorch. In International Conference on Learning Representations (ICLR) Workshop, 2017.
Barak A Pearlmutter. Fast exact multiplication by the hessian. Neural computation, 6(1):147­160, 1994. Fabian Pedregosa. Hyperparameter optimization with approximate gradient. In International Conference on
Machine Learning (ICML), 2016. Jean Ponce, Tamara L Berg, Mark Everingham, David A Forsyth, Martial Hebert, Svetlana Lazebnik, Marcin
Marszalek, Cordelia Schmid, Bryan C Russell, Antonio Torralba, et al. Dataset issues in object recognition. In Toward category-level object recognition, pp. 29­48. Springer, 2006. Ilija Radosavovic, Piotr Dollár, Ross Girshick, Georgia Gkioxari, and Kaiming He. Data distillation: Towards omni-supervised learning. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018. Adriana Romero, Nicolas Ballas, Samira Ebrahimi Kahou, Antoine Chassang, Carlo Gatta, and Yoshua Bengio. Fitnets: Hints for thin deep nets. In International Conference on Learning Representations (ICLR), 2015. Kate Saenko, Brian Kulis, Mario Fritz, and Trevor Darrell. Adapting visual category models to new domains. In European Conference on Computer Vision (ECCV), 2010. Simon Tong and Daphne Koller. Support vector machine active learning with applications to text classification. Journal of Machine Learning Research, 2(Nov):45­66, 2001. Antonio Torralba and Alexei A Efros. Unbiased look at dataset bias. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1521­1528. IEEE, 2011. Matthew D Zeiler and Rob Fergus. Visualizing and understanding convolutional networks. In European Conference on Computer Vision (ECCV), pp. 818­833. Springer, 2014. Bolei Zhou, Aditya Khosla, Agata Lapedriza, Aude Oliva, and Antonio Torralba. Object detectors emerge in deep scene cnns. In International Conference on Learning Representations (ICLR), 2015.
11


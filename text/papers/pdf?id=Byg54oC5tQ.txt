Under review as a conference paper at ICLR 2019
GENERATIVE MODEL FOR MATERIAL IRRADIATION EXPERIMENTS BASED ON PRIOR KNOWLEDGE AND ATTENTION MECHANISM
Anonymous authors Paper under double-blind review
ABSTRACT
Material irradiation experiment is dangerous and complex, which requires large number of high-level expertise in the manual processing of experimental images and data. In this paper, we propose a generative adversarial model based on prior knowledge and attention mechanism to achieve the generation of irradiated material images (data-to-image model), and a prediction model for corresponding industrial performance (image-to-data model). With the proposed models, researchers can skip the dangerous and complex irradiation experiments and obtain the irradiation images and industrial performance parameters directly by inputing some experimental parameters only. We also introduce a new dataset ISMD which contains 22000 irradiated images with 22,143 sets of corresponding parameters. Our model achieved high quality results by compared with several baseline models. The evaluation and detailed analysis are also performed.
1 INTRODUCTION
In recent years, significant progress has been made in the development of deep learning and generation models Kingma & Welling (2013) Goodfellow et al. (2014) Mirza & Osindero (2014). However, they are far less used in natural sciences than in services or the arts, experimental images from natural sciences have rich scientific connotations that can be analyzed using deep learning and generating models (e.g. medical images, fluid experiments, materials experiments etc. Kisilev et al. (2015) Jing et al. (2017) Alom et al. (2018) Li et al. (2018)).
Most of the researchs on analyzing experimental images of natural sciences are based on feature extraction and end-to-end mapping to obtain valuable experimental target parameters Li et al. (2018) Cirean et al. (2013) Jing et al. (2017), which can be called as image-to-data task. And most of these researchs are based on convolutional neural networks(CNN) and semantic segmentation Ronneberger et al. (2015) Menze et al. (2010). Although impressive results have been achieved, none of these researchs have built a data-to-image model.
We propose a generation model based on prior knowledge and attention mechanism to generate images of the experimental results. The overall architecture of the model is shown in Fig.3 .First, we propose an embedding model f c(m) = P hm for the molecular composition of materials, so that we can get the feature vector Cmi for each material composition i, which is called molecule2vec.Then we can embed the feature vector Cmi of the material composition as a priori knowledge into the latent variable sampling of the generation model(see Fig.3 for details).For the irradiated material images, the swelling cavities distribution Hv is the most important feature Ehrlich (1981), so an attention mechanism is introduced to make the model focus on the section with swelling cavities in the material images(see Fig.1).The image generated by the feedforward propagation of the model Ximg = G(Dd, Dc, z) can be encoded by the encoder network to be transformed into the corresponding swelling cavities distribution Hv, we construct the loss term LHv = ||Hv - Hv||2 to measure the distance between the real distribution of the swelling cavities and the feature of the generated image , and then the attention mechanism is optimized by gradient descent.In the end, we can obtain high-quality images of irradiated materials. In addition, we pre-trained the image-to-data network P red(Ximg) = Dr to predict the performance parameters Dr of the material through the
1

Under review as a conference paper at ICLR 2019

(a) Overview

(b) Representation Vector

Figure 1: (a) :data-to-image model and image-to-data model. In the data-to-image task, model generate images based on prior knowledge and attention mechanism. In the image-to-data task, images generated are used to predict the performance parameters. (b): For different alloy materials, the proportion of the elements in their molecular compositions is calculated, which is used by element representation vector m.

corresponding experimental image Ximg, which makes it possible to predict the material's performance parameters Dr by generating image Ximg by P red(Ximg) = Dr.
our contributes:
· data-to-image material images generation model: a generation model P (Ximg|Dd, Dc, z) is built based on prior knowledge and attention mechanism.
· image-to-data material performance prediction model: using the images Ximg and the context association method of CNN+BiLSTM, the network P (Dr|Ximg, Cm) is established to predict the performance parameters Dr by the image Ximg.
· The experimental dataset ISMD for irradiated materials: the images with corresponding data from large number of irradiation experiments and manual annotations.

2 RELATED WORK
Our research is related to deep generation models for image generation, image information mining in the natural sciences, and irradiated material science.
Deep generation model for image generation. The caption-to-image generation model Gregor et al. (2015) Zhang et al. (2016) Mansimov et al. (2015) is the majority, most of these models will first introduce a semantic capture module to extract semantic features, then generate images through RNN Mansimov et al. (2015), stackGAN Zhang et al. (2016), etc. In addition, enhancing the connection between semantics and images by attention mechanism is also popular Zhang et al. (2017).
Image information mining in the natural sciences. Medicine, biology, materials and other fields have recently introduced deep learning models to mine the connotation information of experimental images Jung et al. (2017) Ronneberger et al. (2015) Li et al. (2018). Some researchs are inspired by natural language processing(NLP) using the context information association trick Ssm et al. (2017), which can be seen as an image-to-data process.
Irradiated material science. In recent years, deep learning methods have been introduced into material images processing Li et al. (2018) Rovinelli (2018), which has replaced the traditional artificial technology analysis and achieved good results.
3 MODEL
Recall that our aim is to generate the corresponding experimental image Ximg based on the mechanics and thermodynamic parameters Dd of the material under the irradiation condition Dc by the prior knowledge, the molecular composition feature vector Cm, and the attention mechanism. In
2

Under review as a conference paper at ICLR 2019
addition, after generating experimental image Ximg, CNN+BiLSTM network P (Dr|Ximg) is used to predict the performance parameters Dr for image Ximg.
3.1 EMBEDDING MODEL FOR THE MOLECULAR COMPOSITION OF MATERIALS
Our dataset contains 14 kinds of alloy materials:
Cm  {Inconel718, InconeX750, Zr1, Zr2, Zr4, Zr1N b, Zr2.5N b, 1Cr13, 2Cr13, 00Cr13N i5M o4, Au304, Au317, Cr17T i, Cr25}
The molecular composition of these alloy materials is an important prior knowledge for images generation task, so embedding molecular composition features into the generation model is needed. Let E = {Al, M g, Si, Cu, F e, O...} be the set of all the elements appeared. Let mi  R|E| be the element percentage representation vector of the alloy material i. Inspired by the word2vec method Mikolov et al. (2013) for word embedding, a method for molecular composition of alloy materials feature extracting called moleculars2vec is proposed. P hm = f c(m) is used to learn the thermodynamic properties P hm from the alloy material by the element representation vector m :
P hm = f c(m) = Relu(Wmf c (m))
where f c(·) represents a fully connected network, f c (·) represents the network part of f c(·) without the last layer, Wm  R|E|×dPh is the weight matrix of the last layer, dP h is the dimension of the thermodynamic property P hm. Since the weight matrix Wm has the ability to represent both the molecular composition features and the properties of the alloy material, we take Cmi = Wmi,: as the feature vector of the material i.
3.2 GENERATIVE MODEL BASED ON PRIOR KNOWLEDGE AND ATTENTION MECHANISM
The prior distribution. In the previous section we extracted the prior knowledge as vectors, that is, the features of the molecular composition of the alloy materials. To introduce it into the model, we define the distribution of the latent variable P (z) as follow(prior distribution):
z  N (µ(Cm), (Cm)) µ(Cm) = tanh(WµCm) (Cm) = exp(tanh(WCm))
where Wµ  RD×dim(Cm),W  RD×dim(Cm) are the learnable parameters. Other similar methods introduce the distribution as latent variable dependencies are Mansimov et al. (2015) Bachman & Precup (2015), and our model is optimized in performance after introducing a prior distribution. The attention module. The swelling cavities distribution Hv of the irradiated material image is a statistics count vector for the distribution of different sizes, which is an important feature for material performance prediction Porollo et al. (2000). Therefore the model needs to allocate more attention to the section contains cavities, which requires attention mechanism to achieve. With this design goal in mind, we can set the attention module as an estimate of the corresponding degree of the cavities distribution Hv and the image Ximg, which is the soft attention mechanism Attn(·):
X^ = Att(Xconv, H^v) = Relu(CN N (Ximg), WvHv)
where Xconv  RL×L×C is a feature calculated by the forward propagation of convolutional layers, which with C feature maps and with dimension of L × L. Wv  R|Hv|×dim(Xconv) is a learnable parameter matrix that maps the swelling cavities distribution Hv to the visual space. Finally, X^ = Att(Xconv, H^v) is inputed into the discriminator D(·) to complete feedforward propagation.
3

Under review as a conference paper at ICLR 2019

Figure 2: The data-to-image model based on prior knowledge and attention mechanism achieves the generation of irradiated material images. Prior knowledge distribution is employed to embed the molecular composition of materials into the generative model. The attention mechanism is utilized to generate images with physical connotations(swelling cavities distribution). In the image-to-data task, CNN+BiLSTM is used to extract the features and learn the dependency of the performance variables.

3.3 PREDICTION MODEL FOR THE PERFORMANCE OF THE MATERIALS

After generating the image of the irradiated material, it is necessary to evaluate the various properties of the material under the condition Dc. A network is constructed to achieve the image-to-data task: Fp : V  Rdim(Dr), where V represents the visual space.
Before we design the structure of this performance evaluate network, let's revisit the two challenges in implementing this map: (1) Irradiated material image Ximg and performance parameter Dr are both highly abstract data forms. It is very difficult to establish mapping between two highly abstract data forms Jung et al. (2017). (2) The variables in the performance parameters Dr are not independent to each other, which have mutual influence and physical connection.
To address the challenge 1, we introduce CNN to extract the visual features for the image Ximg, and the molecular composition feature vector Cm is introduced to change the model P (Dr|Ximg) to P (Dr|Ximg, Cm); For the challenge 2, BiLSTM Schuster (1996) is used to learn the dependency of the variables in the performance parameters Dr. The prediction model is defined as:

- Dri hi =

- - = Relu(Wh[hi , hi ] + (Wxf X¯ + Wf h1:i-1

bh) + bf

)

- hi

=

(WxbX¯

+

Wf h>i

+

bb)

X¯ = Relu(WxX^img + WmCm + b)

where

X^img

= CN - -

N

(Ximg

)

is

the

visual

feature

matrix

extracted

by

convolutional

neural net-

works(CNN), hi, hi are the forward and backward propagation hidden state vectors, which are

concatenated together to predict the i-th variable in the performance parameters Dri .

3.4 LEARNING

In order to generate real images with physical connotations, the learning loss consists of two parts: GAN loss LGAN (prompting the model to generate real images) and swelling cavities feature loss

4

Under review as a conference paper at ICLR 2019

Algorithm 1 The training algorithm using minibatch SGD with learning rate 

Input: minibatch images Ximg; minibatch data Dd, Dc, Cm, Hv; minibatch size S;

1: for n = 1 to S do do

2: z  N (µ(Cm), (Cm));

3: Ximg  G(z, Dd, Dc);

4: Lr  D(Attn(Ximg, Hv));

5: Lf  D(Attn(Ximg, Hv));

6: LHv  ||En(Ximg) - Hv||2;

7:

LD



-log(Lr )

-

log(1

-

Lf )

+

 2

LHv

;

8: D  D -  D LD;

9:

LG



-log(Lf )

+

 2

LHv

;

10: G  G -  G LG;

11: end for

LHv (prompting the model to generate images with physical connotations, adjusting the parameters of attention mechanism by backpropagation). In order to calculate swelling cavities feature loss LHv , We need to measure the corresponding degree of the image Ximg and the swelling cavities feature Hv, Which requires encoding the image.
The image encoder: image encoder maps the image Ximg to the space of the swelling cavities feature space by the convolutional neural network(CNN). The middle layer of the CNN can learn local features of the region in the image Ximg. Finally, we map the CNN features of Ximg to the swelling cavities distribution features through the fully connected layers:H¯v = En(Ximg) = W f c(CN N (Ximg)).
The loss and learning steps: the loss function consists of the loss of GAN LGAN and the swelling cavities distribution features loss LHv , and the final loss based on prior knowledge and attention mechanism is defined as L = LGAN + LHv .
where  is a hyperparameter that balances two losses. The first loss represents the unconditional loss of GAN Xu et al. (2017), and the second loss represents the conditional loss constrainted by the swelling cavities features. The loss LHv minimizes the difference between the cavities feature Hv of the generated image Ximg and the true cavities feature Hv, defined as:LHv = ||Hv - Hv||2 = ||En(Ximg) - Hv||2.
The loss of GAN consists of the loss of the generator and the loss of the discriminator. We assign the swelling cavities feature loss LHv to the generator and the discriminator. The generator is trained to generate real images Ximg with swelling cavities features(physical connotations), whose loss is defined as:

 LG = -EXimgPG logD(Ximg, Hv) + 2 LHv
The discriminator is trained to distinguish whether the input material image Ximg is real or fake, whose loss is defined as:

LD

=

-EXimgPdata logD(Ximg, Hv)

-

EXimgPG log(1

-

D(Ximg, Hv))

+

 2

LHv

Finally, we can train the generator and discriminator alternately.

3.5 TRAINING DETAILS
The training algorithm is implemented with the deep learning lib Keras. The networks are randomly intialized without any pre-training and is trained with decayed Adagrad and RMSprop. We train for a total of 2000 epochs and use a batch size of 10, a learning rate of 1 × 10-5, and a weight decay rate of 1.6 × 10-6 for the image generation task. The generated image size is 64 × 64. The molecule2vec embedding vectors are intialized as random vectors.

5

Under review as a conference paper at ICLR 2019
4 EXPERIMENTS
Extensive experiments is carried out to evaluate the proposed model by comparing with multiple baseline models. First, we performed an experiment on image generation task and evaluate the ability of the model to generate experimental material images based on condition parameters(datato-image task). Then the network P (Dr|Ximg) predicting the performance parameters Dr by the image Ximg is tested(image-to-data task), which is also compared with the theoretical models from computational materials science Ehrlich (1981) Boltax et al. (1978) Rest & Hofman (1999).
4.1 DATASET We introduce a dataset called ISMD(Irradiation Swelling Material Dataset) into this task. This dataset includes more than 22,000 images with size of 64 × 64 and with shooting scale of 100nm from irradiation experiments, corresponding the molecular compositions set m of 14 alloys, material thermodynamics and mechanical properties set Dd, experimental condition parameters set Dc, swelling cavities distribution set Hv, and the performance parameters set Dr. More details about dataset ISMD production Details and data formats can be found in Appendix.A;
4.2 BASELINES Baseline models for image generation.
· Pure GAN: the prior distribution and the attention mechanism removed from the proposed model.
· The prior distribution dropped: only the prior distribution of the proposed model removed and is used to evaluate the value of prior distribution.
· The attention mechanism dropped: only the attention mechanism of the proposed model removed and is used to evaluate the value of the attention mechanism.
· Variational Auto-Encoder: To verify whether GAN is a better generation model for this task.
Baseline model for materials performance prediction: The theoretical model from computational material science is used to evaluate the prediction model in image-to-data task.
Figure 3: Above:The images generated by the proposed model and the baseline models.Below: The materials performance prediction model compared to theoretical models and the experimental results (Partial results).
4.3 EVALUATION METRICS Evaluation metrics for image generation: Inception score Salimans et al. (2016) Springenberg (2015)is the evaluation method for generating images. The core idea of the method is: im-
6

Under review as a conference paper at ICLR 2019

Figure 4: The data is collected by instrumentsFigure 5: We put the trained models on the server records, organized and annotated by researchers. and developed a Web App.

ages contain meaningful objects should have a conditional label distribution p(y|x) with low entropy, moreover, the model is expected to generate varied images, so the marginal p(y|x = G(z))dz should have high entropy.Combining these two requirements, the evaluation is defined as exp(EXimg KL(p(m|Ximg)||p(m|Ximg))).
Evaluation metrics for materials performance prediction: the material performance parameters from the performance evaluation experiments is Dr, the parameters calculated by the theoretical models Ehrlich (1981) Boltax et al. (1978) Rest & Hofman (1999) is D^r, and which predicted by the image Ximg in the proposed model P (Dr|Ximg, Cm) is Dr. Finally, the accuracy score is defined as AccuracyScore = 103/||Dr - Dr||2.

4.4 RESULTS
For the data-to-image task, the proposed model performs better on the ISMD dataset than other baseline models, achieving higher scores.For the image-to-data task, our model performs better on the prediction of some material properties compared to the theoretical model. In summary, the combination of the image-to-data model and computational material science models should be better.

Table 1: The data-to-image task.

models Inception score

VAE gan pr+gan att+gan Our model

1.89 ± 0.05 2.30 ± 0.07 2.40 ± 0.02 2.84 ± 0.07 3.87 ± 0.08

Table 2: The accuracy score for materials performance prediction.

prediction tasks img-to-data theoretical models

deltas deltab deltae deltaL HB HRC HV K

2.59 3.07 2.71 3.76 2.80 2.75 2.56 4.17 2.08 3.27 2.01 4.08 2.05 2.00 2.59 2.54

5 CONCLUSION
In this work, we propose the data-to-image model based on prior knowledge and attention mechanism to achieve the generation of irradiated material images. In the image-to-data task, CNN and BiLSTM are used to extract the features and learn the dependency of the variables. In the future work, we plan to generate more controllable and diverse material images with physical connotations and predict the performance data to replace the experiments.

APPENDIX
.1 MATERIAL THERMODYNAMICS AND MECHANICAL PROPERTIES SET Dd
mechanical properties:

7

Under review as a conference paper at ICLR 2019
Figure 7: The swelling cavities distribution Hv. Figure 6: We visualize the distribution of the molecular composition feature vector Cm in twodimensional space.
E: modulus of elasticity (unit: M P a); : Poisson's ratio; ¯: Equivalent stress (unit: M P a); ¯: Equivalent plastic strain (unit: M P a); n: strain hardening index; m: strain rate sensitivity index; K: intensity factor; Lame constant ; Lame constant G;
thermodynamics:
Ct: crystal type; Cd: lattice parameter (unit: nm); Tm: melting point (unit: K); : theoretical density (unit: g/cm3); V^ : Thermal expansion coefficient (unit: 10-6 · K-1); Ch: Thermal conductivity (unit: W · m-1 · C-1); Hc: heat capacity (unit: KJ/mol · K); Eh: hot (unit: KJ/mol); Ce: Seebeck temperature difference electromotive force factor (unit: uV /K); CR: resistivity (unit: 103 ·  · cm);
.2 EXPERIMENTAL CONDITION PARAMETERS SET Dd
Fast neutron injection volume f (unit: 1019n/cm2); Thermal neutron injection amount t(unit: 1019n/cm2); Irradiation flux i(unit: 1019n/cm2); Irradiation temperature Ti(unit: K); Experimental temperature Te(unit: K);
.3 PERFORMANCE PARAMETERS SET Dr
Yield limit s(unit: M P a); Stretch limit b(unit: M P a); Elastic limit e(unit: M P a); The total extension rate is L(unit: %); Brinell hardness HB (unit: kg/mm2); Rockwell hardness HRC (unit: mm) (dimensionless: HR*T); Vickers hardness HV (unit: kg/mm2); Volume expansion rate Kv (unit: %); Irradiation growth rate KL(unit: %); Fracture toughness KIc(unit: M P a/m1/2); Creep performance t(unit: M P a); Crisp feature CHe  {0, 1};
Figure 8: Images generation recording of the baseline models and proposed model from different epochs.
8

Under review as a conference paper at ICLR 2019
REFERENCES
Md Zahangir Alom, Mahmudul Hasan, Chris Yakopcic, Tarek M. Taha, and Vijayan K. Asari. Recurrent residual convolutional neural network based on u-net (r2u-net) for medical image segmentation. IEEE Conf, 2018.
Philip Bachman and Doina Precup. Data generation as sequential decision making. Computer Science, pp. 3249­3257, 2015.
A. Boltax, J. P. Foster, J. E. Kalinowski, and D. C. Swenson. Design applications of irradiation creep and swelling data. Trans. Am. Nucl. Soc.; (United States), 28, 1978.
D. C. Cirean, A Giusti, L. M. Gambardella, and J Schmidhuber. Mitosis detection in breast cancer histology images with deep neural networks. In International Conference on Medical Image Computing and Computer-Assisted Intervention, pp. 411­8, 2013.
Karl Ehrlich. Irradiation creep and interrelation with swelling in austenitic stainless steels. Journal of Nuclear Materials, 100(1):149­166, 1981.
Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In International Conference on Neural Information Processing Systems, pp. 2672­2680, 2014.
Karol Gregor, Ivo Danihelka, Alex Graves, Danilo Jimenez Rezende, and Daan Wierstra. Draw: a recurrent neural network for image generation. Computer Science, pp. 1462­1471, 2015.
Baoyu Jing, Pengtao Xie, and Eric Xing. On the automatic generation of medical imaging reports. ICML, 2017.
Kim Uk Jung, Kim Gu Hak, and Ro Man Yong. Iterative deep convolutional encoder-decoder network for medical image segmentation. IEEE Conf, 2017.
Diederik P Kingma and Max Welling. Auto-encoding variational bayes. Arxiv, 2013.
P. Kisilev, E. Walach, E. Barkan, B. Ophir, S. Alpert, and S. Y. Hashoul. From medical image to automatic medical report generation. Ibm Journal of Research and Development, 59(2/3):2:1­2:7, 2015.
Wei Li, Kevin G. Field, and Dane Morgan. Automated defect analysis in electron microscopic images. Arxiv, 2018.
Elman Mansimov, Emilio Parisotto, Jimmy Lei Ba, and Ruslan Salakhutdinov. Generating images from captions with attention. Computer Science, 2015.
B. H. Menze, Leemput K Van, D Lashkari, M. A. Weber, N Ayache, and P Golland. A generative model for brain tumor segmentation in multi-modal images. Ibm Journal of Research and Development, 13(2):151­9, 2010.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient estimation of word representations in vector space. Computer Science, 2013.
Mehdi Mirza and Simon Osindero. Conditional generative adversarial nets. Computer Science, pp. 2672­2680, 2014.
S. I. Porollo, A. N. Vorobjev, Yu. V. Konobeev, N. I. Budylkin, E. G. Mironova, F. A. Garner, S. I. Porollo, A. N. Vorobjev, Yu. V. Konobeev, and N. I. Budylkin. Irradiation creep and stress-affected swelling in austenitic stainless steel 16cr-15ni-3mo-nb-b irradiated in the bn-350 reactor. Aaps Pharmscitech, 13(4):1386­1395, 2000.
J. Rest and G. L. Hofman. Dart model for irradiation-induced swelling of uranium silicide dispersion fuel elements. Nuclear Technology, 126(1):88­101, 1999.
Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-Net: Convolutional Networks for Biomedical Image Segmentation. Springer International Publishing, 2015.
9

Under review as a conference paper at ICLR 2019
A Rovinelli. Using machine learning and a data-driven approach to identify the small fatigue crack driving force in polycrystalline materials. Arxiv, 2018.
Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training gans. Computer Science, 2016.
Mike Schuster. Bi-directional recurrent neural networks for speech recognition. Ieice Technical Report Speech, 96:7­12, 1996.
Jost Tobias Springenberg. Unsupervised and semi-supervised learning with categorical generative adversarial networks. Computer Science, 2015.
Salehi Ssm, D Erdogmus, and A Gholipour. Auto-context convolutional neural network (auto-net) for brain extraction in magnetic resonance imaging. IEEE Transactions on Medical Imaging, PP (99):1­1, 2017.
Tao Xu, Pengchuan Zhang, Qiuyuan Huang, Han Zhang, Zhe Gan, Xiaolei Huang, and Xiaodong He. Attngan: Fine-grained text to image generation with attentional generative adversarial networks. Arxiv, 2017.
Han Zhang, Tao Xu, and Hongsheng Li. Stackgan: Text to photo-realistic image synthesis with stacked generative adversarial networks. Computer Science, pp. 5908­5916, 2016.
Han Zhang, Tao Xu, Hongsheng Li, Shaoting Zhang, Xiaogang Wang, Xiaolei Huang, and Dimitris N. Metaxas. Stackgan++: Realistic image synthesis with stacked generative adversarial networks. IEEE Transactions on Pattern Analysis and Machine Intelligence, PP(99), 2017.
10


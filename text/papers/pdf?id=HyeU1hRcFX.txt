Under review as a conference paper at ICLR 2019
UNSUPERVISED CONDITIONAL GENERATION USING NOISE ENGINEERED MODE MATCHING GAN
Anonymous authors Paper under double-blind review
ABSTRACT
Conditional generation refers to the process of sampling from an unknown distribution conditioned on semantics of the data. This can be achieved by augmenting the generative model with the desired semantic labels, albeit it is not straightforward in an unsupervised setting where the semantic label of every data sample is unknown. In this paper, we address this issue by proposing a method that can generate samples conditioned on the properties of a latent distribution engineered in accordance with a certain data prior. In particular, a latent space inversion network is trained in tandem with a generative adversarial network such that the modal properties of the latent space distribution are induced in the data generating distribution. We demonstrate that our model despite being fully unsupervised, is effective in learning meaningful representations through its mode matching property. We validate our method on multiple unsupervised tasks such as conditional generation, dataset attribute discovery and inference using three real world image datasets namely MNIST, CIFAR-10 and CelebA and show that the results are comparable to the state-of-the-art methods.
1 INTRODUCTION
Unsupervised learning or extraction of semantically relevant information from unlabelled data is a problem of great interest to the machine learning community. If successfully extracted, such an information can be utilized in a wide range of tasks such as feature learning, supervised and semisupervised classification, data augmentation and reconstruction (Hastie et al. (2009); Hinton et al. (2006); Hinton & Salakhutdinov (2006); Bengio et al. (2013); Yann (1987); Radford et al. (2015); Kingma et al. (2014); Cheung et al. (2014)). From a generative model's perspective, unsupervised learning amounts to dividing the data manifold into semantically interpretable regions and sample new points from each of the regions (Jaakkola & Haussler (1999); Hastie & Tibshirani (1996)). However, distributions of most of the naturally occurring data such as images and speech lie over complex high-dimensional manifolds and do not have tractable forms. Thus simple generative models are not found to be successful in modelling such data.
Deep generative neural networks, specifically, Generative Adversarial Networks (GANs) (Goodfellow et al. (2016)) have been shown to be highly effective in modelling complex distributions for several real-world tasks (Radford et al. (2015); Isola et al. (2017); Van Den Oord et al. (2016); Odena et al. (2016); Denton et al. (2015)). The basic idea of a GAN is to learn to implicitly sample from the data generating distribution without explicitly estimating it. It does so, by adopting an adversarial learning principle where the problem of sampling from an unknown data distribution is formulated as a min-max game between two function approximators called the generator and the discriminator. A generator has to effectively learn to map an input noise distribution to the data generating distribution. This is facilitated by the discriminator that is adversarially trained to distinguish between the samples drawn from the true data generating distribution and the samples generated by the generator. At the point of equilibrium, the distribution of the generated samples converges to the data generating distribution. GANs are similar to the conventional generative models in that they too are designed to learn a mapping from a latent space (usually noise) to the data space. However since GANs do not explicitly model the relation between the latent and the data space, they cannot be readily used for unsupervised learning unlike the conventional generative models where the posterior of the latent variable conditioned on the data is tractable. Thus, if the relation between the
1

Under review as a conference paper at ICLR 2019
latent and the data space can be inferred in a GAN setting, they can be readily used to perform a variety of unsupervised learning tasks.
Owing to this need, several successful recent attempts (although in different contexts) have been made to learn an inverse mapping from the data space to the latent space (Chen et al. (2016); Donahue et al. (2016); Dumoulin et al. (2016); Mescheder et al. (2017); Larsen et al. (2015); Makhzani et al. (2015); Kim et al. (2017); Springenberg (2015)). The basic idea is to train a GAN to simultaneously learn to sample from the data distribution and also to approximate the posterior of latent variables conditioned on data. This technique is also shown to have several desirable side-effects such as stabilization of training and prevention of degenerative solution for the generator (known as mode collapse) (Che et al. (2016); Srivastava et al. (2017); Arjovsky et al. (2017); Salimans et al. (2016)). In conventional generative models although, the prior on the latent variable is chosen in accordance with some guess on the structure of the data. For example, number of clusters in Kmeans is chosen to be ten if there are ten `classes' in the data (Jain et al. (1999)). This suggests that engineering the latent (noise) space in accordance with data priors might lead to the possibility of multiple applications.
Motivated by the above observations, in this paper, we propose a GAN model where an inversion mapping from the generated data space to an engineered latent space is learned such that properties of the data generating distribution are matched to those of the latent distribution. We demonstrate the use of this method in several unsupervised learning tasks such as conditional generation, inference and attribute discovery through experiments on multiple real-world datasets1.
2 PROPOSED METHOD
Suppose PX denotes the data generating distribution from which the samples are to be drawn and X denote its support. If X is a disconnected set, which is a union of k non-empty disjoint connected open subsets (Xi, i = {1, 2...k}), each representing a mode, then PX is termed as a multimodal distribution with k modes. In the following sections of the paper, we describe a procedure to induce multiple modes in the data generating space by engineering the latent space along with inversion. For the sake of simplicity, we shall assume that PX is a bimodal distribution albeit all the analysis hold equally well for multimodal distributions.
Let z denote a latent variable with a distribution z  PZ and Z be the support of PZ. Let g : Z  X be a continuous function mapping from z to x which serves as an estimator for PX . Let X0  X and X1  X represent two modes of PX . We claim that the following properties have to hold for PX to be multimodal.
Proposition 1. Let Z0  Z and Z1  Z respectively be the inverse images of X0 and X1 under g, then X0  X1 =  only if Z0  Z1 = , where  is an empty set.
Proof: In Appendix A.
Also since g is a continuous mapping, Z0  Z and Z1  Z also happen to be open subsets. Thus, from proposition 1, to get a bimodal generated distribution, it is necessary to have a bimodal latent distribution but not sufficient because Proposition 1 is not two way. However, if there exists a mapping h : X  Y^ which maps x, the output of g, to another random variable y^ in such a way that PY^ is bimodal, one can expect to apply proposition 1 again on h and claim bi-modality on PX . The below corollary to proposition 1 is stated to affirm this fact.
Corollary 1.1. Assume Y^ 0  Y^ and Y^ 1  Y^ are two subsets of Y^ such that h : X0  Y^ 0 and h : X1  Y^ 1 then Y^ 0  Y^ 1 =  only if X0  X1 = . Given Z0  Z1 = , Y^ 0  Y^ 1 =  is a sufficient condition for X0  X1 = .
Corollary 1.1 asserts if h maps x to any bimodal distribution, the generated distribution (PX ) will be bimodal. However, since both g and h are continuous mappings, conditional generation of samples from PX (conditioned on modes of z) is possible only if h is in accordance with the modal properties of z. Thus, we introduce a new random variable y that is an indicator of the modes of z. Formally, y := 1zZ1 , which implies that PY (y = 1) = Z1 PZ dz and PY (y = 0) = Z0 PZ dz. With this, we show that if PY and PY^ are close, then the modal properties of PX will match with those of PZ .
1Code for implementation can be found at - https://github.com/NEMGAN/NEMGAN
2

Under review as a conference paper at ICLR 2019

Proposition 2. Define a random variable that is an indicator of modes of PX as follows: x^ := 1xX1 , PX^ (x^ = 1) = X1 PX dx and PX^ (x^ = 0) = X0 PX dx. The minimization of DKL(PY^ ||PY ) is equivalent to the minimization of DKL(PY^ ||PX^ ).
Proof: In appendix A.
Corollary 2.1. Minimizing DKL(PY^ ||PX^ ) leads to matching of modal properties of PX^ and PY , that is, Xi PX dx = PY (y = i), i  {0, 1}.
Proof: In Appendix A.
Proposition 2 and Corollary 2.1. state an important fact - modal properties of the latent space and the data generating space can be matched by choosing h that would minimize DKL(PY^ ||PY ). This also implies that the imbalance (if any) in the modes of the latent distribution is reflected in the data generating distribution. This is extremely useful for datasets containing imbalanced classes, which is often the case in real world applications. The latent space Z can be constructed following the presumed class imbalance ratio in the real data to conditionally generate imbalanced classes.
Using proposition 1 and 2, one can make the data generating distribution to be bimodal, however, produced modes might be degenerated in the sense that Xi's reduce to singletons (mode collapse). To avoid this degenerative case, we propose to decompose h as a composite of two mappings h1 : X  Z^ and h2 : Z^  Y^ . Further, minimizing a norm distance between samples of Z and Z^ will prevent degenerative modes in PX . This is because h2 will enforce unique reconstruction of every sample of z which in turn ensures that a unique sample of x is generated by a unique sample of z.
In the above formulation however, the generated distribution is not constrained to be close to the distribution of the real data. For that, one has to rely on a technique (such as GANs) that would enforce g(z) to be close to the distribution of the real data. Thus training a GAN according to the proposed formulation, would simultaneously enforce the generated data distribution to be close to the real data distribution and match the modal properties of the latent distribution. Conversely, if the latent distribution is chosen in accordance with the modal properties of the real data distribution, the generated data space will be clustered conditioned on the modes of the real data. This is a very desirable property because if the modes of the real data signify semantic separation, then by adopting the proposed procedure one can achieve unsupervised semantically meaningful conditional generation. In the following sections, we detail the procedure to realize the proposed method in a GAN framework and validate the claims on a variety of real world tasks.
3 REALIZATION USING GENERATIVE ADVERSARIAL NETWORKS
GAN realizes the mapping g using a deep neural network. Additionally, to ensure PX is close to the real data distribution a second neural network called the discriminator denoted by d is adversarially trained to differentiate between a true data sample and generated data sample. At the optimum, the generated distribution, PX approaches the distribution of the real data, PXr . We utilize a GAN to realize g and the inversion network h = h1  h2 is also modelled using deep neural networks. All neural networks, g, d and h are simultaneously trained to optimize the following objective function:

min max
g,h12 d

Exr [log d(xr)] + Ez[log (1 - d  g(z)) + ||z - h1  g(z)||p] + DKL(PY^ ||PY )

(1)

The variables and notations in Equation 1 are consistent with the ones in Section 2. The proposed method can be seen as establishing a chain of continuous functions of random variables - y  z  x  z^  y^ and imposing a similar modal structures on all of them. The model being completely unsupervised offers several advantages - (a) Once trained, sampling z from a particular mode confines x to a unique mode leading to conditional generation, (b) when training by imposing a fixed prior on z, unseen attributes in the data can be discovered, (c) post training, the h network can be used independently to cluster the data, (d) if z is well-reconstructed by h1, this architecture can be used for inference (Note that this is not guaranteed since the model convergence can be defined only via convergence of h2 even if z is not reconstructed back by h1), (e) Mode-collapse when generated PX becomes independent of the latent random variable z is avoided in our formulation since g is explicitly forced to produce a multi-modal distribution by the inversion network h.

3

Under review as a conference paper at ICLR 2019
4 RELATED WORK
Data space auto-encoders
In these class of methods, the idea is to learn a mapping from the data to the latent space and back to the data space. Methods such as vanilla autoencoders (AE) (Hinton & Salakhutdinov (2006), variational autoencoders (VAE) Kingma & Welling (2013), adverarial autoencoders (AAE) Makhzani et al. (2015)), mode regularized GANs (Che et al. (2016)), VAE/GAN (Larsen et al. (2015)) fall into this category. While these methods offer several advantages, there are two major issues with this idea -(a) without regularization, a naive AE model ends up learning an identity mapping. Further, a vanilla autoencoder cannot be used as a generative model because latent distribution is not tractable and hence cannot be sampled from, (b) although VAE, AAE, VAE/GAN circumvent this problem by using either variational inference or adversarial training, these models cannot find disentangled mappings and thus cannot be used for unsupervised conditional generation tasks.
Latent space auto-encoders
In an alternative setting, these class of methods essentially learn mappings from the latent space to the data space and backwards, autoencoding the latent/noise space. These methods have an advantage over the data autoencoders in that it is relatively easy to reconstruct the noise vectors as one has control over the distribution from which they are chosen. Methods such as Adversarially learned inference (ALI) (Dumoulin et al. (2016)), Bidirectional GANs (BiGAN) (Donahue et al. (2016), VEEGAN Srivastava et al. (2017)) and structured GANs (Deng et al. (2017)) are examples of this approach. In ALI and BiGAN, an encoder is learned that would map the data space to the latent space and additionally the discriminator is taken to classify between the tuples of encoded data-real data and latent noise-generated data. However in these methods, the latent space encoding is learned independently of the generated data, in the sense that the real-data is inverted back but not the generated data. This is shown to lead to poor quality reconstruction and possibility of mode collapse in the generated data space Srivastava et al. (2017). VEEGAN on the other hand maps both the real and generated data space to a fixed distribution in order to prevent the problem of mode collapse. While these methods are more advantageous over data-space autoencoders for the task at hand, they still do not learn disentangled relation between the data and the latent space as they invert the data to a semantically-meaningless fixed noise distribution.
Regularized information maximizers
Methods such as InfoGAN (Chen et al. (2016)) and CaTGAN (Springenberg (2015)) propose to train the GAN along with a regularization term maximizing the mutual information between the generated data and parts of the latent space, following the idea of regularized information maximization (Krause et al. (2010)). These methods are shown to learn disentangled mappings between the latent and the data space in a completely unsupervised manner albeit they might lead to mode collapse because parts of latent space are not reconstructed. Further since there are no explicit function mapping for noise reconstruction, the learned semantic disentanglement may not be completely faithful (Kim & Mnih (2018); Ghosh et al.). Also inference in the data space is not possible because a major part of the latent space is ignored while information maximization.
4.1 OUR CONTRIBUTIONS
1. Engineering latent/noise space - Almost all the aforementioned methods use unimodal distribution such as Gaussian or Uniform distribution for the latent space. As we have shown in Proposition 1, multiple modes are produced in the data generating space when the latent space has multiple modes. Further, even if reconstructed perfectly, a unimodal noise cannot be used for conditional generation since the semantic relation between the latent and the data space is unknown. In this work we propose to engineer the noise space according to the guess one has on the real data distribution. The specifics for the choice of noise space will depend upon the task and the guess one has on the data prior and will be described in the experiment section.
2. Two-stage latent reconstruction - The latent-space reconstruction network is made a composite of two functions (h1 and h2) one for norm-based reconstruction (g(h1(xr) used in inference) and one to only reproduce the modal properties (g(z) used for conditional gen-
4

Under review as a conference paper at ICLR 2019
eration). Among them, the KL-loss contributes to mode matching and the norm-loss to the variety in the generated data.
3. Devoid of approximate/variational inference - In many formulations such as InfoGAN and VEEGAN, the loss involves an entropy term that requires sampling from an unknown (posterior of the latent variable given the real data) distribution which cannot be computed. These methods rely on minimizing a lower bound on the actual cost via variational inference. However, in our case both the loss terms are exact and hence there is no need for constructing bounds.
Based on the above observations, we call our approach Noise Engineered Mode-matching GAN (NEMGAN). During implementation, a multimodal noise is created by adding uniform noise with a multinomial random variable (akin to y) which is selected in accordance to the assumed data prior. The resulting distribution will be a convolution product of the two distributions resulting in a multimodal distribution with non-overlapping modes.
5 EXPERIMENTS
We conduct three sets of experiments on three real world datasets namely MNIST (LeCun et al. (2010)), CelebA (Liu et al. (2015)) and CIFAR-10 (Krizhevsky & Hinton (2009)), to evaluate our claims. (a) Mode matching - Design noise space in accordance with the semantic imbalance ratio in the data and evaluate conditional generation, (b) Attribute discovery - Impose a fixed but arbitrary prior on noise and see if the generated modes are semantically meaningful, (c) Semantic representation learning - No particular prior is imposed on noise except for the number of modes and evaluate if meaningful representations are learned and (d) Data inference - validate the data reconstruction capabilities of NEMGAN. Note that all experiments are fully unsupervised.
5.1 MODE MATCHING
We take two semantically `distinct' classes (digits 0 and 4) for MNIST based on their t-SNE plots (Maaten & Hinton (2008)), albeit this paradigm is applicable to any combination of digits. Three imbalance ratios of 50:50, 25:75, and 02:98 are considered, signifying no imbalance, slight imbalance and heavy imbalance, respectively in digit 0 and 4. Similar class imbalance is induced in PY and hence in the latent space PZ, by construction. Figure 1 depicts the images generated by NEMGAN corresponding to two noise modes. It can be seen that even with an imbalance ratio of 02:98, the network is able to conditionally generate semantically separated images. Further to illustrate the mode matching, we plot the class-wise histograms of the distances between the individual samples and their corresponding means in Figure 2, for both the noise and the generated data space. It can be seen that there is evidence for mode matching between latent and generated spaces. As another level of sanity check, we evaluate the classification accuracy of the reconstruction network on the real data without imbalance h(xr) and observed accuracy of about 99% for first two and 90% for the case with a class imbalance of 02:98 in the training data. Next to evaluate the semantic separation, we consider images corresponding to digits 3 and 5, which have a significant overlap in the t-SNE with an imbalance ratio of 30:70. The left pane of Figure 3 shows the conditional generation for the considered classes (3 and 5) which shows no overlap between the classes in spite of their closeness in the data manifold. In fact, the inversion network h(xr) achieves a classification accuracy of 98.2 % on real test samples of the digits 3 and 5 in these settings.
We conduct the mode matching experiments with complex color data sets - CIFAR-10 and CelebA. CIFAR in particular is known to have very high intra-class variability. First, we consider two relatively distinct classes namely airplane and frogs, with an imbalance ratio of 30:70. Once again, the modes are matching leading to conditional generation results shown in right pane of Figure 3 with an inversion network classification accuracy of 81.05% on unseen real data. Similarly, the facial images generated conditioned on presence and absence of glasses, when the latent space is induced with an imbalance of 07:93 are shown in first two panes of Figure 4. Furthermore, for an imbalance of 05:95 the network learns to latch on to another facial attribute namely presence or absence of rosy cheeks as observed in last two panes of Figure 4. Another interesting observation in all these experiments is that there is considerable amount of visual variety in the images from each class suggesting reduction of mode collapse even within a single mode with 2% occurrence. These experiments
5

Under review as a conference paper at ICLR 2019

(a) 50:50

(b) 25:75

(c) 02:98

Figure 1: Sample images generated in the experiments with different class ratios. Surprisingly the network is able to do conditional generation even with an imbalance of 02:98 with variety in class corresponding to 2 % (Digit 4).

(a) 50:50

(b) 25:75

(c) 02:98

Figure 2: Distribution of sample distances from the corresponding class means for different ratios. The plots indicate mode matching between the latent and generated data space.

Figure 3: Sample images generated in the experiments with class ratio of 30:70 for digits 3 and 5 in MNIST (left pane), Airplanes and Frogs from CIFAR dataset (right pane). Conditional generation is observed with respective clustering accuracies of 98 and 81.05%.
thus demonstrate the ability of NEMGAN to perform an unsupervised generation conditioned on semantically meaningful classes through mode matching.
5.2 ATTRIBUTE DISCOVERY
In a few real-life scenarios, the class imbalance ratio is unknown apriori. In such cases, an unsupervised technique should discover semantically plausible regions in the data space. To evaluate NEMGANs ability to perform such a task, we perform experiments where sample from PY are drawn with an assumed class ratio rather than a known ratio. Two experiments are performed on CelebA, first with the assumption of 2 classes having a ratio of 70:30 and the second with the assumption of
6

Under review as a conference paper at ICLR 2019

Glasses

Without glasses

Rosy cheeks

Non-rosy cheeks

Figure 4: Sample images generated in the experiments with class ratio of 07:93 for glass and nonglass faces, 05:95 for rosy cheeks and non rosy cheeks for the CelebA dataset.

3 classes having a ratio of 10:30:60. In the first experiment, the network discovers visibility of teeth as an attribute to the faces whereas in the second it learns to differentiate between the facial pose angles. Conditional generation from both the experiments are shown in figure 5 and 6, respectively. Note that these attributes are not labelled in the dataset but are discovered by NEMGAN.

Figure 5: Discovery of the facial attribute smile with teeth visible. Sample images generated in the experiments with class ratio of 70:30 for faces from the CelebA dataset.

Figure 6: Discovery of the attribute facial pose-angle. Sample images generated in the experiments with class ratio of 10:30:60 for from the CelebA dataset.
5.3 SEMANTIC REPRESENTATION LEARNING
In these experiments, we use a PZ that has k uniform modes and train NEMGAN on MNIST and CIFAR-10. In the first experiment, k was set to 10 for both the datasets based on the fact that there are 10 uniformly distributed semantically distinct classes in both of them. Figure 7 depicts the sample from generated images using the MNIST dataset - it can be seen that every generated mode correspond to a digit type. Further a classification accuracy of 96.09% was obtained on the real data from h(xr). To quantify the separability, we compute the class-wise KL divergence between the real and the generated digits (for each class) using a nearest neighbor density estimation technique (Duda et al. (2012)), shown in Figure 7. It can be seen that each mode (class) in the generated data has a unique digit with which the KL is least, signifying a digit-wise semantic separation. Next we repeat the experiment with k = 10 on CIFAR-10. Given the huge intra and inter class variety in the CIFAR it is difficult for a fully unsupervised method to generate ten semantically separated modes. However, as seen in the left most pane of Figure 8, NEMGAN is surprisingly able to generate ten modes with every mode dominated by one CIFAR class. Further we conducted the CIFAR experiment with k = 5 (5 modes) taking five semantically `distinct' classes from CIFAR (horses, airplanes, dogs, frogs and automobiles) and the results are shown in the middle pane of Figure 8. It can be again seen that class-wise separation is qualitatively more in this case as expected since the classes are `more' distinct which is also corroborated from the class-wise KL plot in the rightmost pane of Figure 8. To the best of our knowledge, ours is the first study to report unsupervised class-conditional generation on CIFAR images.
7

Under review as a conference paper at ICLR 2019

Figure 7: Conditional generation of ten MNIST classes using NEMGAN - left pane: Generated digits, right pane: Class-wise KL between generated and real data.

Figure 8: Conditional generation of CIFAR classes using NEMGAN. Every mode in the generated space (one row) is dominated by one CIFAR class in the left pane. This is more so in the 5-class (second pane) case where a better separation is seen evident from the class-wise KL (third pane).
5.4 INFERENCE
In the final set of experiments, though not the main objective of the paper, we qualitatively evaluate NEMGAN on an inference task where the real images are reconstructed using the outputs of the norm (L1, used in this work) loss layer. Figure 9 depicts the inference for arbitrarily chosen 10 digits from MNIST and CIFAR samples from the two class (Frog Vs. Airplane) experiment (more images with qualitative comparison is given in the Appendix). We have observed that the semantic category is almost always preserved. Given that the network was trained with a skewed class ratio of 70-30, it is significant that a class-wise faithful reconstruction is obtained.

(a) MNIST

(b) CIFAR

Figure 9: Inference results of MNIST and CIFAR data. Samples from real test data(top row) are passed through the g(h1(xr)) network to generate the images(bottom row).

6 CONCLUSION
We introduced a method for unsupervised conditional generation using generative adversarial networks through engineered latent space inversion. We demonstrated through multiple experiments, that certain desirable properties could be induced in the generated data by creating a latent noise space that is engineered in accordance to the need. Being fully unsupervised, NEMGAN can be used in multiple applications spanning from conditional generation to data space clustering. It also has several desirable side effects like reduction of the infamous mode collapse. Even though NEMGAN is fully unsupervised, we believe its performance can be improved by augmenting it with some supervision.

8

Under review as a conference paper at ICLR 2019

REFERENCES
Martin Arjovsky, Soumith Chintala, and Le´on Bottou. arXiv:1701.07875, 2017.

Wasserstein gan.

arXiv preprint

Yoshua Bengio, Aaron Courville, and Pascal Vincent. Representation learning: A review and new perspectives. IEEE transactions on pattern analysis and machine intelligence, 35(8):1798­1828, 2013.

Tong Che, Yanran Li, Athul Paul Jacob, Yoshua Bengio, and Wenjie Li. Mode regularized generative adversarial networks. arXiv preprint arXiv:1612.02136, 2016.

Xi Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya Sutskever, and Pieter Abbeel. Infogan: Interpretable representation learning by information maximizing generative adversarial nets. In Advances in neural information processing systems, pp. 2172­2180, 2016.

Brian Cheung, Jesse A Livezey, Arjun K Bansal, and Bruno A Olshausen. Discovering hidden factors of variation in deep networks. arXiv preprint arXiv:1412.6583, 2014.

Zhijie Deng, Hao Zhang, Xiaodan Liang, Luona Yang, Shizhen Xu, Jun Zhu, and Eric P Xing. Structured generative adversarial networks. In Advances in Neural Information Processing Systems, pp. 3899­3909, 2017.

Emily L Denton, Soumith Chintala, Rob Fergus, et al. Deep generative image models using a laplacian pyramid of adversarial networks. In Advances in neural information processing systems, pp. 1486­1494, 2015.

Jeff Donahue, Philipp Kra¨henbu¨hl, and Trevor Darrell. Adversarial feature learning. arXiv preprint arXiv:1605.09782, 2016.

Richard O Duda, Peter E Hart, and David G Stork. Pattern classification. John Wiley & Sons, 2012.

Vincent Dumoulin, Ishmael Belghazi, Ben Poole, Olivier Mastropietro, Alex Lamb, Martin Arjovsky, and Aaron Courville. Adversarially learned inference. arXiv preprint arXiv:1606.00704, 2016.

Arnab Ghosh, Viveka Kulharia, Vinay Namboodiri, Philip HS Torr, and Puneet K Dokania. Multiagent diverse generative adversarial networks.

Ian Goodfellow, Yoshua Bengio, Aaron Courville, and Yoshua Bengio. Deep learning, volume 1. MIT Press, 2016.

Trevor Hastie and Robert Tibshirani. Discriminant analysis by gaussian mixtures. Journal of the Royal Statistical Society. Series B (Methodological), pp. 155­176, 1996.

Trevor Hastie, Robert Tibshirani, and Jerome Friedman. Unsupervised learning. In The elements of statistical learning, pp. 485­585. Springer, 2009.

Geoffrey E Hinton and Ruslan R Salakhutdinov. Reducing the dimensionality of data with neural networks. science, 313(5786):504­507, 2006.

Geoffrey E. Hinton, Simon Osindero, and Yee Whye Teh. A fast learning algorithm for deep belief nets. Neural Computation, 18:1527­1554, 2006.

Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A Efros. Image-to-image translation with conditional adversarial networks. In 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 5967­5976. IEEE, 2017.

Tommi Jaakkola and David Haussler. Exploiting generative models in discriminative classifiers. In Advances in neural information processing systems, pp. 487­493, 1999.

Anil K Jain, M Narasimha Murty, and Patrick J Flynn. Data clustering: a review. ACM computing surveys (CSUR), 31(3):264­323, 1999.

9

Under review as a conference paper at ICLR 2019
Hyunjik Kim and Andriy Mnih. Disentangling by factorising. arXiv preprint arXiv:1802.05983, 2018.
Taeksoo Kim, Moonsu Cha, Hyunsoo Kim, Jung Kwon Lee, and Jiwon Kim. Learning to discover cross-domain relations with generative adversarial networks. arXiv preprint arXiv:1703.05192, 2017.
Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013.
Diederik P Kingma, Shakir Mohamed, Danilo Jimenez Rezende, and Max Welling. Semi-supervised learning with deep generative models. In Advances in Neural Information Processing Systems, pp. 3581­3589, 2014.
Andreas Krause, Pietro Perona, and Ryan G Gomes. Discriminative clustering by regularized information maximization. In Advances in neural information processing systems, pp. 775­783, 2010.
Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. Technical report, Citeseer, 2009.
Anders Boesen Lindbo Larsen, Søren Kaae Sønderby, Hugo Larochelle, and Ole Winther. Autoencoding beyond pixels using a learned similarity metric. arXiv preprint arXiv:1512.09300, 2015.
Yann LeCun, Corinna Cortes, and CJ Burges. Mnist handwritten digit database. AT&T Labs [Online]. Available: http://yann. lecun. com/exdb/mnist, 2, 2010.
Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild. In Proceedings of the IEEE International Conference on Computer Vision, pp. 3730­3738, 2015.
Laurens van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of machine learning research, 9(Nov):2579­2605, 2008.
Alireza Makhzani, Jonathon Shlens, Navdeep Jaitly, Ian Goodfellow, and Brendan Frey. Adversarial autoencoders. arXiv preprint arXiv:1511.05644, 2015.
Lars Mescheder, Sebastian Nowozin, and Andreas Geiger. Adversarial variational bayes: Unifying variational autoencoders and generative adversarial networks. arXiv preprint arXiv:1701.04722, 2017.
Augustus Odena, Christopher Olah, and Jonathon Shlens. Conditional image synthesis with auxiliary classifier gans. arXiv preprint arXiv:1610.09585, 2016.
Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep convolutional generative adversarial networks. arXiv preprint arXiv:1511.06434, 2015.
Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training gans. In Advances in Neural Information Processing Systems, pp. 2234­2242, 2016.
Jost Tobias Springenberg. Unsupervised and semi-supervised learning with categorical generative adversarial networks. arXiv preprint arXiv:1511.06390, 2015.
Akash Srivastava, Lazar Valkoz, Chris Russell, Michael U Gutmann, and Charles Sutton. Veegan: Reducing mode collapse in gans using implicit variational learning. In Advances in Neural Information Processing Systems, pp. 3308­3318, 2017.
Aa¨ron Van Den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves, Nal Kalchbrenner, Andrew W Senior, and Koray Kavukcuoglu. Wavenet: A generative model for raw audio. In SSW, pp. 125, 2016.
L Yann. Modeles connexionnistes de lapprentissage. PhD thesis, PhD thesis, These de Doctorat, Universite Paris 6, 1987.
10

Under review as a conference paper at ICLR 2019

A. Proofs for Propositions

Proposition 1: Let Z0  Z and Z1  Z respectively be the inverse images of X0 and X1 under g, then X0  X1 =  only if Z0  Z1 = , where  is an empty set.
Proof: Given X0  X1 = , assume, Z0  Z1 =  = zi  Z0  Z1. Given zi  Z0, let g(zi) = xi0  X0 and similarly, given zi  Z1, g(zi) = xi1  X1. Since g is a continuous function, xi0 = xi1 = xi = xi  X0  X1 contradicting the fact that X0  X1 = , hence Z0  Z1 = .

Proposition 2: Define a random variable that is an indicator of modes of PX as follows: x^ := 1xX1 , PX^ (x^ = 1) = X1 PX dx and PX^ (x^ = 0) = X0 PX dx. The minimization of DKL(PY^ ||PY ) is equivalent to the minimization of DKL(PY^ ||PX^ ).
Proof: As we know

DKL(PY^ ||PY ) =

y^

PY^

log

PY^ PY

(2)

= PY^ log PY^ - PY^ log PY
y^

(3)

= PY^ log PY^ - PY^ (y^ = 0) log PY (y = 0) - PY^ (y^ = 1) log PY (y = 1) (4)
y^

= PY^ log PY^ - PY^ (y^ = 0) log PZ dz - PY^ (y^ = 1) log PZ dz
y^ Z0 Z1
Since Zi PZ dz = Xi PX dx, equation 3 can be written as

(5)

DKL(PY^ ||PY ) = PY^ log PY^ - PY^ (y^ = 0) log PX dx - PY^ (y^ = 1) log PX dx (6)
y^ X0 X1

Since Xi PX dx = PX^ (x^ = i), by definition, equation 6 can be written as

DKL(PY^ ||PY ) = PY^ log PY^ - PY^ (y^ = 0) log PX^ (x^ = 0) - PY^ (y^ = 1) log PX^ (x^ = 1) (7)
y^

= PY^ log PY^ - PY^ log PX^
y^
= DKL(PY^ ||PX^ )

(8) (9)

Corollary 2.1: Minimizing DKL(PY^ ||PX^ ) leads to matching of modal properties of PX^ and PY , that is, Xi PX dx = PY (y = i), i  {0, 1}.
Proof: The optimum value of DKL(PY^ ||PX^ ) is achieved when PX^ (x^ = i) = P(y = i). Further from equation 9, P(y = i) = P^(y^ = i). Hence, PX^ (x^ = i) = PY (y = i), i  {0, 1}.

11

Under review as a conference paper at ICLR 2019 B.1. Qualitative Comparison with existing architectures

(a) Conditional generation using NEMGAN. (b) Conditional generation using InfoGAN. Figure 10: Conditional generation comparison.
B.2. Inference comparison

(a) Real images from MNIST

(b) Images inferred using g(h1(xr)).

Figure 11: Inference results of MNIST data. Samples from real test data are passed through the g(h1(xr)) network to generate the images.

(a) Real images from MNIST

(b) Images inferred using BiGAN

Figure 12: Inference results of MNIST data from BiGAN.

12

Under review as a conference paper at ICLR 2019

(a) Top: real images of frogs from CIFAR dataset, (b) Top: real images of airplane from CIFAR dataset, bottom: Inference results obtained using NEMGAN. bottom: Inference results obtained using NEMGAN.
Figure 13: Top: real images from CIFAR, bottom: Inference results obtained using NEMGAN.

(a) Top: real images of frogs from CIFAR dataset, (b) Top: real images of airplane from CIFAR dataset,

bottom: Inference results obtained using ALI.

bottom: Inference results obtained using ALI.

Figure 14: Top: real images from CIFAR dataset, bottom: Inference results obtained using ALI.

C. Conditional generation of CIFAR images

Figure 15: Conditional generation of CIFAR 5 classes using NEMGAN. It is seen that the every mode in the generated space (one row) represents one CIFAR class.

13

Under review as a conference paper at ICLR 2019

Figure 16: Conditional generation of CIFAR 10 classes using NEMGAN. It is seen that the every mode in the generated space (one row) is dominated by one CIFAR class.

D. Network architectures used in the work

Layer
Input Fully Connected Fully Connected
Reshape Dropout UpSampling Conv2DTranspose UpSampling Conv2DTranspose Conv2DTranspose Conv2DTranspose Conv2DTranspose Conv2DTranspose Conv2DTranspose

Table 1: MNIST Generator architecture Output dimension Kernel size Batch Normalization

50
1024
6272 7×7×128 7×7×128 14×14×128 14×14×64 28×28×64 28×28×32 28×28×16 28×28×8 28×28×4 28×28×2 28×28×1

5×5 5×5 5×5 5×5 5×5 5×5 5×5

Yes Yes No No No Yes No Yes Yes Yes Yes Yes Yes

Activation
LeakyReLU LeakyReLU
LeakyReLU LeakyReLU LeakyReLU LeakyReLU LeakyReLU LeakyReLU Sigmoid

14

Under review as a conference paper at ICLR 2019

Layer

Table 2: MNIST Discriminator architecture Output dimension Kernel size Activation

Input

28×28×1

-

-

Conv2D

14×14×64

5×5 LeakyReLU

Conv2D

7×7×128

5×5 LeakyReLU

Conv2D

4×4×256

5×5 LeakyReLU

Conv2D

4×4×512

5×5 LeakyReLU

Flatten

8192

--

Fully Connected

1

- Sigmoid

Dropout
Yes Yes Yes Yes No No

Layer

Table 3: MNIST Inversion Network Output dimension Kernel size

Input

28×28×1

-

Conv2D

14×14×32

5×5

Conv2D Conv2D

7×7×64 4×4×128

5×5 5×5

Conv2D

2×2×256

5×5

Conv2D

1×1×512

5×5

Flatten

512 -

Fully Connected

50

-

Additive Margin Softmax

10

-

Activation
LeakyReLU LeakyReLU LeakyReLU LeakyReLU LeakyReLU
LeakyReLU
-

Dropout
Yes Yes Yes Yes Yes No No
-

Layer
Input Fully Connected
Reshape Conv2DTranspose
UpSampling Conv2DTranspose
UpSampling Conv2DTranspose
UpSampling Conv2DTranspose Conv2DTranspose Conv2DTranspose

Table 4: CIFAR Generator architecture Output dimension Kernel size Batch Normalization

50
4096 4×4×256 4×4×256 8×8×256 8×8×128 16×16×128 16×16×64 32×32×64 32×32×32 32×32×16 32×32×3

5×5 3×3 3×3 5×5 3×3 3×3

Yes No Yes No Yes No Yes No Yes Yes Yes

Activation
LeakyReLU
LeakyReLU
LeakyReLU
LeakyReLU
LeakyReLU LeakyReLU
Tanh

Layer

Table 5: CIFAR Discriminator architecture Output dimension Kernel size Activation

Input Conv2D Conv2D Conv2D Conv2D Conv2D Conv2D Conv2D Conv2D

32×32×3 14×14×64 12×12×64 10×10×128 8×8×128 6×6×256 4×4×256 2×2×512 1×1×512

-5×5 LeakyReLU 3×3 LeakyReLU 3×3 LeakyReLU 3×3 LeakyReLU 3×3 LeakyReLU 3×3 LeakyReLU 3×3 LeakyReLU 2×2 LeakyReLU

Flatten 512 - -

Fully Connected

1

1 Sigmoid

Dropout
Yes Yes Yes Yes Yes Yes Yes Yes No No

15

Under review as a conference paper at ICLR 2019

Layer

Table 6: CIFAR Inversion Network Output dimension Kernel size

Input

32×32×3

-

Conv2D

14×14×48

5×5

Conv2D

10×10×48

3×3

Conv2D

6×6×96

3×3

Conv2D

6×6×96

3×3

Conv2D

4×4×384

3×3

Conv2D

4×4×384

3×3

Flatten

6144

-

Fully Connected

50 50

Additive Margine Softmax

10

-

Activation
LeakyReLU LeakyReLU LeakyReLU LeakyReLU LeakyReLU LeakyReLU
Sigmoid
-

Dropout
Yes Yes Yes Yes Yes Yes No No
-

16


Under review as a conference paper at ICLR 2019
STOCHASTIC GRADIENT/MIRROR DESCENT: MINIMAX OPTIMALITY AND IMPLICIT REGULARIZATION
Anonymous authors Paper under double-blind review
ABSTRACT
Stochastic descent methods (of the gradient and mirror varieties) have become increasingly popular in optimization. In fact, it is now widely recognized that the success of deep learning is not only due to the special deep architecture of the models, but also due to the behavior of the stochastic descent methods used, which play a key role in reaching "good" solutions that generalize well to unseen data. In an attempt to shed some light on why this is the case, we revisit some minimax properties of stochastic gradient descent (SGD) for the square loss of linear models--originally developed in the 1990's--and extend them to general stochastic mirror descent (SMD) algorithms for general loss functions and nonlinear models. In particular, we show that there is a fundamental identity which holds for SMD (and SGD) under very general conditions, and which implies the minimax optimality of SMD (and SGD) for sufficiently small step size, and for a general class of loss functions and general nonlinear models. We further show that this identity can be used to naturally establish other properties of SMD (and SGD), namely convergence and implicit regularization for over-parameterized linear models (in what is now being called the "interpolating regime"), some of which have been shown in certain cases in prior literature. We also argue how this identity can be used in the so-called "highly over-parameterized" nonlinear setting (where the number of parameters far exceeds the number of data points) to provide insights into why SMD (and SGD) may have similar convergence and implicit regularization properties for deep learning.
1 INTRODUCTION
Deep learning has proven to be extremely successful in a wide variety of tasks (Krizhevsky et al., 2012; LeCun et al., 2015; Mnih et al., 2015; Silver et al., 2016; Wu et al., 2016). Despite its tremendous success, the reasons behind the good generalization properties of these methods to unseen data is not fully understood (and, arguably, remains somewhat of a mystery to this day). Initially, this success was mostly attributed to the special deep architecture of these models. However, in the past few years, it has been widely noted that the architecture is only part of the story, and, in fact, the optimization algorithms used to train these models, typically stochastic gradient descent (SGD) and its variants, play a key role in learning parameters that generalize well.
In particular, it has been observed that since these deep models are highly over-parameterized, they have a lot of capacity, and can fit to virtually any (even random) set of data points (Zhang et al., 2016). In other words, highly over-parametrized models can "interpolate" the data, so much so that this regime has been called the "interpolating regime" (Ma et al., 2018). In fact, on a given dataset, the loss function often has (uncountably infinitely) many global minima, which can have drastically different generalization properties, and it is not hard to construct "trivial" global minima that do not generalize. Which minimum among all the possible minima we pick in practice is determined by the optimization algorithm that we use for training the model. Even though it may seem at first that, because of the non-convexity of the loss function, the stochastic descent algorithms may get stuck in local minima or saddle points, in practice they almost always achieve a global minimum (Kawaguchi, 2016; Zhang et al., 2016; Lee et al., 2016), which perhaps can also be justified by the fact that these models are highly over-parameterized. What is even more interesting is that not only do these stochastic descent algorithms converge to global minima, but they converge to "special" ones that generalize well, even in the absence of any explicit regularization or early stopping (Zhang
1

Under review as a conference paper at ICLR 2019

et al., 2016). Furthermore, it has been observed that even among the common optimization algorithms, namely SGD or its variants (AdaGrad (Duchi et al., 2011), RMSProp (Tieleman & Hinton, 2012), Adam (Kingma & Ba, 2014), etc.), there is a discrepancy in the solutions achieved by different algorithms and their generalization capabilities (Wilson et al., 2017), which again highlights the important role of the optimization algorithm in generalization.
There have been many attempts in recent years to explain the behavior and properties of these stochastic optimization algorithms, and many interesting insights have been obtained (Achille & Soatto, 2017; Chaudhari & Soatto, 2018; Shwartz-Ziv & Tishby, 2017; Soltanolkotabi et al., 2017). In particular, it has been argued that the optimization algorithms perform an implicit regularization, while optimizing the loss function, which is why the solution generalizes well (Neyshabur et al., 2017; Ma et al., 2017; Gunasekar et al., 2017; 2018; Soudry et al., 2018). Despite this recent progress, most results explaining the behavior of the optimization algorithm, even for SGD, are limited to linear or very simplistic models. Therefore, a general characterization of the behavior of stochastic descent algorithms for more general models would be of great interest.
1.1 OUR CONTRIBUTION
In this paper, we present an alternative explanation of the behavior of SGD, and more generally, the stochastic mirror descent (SMD) family of algorithms, which includes SGD as a special case. We do so by obtaining a fundamental identity for such algorithms. We show that, for general nonlinear models and general loss functions, when the step size is sufficiently small, SMD (and therefore also SGD) is the optimal solution of a certain minimax filtering (or online learning) problem. Our results are inspired by and rooted in H filtering theory, which was originally developed in the 1990's in the context of robust control theory (Hassibi et al., 1999; Simon, 2006; Hassibi et al., 1996), and we generalize several results from this literature, e.g., (Hassibi et al., 1994; Kivinen et al., 2006). Furthermore, we show that many properties recently proven in the literature, such as the implicit regularization of SMD in the over-parameterized linear case--when convergence happens--(Gunasekar et al., 2018), naturally follows naturally from this theory. The theory also allows us to establish new results, such as the convergence (in a deterministic sense) of SMD in the over-parametrized linear case. We also use the theory developed in this paper to provide some speculative arguments into why SMD (and SGD) may have similar convergence and implicit regularization properties in the socalled "highly over-parameterized" nonlinear setting (where the number of parameters far exceeds the number of data points) common to deep learning.
In an attempt to make the paper easier to follow, we first describe the main ideas and results in a simpler setting, namely, SGD on the square loss of linear models, in Section 3, and mention the connections to H theory. The full results, for SMD on a general class of loss functions and for general nonlinear models, are presented in Section 4. We demonstrate some implications of this theory, such as deterministic convergence and implicit regularization, in Section 5, and we finally conclude with some remarks in Section 6. Most of the formal proofs are relegated to the appendix.

2 PRELIMINARIES

Denote the training dataset by {(xi, yi) : i = 1, . . . , n}, where xi  Rd are the inputs, and yi  R are the labels. We assume that the data is generated through a (possibly nonlinear) model fi(w) = f (xi, w) with some parameter vector w  Rm, plus some noise vi, i.e., yi = f (xi, w) + vi for i = 1, . . . , n. The noise can be due to actual measurement error, or it can be due to modeling error (if the model f (xi, ·) is not rich enough to fully represent the data), or it can be a combination of both. As a result, we do not make any assumptions on the noise (such as stationarity, whiteness, Gaussianity, etc.).
Since typical deep models have a lot of capacity and are highly over-parameterized, we are particularly interested in the over-parameterized (so-caled interpolating) regime, i.e., when m > n. In this case, there are many parameter vectors w (in fact, uncountably infinitely many) that are consistent with the observations. We denote the set of these parameter vectors by

W = {w  Rm | yi = f (xi, w), i = 1, . . . , n} .

(1)

2

Under review as a conference paper at ICLR 2019

(Note the absence of the noise term, since in this regime we can fully interpolate the data.) The set
W is typically an (m - n)-dimensional manifold and depends only on the training data {(xi, yi) : i = 1, . . . , n} and nonlinear model f (·, ·).

The total loss (empirical risk) on the training set can be denoted by L(w) =

n i=1

Li(w),

where

Li(·) is the loss on the individual data point i. We assume that the loss Li(·) depends only on the

residual, i.e., the difference between the prediction and the true label. In other words,

Li(w) = l(yi - f (xi, w)),

(2)

where l(·) can be any nonnegative differentiable function with l(0) = 0. Typical examples of l(·)
include square (l2) loss, We remark that, in the interpolating regime, every parameter vector in the set W renders each individual loss zero, i.e., Li(w) = 0, for all w  W.

3 WARM-UP: REVISITING SGD ON SQUARE LOSS OF LINEAR MODELS

In this section, we describe the main ideas and results in a simple setting, i.e., stochastic gradient

descent (SGD) for the square loss of a linear model, and we revisit some of the results from H

theory (Hassibi et al., 1999; Simon, 2006). In this case, the data model is simply yi = xTi w + vi,

i

=

1, . . . , n

and

the

loss

function

is

Li(w)

=

1 2

(yi

- xTi w)2.

Assuming the data is indexed randomly, the SGD updates are defined as wi = wi-1 - Li(wi-1), where  > 0 is the step size or learning rate.1 The update in this case can be expressed as

wi = wi-1 + xi yi - xTi wi-1 ,

(3)

for i  1 (we cycle through the data, or select them at random, for i > n).

3.1 CONSERVATION OF UNCERTAINTY

Consider the general setting where the labels are generated by yi = f (xi, w)+vi, for some unknown parameter vector w and some unknown noise vi. Prior to the i-th step of any optimization algorithm, we have two sources of uncertainty: our uncertainty about the unknown parameter vector w, which we can represent by w - wi-1, and our uncertainty about the i-th data point (xi, yi), which we can represent by the noise vi. After the i-th step, the uncertainty about w is transformed to w - wi. But what about the uncertainty in vi? What is it transformed to? In fact, we will view any optimization algorithm as one which redistributes the uncertainties at time i - 1 to new uncertainties at time i.
The two uncertainties, or error terms, we will consider are ei and ep,i, defined as follows.

ei := yi - xTi wi-1, and ep,i := xTi w - xiT wi-1.

(4)

ei is often referred to as the innvovations and is the error in predicting yi, given the input xi. ep,i
is sometimes called the prediction error, since it is the error in predicting the noiseless output xiT w, i.e., in predicting what the best output of the model is. In the absence of noise, ei and ep,i coincide.

One can show that SGD transforms the uncertainties in the fashion specified by the following lemma, which was first noted in (Hassibi et al., 1996).
Lemma 1. For any parameter w and noise values {vi} that satisfy yi = xTi w + vi for i = 1, . . . , n, and for any step size  > 0, the following relation holds for the stochastic gradient descent updates {wi} given in Eq. (3)

w - wi-1 2 + vi2 = w - wi 2 +  1 -  xi 2 e2i + e2p,i, i  1.

(5)

As illustrated in Figure 1, this means that each step of SGD can be thought of as a lossless transformation of the input uncertainties to the output uncertainties, with the specified coefficients.

Once one yi - xTi w

knows this as vi = (yi

result, proving - xTi wi-1) -

(ixt iTiswst-raixgiThtwfoi-rw1a).rdM. uTlotipselyeinthgabt,onthotseidtehsatbwy eca,nwwerhitaevvei

=

 vi

=

 (yi

-

xiT

wi-1)

-

(xiT

w

-

xiT

wi-1).

(6)

1For the sake of simplicity of presentation, we present the results for constant step size. We show in the appendix that all the results extend to the case of time-varying step-size.

3

Under review as a conference paper at ICLR 2019

Figure 1: Each step of SGD can be viewed as a transformation of the uncertainties with the right coefficients.

On the other hand, subtracting both sides of the update rule (3) from w yields

w - wi = (w - wi-1) -  yi - xiT wi-1 xi.

(7)

Squaring both sides of (6) and (7), and subtracting the results leads to Equation (5).

A nice property of Equation (5) is that, if we sum over all i = 1, . . . , T , the terms w - wi 2 and w - wi-1 2 on different sides cancel out telescopically, leading to the following important lemma.
Lemma 2. For any parameter w and noise values {vi} that satisfy yi = xiT w + vi for i = 1, . . . , n, any initialization w0, any step size  > 0, and any number of steps T  1, the following relation holds for the stochastic gradient descent updates {wi} given in Eq. (3)

T TT

w - w0 2 +  vi2 = w - wT 2 + 

1 -  xi 2 ei2 +  ep2,i.

i=1 i=1 i=1

(8)

As we will show next, this identity captures most properties of SGD, and implies several important results in a very transparent fashion. For this reason, this relation can be viewed as a "fundamental identity" for SGD.

3.2 MINIMAX OPTIMALITY OF SGD

If we were in an online setting, for a given number of steps T , we would like the total prediction

errors

T i=1

ep2,i

to

be

small,

while

also

achieving

a

final

wT

that

is

close

to

the

true

w.

On

the

other

hand, the overall incoming uncertainty/disturbance from nature consists of the total noise

T i=1

vi2

and the error in the initial guess w0 from the true w. So ideally, we would like to have the ratio of

a weighted combination of w - wT 2 and

T i=1

ep2,i

over

a

weighted

combination

of

w - w0 2

and

T i=1

vi2

to

be

small.

If

we

do

not

have

any

knowledge

of

the

disturbances

or

their

distribution,

then a conservative choice would be to minimize this ratio for the worst-case disturbance. It turns

out that that is exactly what SGD does, and the weights are nothing but the learning rate.

Theorem 3. For any initialization w0, any step size 0 <   mini

1 xi

2,

and

any

number

of

steps

T  1, the stochastic gradient descent iterates {wi} given in Eq. (3) are the optimal solution to the

following minimization problem

min max w - wT 2 +  {wi} w,{vi} w - w0 2 +  Furthermore, the optimal value (achieved by SGD) is 1.

T i=1 T i=1

ep2,i vi2

.

(9)

This result in fact states that SGD is choosing the best estimate that safeguards against the worstcase disturbances, which is a conservative choice. However, this choice may actually be the rational thing to do in situations when we do not have much knowledge about the disturbances, which is the case in many machine learning tasks.

Theorem 3 holds for any horizon T  1. A variation of this result, i.e., when T   and without

the w - wT 2 term in the numerator, was first shown in (Hassibi et al., 1994; 1996). In that case,

the ratio that maps

wth-ewu0nk2i+=n1owe2pn,ii=d1isvti2urinbatnhceems i(nwim-axwp0r,o{blemvii}s)intofathcet

the H norm of the transfer prediction errors {ep,i}.

operator

4

Under review as a conference paper at ICLR 2019

3.3 CONVERGENCE AND IMPLICIT REGULARIZATION

The over-parameterized (interpolating) linear regression regime is a simple but instructive setting,

recently considered in some papers (Gunasekar et al., 2018; Zhang et al., 2016). In this setting, we

can show that, for sufficiently small step, i.e.  < mini

1 xi

2,

SGD

always

converges

to

a

special

solution among all the solutions W, in particular to the one with the smallest l2 distance from w0.

In other words, if, for example, initialized at zero, SGD implicitly regularizes the solution according

to an l2 norm. This result follows directly from Lemma 2.

To see that, note that in the interpolating case the vi are zero, and we have ei = ep,i = yi-xTi wi-1 = xTi w - xiT wi-1. Hence, identity (8) reduces to

T

w - w0 2 = w - wT 2 + 

2 -  xi 2 ei2,

i=1

(10)

for all w  W. By dropping the w - wT 2 term and taking T  , we have

 i=1

2 -  xi

2

ei2



w - w0

2, which implies that, for 

<

mini

2 xi

2,

we

must

have

ei  0 as i  . When ei = yi - xTi wi-1 goes to zero, the updates in (3) vanish and we get

convergence, i.e., w  w. Further, again because ei  0, all the data points are being fit, which

means w  W. Moreover, it is again very straightforward to see from (10) that the solution con-

verged to is the one with minimum Euclidean norm from the initial point. To see that, notice that

the summation term in Eq. (10) is independent of w (it depends only on xi, yi and w0). Therefore, by taking T   and minimizing both sides with respect to w  W, we get

w = arg min w - w0 .
wW

(11)

Once again, this also implies that if SGD is initialized at the origin, i.e., w0 = 0, then it converges to the minimum-l2-norm solution, among all the solutions.

4 MAIN RESULT: GENERAL CHARACTERIZATION OF STOCHASTIC MIRROR DESCENT

Stochastic Mirror Descent (SMD) (Nemirovskii et al., 1983; Beck & Teboulle, 2003; Cesa-Bianchi et al., 2012; Zhou et al., 2017) is one of the most widely used family of algorithms for stochastic optimization, which includes SGD as a special case. In this section, we provide a characterization of the behavior of general SMD, on general loss functions and general nonlinear models, in terms of the fundamental identity and minimax optimality.

For any strictly convex and differentiable potential (·), the corresponding SMD updates are defined

as

wi = arg min wT Li(wi-1) + D(w, wi-1),

(12)

w

where

D(w, wi-1) = (w) - (wi-1) - (wi-1)T (w - wi-1)

(13)

is the Bregman divergence with respect to the potential function (·). Since the potential function

is strictly convex, the updates can be equivalently written as

(wi) = (wi-1) - Li(wi-1),

(14)

which are uniquely defined because of the invertibility of  (implied by the strict convexity of (·)). In other words, stochastic mirror descent can be thought of as transforming the variable w, with a mirror map (·), into (w), and performing SGD on the new variable. For this reason, (w) is often referred to as the dual variable, while w is the primal variable.

Different choices of the potential function (·) yield different optimization algorithms, which, as

we will see, result in different implicit regularizations. To name a few examples: For the potential

function

(w)

=

1 2

w

2,

the

Bregman

divergence

is

D(w, w

)

=

1 2

w-w

2, and the update rule

reduces to that of SGD. For (w) = j wj log wj, the Bregman divergence becomes the unnormal-

ized relative entropy (Kullback-Leibler divergence) D(w, w ) =

j

wj

log

wj wj

-

j wj +

j wj ,

5

Under review as a conference paper at ICLR 2019

which corresponds to the exponentiated gradient descent (aka the exponential weights) algorithm.

Other examples include (w) =

1 2

w

2 Q

=

1 2

wT

Qw

for

a

positive

definite

matrix

Q,

which

yields

D(w, w ) =

1 2

(w

-

w

)T

Q(w

-

w

),

and

the

q-norm

squared

(w)

=

1 2

w

2 q

with

1 p

+

1 q

= 1,

which yields the p-norm algorithms (Grove et al., 2001; Gentile, 2003).

In order to derive an equivalent "conservation law" for SMD, similar to the identity (5), we first need to define a new measure for the difference between the parameter vectors w and w according to the loss function Li(·). To that end, let us define

DLi (w, w ) := Li(w) - Li(w ) - Li(w )T (w - w ),

(15)

which is defined in a similar way to a Bregman divergence for the loss function Li. The difference though is that, unlike the potential function of the Bregman divergence, the loss function Li does not have to be convex, and as a result, the difference DLi (w, w ) does not have to be nonnegative. The following result, which is the general counterpart of Lemma 1, states the identity that characterizes
SMD updates in the general setting.

Lemma 4. For any (nonlinear) model f (·, ·), any differentiable loss l(·), any parameter w and noise
values {vi} that satisfy yi = f (xi, w) + vi for i = 1, . . . , n, and any step size  > 0, the following relation holds for the stochastic mirror descent updates {wi} given in Eq. (14)

D(w, wi-1) + l(vi) = D(w, wi) + Ei(wi, wi-1) + DLi (w, wi-1), for all i  1, where

(16)

Ei(wi, wi-1) := D(wi, wi-1) - DLi (wi, wi-1) + Li(wi).

(17)

The proof is provided in Appendix A. Note that Ei(wi, wi-1) is not a function of w. Furthermore, even though it does not have to be nonnegative in general, for  sufficiently small, it becomes nonnegative, because the Bregman divergence D(., .) is nonnegative.
Summing Equation (16) over all i = 1, . . . , T leads to the following identity, which is the general counterpart of Lemma 2.
Lemma 5. For any (nonlinear) model f (·, ·), any differentiable loss l(·), any parameter w and noise values {vi} that satisfy yi = f (xi, w) + vi for i = 1, . . . , n, any initialization w0, any step size  > 0, and any number of steps T  1, the following relation holds for the stochastic mirror descent updates {wi} given in Eq. (14)

TT
D(w, w0) +  l(vi) = D(w, wT ) + (Ei(wi, wi-1) + DLi (w, wi-1)) .
i=1 i=1

(18)

We should reiterate that Lemma 5 is a fundamental property of SMD, which allows one to prove many important results, in a direct way.

In particular, in this setting, we can show that SMD is minimax optimal in a manner that generalizes Theorem 3 of Section 3, in the following 3 ways: 1) General mirror descent, 2) General model f (·, ·), and 3) General loss function l(·). The result is as follows.
Theorem 6. Consider any (nonlinear) model f (·, ·), any differentiable loss l(·) with property l(0) = l (0) = 0, and any initialization w0. For sufficiently small step size, i.e., for any  for which (w) - Li(w) is convex for all i, and for any number of steps T  1, the stochastic mirror descent iterates {wi} given by Eq. (14), w.r.t. any strictly convex potential (·), are the optimal solution to the following minimization problem

min
{wi }

max
w,{vi }

D(w, wT ) +  D(w, w0)

+

T i=1


DLi
T i=1

(w, wi-1 l(vi)

)

.

Furthermore, the optimal value (achieved by SMD) is 1.

(19)

The proof is provided in Appendix B. For the case of square loss and a linear model, the result reduces to the following form.

6

Under review as a conference paper at ICLR 2019

Corollary 7. For Li

i.e., 0 <  

 xi 2

(w)

=

1 2

(yi

-

xiT

w)2

, and any number of

, for any steps T

initialization w0, any sufficiently small  1, the SMD iterates {wi} given by

step Eq.

size, (14),

w.r.t. any -strongly convex potential (·), are the optimal solution to the following minimization

problem

min

max

D (w,

wT )

+

 2

{wi} w,{vi}

D(w, w0)

+

 2

T i=1 T i=1

ep2,i vi2

.

(20)

The optimal value (achieved by SMD) is 1.

We should remark that Theorem 6 and Corollary 7 generalize several known results in the literature.

In particular, as mentioned in Section 3, the result of (Hassibi et al., 1994) is a special case of Corol-

lary 7 for (w) =

1 2

w

2.

Furthermore, our result generalizes the result of (Kivinen et al., 2006),

which is the special case for the p-norm algorithms, again, with square loss and a linear model. An-

other interesting connection to the literature is that it was shown in (Hassibi & Kailath, 1995) that SGD is locally minimax optimal, with respect to the H norm. Strictly speaking, our result is not

a generalization of that result; however, Theorem 6 can be interpreted as SGD/SMD being globally

minimax optimal, but with respect to a different metric in the numerator and denominator.

5 CONVERGENCE AND IMPLICIT REGULARIZATION IN OVER-PARAMETERIZED MODELS

In this section, we show some of the implications of the theory developed in the previous section. In particular, we show convergence and implicit regularization, in the over-parameterized (so-called interpolating) regime, for general SMD algorithms. We first consider the linear interpolating case, which has been studied in the literature, and show that the known results follow naturally from our Lemma 5. Further, we shall obtain some new convergence results. Finally, we discuss the implications for nonlinear models, and argue that qualitatively the same results hold in highly-overparameterized settings, which is a typical scenario in deep learning.

5.1 OVER-PARAMETERIZED LINEAR MODELS

In this setting, the vi are zero, W = w | yi = xiT w, i = 1, . . . , n , and Li(w) = l(yi - xTi w), with any differentiable loss l(·). Therefore, Eq. (18) reduces to

T
D(w, w0) = D(w, wT ) + (Ei(wi, wi-1) + DLi (w, wi-1)) ,
i=1

(21)

for all w  W, where

DLi (w, wi-1) = Li(w) - Li(wi-1) - Li(wi-1)T (w - wi-1) = 0 - l(yi - xTi wi-1) + l (yi - xiT wi-1)xTi (w - wi-1) = -l(yi - xTi wi-1) + l (yi - xTi wi-1)(yi - xiT wi-1)

(22) (23) (24)

which is notably independent of w. As a result, we can easily minimize both sides of Eq. (21) with respect to w  W, which leads to the following result.
Proposition 8. For any differentiable loss l(·), any initialization w0, and any step size , consider the stochastic mirror descent iterates given in Eq. (14) with respect to any strictly convex potential (·). If the iterates converge to a solution w  W, then

w = arg min D(w, w0).
wW

(25)

Remark. In particular, for the initialization w0 = arg minwRm (w), if the iterates converge to a solution w  W, then

w = arg min (w).

(26)

wW

An equivalent form of Proposition 8 has been shown recently in, e.g., (Gunasekar et al., 2018). Note that the result of (Gunasekar et al., 2018) does not say anything about whether the algorithm

7

Under review as a conference paper at ICLR 2019

converges or not. However, our fundamental identity of SMD (Lemma 5) allows us to also establish convergence to the regularized point, for some common cases, which will be shown next.
What Proposition 8 says is that depending on the choice of the potential function (·), the optimization algorithm can perform an implicit regularization without any explicit regularization term. In other words, for any desired regularizer, if one chooses a potential function that approximates the regularizer, we can run the optimization without explicit regularization, and if it converges to a solution, the solution must be the one with the minimum potential.
Next we establish convergence to the regularized point for the convex case.
Proposition 9. Consider the following two cases.

(i) l(·) is a differentiable and strictly convex function, (·) is a strictly convex function, and  > 0 is such that  - Li is convex for all i, or

(ii) l(·) is a strongly quasi-convex function, (·) is an -strongly convex function, and 0 <  

min .i

|yi-xiT wi-1| xi 2|l (yi-xTi wi-1)|

If either (i) or (ii) holds, then for any initialization w0, the stochastic mirror descent iterates given in Eq. (14) converge to

w = arg min D(w, w0).
wW

(27)

The proof is provided in Appendix C.

5.2 DISCUSSION OF HIGHLY OVER-PARAMETERIZED NONLINEAR MODELS

Let us consider the highly-overparameterized nonlinear model

yi = f (xi, w), i = 1, . . . , n

(28)

where by highly-overparameterized we mean m n. Since the model is highly over-parameterized,
it is assumed that we can perfectly interpolate the data points (xi, yi) so that the noise vi is zero. In this case, the set of parameter vectors that interpolate the data is given by W = {w  Rm | yi = f (xi, w), i = 1, . . . , n}, and Eq. (18), again, reduces to

T
D(w, w0) = D(w, wT ) + (Ei(wi, wi-1) + DLi (w, wi-1)) ,
i=1

(29)

for all w  W. In order to show convergence, using the above identity, one needs to show that
the terms inside the summation are non-negative. Because if that is the case, both Ei(wi, wi-1) and DLi (w, wi-1) must approach zero as i  , which would imply the convergence of wi to a point in W. Since Ei(wi, wi-1) = D(wi, wi-1) - DLi (wi, wi-1) + Li(wi), by taking  to be small enough, one can guarantee that Ei(wi, wi-1)  0. To see why, simply take  small enough so that (·) - Li(·) is strictly convex (which can always be done since (·) is strictly convex) which means that its Bregman divergence, D(wi, wi-1) - DLi (wi, wi-1), is non-negative. Since the loss Li(·) is also non-negative, Ei(wi, wi-1)  0.

Therefore, to establish convergence, all we need to do is to guarantee DLi (w, wi-1)  0. But this

is clearly not always true because for a general nonlinear model f (·, ·), the loss function Li(w) =

l(yi-f (xi, w))

is not

convex

(Take,

for example,

even the

square loss,

Li(w)

=

1 2

(yi

-f

(xi,

w))2.).

However, in any small enough neighborhood around any point w  W, the loss function Li(w) will

be convex (since the Li(w) attain their minimum on W and so will be convex in a neighborhood

around W).

The upshot of this discussion is that, if w0 is close enough to W, then all the DLi (w, wi-1)  0 and hence wi  w  W. Of course, this is simply stating that, if w0 is close enough to W then we
converge to a global minimum, which may not be too surprising a statement.

The next question is what do we converge to? Looking again at (29) we note that E(wi, wi-1) is independent of w  W, as before. However, unlike the linear case, DLi (w, wi-1) = Li(w) -

8

Under review as a conference paper at ICLR 2019

Li(wi-1) - Li(wi-1)T (w - wi-1) = -Li(wi-1) - Li(wi-1)T (w - wi-1) does depend on w. Therefore taking T  , all we can write is



arg min D(w, w0) = arg min D(w, w) +  DLi (w, wi-1) .

wW

wW

i=1

(30)

At this point, one can hope that, if  is small and w0 is close enough to W, then the second term on the right-hand side of the above is small and so

arg min D(w, w0)  arg min D(w, w).

wW

wW

(31)

In other words, if  is small and w0 is close enough to W, then SMD converges to a point in W that is close to the nearest point to w0 in Bregman divergence, i.e., SMD approximately performs implicit regularization.

This might appear to be an unsatisfying result, since it is predicated on w0 being close to W. How-

ever, one may argue that in the highly-overparameterized regime, m n, it perhaps is not. The

dimension of the manifold W is m - n, which means that any randomly chosen w0 will have a very

large component when projected onto W. For example, if W were an (m - n)-dimensional lin-

ear space, the distance of a randomly chosen w0 to this space would simply be

n m

w0

w0 .

Thus, we may expect that, when m n, the distance of any randomly chosen w0 to W will be small

and so SMD will converge to a point on W that approximately performs implicit regularization.

Of course, this was a very heuristic argument that merits a much more careful analysis. But it is suggestive of the fact that SGD and SMD, when performed on highly-overparameterized nonlinear models, as occurs in deep learning, may have implicit regularization properties.

6 CONCLUDING REMARKS

In this paper, we derived a fundamental identity for characterizing the behavior of general stochastic mirror descent algorithms, and showed that many fundamental properties of the algorithm can be deduced directly from this identity. In particular, we showed that for sufficiently small step size, SMD is the optimal solution to a minimax problem, for general loss functions and general nonlinear models. Further, we demonstrated that in many cases both convergence and implicit regularization can be naturally derived from this theory.
We should remark that all the results stated throughout the paper extend to the case of time-varying step size i, with minimal modification. In particular, it is easy to show that in this case, the identity (the counterpart of Eq. (18)) becomes

TT
D(w, w0) + il(vi) = D(w, wT ) + (Ei(wi, wi-1) + iDLi (w, wi-1)) ,
i=1 i=1

(32)

where Ei(wi, wi-1) = D(wi, wi-1) - iDLi (wi, wi-1) + iLi(wi). As a consequence, our main result will be the same as in Theorem 6, with the only difference that the small-step-size condition in this case is the convexity of (w) - iLi(w) for all i, and the SMD with time-varying step size will be the optimal solution to the following minimax problem

min
{wi }

max
w,{vi }

D(w, wT ) + D(w, w0)

T i=1
+

iDLi (w, wi-1)

T i=1

il(vi)

.

(33)

Similarly, the convergence and implicit regularization results can be proven under the same conditions (See Appendix D for more details on the time-varying case).

This paper opens up a variety of important directions for future work. Most of the analysis developed here is general, in terms of the model, the loss function, and the potential function. Therefore, it would be interesting to study the implications of this theory for specific classes of models (such as different neural networks), specific losses, and specific mirror maps (which induce different regularization biases), something for future work.

9

Under review as a conference paper at ICLR 2019
REFERENCES
Alessandro Achille and Stefano Soatto. On the emergence of invariance and disentangling in deep representations. arXiv preprint arXiv:1706.01350, 2017.
Amir Beck and Marc Teboulle. Mirror descent and nonlinear projected subgradient methods for convex optimization. Operations Research Letters, 31(3):167­175, 2003.
Nicolo Cesa-Bianchi, Pierre Gaillard, Ga´bor Lugosi, and Gilles Stoltz. Mirror descent meets fixed share (and feels no regret). In Advances in Neural Information Processing Systems, pp. 980­988, 2012.
Pratik Chaudhari and Stefano Soatto. Stochastic gradient descent performs variational inference, converges to limit cycles for deep networks. In International Conference on Learning Representations, 2018.
John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and stochastic optimization. Journal of Machine Learning Research, 12(Jul):2121­2159, 2011.
Claudio Gentile. The robustness of the p-norm algorithms. Machine Learning, 53(3):265­299, 2003.
Adam J Grove, Nick Littlestone, and Dale Schuurmans. General convergence results for linear discriminant updates. Machine Learning, 43(3):173­210, 2001.
Suriya Gunasekar, Blake E Woodworth, Srinadh Bhojanapalli, Behnam Neyshabur, and Nati Srebro. Implicit regularization in matrix factorization. In Advances in Neural Information Processing Systems, pp. 6152­6160, 2017.
Suriya Gunasekar, Jason Lee, Daniel Soudry, and Nathan Srebro. Characterizing implicit bias in terms of optimization geometry. arXiv preprint arXiv:1802.08246, 2018.
Babak Hassibi and Thomas Kailath. Hoo optimal training algorithms and their relation to backpropagation. In Advances in Neural Information Processing Systems 7, pp. 191­198. 1995.
Babak Hassibi, Ali H. Sayed, and Thomas Kailath. Hoo optimality criteria for LMS and backpropagation. In Advances in Neural Information Processing Systems 6, pp. 351­358. 1994.
Babak Hassibi, Ali H Sayed, and Thomas Kailath. Hoo optimality of the LMS algorithm. IEEE Transactions on Signal Processing, 44(2):267­280, 1996.
Babak Hassibi, Ali H Sayed, and Thomas Kailath. Indefinite-Quadratic Estimation and Control: A Unified Approach to H2 and H-infinity Theories, volume 16. SIAM, 1999.
Kenji Kawaguchi. Deep learning without poor local minima. In Advances in Neural Information Processing Systems, pp. 586­594, 2016.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.
Jyrki Kivinen, Manfred K Warmuth, and Babak Hassibi. The p-norm generalization of the LMS algorithm for adaptive filtering. IEEE Transactions on Signal Processing, 54(5):1782­1793, 2006.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. In Advances in Neural Information Processing Systems, pp. 1097­1105, 2012.
Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. Nature, 521(7553):436, 2015.
Jason D Lee, Max Simchowitz, Michael I Jordan, and Benjamin Recht. Gradient descent only converges to minimizers. In Conference on Learning Theory, pp. 1246­1257, 2016.
Cong Ma, Kaizheng Wang, Yuejie Chi, and Yuxin Chen. Implicit regularization in nonconvex statistical estimation: Gradient descent converges linearly for phase retrieval, matrix completion and blind deconvolution. arXiv preprint arXiv:1711.10467, 2017.
10

Under review as a conference paper at ICLR 2019
Siyuan Ma, Raef Bassily, and Mikhail Belkin. The power of interpolation: Understanding the effectiveness of SGD in modern over-parametrized learning. In Proceedings of the 35th International Conference on Machine Learning, volume 80, pp. 3325­3334. PMLR, 2018.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control through deep reinforcement learning. Nature, 518(7540):529, 2015.
Arkadii Nemirovskii, David Borisovich Yudin, and Edgar Ronald Dawson. Problem complexity and method efficiency in optimization. 1983.
Behnam Neyshabur, Ryota Tomioka, Ruslan Salakhutdinov, and Nathan Srebro. Geometry of optimization and implicit regularization in deep learning. arXiv preprint arXiv:1705.03071, 2017.
Ravid Shwartz-Ziv and Naftali Tishby. Opening the black box of deep neural networks via information. arXiv preprint arXiv:1703.00810, 2017.
David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering the game of go with deep neural networks and tree search. Nature, 529(7587):484­489, 2016.
Dan Simon. Optimal state estimation: Kalman, H infinity, and nonlinear approaches. John Wiley & Sons, 2006.
Mahdi Soltanolkotabi, Adel Javanmard, and Jason D Lee. Theoretical insights into the optimization landscape of over-parameterized shallow neural networks. arXiv preprint arXiv:1707.04926, 2017.
Daniel Soudry, Elad Hoffer, and Nathan Srebro. The implicit bias of gradient descent on separable data. In International Conference on Learning Representations, 2018.
Tijmen Tieleman and Geoffrey Hinton. Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude. COURSERA: Neural networks for machine learning, 4(2):26­ 31, 2012.
Ashia C Wilson, Rebecca Roelofs, Mitchell Stern, Nati Srebro, and Benjamin Recht. The marginal value of adaptive gradient methods in machine learning. In Advances in Neural Information Processing Systems, pp. 4151­4161, 2017.
Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google's neural machine translation system: Bridging the gap between human and machine translation. arXiv preprint arXiv:1609.08144, 2016.
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep learning requires rethinking generalization. arXiv preprint arXiv:1611.03530, 2016.
Zhengyuan Zhou, Panayotis Mertikopoulos, Nicholas Bambos, Stephen Boyd, and Peter W Glynn. Stochastic mirror descent in variationally coherent optimization problems. In Advances in Neural Information Processing Systems, pp. 7043­7052, 2017.
11

Under review as a conference paper at ICLR 2019

Supplementary Material

A PROOF OF LEMMA 4

Proof. Let us start by expanding the Bregman divergence D(w, wi) based on its definition

D(w, wi) = (w) - (wi) - (wi)T (w - wi).

By plugging the SMD update rule (wi) = (wi-1) - Li(wi-1) into this, we can write it as
D(w, wi) = (w) - (wi) - (wi-1)T (w - wi) + Li(wi-1)T (w - wi). (34)
Using the definition of Bregman divergence for (w, wi-1) and (wi, wi-1), i.e., D(w, wi-1) = (w) - (wi-1) - (wi-1)T (w - wi-1) and D(wi, wi-1) = (wi) - (wi-1) - (wi-1)T (wi - wi-1), we can express this as

D(w, wi) = D(w, wi-1) + (wi-1) + (wi-1)T (w - wi-1) - (wi)

- (wi-1)T (w - wi) + Li(wi-1)T (w - wi) (35)

= D(w, wi-1) + (wi-1) - (wi) + (wi-1)T (wi - wi-1)

+ Li(wi-1)T (w - wi) (36)

= D(w, wi-1) - D(wi, wi-1) + Li(wi-1)T (w - wi).

(37)

Expanding the last term using w - wi = (w - wi-1) - (wi - wi-1), and following the definition of DLi (., .) from (15) for (w, wi-1) and (wi, wi-1), we have

D(w, wi) = D(w, wi-1) - D(wi, wi-1) + Li(wi-1)T (w - wi-1)
- Li(wi-1)T (wi - wi-1) (38) = D(w, wi-1) - D(wi, wi-1) +  (Li(w) - Li(wi-1) - DLi (w, wi-1))
-  (Li(wi) - Li(wi-1) - DLi (wi, wi-1)) (39) = D(w, wi-1) - D(wi, wi-1) +  (Li(w) - DLi (w, wi-1))
-  (Li(wi) - DLi (wi, wi-1)) (40)

Defining Ei(wi, wi-1) := D(wi, wi-1) - DLi (wi, wi-1) + Li(wi), we can write the above equality as

D(w, wi) = D(w, wi-1) - Ei(wi, wi-1) +  (Li(w) - DLi (w, wi-1)) .

(41)

Notice that for any model class with additive noise, and any loss function Li that depends only on the residual (i.e. the difference between the prediction and the true label), the term Li(w) depends only on the noise term, for any "true" parameter w. In other words, for all w that satisfy yi = f (xi, w) + vi, we have Li(w) = l(yi - f (xi, w)) = l(yi - (yi - vi)) = l(vi) . Finally, reordering
the terms leads to

D(w, wi) + DLi (w, wi-1) + Ei(wi, wi-1) = D(w, wi-1) + l(vi), which concludes the proof.

(42)

B PROOF OF THEOREM 6

Proof. We prove the theorem in two parts. First, we show that the value of the minimax is at least 1. Then we prove that the values is at most 1, and is achieved by stochastic mirror descent for small enough step size.

1. Consider the maximization problem

max
w,{vi }

D(w, wT ) +  D(w, w0)

+

T i=1


DLi
T i=1

(w, wi-1 l(vi)

)

.

12

Under review as a conference paper at ICLR 2019

Clearly, the optimal solution and the optimal values of this problem can, and will, be a
function of {wi}. Similarly, we can also choose feasible points that depend on {wi}. Any choice of a feasible point (w^, {v^i}) gives a lower bound on the value of the problem.
Before choosing a feasible point, let us first expand the DLi (w,i-1 ) term in the numerator, according to its definition.

DLi (w, wi-1) = l(vi)-l(yi-fi(wi-1))+l (yi-fi(wi-1))f (wi-1)T (w-wi-1), (43) where we have used the fact that l(yi - fi(w)) = l(vi) for all consistent w, in the first term. Now, we choose a feasible point as follows

v^i = fi(wi-1) - fi(w^),

(44)

where w^ is the choice of w, as will be described soon. The reason for choosing this value for the noise is that it "fools" the estimator by making its loss on the corresponding data point zero. In other words, for this choice, we have

DLi (w, wi-1) = l(v^i) - l(0) + l (0)f (wi-1)T (w^ - wi-1) = l(v^i)

because l(0) = l (0) = 0. It should be clear at this point that this choice makes the second terms in the numerator and the denominator equal, independent of the choice of w^. What remains to do, in order to show the 1 lower-bound, is to take care of the other two terms, i.e., D(w, wT ) and D(w, w0). As we would like to make the ratio equal to one, we would like to have D(w, wT ) = D(w, w0), which is equivalent to having

(w) - (wT ) - (wT )T (w - wT ) = (w) - (w0) - (w0)T (w - w0)

which is, in turn, equivalent to

((wT ) - (w0))T w = -(wT ) + (w0) + (wT )T wT - (w0)T w0. (45)

Since  is an invertible function, (wT ) - (w0) = 0, if wT = w0. Therefore, the

above equation has a solution for w, if wT = w0. As a result, choosing w^ to be a solution

to (45) makes D(w^, wT ) = D(w^, w0), if wT = w0. For the case when wT = w0, it

is trivial that D(w^, wT ) = D(w^, w0) for any choice of w^. In this case, we only need

to choose w^ to be different from w0, to avoid making the ratio

0 0

.

Hence, we have the

following choice

w^ = a solution of (45)

for wT = w0

w0 + w for some w = 0 for wT = w0

Choosing the feasible point w^, {vi} according to (46) and (44) leads to

(46)

max
w,{vi }

D(w, wT ) +  D(w, w0)

+

T i=1


DLi (w, wi-1)

T i=1

l(vi)

 D(w^, wT ) +  D(w^, w0) + 

T i=1 T i=1

l(fi(wi-1) l(fi(wi-1)

- -

fi(w^)) fi(w^))

.

Taking the minimum of both sides with respect to {wi}, we have

(47)

min
{wi }

max
w,{vi }

D(w, wT ) +  D(w, w0)

+

T i=1


DLi (w, wi-1)

T i=1

l(vi)

 min D(w^, wT ) +  {wi} D(w^, w0) + 

T i=1

l(fi(wi-1

)

-

fi(w^))

T i=1

l(fi

(wi-1)

-

fi

(w^))

=

1.

(48)

The equality to 1 comes from the fact the that the optimal solution of the minimization either has wT = w0 or wT = w0, and in both cases the ratio is equal to 1.

13

Under review as a conference paper at ICLR 2019

2. Now we prove that, under the small step size condition (convexity of (w) - Li(w) for all i), SMD makes the minimax value at most 1, which means that it is indeed an optimal
solution. Recall from Lemma 5 that

T TT
D(w, w0) +  l(vi) = D(w, wT ) +  DLi (w, wi-1) + Ei(wi, wi-1),
i=1 i=1 i=1

where Ei(wi, wi-1) = D(wi, wi-1) - DLi (wi, wi-1) + Li(wi).
It is easy to check that when (w) - Li(w) is convex, D(wi, wi-1) - DLi (wi, wi-1) is in fact a Bregman divergence (i.e. the Bregman divergence with respect to the potential (w) - Li(w)), and therefore it is nonnegative for any wi and wi-1. Furthermore, we know that the loss Li(wi) is also nonnegative for all wi. It follows that Ei(wi, wi-1) is nonnegative for all values of wi, wi-1 and i. As a result, we have the following bound.

TT
D(w, w0) +  l(vi)  D(w, wT ) +  DLi (w, wi-1).
i=1 i=1

(49)

Since the Bregman divergence D(w, w0) and the loss l(vi) are nonnegative, the left-hand side expression is nonnegative, and it follows that

D(w, wT ) +  D(w, w0)

T i=1
+

DLi (w, wi-1)

T i=1

l(vi

)



1.

(50)

In fact, this means that independent of the choice of the maximizer (i.e. for all {vi} and w), as long as the step size condition is met, SMD makes the ratio less than or equal to 1.

Combining the results of 1 and 2 above concludes the proof.

B.1 PROOF OF THEOREM 3

Proof. This result is a special case of Theorem 6, which was proven above. In this case, (w) =

1 2

w

2,

f (xi, w)

=

xiT w,

and

l(z)

=

1 2

z2.

Therefore,

D(w, wT )

=

1 2

w - wT

2, D(w, w0) =

1 2

w - w0

2,

DLi (w, wi-1)

=

1 2

(xTi

w

-

xiT

wi-1)2,

and

l(vi)

=

1 2

vi2,

which

leads

to the

result.

C PROOF OF PROPOSITION 9

Proof. To prove convergence, we appeal again to Equation (21), i.e.
T
D(w, w0) = D(w, wT ) + (Ei(wi, wi-1) + DLi (w, wi-1)) ,
i=1
for all w  W. We prove the two cases separately.

(51)

1. The proof of case (i) is straightforward. When l(·) is differentiable and strictly convex,

Li is also strictly convex. Therefore, DLi (w, wi-1) is a Bregman divergence and is nonnegative, and it is zero only when w = wi-1. Moreover, when (w) - Li(w) is con-

vex, Ei(wi, wi-1) is also nonnegative. Therefore, the summand in Eq. (51) is nonneg-

ative, and has to go to zero for i  . That is because as T  , the sum should

remain bounded, i.e.,

 i=1

(Ei(wi,

wi-1)

+

DLi

(w,

wi-1))



D(w, w0).

As a re-

sult of the non-negativity of both terms in the sum, we have both Ei(wi, wi-1)  0 and

DLi (w, wi-1)  0 as i  , which imply convergence of wi to a point w  W.

2. To prove case (ii), note that we have

DLi (w, wi-1) = Li(w) - Li(wi-1) - Li(wi-1)T (w - wi-1) = 0 - l(yi - xTi wi-1) + l (yi - xiT wi-1)xiT (w - wi-1) = -l(yi - xiT wi-1) + l (yi - xiT wi-1)(yi - xTi wi-1),

(52) (53) (54)

14

Under review as a conference paper at ICLR 2019

and

Ei(wi, wi-1) = D(wi, wi-1) - DLi (wi, wi-1) + Li(wi) = D(wi, wi-1) +  Li(wi-1) + Li(wi-1)T (wi - wi-1)

(55) (56)

= D(wi, wi-1) +  l(yi - xTi wi-1) - l (yi - xiT wi-1)xTi (wi - wi-1) . (57)

It follows from (54) and (57) that the summand in Equation (51) is

Ei(wi, wi-1) + DLi (w, wi-1) = D(wi, wi-1) + l (yi - xiT wi-1)(yi - xiT wi). (58)

The first term is a Bregman divergence, and is therefore nonnegative. In order to establish
convergence, one needs to argue that the second term is nonnegative as well, so that the
summand goes to zero as i  . Since l(·) is increasing for positive values and decreasing for negative values, it is enough to show that yi-xiT wi-1 and yi-xiT wi have the same sign, in order to establish nonnegativity. It is not hard to see that if the distance between the two points is less than or equal to the distance of yi-xiT wi from the origin, then the signs are the same. In other words, if |(yi-xTi wi)-(yi-xTi wi-1)| = |xTi (wi-wi-1)|  |yi-xTi wi-1|, then the sign are the same.

Note that by the definition of -strong convexity of (·), we have

((wi) - (wi-1))T (wi - wi-1)   wi - wi-1 2,

(59)

which implies

- Li(wi-1)T (wi - wi-1)   wi - wi-1 2,

(60)

by substituting from the SMD update rule. Upper-bounding the left-hand side by

 Li(wi-1) (wi - wi-1) implies

 Li(wi-1)   wi - wi-1 .

(61)

This implies that we have the following bound

|xiT (wi - wi-1)|  xi

wi - wi-1   xi

Li(wi-1) . 

(62)

It follows that if  

|yi-xiT wi-1| xi Li(wi-1)

,

for

all

i,

then

the

signs

are

the

same,

and

the

summand in Eq.(51) is indeed nonnegative. This condition can be equivalently expressed

as   mini

|yi-xTi wi-1| xi 2|l (yi-xiT wi-1)|

for

all

i,

or





mini

,|yi-xiT wi-1|
xi 2|l (yi-xiT wi-1)|

which

is

the

condition in the statement of the proposition.

Now that we have argued that the summand is nonnegative, the convergence to w  W is immediate. The reason is that both D(wi, wi-1)  0 and l (yi -xiT wi-1)(yi -xiT wi)  0, as i  . The first one implies convergence to a point w. The second one implies that either yi - xTi wi-1 = 0 or yi - xTi wi = 0, which, in turn, implies w  W.

D TIME-VARYING STEP-SIZE

The update rule for the stochastic mirror descent with time-varying step size is as follows.

wi = arg min iwT Li(wi-1) + D(w, wi-1),
w

(63)

which can be equivalently expressed as (wi) = (wi-1) - iLi(wi-1), for all i. The main results in this case are as follows.

Lemma 10. For any (nonlinear) model f (·, ·), any differentiable loss l(·), any parameter w and
noise values {vi} that satisfy yi = f (xi, w) + vi for i = 1, . . . , n, any initialization w0, any step size sequence {i}, and any number of steps T  1, the following relation holds for the stochastic mirror descent updates {wi} given in Eq. (63)

TT

D(w, w0) + il(vi) = D(w, wT ) + (Ei(wi, wi-1) + iDLi (w, wi-1)) ,
i=1 i=1

(64)

15

Under review as a conference paper at ICLR 2019

Proof. The proof is straightforward by summing the following equation for all i = 1, . . . , T

D(w, wi-1) + il(vi) = D(w, wi) + Ei(wi, wi-1) + iDLi (w, wi-1), which can be easily shown in the same way as in the proof of Lemma 4 in Appendix A.

(65)

Theorem 11. Consider any general model f (·, ·), and any differentiable loss function l(·) with
property l(0) = l (0) = 0. For sufficiently small step size, i.e., for any sequence {i} for which (w) - iLi(w) is convex for all i, the stochastic mirror descent iterates {wi} given by Eq. (63) are
the optimal solution to the following minimization problem

min
{wi }

max
w,{vi }

D(w, wT ) + D(w, w0)

T i=1
+

iDLi (w, wi-1)

T i=1

il(vi)

.

Furthermore, the optimal value (achieved by SMD) is 1.

(66)

Proof. The proof is similar to that of Theorem 6, as presented in Appendix B. The argument for the upper-bound of 1 is exactly the same. For the second part of the proof, we use the previous Lemma. It follows from the convexity of (w) - iLi(w) that Ei(wi, wi-1)  0, and as a result we have

D(w, wT ) +

T i=1

iDLi

(w,

wi-1)

D(w, w0) +

T i=1

i

l(vi)



1

(67)

for SMD updates, which concludes the proof.

The convergence and implicit regularization results hold similarly, and can be formally stated as follows.
Proposition 12. Consider the following two cases.

(i) l(·) is a differentiable and strictly convex function, (·) is a strictly convex function, and the positive sequence {i} is such that  - iLi is convex for all i, or

(ii) l(·) is a strongly quasi-convex function, (·) is an -strongly convex function, and 0 <

i 

|yi-xiT wi-1| xi 2|l (yi-xiT wi-1)|

for

all

i.

If either (i) or (ii) holds, then for any initialization w0, the stochastic mirror descent iterates given

in Eq. (63) converge to

w = arg min D(w, w0).
wW

(68)

Proof. The proof is similar to that of Proposition 9, as provided in Appendix C.

16


Under review as a conference paper at ICLR 2019
ON THE MARGIN THEORY OF FEEDFORWARD NEURAL NETWORKS
Anonymous authors Paper under double-blind review
ABSTRACT
Past works have shown that, somewhat surprisingly, over-parametrization can help generalization in neural networks. Towards explaining this phenomenon, we adopt a margin-based perspective. We establish: 1) for multi-layer feedforward relu networks, the global minimizer of a weakly-regularized cross-entropy loss has the maximum normalized margin among all networks, 2) as a result, increasing the over-parametrization improves the normalized margin and generalization error bounds for two-layer networks. In particular, an infinite-size neural network enjoys the best generalization guarantees. The typical infinite feature methods are kernel methods; we compare the neural net margin with that of kernel methods and construct natural instances where kernel methods have much weaker generalization guarantees. We validate this gap between the two approaches empirically. Finally, this infinite-neuron viewpoint is also fruitful for analyzing optimization. We show that a perturbed gradient flow on infinite-size networks finds a global optimizer in polynomial time.
1 INTRODUCTION
In deep learning, over-parametrization refers to the widely-adopted technique of using more parameters than necessary (Krizhevsky et al., 2012; Livni et al., 2014). Both computationally and statistically, over-parametrization is crucial for learning neural nets. Controlled experiments demonstrate that over-parametrization eases optimization by smoothing the non-convex loss surface (Livni et al., 2014; Sagun et al., 2017). Statistically, increasing model size without any regularization still improves generalization even after the model interpolates the data perfectly (Neyshabur et al., 2017b). This is surprising given the conventional wisdom on the trade-off between model capacity and generalization.
In the absence of an explicit regularizer, algorithmic regularization is likely the key contributor to good generalization. Recent works have shown that gradient descent finds the minimum norm solution fitting the data for problems including logistic regression, linearized neural networks, and matrix factorization (Soudry et al., 2018; Gunasekar et al., 2018b; Li et al., 2018; Gunasekar et al., 2018a; Ji & Telgarsky, 2018). Many of these proofs require a delicate analysis of the algorithm's dynamics, and some are not fully rigorous due to assumptions on the iterates. To the best of our knowledge, it is an open question to prove analogous results for even two-layer relu networks. (For example, the technique of Li et al. (2018) on two-layer neural nets with quadratic activations still falls within the realm of linear algebraic tools, which apparently do not suffice for other activations.)
We propose a different route towards understanding generalization: making the regularization explicit. The motivations are: 1) with an explicit regularizer, we can analyze generalization without fully understanding optimization; 2) it is unknown whether gradient descent provides additional implicit regularization beyond what 2 regularization already offers; 3) on the other hand, with a sufficiently weak 2 regularizer, we can prove stronger results that apply to multi-layer neural nets with relu activations. Additionally, explicit regularization is perhaps more relevant because 2 regularization is typically used in practice.
Concretely, we add a norm-based regularizer to the cross entropy loss of a multi-layer feedforward neural network with relu activations. We show that the global minimizer of the regularized objective achieves the maximum normalized margin among all the models with the same architecture, if the regularizer is sufficiently weak (Theorem 2.1). Informally, for models with norm 1 that perfectly classify the data, the margin is the smallest difference across all datapoints between the classifier
1

Under review as a conference paper at ICLR 2019
score for the true label and the next best score. We are interested in normalized margin because its inverse bounds the generalization error (see recent work (Bartlett et al., 2017; Neyshabur et al., 2017a; 2018) and our Theorem 3.1). Our work explains why optimizing the training loss can lead to parameters with a large margin and thus, better generalization error.
At a first glance, it might seem counterintuitive that decreasing the regularizer is the right approach. At a high level, we show that the regularizer only serves as a tiebreaker to steer the model towards choosing the largest normalized margin. Our proofs are simple, oblivious to the optimization procedure, and apply to any norm-based regularizer. We also show that an exact global minimum is unnecessary: if we approximate the minimum loss within a constant, we obtain the max-margin within a constant (Theorem 2.2).
We further study the margin of two-layer networks: let  ,m be the max normalized margin of a neural net with m hidden units (formally defined in Section 3.1). Let  , supm  ,m be the largest possible margin of an infinite two-layer network. We will show three properties of the margins:
1. In Theorem 3.2, we show that the optimal normalized margin of two-layer networks is non-decreasing as the width of the architecture grows, so the generalization error bound only improves with a wider network. Thus, even if the dataset is already separable, it could still be useful to increase the width to achieve larger margin and better generalization. More formally, let n be the number of training examples. We additionally approach the maximum possible margin  , after over-parameterizing with m  n neurons: m  n,  ,m =  ,.
2. The max-margin of infinite-size nets,  ,, equals half the margin of the 1-norm SVM (Zhu et al., 2004) over the lifted feature space defined by the activation function applied to all possible hidden units. (See Theorem 3.3.)
3. We compare the neural net margin  , to the standard margin for the kernel SVM on the same features. We design a simple data distribution (Figure 1) where neural net margin  , is large but the kernel margin is small. This translates to an ( d) factor gap between the generalization error bounds for the two approaches and demonstrates the power of neural nets compared to kernel methods. We experimentally confirm that a gap does indeed exist.
In the context of bullet 2, our work is closely related to that of Rosset et al. (2007) and Neyshabur et al. (2014), who show that optimizing the loss over the parameters of a two-layer relu network is equivalent to optimizing the loss of a "convex neural net" parametrized by a distribution over hidden units. We go one step further and connect the weakly regularized training loss to the 1 SVM.
We will also adopt this view of infinite-size neural networks to study how over-parametrization helps optimization. Prior works (Mei et al., 2018; Chizat & Bach, 2018) show that gradient descent on two-layer networks becomes Wasserstein gradient flow over parameter distributions in the limit of infinite neurons. For this setting, we prove that perturbed Wasserstein gradient flow finds a global optimizer in polynomial time.
Finally, we empirically validate several of the claims made in this paper. First, we train a two-layer network on a one-dimensional classification task that is simple to visualize. In one dimension, it is possible to brute-force approximate the maximum neural network margin and we show that training with an progressively smaller regularizer results in convergence to this margin. Second, we compare the generalization performance of neural networks and kernel methods and confirm that neural networks do achieve better generalization, as our theory predicts.
1.1 ADDITIONAL RELATED WORK
Zhang et al. (2016) and Neyshabur et al. (2017b) show that neural network generalization defies conventional explanations and requires new ones. One proposed explanation is the inductive bias of the training algorithm. Recent papers (Hardt et al., 2015; Brutzkus et al., 2017; Chaudhari et al., 2016) study inductive bias through training time and sharpness of local minima. Neyshabur et al. (2015a) propose a new steepest descent algorithm in a geometry invariant to weight rescaling and show that this improves generalization. Morcos et al. (2018) relate generalization in deep nets to the number of "directions" in the neurons. Other papers (Gunasekar et al., 2017; Soudry et al., 2018; Gunasekar et al., 2018b; Li et al., 2018; Gunasekar et al., 2018a) study implicit regularization towards a specific solution. Ma et al. (2017) show that implicit regularization can help gradient descent avoid
2

Under review as a conference paper at ICLR 2019

overshooting optima. Rosset et al. (2004) study logistic regression with a weak regularization and show convergence to the max margin solution. We adopt their techniques and extend their results.
Recent works have also derived tighter Rademacher complexity bounds for deep neural networks (Neyshabur et al., 2015b; Bartlett et al., 2017; Neyshabur et al., 2017a; Golowich et al., 2017) and new compression based generalization properties (Arora et al., 2018b). Dziugaite & Roy (2017) manage to compute non-vacuous generalization bounds from PAC-Bayes bounds. Neyshabur et al. (2018) investigate the Rademacher complexity of two-layer networks. Liang & Rakhlin (2018) and Belkin et al. (2018) study the generalization of kernel methods.
On the optimization side, Soudry & Carmon (2016) explain why over-parametrization can remove bad local minima. Safran & Shamir (2016) show that over-parametrization can improve the quality of the random initialization. Haeffele & Vidal (2015), Nguyen & Hein (2017), and Venturi et al. (2018) show that for sufficiently overparametrized networks, all local minima are global, but do not show how to find these minima via gradient descent. Du & Lee (2018) show that for two-layer networks with quadratic activations, all second-order stationary points are global minimizers. Arora et al. (2018a) interpret over-parametrization as a means of implicit acceleration during optimization. Mei et al. (2018), Chizat & Bach (2018), and Sirignano & Spiliopoulos (2018) take a distributional view of over-parametrized networks. Chizat & Bach (2018) show that Wasserstein gradient flow converges to global optimizers under structural assumptions. We extend this to a polynomial-time result.

1.2 NOTATION

Let R denote the set of real numbers. We will use · to indicate a general norm, with · 1, · 2, · 

denoting the 1, 2,  norms on finite dimensional vectors, respectively, and · F denoting the

Frobenius norm on a matrix. In general, we use ¯ on top of a symbol to denote a unit vector:

when applicable, u¯ u/ u , where the norm · will be clear from context. Let Sd-1 {u¯  Rd : u¯ 2 = 1} be the unit sphere in d dimensions. Let Lp(Sd-1) be the space of functions on Sd-1 for which the p-th power of the absolute value is Lebesgue integrable. For   Lp(Sd-1),

we overload notation and write  p

Sd-1 |(u¯)|pdu¯ 1/p. Additionally, for 1  L1(Sd-1)

and 2  L(Sd-1) or 1, 2  L2(Sd-1), we can define 1, 2

Sd-1 1(u¯)2(u¯)du¯ < .

Furthermore, we will use Vol(Sd-1) Sd-1 1du¯.

Throughout this paper, we reserve the symbol X = [x1, . . . , xn] to denote the collection of datapoints (as a matrix), and Y = [y1, . . . , yn] to denote labels. We use d to denote the dimension of our data. We often use  to denote the parameters of a prediction function f , and f (; x) to denote the
prediction of f on datapoint x.

We will use the notation , to mean less than or greater than up to a universal constant, respectively. Unless stated otherwise, we use O(·), (·) as a placeholders for some universal constant in upper and lower bounds, respectively. We will use poly to denote some universal constant-degree polynomial in the arguments.

2 WEAK REGULARIZER GUARANTEES MAX MARGIN SOLUTIONS

In this section, we will show that when we add a weak regularizer to cross-entropy loss with a positive-homogeneous prediction function, the normalized margin of the optimum converges to some max-margin solution. As a concrete example, feedforward relu networks are positive-homogeneous.
Let l be the number of labels, so the i-th example has label yi  [l]. We work with a family F of prediction functions f (; ·) : Rd  Rl that are a-positive-homogeneous in their parameters for some a > 0: f (c; x) = caf (; x), c > 0. We additionally require that f is continuous in . For some general norm · , we study the -regularized cross-entropy loss L, defined as

L()

n
- log
i=1

exp(fyi (; xi))

l j=1

exp(fj

(;

xi

))

+



r

(2.1)

3

Under review as a conference paper at ICLR 2019

for fixed r > 0. Let   arg min L().1 We define the normalized margin of  as:



min
i

fyi

(¯ ;

xi)

-

max
j=yi

fj

(¯ ;

xi)

Define the · -max normalized margin as

(2.2)



max min
 1 i

fyi

(;

xi

)

-

max
j=yi

fj

(;

xi

)

and let  be a parameter achieving this maximum. We show that with sufficiently small regularization level , the normalized margin  approaches the maximum margin  .
Theorem 2.1. Assume the training data is separable by a network f ( ; ·)  F with an optimal normalized margin  > 0. Then, the normalized margin of the global optimum of the weaklyregularized objective (equation 2.1) converges to  as the strength of the regularizer goes to zero. Mathematically, let  be defined in equation 2.2. Then

   as   0

An intuitive explanation for our result is as follows: because of the homogeneity, the loss L() roughly satisfies the following (for small , and ignoring problem parameters such as n):
L()  exp(-  a) +   r
Thus, the loss focuses on choosing parameters with larger margin, and the regularization term biases the loss to select parameters with a smaller norm. The full proof of the theorem is deferred to Section A.1.
We can also provide an analogue of Theorem 2.1 for the binary classification setting. For this setting, our prediction is now a single real output and we train using logistic loss. We provide formal definitions and results in Section A.2. Our theory for two-layer neural networks (see Section 3) is based in this setting.

2.1 OPTIMIZATION ACCURACY

Since L is typically hard to optimize exactly for neural nets, it would be ideal to relax the condition that  minimizes L. Thus, we ask, how accurately do we need to optimize L to obtain a margin that approximates  up to a constant? The following theorem shows that if suffices to find 
achieving a constant factor multiplicative approximation of L(), where  is some sufficiently small polynomial in n, l,  . Though our theorem is stated for the general multi-class setting, our
result applies for binary classification as well. We provide the proof in Section A.3.

Theorem 2.2.

In the setting of Theorem 2.1, suppose that we choose  =

( )r/a nc (l-1)c

for sufficiently

large c (that only depends on r/a). Let  denote a 2-approximate 2 minimizer of L, so L( ) 

2L(). Denote the normalized margin of  by  . Then



 2 · 4a/r

.

3 MARGINS OF OVER-PARAMETERIZED TWO-LAYER HOMOGENEOUS NEURAL NETS
In Section 2 we showed that a weakly-regularized logistic loss leads to the maximum normalized margin. In this section, we analyze the properties of the max-margin of neural nets more closely. We will contrast neural networks with kernel methods, for which margins have already been extensively studied. Towards a first-cut understanding, we focus on two-layer networks for binary classification.
1We formally show that L has a minimizer in Claim A.1 of Section A. 2The exact approximation constant is not important, so we choose 2 for simplicity.

4

Under review as a conference paper at ICLR 2019

First, in Section 3.1 we provide a bound stating that the generalization error is roughly linear in the inverse of the margin, establishing that a larger margin implies better generalization. In Section 3.2, we show that the maximum normalized margin is non-decreasing with the hidden layer size and stays constant as soon as there are more hidden units than data points. This suggests that increasing the size of the network improves the generalization of the solution.
Second, in Section 3.3, we draw an analogy to classical kernel methods by proving that the maximum 2-normalized margin of an over-parameterized neural net is equal to half the maximum possible 1-normalized margin of linear functionals on a lifted feature space. In other words, we establish an equivalence between neural networks and the 1-norm SVM (Zhu et al., 2004) on the lifted features. These features are constructed by applying the activation function on all possible hidden layer weights.
Third, continuing this analogy, we will compare the generalization power of a two-layer neural network to that of a kernel method on the lifted space. This kernel method corresponds to fixing random weights for the hidden layer and solving a 2-norm max-margin problem on the top layer weights. We demonstrate instances where two layer neural networks give better generalization error guarantees than the kernel method.

3.1 SETUP AND MARGIN-BASED GENERALIZATION ERROR

In the rest of the paper, we work with two-layer neural networks with a single output for binary classification. We use m to denote the number of hidden units, w1, . . . , wm  Rd for the weight
vectors on the first layer, and u1, . . . , um  R for the weights on the second layer. We let j (wj, uj), and we use  to denote the collection of all the parameters. We assume in this section that the activation  : R  R is 1-homogeneous and 1-Lipschitz. The network thus computes a single
score
m

f (; x)

wj(uj x)

j=1

We consider 2 regularization from here on. The regularized logistic loss of the architecture with m

hidden units is therefore

L,m

1 n

n

log(1 + exp(-yif (; xi))) + 



2 2

i=1

(3.1)

where  2 denotes the Euclidean norm of all the parameters in . We note that f and the regularizer are both 2-homogeneous in , so the results of Section 2 apply to L,m.3

Following our conventions from Section 2, we denote the optimizer of L,m by ,m, the normalized margin of ,m by ,m, the max-margin solution by  ,m, and the max-margin by  ,m. We emphasize the size of the network in our notation. Since our classifier f now predicts a single real value, we need to redefine
,m min yif (¯ ,m; xi)
i
 ,m max min yif (; xi)
 1 i
When the data is not separable by a m-unit neural net,  ,m is zero by definition.

Recall that X = [x1, . . . , xn] denotes the matrix with all the data points as columns, and Y = [y1, . . . , yn] denotes the labels. We sample X and Y i.i.d. from the data generating distribution pdata, which is supported on X × {-1, +1}. We can define the population 0-1 loss and the training 0-1
loss of the network  as

L() = Pr [yf (; x)  0]
(x,y)pdata

We will let D

X

2 F

n

be the average norm squared of the data and C

supxX x 2 be an upper

bound on the norm of a single datapoint. The following theorem shows that the generalization error

only depends on the parameters through the inverse of the margin on the training data. We provide a

proof in Section C.1.

3Although Theorem 2.1 is written in the language of multi-class prediction where the classifier outputs l  2 scores, the results translate to single-output binary classification. See Section A.2.

5

Under review as a conference paper at ICLR 2019

Theorem 3.1. Suppose  is 1-Lipschitz and 1-homogeneous. Then for any  that separates the data with margin  mini yif (¯ ; xi) > 0, with probability at least 1 -  over the draw of X, Y ,

L()  6

D + ()

n

(3.2)

where ()

+log log2

4C 

n

log(1/) 2n

.

Note

that

() is typically small, and thus the above bound

mainly

scales

with

1 

D n

.

As

a

corollary,

with

probability

1

-

,4

6

lim L(,m) 
0



,m

D +

(

,m)

n

(3.3)

Above we implicitly assume  ,m > 0, since otherwise the right hand side of the bound is vacuous.

One consequence of the above theorem and Theorem 2.2 is that if  is polynomially small in  ,m and n, we only need to optimize L,m up to a constant multiplicative factor to obtain parameters with generalization bounds roughly as good as those for  ,m.

3.2 THE MAX MARGIN IS NON-DECREASING IN THE HIDDEN LAYER SIZE

Now we show that the maximum normalized margin is nondecreasing with the hidden layer size and

stays constant once we have more hidden units than examples.

Theorem 3.2. In the setting of Section 3.1, recall that  ,m denotes the max normalized margin of a

two-layer neural network with hidden layer size m. Then,

 ,1   ,2 · · ·   ,n =  ,n+1 =  ,n+2 = · · ·

(3.4)

We note that  ,n will be positive when  is a sufficiently powerful activation such as relu or sigmoid and the data points are not repetitive, so the neural network can fit any function of the data. We prove Theorem 3.2 in Section B. Theorem 3.2 can explain why additional over-parametrization has been observed to improve generalization in two-layer networks Neyshabur et al. (2017b). Our margin does not decrease with a larger network size, and therefore Theorem 3.1 gives a better generalization bound. We precisely characterize the value of  ,n in the following section.

3.3 THE MAX MARGIN OF NEURAL NETS IS EQUIVALENT TO 1 SVM IN LIFTED SPACE

We link infinite-size neural networks to the 1 SVM over a lifted space, defined via a lifting function  : Rd  L(Sd-1) mapping data to an infinite feature vector:

x  Rd  (x)  L(Sd-1) satisfying (x)[u¯] = (u¯ x)

(3.5)

We look at the margin of linear functionals corresponding to   L1(Sd-1) . The 1-norm SVM over the lifted feature (x) solves for the maximum margin:

1

max min yi , (xi)
 i[n]

(3.6)

subject to  1  1

where we rely on the inner product and 1-norm defined in Section 1.2. A priori, it is unclear how to

optimize this since the kernel trick does not work for 1 norm. Here we will show that optimizing two-layer neural networks with weak regularization is equivalent to solving equation 3.6.

Theorem 3.3. Let  1 be defined in equation 3.6, and  ,m be defined in Section 3.1. For any m  n,

 ,m =  1

(3.7)

2

Rosset et al. (2007) and Neyshabur et al. (2014) show a similar equivalence, but between a lifted logistic regression problem and equation 3.1. In contrast, the above theorem, proved in Section B, shows the equivalence between equation 3.1 and the 1-norm SVM when the regularizer is small.

3.4 COMPARISON TO KERNEL METHODS
4The quantity lim0 L(,m) does not necessarily exist, but here we take it to mean  > 0, 0 > 0 with L(,m) at most the RHS of equation 3.3 plus  for all  < 0.

6

Under review as a conference paper at ICLR 2019

We compare the 1 SVM margin, attainable by a finite neural network, to the 2 margin attainable via kernel methods. Following the setup of Section 3.3, we define the kernel problem over   L2(Sd-1):

2

max


min
i[n]

yi

, (xi)



subject to   2  1

(3.8)

where 

Vol(Sd-1). (We scale

  2 by  to make the lemma

statement below cleaner.) First,  2 can be used to obtain a standard upper bound on the generalization error of the kernel SVM. Follow-

ing the notation of Section 3.1, we will let L 2-svm denote the 0-1 population classification error for the optimizer of equation 3.8.

Lemma 3.4. In the setting of Theorem 3.1, with probability at least 1 - , the generalization error of the standard kernel SVM with
relu feature (defined in equation 3.8) is bounded by

Figure 1: A visualization of 60 sampled points from D in 3 dimensions. Red points denote negative examples and blue points denote positive examples.

where 2

1D

L 2-svm

2

+ dn

2

log max



log2

C 

/d ,2
2

n

+

log(1/) n

is

typically

a

lower-order

term.

(3.9)

The bound above follows from standard techniques (Bartlett & Mendelson, 2002), and we provide a full proof in Section C.1. We construct a data distribution for which this lemma does not give a good bound for kernel methods, but Theorem 3.1 does imply good generalization for two-layer networks.
Theorem 3.5. There exists a data distribution pdata such that the 1 SVM with relu features has a good margin:

1 1 and with probability 1 -  over the choice of i.i.d. samples from pdata, obtains generalization error

L 1-svm

d

+ n

1

where 1

log log(d log n) n

+

log(1/) n

is

typically

a

lower

order

term.

Meanwhile,

with

high

probability the 2 SVM has a small margin:

 2 max

log n , 1/d
n

and therefore the generalization upper bound from Lemma 3.4 is at least  min 1 , d log n n

We briefly overview the construction of pdata here and defer the full proof of Theorem 3.5 to Section D.1.

Proof sketch for Theorem 3.5. We base pdata on the distribution D of examples (x, y) described below. Here ei is the i-th standard basis vector and we use x ei to represent the i-coordinate of x (since the subscript is reserved to index training examples).

e3 x

 

...

 



N

(0,

Id-2),

and


  

y = +1, y = +1, y = -1,

ed x



 

y = -1,

x e1 = +1, x e1 = -1,
x e1 = +1, x e1 = -1,

x e2 = +1 x e2 = -1 x e2 = -1
x e2 = +1

w/ prob. 1/4 w/ prob. 1/4 w/ prob. 1/4 w/ prob. 1/4

7

Under review as a conference paper at ICLR 2019

Figure 1 shows samples from D when there are 3 dimensions. From the visualization, it is clear that there is no linear separator for D. As Lemma D.1 shows, a relu network with four neurons can fit this
relatively complicated decision boundary. On the other hand, for kernel methods, we prove that the symmetries in D induce cancellation in feature space. The following lemmas, proved in Section D.1,
formalize this cancellation and show that it results in a small margin for kernel methods.

Lemma 3.6 (Margin upper bound tool). In the setting of Theorem 3.5, we have



2



1 

·

1 n

n
(xi)yi

i=1 2

Lemma from D.

3.7. In the setting of Theorem 3.5, let (xi, yi)ni=1 Let  be defined in equation 3.5 with  = relu.

be n With

i.i.d samples and high probability

corresponding labels (at least 1 - dn-10),

we have

1n n (xi)yi
i=1 2

 /n log n + /d

Combining these lemmas gives us the desired bound on  2 .

Gap in regression setting: We are able to prove an even larger ( n/d) gap between neural networks and kernel methods in the regression setting where we wish to interpolate continuous labels. Analogously to the classification setting, optimizing a regularized squared error loss on neural networks is equivalent to solving a minimum 1-norm regression problem (see Theorem D.3). Furthermore, kernel methods correspond to a minimum 2-norm problem. We construct distributions
pdata where the 1-norm solution will have a generalization error bound of O( d/n), whereas the 2norm solution will have a generalization error bound that is (1) and thus vacuous. In Section D.2, we define the 1-norm and 2-norm regression problems. In Theorem D.6 we formalize our construction.

4 PERTURBED WASSERSTEIN GRADIENT FLOW FINDS GLOBAL OPTIMIZERS IN
POLYNOMIAL TIME
In the prior section, we studied the limiting behavior of the generalization of a two-layer network as its width goes to infinity. In this section, we will now study the limiting behavior of the optimization algorithm, gradient descent. Prior work (Mei et al., 2018; Chizat & Bach, 2018) has shown that as the hidden layer size grows to infinity, gradient descent for a finite neural network approaches the Wasserstein gradient flow over distributions of hidden units (defined in equation 4.1). Chizat & Bach (2018) also prove that Wasserstein gradient flow converges to a global optimizer in this setting but do not specify a convergence rate.
We show that a perturbed version of Wasserstein gradient flow converges in polynomial time. The informal take-away of this section is that a perturbed version of gradient descent converges in polynomial time on infinite-size neural networks (for the right notion of infinite-size.)
Formally, we optimize the following functional over distributions  on Rd+1:
L[] R d + V d
where  : Rd+1  Rk, R : Rk  R, and V : Rd+1  R. In this work, we consider 2-homogeneous  and V . We will additionally require that R is nonnegative and V is positive on the unit sphere. Finally, we need standard regularity assumptions on R, , and V : Assumption 4.1 (Regularity conditions on , R, V ).  and V are differentiable as well as upper bounded and Lipschitz on the unit sphere. R is Lipschitz and its Hessian has bounded operator norm.
We provide more details on the specific parameters (for boundedness, Lipschitzness, etc.) in Section E.1. We note that relu networks satisfy every condition but differentiability of .5 We can fit a neural network under our framework as follows:
5The relu activation is non-differentiable at 0 and hence the gradient flow is not well-defined. Chizat & Bach (2018) acknowledge this same difficulty with relu.

8

Under review as a conference paper at ICLR 2019

Example 4.2 (Logistic loss for neural networks). We interpret  as a distribution over the parameters

of the network. Let k n and i() w(u xi) for  = (w, u). In this case, d is a distributional neural network that computes an output for each of the n training examples (like a

standard neural network, it also computes a weighted sum over hidden units). We can compute the

distributional version of the regularized logistic loss in equation 3.1 by setting V ()





2 2

and

R(a1, . . . , an)

n i=1

log(1

+

exp(-yi

ai)).

We will define L [] : Rd+1  R with L []() R ( d), () + V () and v[]() -L [](). Informally, L [] is the gradient of L with respect to , and v is the induced velocity field. For the standard Wasserstein gradient flow dynamics, t evolves according to

d dt t

=

-

·

(v[t]t)

(4.1)

where · denotes the divergence of a vector field. For neural networks, these dynamics formally

define continuous-time gradient descent when the hidden layer has infinite size (see Theorem 2.6

of Chizat & Bach (2018), for instance).

We propose the following modification of the Wasserstein gradient flow dynamics:

d dt t

=

-t

+

U

d

-



·

(v[t]t)

(4.2)

where U d is the uniform distribution on Sd. In our perturbed dynamics, we add uniform noise over U d. For infinite-size neural networks, one can informally interpret this as re-initializing a very small

fraction of the neurons at every step of gradient descent. We prove convergence to a global optimizer

in time polynomial in 1/ , d, and the regularity parameters.

Theorem 4.3 (Theorem E.4 with regularity parameters omitted). Suppose that  and V are 2homogeneous and the regularity conditions of Assumption 4.1 are satisfied. Also assume that from

starting distribution 0, a solution to the dynamics in equation 4.2 exists. Define L inf L[].

Let > 0 be a desired error threshold and choose  exp(-d log(1/ )poly(k, L[0] - L )) and

t

d2
4

poly(log(1/

), k, L[0] - L

), where the regularity parameters for , V , and R are hidden

in the poly(·). Then, perturbed Wasserstein gradient flow converges to an -approximate global

minimum in t time:

min L[t] - L  .
0tt

We provide a theorem statement that includes regularity parameters in Section E.1. We prove the theorem in Section E.2.

As a technical detail, Theorem 4.3 requires that a solution to the dynamics exists. We can remove this assumption by analyzing a discrete-time version of equation 4.2:
t+1 t + (-t + U d -  · (v[t]t))
and additionally assuming  and V have Lipschitz gradients. In this setting, a polynomial time convergence result also holds. We state the result in Section E.3.

5 SIMULATIONS
We first verify the normalized margin convergence on a two-layer networks with one-dimensional input. A single hidden unit computes the following: x  ajrelu(wjx + bj). We add · 22regularization to a, w, and b and compare the resulting normalized margin to that of an approximate solution of the 1 SVM problem with features relu(wxi + b) for w2 + b2 = 1. Writing this feature vector is intractable, so we solve an approximate version by choosing 1000 evenly spaced values of (w, b). Our theory predicts that with decreasing regularization, the margin of the neural network converges to the 1 SVM objective. In Figure 2, we plot this margin convergence and visualize the final networks and ground truth labels. The network margin approaches the ideal one as   0, and the visualization shows that the network and 1 SVM functions are extremely similar.
Next, we experiment on synthetic data in a higher-dimensional setting. For classification and regression, we compare the generalization error and predicted generalization upper bounds6 (from
6We compute the leading term that is linear in the norm or inverse margin.

9

Under review as a conference paper at ICLR 2019
Figure 2: Neural network with input dimension 1. Left: Normalized margin as we decrease . Right: Visualization of the normalized functions computed by the neural network and 1 SVM solution for   10-14.
Figure 3: Comparing neural networks and kernel methods. Left: Classification. Right: Regression.
Theorem 3.1 and Lemmas 3.4, D.4, and D.5) of a trained neural network against a 2 kernel SVM with relu features as we vary n. For classification we plot 0-1 error, whereas for regression we plot squared error. Our ground truth comes from a random neural network with 6 hidden units. For classification, we used rejection sampling to obtain datapoints with unnormalized margin of at least 0.1 on the ground truth network. We use a fixed dimension of d = 20. For all experiments, we train the network for 20000 steps with  = 10-8 and average over 100 trials for each plot point. The plots in Figure 3 show that two-layer networks clearly outperform kernel methods in test error as n grows. However, there seems to be looseness in the upper bounds for kernel methods: the kernel generalization bound appears to stay constant with n (as predicted by our theory for regression), but the kernel test error decreases. There is also some variance in the neural network generalization bound for classification. This occured likely because we did not tune learning rate and training time, so the optimization failed to find the best margin. In Section F, we include additional experiments training modified WideResNet architectures on CIFAR10 and CIFAR100. Although ResNet is not homogeneous, we still report interesting increases in generalization performance from annealing the weight decay during training, versus staying at a fixed decay rate.
6 CONCLUSION
We have made the case that maximizing margin is one of the inductive biases of relu networks with cross-entropy loss. We show that we can obtain a maximum normalized margin by training with a weak regularizer. We also prove that larger 2-normalized margin indicates better generalization for two-layer nets. Our work leaves open the question of how the 2-normalized margin relates to generalization in much deeper neural networks. This is a fascinating theoretical and empirical question for future work. On the optimization side, we make progress towards understanding overparametrized gradient descent by analyzing infinite-size neural networks. A natural direction for future work is to apply our theory to optimize the margin of finite-sized neural networks.
10

Under review as a conference paper at ICLR 2019
REFERENCES
Sanjeev Arora, Nadav Cohen, and Elad Hazan. On the optimization of deep networks: Implicit acceleration by overparameterization. arXiv preprint arXiv:1802.06509, 2018a.
Sanjeev Arora, Rong Ge, Behnam Neyshabur, and Yi Zhang. Stronger generalization bounds for deep nets via a compression approach. arXiv preprint arXiv:1802.05296, 2018b.
Keith Ball et al. An elementary introduction to modern convex geometry. Flavors of geometry, 31: 1­58, 1997.
Peter L Bartlett and Shahar Mendelson. Rademacher and gaussian complexities: Risk bounds and structural results. Journal of Machine Learning Research, 3(Nov):463­482, 2002.
Peter L Bartlett, Dylan J Foster, and Matus J Telgarsky. Spectrally-normalized margin bounds for neural networks. In Advances in Neural Information Processing Systems, pp. 6240­6249, 2017.
Mikhail Belkin, Siyuan Ma, and Soumik Mandal. To understand deep learning we need to understand kernel learning. arXiv preprint arXiv:1802.01396, 2018.
Alon Brutzkus, Amir Globerson, Eran Malach, and Shai Shalev-Shwartz. Sgd learns overparameterized networks that provably generalize on linearly separable data. arXiv preprint arXiv:1710.10174, 2017.
Pratik Chaudhari, Anna Choromanska, Stefano Soatto, Yann LeCun, Carlo Baldassi, Christian Borgs, Jennifer Chayes, Levent Sagun, and Riccardo Zecchina. Entropy-sgd: Biasing gradient descent into wide valleys. arXiv preprint arXiv:1611.01838, 2016.
Lenaic Chizat and Francis Bach. On the global convergence of gradient descent for over-parameterized models using optimal transport. arXiv preprint arXiv:1805.09545, 2018.
Simon S Du and Jason D Lee. On the power of over-parametrization in neural networks with quadratic activation. arXiv preprint arXiv:1803.01206, 2018.
Simon S Du, Jason D Lee, Yuandong Tian, Barnabas Poczos, and Aarti Singh. Gradient descent learns one-hidden-layer cnn: Don't be afraid of spurious local minima. arXiv preprint arXiv:1712.00779, 2017.
Gintare Karolina Dziugaite and Daniel M Roy. Computing nonvacuous generalization bounds for deep (stochastic) neural networks with many more parameters than training data. arXiv preprint arXiv:1703.11008, 2017.
Noah Golowich, Alexander Rakhlin, and Ohad Shamir. Size-independent sample complexity of neural networks. arXiv preprint arXiv:1712.06541, 2017.
Suriya Gunasekar, Blake E Woodworth, Srinadh Bhojanapalli, Behnam Neyshabur, and Nati Srebro. Implicit regularization in matrix factorization. In Advances in Neural Information Processing Systems, pp. 6151­6159, 2017.
Suriya Gunasekar, Jason Lee, Daniel Soudry, and Nathan Srebro. Characterizing implicit bias in terms of optimization geometry. arXiv preprint arXiv:1802.08246, 2018a.
Suriya Gunasekar, Jason Lee, Daniel Soudry, and Nathan Srebro. Implicit bias of gradient descent on linear convolutional networks. arXiv preprint arXiv:1806.00468, 2018b.
Benjamin D Haeffele and René Vidal. Global optimality in tensor factorization, deep learning, and beyond. arXiv preprint arXiv:1506.07540, 2015.
Moritz Hardt, Benjamin Recht, and Yoram Singer. Train faster, generalize better: Stability of stochastic gradient descent. arXiv preprint arXiv:1509.01240, 2015.
Ziwei Ji and Matus Telgarsky. Risk and parameter convergence of logistic regression. arXiv preprint arXiv:1803.07300, 2018.
11

Under review as a conference paper at ICLR 2019
Sham M Kakade, Karthik Sridharan, and Ambuj Tewari. On the complexity of linear prediction: Risk bounds, margin bounds, and regularization. In Advances in neural information processing systems, pp. 793­800, 2009.
Vladimir Koltchinskii, Dmitry Panchenko, et al. Empirical margin distributions and bounding the generalization error of combined classifiers. The Annals of Statistics, 30(1):1­50, 2002.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. In Advances in neural information processing systems, pp. 1097­1105, 2012.
Yuanzhi Li, Tengyu Ma, and Hongyang Zhang. Algorithmic regularization in over-parameterized matrix sensing and neural networks with quadratic activations. In Conference On Learning Theory, pp. 2­47, 2018.
T. Liang and A. Rakhlin. Just Interpolate: Kernel "Ridgeless" Regression Can Generalize. ArXiv e-prints, August 2018.
Roi Livni, Shai Shalev-Shwartz, and Ohad Shamir. On the computational efficiency of training neural networks. In Advances in Neural Information Processing Systems, pp. 855­863, 2014.
Cong Ma, Kaizheng Wang, Yuejie Chi, and Yuxin Chen. Implicit regularization in nonconvex statistical estimation: Gradient descent converges linearly for phase retrieval, matrix completion and blind deconvolution. arXiv preprint arXiv:1711.10467, 2017.
Song Mei, Andrea Montanari, and Phan-Minh Nguyen. A mean field view of the landscape of two-layers neural networks. arXiv preprint arXiv:1804.06561, 2018.
Ari S Morcos, David GT Barrett, Neil C Rabinowitz, and Matthew Botvinick. On the importance of single directions for generalization. arXiv preprint arXiv:1803.06959, 2018.
Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro. In search of the real inductive bias: On the role of implicit regularization in deep learning. arXiv preprint arXiv:1412.6614, 2014.
Behnam Neyshabur, Ruslan R Salakhutdinov, and Nati Srebro. Path-sgd: Path-normalized optimization in deep neural networks. In Advances in Neural Information Processing Systems, pp. 2422­2430, 2015a.
Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro. Norm-based capacity control in neural networks. In Conference on Learning Theory, pp. 1376­1401, 2015b.
Behnam Neyshabur, Srinadh Bhojanapalli, David McAllester, and Nathan Srebro. A pacbayesian approach to spectrally-normalized margin bounds for neural networks. arXiv preprint arXiv:1707.09564, 2017a.
Behnam Neyshabur, Srinadh Bhojanapalli, David McAllester, and Nati Srebro. Exploring generalization in deep learning. In Advances in Neural Information Processing Systems, pp. 5947­5956, 2017b.
Behnam Neyshabur, Zhiyuan Li, Srinadh Bhojanapalli, Yann LeCun, and Nathan Srebro. Towards understanding the role of over-parametrization in generalization of neural networks. arXiv preprint arXiv:1805.12076, 2018.
Quynh Nguyen and Matthias Hein. The loss surface of deep and wide neural networks. arXiv preprint arXiv:1704.08045, 2017.
Saharon Rosset, Ji Zhu, and Trevor Hastie. Boosting as a regularized path to a maximum margin classifier. Journal of Machine Learning Research, 5(Aug):941­973, 2004.
Saharon Rosset, Grzegorz Swirszcz, Nathan Srebro, and Ji Zhu. l1 regularization in infinite dimensional feature spaces. In International Conference on Computational Learning Theory, pp. 544­558. Springer, 2007.
Itay Safran and Ohad Shamir. On the quality of the initial basin in overspecified neural networks. In International Conference on Machine Learning, pp. 774­782, 2016.
12

Under review as a conference paper at ICLR 2019
Levent Sagun, Utku Evci, V Ugur Guney, Yann Dauphin, and Leon Bottou. Empirical analysis of the hessian of over-parametrized neural networks. arXiv preprint arXiv:1706.04454, 2017.
Justin Sirignano and Konstantinos Spiliopoulos. Mean field analysis of neural networks. arXiv preprint arXiv:1805.01053, 2018.
Daniel Soudry and Yair Carmon. No bad local minima: Data independent training error guarantees for multilayer neural networks. arXiv preprint arXiv:1605.08361, 2016.
Daniel Soudry, Elad Hoffer, and Nathan Srebro. The implicit bias of gradient descent on separable data. In International Conference on Learning Representations, 2018. URL https://openreview. net/forum?id=r1q7n9gAb.
Ryan J Tibshirani et al. The lasso problem and uniqueness. Electronic Journal of Statistics, 7: 1456­1490, 2013.
Twan van Laarhoven. L2 regularization versus batch and weight normalization. arXiv preprint arXiv:1706.05350, 2017.
Luca Venturi, Afonso Bandeira, and Joan Bruna. Neural networks with finite intrinsic dimension have no spurious valleys. arXiv preprint arXiv:1802.06384, 2018.
Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. arXiv preprint arXiv:1605.07146, 2016.
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep learning requires rethinking generalization. arXiv preprint arXiv:1611.03530, 2016.
Ji Zhu, Saharon Rosset, Robert Tibshirani, and Trevor J Hastie. 1-norm support vector machines. In Advances in neural information processing systems, pp. 49­56, 2004.
A MISSING PROOFS IN SECTION 2
We first show that L does indeed have a global minimizer. Claim A.1. In the setting of Theorems 2.1 and A.3, arg min L() exists.
Proof. We will argue in the setting of Theorem 2.1 where L is the multi-class cross entropy loss, because the logistic loss case is analogous. We first note that L is continuous in  because f is continuous in  and the term inside the logarithm is always positive. Next, define b inf L() > 0. Then we note that for  > (b/)1/r M , we must have L() > b. It follows that inf  M L() = inf L(). However, there must be a value  which attains inf  M L(), because { :   M } is a compact set and L is continuous. Thus, inf L() is attained by some .
A.1 MISSING PROOFS FOR MULTI-CLASS SETTING
Towards proving Theorem 2.1, we first show as we decrease , the norm of the solution  grows. Lemma A.2. In the setting of Theorem 2.1, as   0, we have   .
To prove Theorem 2.1, we rely on the exponential scaling of the cross entropy: L can be lower bounded roughly by exp(-  ), but also has an upper bound that scales with exp(-   ). By Lemma A.2, we can take large  so the gap  -  vanishes. This proof technique is inspired by that of Rosset et al. (2004).
13

Under review as a conference paper at ICLR 2019

Proof of Theorem 2.1. For any M > 0 and  with  mini f (¯ ; xi) - maxj=yi f (¯ ; xi) ,

1 L(M ) = n

n

- log

i=1

exp(M afyi (; xi))

l j=1

exp(M

afj

(;

xi))

+

M r



r

(by the homogeneity of f )

1 =

n
- log

n 1+

i=1

j=yi

1 exp(M a(fj(;

xi)

-

fyi (;

xi)))

+

M r



r

(A.1)

 log(1 + (l - 1) exp(-M a)) + M r  r

(A.2)

We can also apply j=yi exp(M a(fj(; xi) - fyi (; xi)))  max exp(M a(fj(; xi) - fyi (; xi))) = exp  in order to lower bound equation A.1 and obtain

L(M ) 

1 n

log(1

+

exp(-M a))

+

M r



r

(A.3)

Applying equation A.2 with M =  and  =  , noting that   1, we have: L(  )  log(1 + (l - 1) exp(-  a )) +   r

(A.4)

Next we lower bound L() by applying equation A.3,

L() 

1 log(1 + exp(- n



a)) + 



r

(A.5)

Combining equation A.4 and equation A.5 with the fact that L()  L(  ) (by the global optimality of ), we have
 > 0, n log(1 + (l - 1) exp(-  a ))  log(1 + exp(-  a))

Recall that by Lemma A.2, as   0, we have   . Therefore, exp(-  a ), exp(-  a)  0. Thus, we can apply Taylor expansion to the equation above with respect to exp(-  a ) and exp(-  a). If max{exp(-  a ), exp(-  a)} < 1, then we obtain n(l - 1) exp(-  a )  exp(-  a) - O(max{exp(-  a )2, exp(-  a)2})

We claim this implies that   lim inf0 . If not, we have lim inf0  <  , which implies that the equation above is violated with sufficiently large  (  log(2( - 1)n)1/a would suffice). By Lemma A.2,    as   0 and therefore we get a contradiction.
Finally, we have    by definition of  . Hence, lim0  exists and equals  .

Now we fill in the proof of Lemma A.2.

Proof of Lemma A.2. For the sake of contradiction, we assume that C > 0 such that for any 0 > 0,

there exists 0 <  < 0 with   C. We will determine the choice of 0 later and pick  such that

  C. Then the logits (the prediction fj(, xi) before softmax) are bounded in absolute value

by some constant (that depends on C), and therefore the loss function - log

exp(fyi (;xi))

l j=1

exp(fj (;xi

))

for

every example is bounded from below by some constant D > 0 (depending on C but not .)

Let M = -1/(r+1), we have that

0 < D  L()  L(M  )

(by the optimality of )

 - log

1 1 + (l - 1) exp(-M a

)

+ M r

(by equation A.2)

= log(1 + (l - 1) exp(--a/(r+1) )) + 1/(r+1)

 log(1 + (l - 1) exp(--0 a/(r+1) )) + 10/(r+1)

Taking a sufficiently small 0, we obtain a contradiction and complete the proof.

14

Under review as a conference paper at ICLR 2019

A.2 FULL BINARY CLASSIFICATION SETTING

For completeness, we state and prove our max-margin results for the setting where we fit binary labels yi  {-1, +1} (as opposed to indices in [l]) and redefining f (; ·) to assign a single real-valued score (as opposed to a score for each label). This lets us work with the simpler -regularized logistic
loss:

L()

1 n

n

log(1 + exp(-yif (; xi))) +   r

i=1

As before, let   arg min L(), and define the normalized margin  by  mini yif (¯ ; xi). Define the maximum possible normalized margin

 max min yif (; xi)
 1 i

(A.6)

Theorem A.3. Assume  > 0 in the binary classification setting with logistic loss. Then as   0,    .

The proof follows via simple reduction to the multi-class case.

Proof of Theorem A.3. We prove this theorem via reduction to the multi-class case with l = 2.

Construct f~ :

Rd



R2

with

f~1(; xi)

=

-

1 2

f

(;

xi)

and

f~2(; xi)

=

1 2

f

(;

xi

).

Define

new

labels y~i = 1 if yi = -1 and y~i = 2 if yi = 1. Now note that f~y~i (; xi)-f~j=y~i (; xi) = yif (; xi),

so the multi-class margin for  under f~ is the same as binary margin for  under f . Furthermore,

defining

L~()

1 n - log n
i=1

exp(f~y~i (; xi))

2 j=1

exp(f~j

(;

xi

))

+



r

we get that L~() = L(), and in particular, L~ and L have the same set of minimizers. Therefore we can apply Theorem 2.1 for the multi-class setting and conclude    in the binary classification
setting.

A.3 MISSING PROOF FOR OPTIMIZATION ACCURACY

Proof of Theorem 2.2. Choose B computing

1 

log

(l-1)( 

)r/a

1/a
.

We can upper bound L( ) by

L( )  2L()  2L(B )  2 log(1 + (l - 1) exp(-Ba )) + 2Br  2(l - 1) exp(-Ba ) + 2Br

(by equation A.2) (using log(1 + x)  x)

 1 (l - 1)( )r/a r/a

2 (

)r/a

+ 2

log 



 4

1 (l - 1)( )r/a r/a log

L(U B)



Furthermore, it holds that



r



.L(U B)


Now

we

note

that

L(

)



L(U B)



[(c + 1) log(n(l - 1))]r/a 4 nc(l - 1)c



1 2n

for

sufficiently

large c.

Now

using the

fact

that

log(x)



x 1+x

x



-1,

we

additionally

have the

lower bound

L( )



1 n

log(1

+

exp(-



a))



1 n

exp(- 1+exp(-

 

a) a)

.

Since L(UB)



1, we

can rearrange to get





- log

nL( ) 1-nL( )

a



- log

nL(U B) 1-nL(U B)

a



- log(2nL(UB)) a

15

Under review as a conference paper at ICLR 2019

The

middle

inequality

followed

because

x 1-x

is

increasing

in

x

for

0



x

<

1,

and

the

last

because

L(U B)



1 2n

.

Since

- log

2nL(U B)

>

0

we

can

also

apply

the

bound



r



L(U B) 

to get



 -a/r log 2nL(UB) (L(U B) )a/r

- log =
 4a/r

8n

1 

log

(l-1)( 

)r/a

r/a

4a/r 

log

(l-1)( 

)r/a

log(

( )r/a 8n

)

-

r a

log log

(l-1)( 

)r/a

log

(l-1)( 

)r/a

log

(l-1)( 

)r/a

(by definition of L(UB))

By assumption, log (

)r/a 



c

log

n(l

-

1),

and

so

log(

( )r/a 8n

)



c-2 c+1

log

(l-1)( 

.)r/a

Thus, if

c  4,

-a/r log L(UB)  

(L(U B) )a/r

4a/r

for c sufficiently large.

3

-

r a

log

log

(l-1)( 

)r/a

5

log

(l-1)( 

)r/a

 2 · 4a/r

B MISSING PROOFS IN SECTIONS 3.2 AND 3.3

We first show Theorem 3.3 and then complete the proof of Theorem 3.2. The proof of Theorem 3.3 will consist of two steps: first, show that equation 3.6 has an optimal solution with sparsity n, and second, show that sparse solutions to equation 3.6 can be mapped to a neural network with the same margin, and vice versa. The following lemma and proof are based on Lemma 14 of Tibshirani et al. (2013).
Lemma B.1. Let supp() {u¯ : |(u¯)| > 0}. There exists an optimal solution  to equation 3.6 with |supp( )|  n.

For the proof of this lemma, we find it convenient to work with a minimum norm formulation which we show is equivalent to equation 3.6:

min


1

subject to yi , (xi)  1 i

(B.1)

Claim B.2. Let S  L1(Sd-1) be the set of optimizers for equation 3.6, and let S  L1(Sd-1) be

the set of optimizers for equation B.1.

If equation B.1 is feasible, for any 

 S,

 1

S

, and for

any   S ,

 

1

 S.

Proof. Let opt denote the optimal objective for equation B.1.

We note that

 1

is

feasible

for

equation

3.6

with

objective

1 opt

,

and

therefore

1



1 opt

.

Furthermore,

1 2

1

yi

u¯Sd-1 (u¯)(u¯

xi)du¯



1

i,

and

so

 1

is feasible for equation B.1 with objective

1 1

.

Therefore,

opt

optimal for equation



1 1

.

3.6, and

As
 1

a result, it is optimal

must hold that opt for equation B.1, as

=

1 1

,

which

desired.

means

that

 

1

is

First, note that if equation B.1 is not feasible, then  1 = 0 and equation 3.6 has a trivial sparse solution, the all zeros function. Thus, it suffices to show that an optimal solution to equation B.1
exists that is n-sparse, since by Lemma B.2 equation B.1 and equation 3.6 have equivalent solutions
up to a scaling. We begin by taking the dual of equation B.1.

16

Under review as a conference paper at ICLR 2019

Claim B.3. The dual of equation B.1 has form

max  1
Rn

n

subject to

iyi(u¯ xi)  1 u¯  Sd-1

i=1

i  0

For any primal optimal solution  and dual optimal solution  , it must hold that

n
i yi(u¯ xi) = sign( (u¯))   (u¯) = 0
i=1

(B.2)

Proof. The dual form can be solved for by computation. By strong duality, equation B.2 must follow from the KKT conditions.

Now define the mapping v : Sd-1  Rn with vi(u¯) yi(u¯ xi). We will show a general result about linearly dependent v(u¯) for u¯  supp( ), after which we can reduce directly to the proof of Tibshirani et al. (2013).
Claim B.4. Let  be any optimal solution. Suppose that there exists S  supp( ) such that {v(u¯) : u¯  S} forms a linearly dependent set, i.e.

for coefficients c. Then

cu¯v(u¯) = 0
u¯S
u¯S cu¯ sign( (u¯)) = 0.

(B.3)

Proof. Let  be any dual optimal solution, then  v(u¯) = sign( (u¯)) u¯  supp( ) by Claim B.3. Thus, we apply  to both sides of equation B.3 to get the desired statement.

Proof of Lemma B.1. The rest of the proof follows Lemma 14 in Tibshirani et al. (2013). The lemma argues that if the conclusion of Claim B.4 holds and an optimal solution  has S  supp( ) with {v(u¯) : u¯  S} linearly dependent, we can construct a new  with  1 =  1 and supp( )  supp( ) (where the inclusion is strict). Thus, if we consider an optimal  with minimal support, it must follow that {v(u¯) : u¯  supp( )} is a linearly independent set, and therefore |supp( )|  n.

We can now complete the proof of Theorem 3.3.

Proof of Theorem 3.3. We first apply Lemma B.1 to conclude that equation 3.6 admits a n-sparse

optimal solution  . Because of sparsity, we can now abuse notation and treat  as a real-

valued function such that u¯supp( ) | (u¯)|  1. We construct  with normalized mar-

gin

at

least

1 2

.

For every u¯  supp(), we let  have a corresponding hidden unit j with

(wj, uj) =

sign( (u¯))

|

(u¯)| 2

,

|

(u¯)| 2

u¯

, and set the remaining hidden units to 0.

This

is possible because m  n. Now

m1

f (; x) = wj(uj x) = 2

 (u¯)(u¯ x)

j=1

u¯supp( )

Furthermore,

m



2 2

=

wj2 +

uj

2 2

=

| (u¯)| | (u¯)| +
22

u¯

2 2

=

| (u¯)|  1

j=1

u¯supp()

u¯supp()

Thus it follows that  has normalized margin at least  1 /2, so  ,m   1 /2.

17

Under review as a conference paper at ICLR 2019

To conclude, margin  ,m

we show that  ,m with hidden units

(wj,m1,/u2j.

Let ,m)

 for

,m denote the j  [m]. We

parameters obtaining optimal m-unit can construct  to put a scaled delta

mass of 2wj ,m uj,m 2 on u¯j,m for j  [m]. It follows that

mm

 1=

2|wj ,m| uj,m 2 

wj ,m2 +

uj ,m

2 2

=



,m

2 2



1

j=1

j=1

Furthermore,

mm

(u¯)(u¯ x) = 2 wj ,m uj,m 2((u¯j,m) x) = 2 wj ,m(uj,m x) = 2f ( ,m; x)

Sd-1

j=1

j=1

Thus,  is a feasible solution to equation 3.6 with objective value at least 2 ,m. Therefore,  1  2 ,m, so  ,m =  1 /2.

Theorem 3.1 follows almost immediately.

Proof of Theorem 3.1. Let  ,m denote the parameters obtaining optimal m-unit margin  ,m. It is simple to see that  ,m+1   ,m: we can set m units of our m + 1-hidden unit network to  ,m, and set the remaining unit to 0. Then this network will compute f ( ,m; ·) using m + 1 units and have the same parameter norm.
Finally, by Theorem 3.3 we have  1 /2 =  ,n =  ,n+1 = · · ·

C RADEMACHER COMPLEXITY AND GENERALIZATION ERROR

We prove the generalization error bounds stated in Theorem 3.1 and Lemma 3.4 via Rademacher complexity. In this section, we will introduce the margin-based generalization error bounds for Rademacher complexity and prove Rademacher complexity bounds for the 1 and 2 norms.

Assume that our data X, Y are drawn i.i.d. from ground truth distribution pdata supported on X × Y. For some hypothesis class F of real-valued functions, we define the empirical Rademacher complexity
R^ (F) as follows:

R^ (F)

1n

nE i

sup if (xi)
f F i=1

where i are independent Rademacher random variables. Specifically, we consider the hypothesis classes of linear functionals in lifted feature space with bounded norm: for activation , define f (; ·) : Rd  R with f (; x) (x),  , and let FBp, {f (; ·) :   Lp,  p  B}. Recall that we defined (x)  L(Sd-1) in equation 3.5.

For the binary classification setting, we will use the following ramp loss, which can be thought of a as a Lipschitz version of the standard 0-1 loss:

l (a)

1 a  0) 
1 - a/ 0 < a  
0 a > 

Following Section 3.1, we will use L() Pr(x,y)pdata (yf (; x)  0) to denote the population 0-1 loss of the classifier f (; ·). The following theorem follows from classical results in the literature

(Koltchinskii et al., 2002), (Kakade et al., 2009).

Theorem C.1. Let (xi, yi)in=1 be drawn iid from pdata. We work in the binary classification setting,

so Y = {-1, 1}. Let q

p p-1

,

C

supxX (x) q. Then with probability at least 1 -  over the

random draws of the data, every f (; ·)  F1p, satisfies

L()  1 n

n

l (yif

(;

xi))

+

4R^ (F1p,) 

+

i=1

log log2

4C 

+

n

log(1/) 2n

(C.1)

18

Under review as a conference paper at ICLR 2019

In particular, for any  that classifies all training data correctly with p-norm normalized margin



mini yif (

 

p

;

xi)

>

0,

with

probability

at

least

1

-

,

L()  4R^ (F1p,) +

log log2

4C 

+

log(1/)

 n 2n

(C.2)

Proof. Our starting point is Theorem 2 of (Kakade et al., 2009), which states the same claim for C supf(;·)F1p, supxX f (; x). We note that

sup sup f (; x) = sup sup , (x)

f (;·)F1p, xX

f (;·)F1p, xX

 sup sup  p (x) q
f (;·)F1p, xX

C

(by the duality of p and q norms)

For the second statement, we simply apply equation C.1 to ¯ /  p and use the fact that L() = L(¯).

Next, we focus on analyzing the Rademacher complexity R^ (FBp,), specifically for the cases p = 1, 2. Since our general analysis only involves duality of norms, similar derivations have been done in the past (Bartlett & Mendelson, 2002). We include our derivations here for completeness.

Lemma C.2.

R^ (FB1,) 

1 n

BE

i

n i=1

i(xi)

.

Lemma C.3.

R^ (FB2,) 

1 n

B

n i=1

(xi) 22.

Proof of Lemmas C.2 and C.3. For p = 1, 2, let · p, denote the dual norm of · p, so · 1, = ·  and · 2, = · 2. We write

R^ (FBp,)

=

1 nE

i

n

sup , i(xi)

FBk

i=1



1n



nE

i

 sup
FBk

k

i(xi) 
i=1 k,



1n



B n

·

E

i



i(xi) 

i=1 k,

Lemma C.2 immediately follows. To obtain Lemma C.3, we have

R^ (FB2,)



1 nBE

i

n
i(xi)
i=1 2



1 B

n


n

2

E i

i(xi) 

i=1 2

(via Jensen's inequality)



1 B

n


nn



E i

i j (xi), (xi) 

i=1 j=1



1 B

n

n

(xi)

2 2

i=1

(terms where i = j cancel out)

19

Under review as a conference paper at ICLR 2019

We will now complete the bound on R^ (FB1,) for general Lipschitz activations . Lemma C.4. Suppose that our activation  is M -Lipschitz. Then

R^ (FB1,)  B(

n i=1

(xi)

2 

+

2M

n

n i=1

xi 22)

Proof. We will show that

nn

n

Ei

i(xi)



(xi)

2 

+

2M

xi

2 2

i=1  i=1

i=1

from which the statement of the lemma follows via Lemma C.2. Fix any u¯  Sd-1. Then we get the decomposition

nn

E i sup

i(u¯ xi)  E i

i(u¯ xi) +

u¯Sd-1 i=1

i=1

nn

Ei

sup i(u¯ xi) - inf

i(u¯ xi)

u¯Sd-1 i=1

u¯Sd-1 i=1

(C.3)

We can bound the first term as

n


n

2

Ei

i(u¯ xi)  E i 

i(u¯ xi) 

i=1 i=1

n
 (u¯ xi)2
i=1

(C.4)

n



(xi)

2 

i=1

We can bound the second term of equation C.3 by

nn

Ei

sup i(u¯ xi) - inf

i(u¯ xi)

u¯Sd-1 i=1

u¯Sd-1 i=1

nn

Ei

sup i(u¯ xi) - inf

i(u¯ xi)

u¯Sd-1 i=1

u¯Sd-1 i=1

n

 2E i sup

i(u¯ xi)

u¯Sd-1 i=1

(C.5)

This simply gives an empirical Rademacher complexity of the hypothesis class F {x  (u¯ x) : u¯  Sd-1} scaled by n. By the Lipschitz contraction property of Rademacher complexity, using the fact that  is M -Lipschitz, we can therefore bound equation C.5 by

n

2E i sup

i(u¯ xi)  2M

u¯Sd-1 i=1

n

xi

2 2

i=1

Plugging equation C.4 and equation C.6 back into equation C.3 gives the desired bound.

(C.6)

As an example, we can apply this bound to relu features:

Corollary C.5. Suppose that  is the relu activation. Let 



and R^ (FB2,)

B X F .
nd

Vol(Sd-1). Then R^ (FB1,) 

3B

X n

F,

20

Under review as a conference paper at ICLR 2019

Proof. In the setting of Lemma C.4, we have (xi)   xi 2 and M = 1. Applying this in Lemma C.4 gives the desired 1-norm Rademacher bound.

For the 2-norm case, we first show that

(xi)

2 2

=



 d

xi

2 2

.

We can compute

(xi)

2 2

= =

VdoEl(u¯SdS-d-1)1E[ru¯eluS(d-1d[u¯relux(iu¯)2

xi ]

)2

]

=

 d

1 M2

EuN

(0,Id×d)[relu(uT

xi)2]

=

 d

xi

2 2

(M2 is the second moment of N (0, 1)) (C.7)

where the last line uses the computation provided in Lemma A.1 by Du et al. (2017). Now we plug this into Lemma C.3 to get the desired bound.

C.1 PROOF OF NETWORK AND KERNEL GENERALIZATION BOUNDS
We use our Rademacher complexity-based generalization bounds to provide a proof of Theorem 3.1 and Lemma 3.4.

Proof of Theorem 3.1. Since  is 1-homogeneous and 1-Lipschitz, it must follow that |(u¯ x)| 

|u¯ x|, and so (x)   x 2. Thus, supxX (x)   supxX x 2. Furthermore,

R^ (F11,) 

3

X n

F.

Finally, we note that the mapping given in the proof of Theorem 3.3 be-

tween neural network and linear functional on (·) applies here as well: we construct  to put a mass

of 2wj uj 2 on u¯j for each (wj, uj)  . Then

 1



2 2

,

and

, (x)

= 2f (; x). Thus,

following the notation of Theorem C.1,

 2





mini yif (¯ ; xi). Now we can apply equation C.2

of Theorem C.1 to get with probability 1 - ,

L() = L()

 4R^ (F11,) + 

log

log2

4 supxX 

x2
+

n

6 X F +

log log2

4C 

+

log(1/)

n n

2n

log(1/) 2n
(plugging in our bound for R^ (F11,))

which gives us equation 3.2. To conclude equation 3.3, we apply the above on ,m and use Theorem A.3.

We will now prove Lemma 3.4.

Proof of Lemma 3.4. From equation C.7, we first obtain supxX (x) 2

C

 d

.

Denote the

optimizer for equation 3.8 by  2 . Since  2 has normalized margin  2 , we apply Theorem C.1

to get with probability 1 - ,

L

2 -svm

=

L(

2)



4R^(F12,)  2

+

log log2

4 supxX 2

(x)

2

+

n

log(1/) 2n



X F + n 2 d

log max

log2

C



/d ,
2

2

n

+

log(1/) n

(applying Corollary C.5)

21

Under review as a conference paper at ICLR 2019

D MISSING PROOFS FOR COMPARISON TO KERNEL METHODS

D.1 CLASSIFICATION

In this section we will complete a proof of Theorem 3.5. Recall the construction of the distribution D provided in Section 3.4. We first provide a classifier of this data with small 1 norm.

Lemma D.1. In the setting of Theorem 3.5, we have that



1

2 .
4



Proof.

Consider the network f (x) =

1 4

(x

(e1 + e2)/

2)+ + (x

(-e1 - e2)/

2)+ - (x

(-e1 +





e2)/

2)+ - (x

(e1 - e2)/

2)+ . The attained margin  =

2 4

,

so



1



2 4

.

Now we will upper bound the margin attainable by the 2 SVM.

 Proof of Lemma 3.6. By the definition of  2 , we have that for any  with   2  1, we have



2



 max
  21

1 n

n i=1

, yi(xi)

Setting



=

1 

1 n

n i=1

(xi

)yi/

1 n

n i=1

(xi

)yi

2 completes the proof. (Attentive readers may

realize that this is equivalent to setting the dual variable of the convex program 3.8 to all 1's

function.)

Proof of Lemma 3.7. Let Wi = (xi)yi. We will bound several quantities regarding Wi's. In the

rest of the proof, we will condition on the event E that i,

xi

2 2

d log n. Note that E is a high

probability event and conditioned on E, xi's are still independent. We omit the condition on E in the

rest of the proof for simplicity.

We first show that assuming the following three inequalities that the conclusion of the Lemma follows.

1.

i,

Wi

2 2

 log n .

2. 2 Var[ i Wi]

n i=1

E[

Wi - E Wi

22]



3. E [ Wi] 2 n/d.

n log n

By bullets 1, 2, and Bernstein inequality, we have that with probability at least 1 - dn-10 over the randomness of the data (X, Y ),

nn

Wi - E

Wi

i=1 i=1

2

 

log1.5

n

+

n log2 n

n log2 n

By bullet 3 and equation above, we complete the proof with triangle inequality:

nn

Wi  E

Wi

+ n log2 n

i=1 2

i=1 2

n

log2

n

+

 n/d

Therefore, it suffices to prove bullets 1, 2 and 3. Note that 2 is a direct corollary of 1 so we will only prove 1 and 3. We start with 3:

By the definition of the 2 norm in L2(Sd-1) and the independence of (xi, yi)'s, we can rewrite

n
E Wi
i=1

22

=  · n2 E

E (x)[u¯] · y

2 u¯Sd-1 (x,y)D

(D.1)

22

Under review as a conference paper at ICLR 2019

Let u¯ = (u¯1, . . . , u¯d) and u¯-2 = (u¯3, . . . , u¯d)  Rd-2, and define 

u¯-2 2. Let x-2 =

(x e3, . . . , x ed). Note that (x)[u¯]y = y[u¯1 · x e1 + u¯2 · x e2 + u¯-2x-2] and u¯-2x-2 has

distribution u¯-2 2 · N (0, 1) =  · N (0, 1). Let z = u¯-2x-2/ , and therefore z has standard

normal distribution. With this change of the variables, by the definition of the distribution D, we have

E
(x,y)D

(x)[u¯]

·

y

=

1 4

E [(u¯1
zN (0,1)

+

u¯2

+

 z)+]

+

1 4

E [(-u¯1
zN (0,1)

-

u¯2

+

 z)+]

-

1 4

E [(u¯1
zN (0,1)

-

u¯2

+

 z)+]

-

1 4

E [(+u¯1
zN (0,1)

-

u¯2

+

 z)+]

By claim D.2, and the 1-homogeneity of relu, we can simplify the above equation to

1

E
(x,y)D

(x)[u¯]

·

y

=

 4

·

2c1 + O(min{|u¯1 + u¯2|/, |u¯1 + u¯2|2/ 2})

- 4

2c1 - O(min{|u¯1 - u¯2|/, |u¯1 - u¯2|2/ 2})

min{|u¯1| + |u¯2|, (|u¯1| + |u¯2|)2/ }

It follows that

2
E E (x)[u¯] · y
u¯Sd-1 (x,y)D

Eu¯

min{(|u¯1| + |u¯2|)2, (|u¯1| + |u¯2|)4/

u¯-2

2 2

}

Eu¯

(|u¯1| + |u¯2|)2 · 1[

u¯-2

2  1/2]

+ Eu¯

(|u¯1| + |u¯2|)4/

u¯-2

2 2

·

1[

u¯-2

2  1/2]

exp(- d) + Eu¯ (|u¯1| + |u¯2|)4 1/d2

(D.2)

Combining equation D.1 and equation D.2 we complete the proof of bullet 3. Next we prove
bullet 1. Note that (x)[u¯]y is bounded by |u¯1| + |u¯2| + u¯-2x-2 2. Therefore, conditioned on xi 2 d log n

Wi

2 2



E

(|u¯1| + |u¯2| + u¯-2x-2 2)2

u¯Sd-1

E |u¯1|2 + E |u¯2|2 + E

u¯Sd-1

u¯Sd-1

u¯Sd-1

1/d + x-2 22/d log n

u¯-2x-2

2 2

Hence we complete the proof.

Claim D.2. Let Z  N (0, 1) and a  R. Then, there exists a universal constant c1 and c2 such that |E [(a + Z)+ + (-a + Z)+] - 2c1|  c2 min{|a|, a2}.

Proof. Without loss of generality we can assume a  0. Then,

E [(a + Z)+ + (-a + Z)+] = E [(a + Z)1[Z  -a]] + E [(Z - a)1[Z  a]]

= E [a · 1[Z  -a]] + E [Z · 1[Z  -a]] - E [a · 1[Z  a]] + E [Z · 1[Z  a]]

= E [a1[-a  Z  a]] + 2 E [Z · 1[Z  a]]

(by E [Z · 1[-a  Z  a]] = 0)

= E [a1[-a  Z  a]] + 2 E [Z · 1[Z  0]] - 2 E [Z · 1[a  Z  0]]

= 2c1 + O(min{a, a2})

where the last equality uses the fact that c1 aE [1[-a  Z  a]] a min{1, a}.

E [Z · 1[Z  0]] and E [a1[-a  Z  a]] 

Now we will prove Theorem 3.5.

Proof of Theorem 3.5. To circumvent the technical issue of bounded support in Theorem 3.1 and

Lemma 3.4, we construct pdata to be a slightly modified version of D: perform rejection sampling

of (x, y)  D until we obtain a sample with

x

2 2

d log n. Since this occurs with very high

23

Under review as a conference paper at ICLR 2019

probability, the high probability result of Lemma 3.7 still translates to pdata. Now apply Lemma 3.6

to conclude that  2

log n n

+

1 d

.

Furthermore,

Lemma

D.1

allows

us

to

conclude

that



1

1.



With high probability over the draws of the samples, we will also get that X F / n d. We can

therefore apply Theorem 3.1, and conclude that with probability 1 - ,

L 1-svm

d log log(d log n) log(1/)

++

nn

n

Furthermore, plugging  2 into the bound of Lemma 3.4 gives us

min

1 , d

+

log log(dn) +

log(1/)

log n n

n

n

D.2 REGRESSION

We will first define the 1-norm and 2-norm regression problems. The regression equivalent of equation 3.6 for   L1(Sd-1) is as follows:

 1  arg min  1

subject to , (xi) = yi

(D.3)

Next we define the regression version of equation 3.8:

 2  arg min  2

subject to , (xi) = yi

(D.4)

where   L2(Sd-1).

We will briefly motivate our study of the regression setting by connecting the minimum 1-norm solution to neural networks. To compare, in the classification setting, optimizing the weakly regularized loss over neural networks is equivalent to solving the 1 SVM. In the regression setting, solving the weakly regularized squared error loss is equivalent is equivalent to finding the minimum 1-norm solution that fits the datapoints exactly.

Theorem D.3. Let f (; ·) be some two-layer neural network with m  n hidden units parametrized by , as in Section 3.1. Define the -regularized squared error loss

L,m()

1 n

n

(f (; xi) - yi)2 + 



2 2

i=1

with ,m  arg min L,m(). Suppose that equation D.3 is feasible with optimal solution  1 .

Then as   0, L,m(,m)  0 and

,m

2 2



2

1

1.

Proof. We can see that equation D.3 will have a n-sparse solution  using the same reasoning as the

proof of Lemma B.1. Furthermore, following the proof of Theorem 3.3, the function x   , (x)

is implementable by a neural network  ,m with



,m

2 2

=2



1 = 2  1 1. Following the

same reasoning as before, we can also conclude that  ,m is an optimal solution for:

min




2 2

subject to f (; xi) = yi

(D.5)

Now we note that 

,m

2 2

 L,m(,m)  L,m(

,m) = 



,m

22, so as   0, and also

,m 2   ,m 2. Now assume for the sake of contradiction that B with ,m 2  B <

 ,m 2 for arbitrarily small . We define

r

1 min
n

n
(f (; xi) - yi)2

i=1

subject to  2  B

Note that r > 0 since  ,m is optimal for equation D.5. However, L,m  r for arbitrarily small

, a contradiction. Thus, lim0



,m

2 2

=



,m

2 2

.

24

Under review as a conference paper at ICLR 2019

We proceed to provide similar generalization bounds as the classification setting. This time, our bounds depend on the norms of the solution. We borrow notation from Section C: again let f (; ·) x  , (x) (for (x) defined in equation 3.5), and define hypothesis classes FBp, of linear functionals bounded by B in p-norm. As before, let R^ (F) denote the empirical Rademacher complexity of hypothesis class F. The following is a generalization bound based on the 1-norm:
Lemma D.4. Let l(·; y) : R  [-c, c] be a bounded M -Lipschitz loss function. Assume that  is a 1-homogeneous and 1-Lipschitz activation. Let (xi, yi)ni=1 be drawn i.i.d from pdata. Then with probability at least 1 -  over the dataset, every   L1(Sd-1) satisfies

E(x,y)pdata [l(f (; x); y)] 

1 n

n

l(f (; xi); yi) + 12M max{1,

1X n

F} +c

i=1

log(1/) + log(max{1, 2  1 X F }) 2n

Proof. Our starting point is Theorem 1 of Kakade et al. (2009), which states that with probability 1 - , for any fixed hypothesis class F and f  F,

1 E(x,y)pdata [l(f (x); y)]  n

n

l(f (xi); yi) + 2M R^ (F ) + c

log(1/) n

i=1

(D.6)

We define Bj

2j XF

for j



0.

We

note

that

by

Lemma

C.4,

R^ (FB1,j)



3

2j n

.

and

apply

the

above

on FB1,j using j

 2j+1

.

Then

using

a

union

bound,

with

probability

1

-

 j=0

j

=

1

-

,

for

all

j  0 and f (; ·)  FB1,j

E(x,y)pdata [l(f (; x); y)]



1 n

n

l(f (; xi); yi) + 2M R^ (FB1,j) + c

log(1/j ) n

i=1

1 n

n

l(f (; xi); yi) +

2j 6M
n

+

c

log(1/) + log(2j+1) n

i=1

Now for every  with

 1<

1 X

F

,

we

use

the

inequality

for

FB1,0,

and

for

every

other

,

we

apply

the

inequality

corresponding

to

F 1,
Bj+1

,

where

2j



 1 X F  2j+1. This gives the desired

statement.

We can also provide the same generalization error bound for the 2-norm and relu features:
Lemma D.5. In the setting of Lemma D.4, choose  to be the relu activation. Then with probability 1 - ,

1 n

n i=1

l(f (;

xi); yi)

+

M

 

max{1, nd

2

X

F} +c

E(x,y)pdata [l(f (; x); y)]
log(1/) + log(max{1,  2 X F }) 2n

Proof. We proceed time have R^ (FB2,j)

the

same
2j nd

way as in the from Lemma

proof C.5.

of Lemma D.4. We define Bj as before, and this Thus, again union bounding over all j, equation

equation D.6 gives with probability 1 - , for all j  0 and f (; ·)  FB2,j

E(x,y)pdata [l(f (; x); y)]

1 n

n i=1

l(f (; xi); yi) +

M

2j nd

+

c

log(1/) + log(2j+1) n

Now we assign the  to different j as before to obtain the statement in the lemma.

25

Under review as a conference paper at ICLR 2019

Note that if l is some bounded loss such that l(y; y) = 0 (for example, truncated squared error), for
 1 and  2 the loss terms over the datapoints (in the bounds of Lemmas D.4 and D.5) vanish. For loss l, define

L 1-reg L 2-reg

Ex,ypdata [l(f ( 1 ; x); y)] Ex,ypdata [l(f ( 2 ; x); y)]

Next, we will define the kernel matrix K with Kij = (xi), (xj) . Now we are ready to state and prove the formal theorem describing the gap between the 1-norm solution and 2-norm solution.

Theorem D.6. Recall the definitions of  1 and  2 in equation D.3 and equation D.4. For any activation  with the property that K is full rank for any X with no repeated datapoints, there exists
a distribution pdata such that with probability 1,

1 11

On the other hand,

E [(xi,yi)ni=1iidpdata

2

22]

=

n 

For i.i.d samples from this choice of pdata, if l is bounded (l(·; y) : R  [-1, 1]), 0 on correct predictions (l(y; y) = 0), and 1-Lipschitz, then with probability 1 - ,

L 1-reg

d log(1/) + log n +
nn

Meanwhile, in the case that

2

2 2



n 

,

the

upper

bound

on

L

2 -reg

from

Lemma

D.5

is

(1)

and

in

particular does not decrease with n.

We will first show that for any dataset X, there is a distribution over Y such that the expectation of

 2 2 is large. When it is clear from context, y will denote the vector corresponding to Y .

Lemma D.7. There is a distribution A over L1(Sd-1) such that for any dataset X with yi (xi),  for   A,

E [

2

22]



n 

and with probability 1,

1 11

We note the order of the quantifiers in Lemma D.7: the distribution A must not depend on the dataset

X. We first provide a simple closed-form expression for

2

2 2

.

Claim D.8. If K is full rank, then

2

2 2

=

y

K -1 y .

Proof. This follows by taking the dual of equation D.4.

Proof of Lemma D.7. We sample   A as follows: first sample u¯  Sd-1 uniformly. Then set  to
have a delta mass of 1 at u¯ and be 0 everywhere else. Define the vector vu¯ [(u¯ x1) · · · (u¯ xn)]; then it follows that we set our labels y to vu¯. It is immediately clear that  1 1   1  1.

To lower bound E[  2 22], from Claim D.8 we get

EA[

2

2 2

]

=

Eu¯Sd-1 [vu¯

K -1 vu¯ ]

= Eu¯Sd-1 [traceK-1(vu¯vu¯ )]

= trace(K-1Eu¯Sd-1 [vu¯vu¯ ]

= 1 trace(K-1K)  n
= 

(by definition of K)

26

Under review as a conference paper at ICLR 2019

Proof of Theorem D.6. We note that since the distribution A of Lemma D.7 does not depend on the

dataset X, it must follow that

E(xi)in=1iidN (0,Id×d)

EA[  2

2 2

]

n =


EA

E [ (xi)ni=1iidN (0,Id×d)

2

2 2

]

n =


Thus, there exists  such that if we sample X i.i.d. from the standard normal and set yi =

(xi), 

, the expectation of

2

2 2

is

at

least

n 

.

We

choose

pdata

corresponding

to

this



, with

x sampled from the standard normal. Now it is clear that pdata will satisfy the norm conditions of

Theorem D.6.



For the generalization bounds, with high probability X F = ( nd) as x is sampled from the

standard normal distribution. Thus, Lemma D.4 immediately gives the desired generalization error

bounds for L 1-reg. On the other hand, if  2 2  

n 

,

then

the

bound

of

Lemma

D.5

is

at

least

  22 X F  (1)

nd

E MISSING PROOFS IN SECTION 4

E.1 DETAILED SETUP

We first write our regularity assumptions on , R, and V in more detail:
Assumption E.1 (Regularity conditions on , R, V ). R is nonnegative, Lipschitz, and smooth: MR, CR such that 2R op  CR, and R 2  MR. Assumption E.2.  is differentiable, bounded and Lipschitz on the sphere: B, M such that (¯)  B ¯  Sd, and |i(¯) - i(¯ )|  M ¯ - ¯ 2 ¯, ¯  Sd. Assumption E.3. V is Lipschitz and upper and lower bounded on the sphere: bV , BV , MV such that 0 < bV  V (¯)  BV ¯  Sd, and V (¯) 2  MV ¯  Sd.

We state the version of Theorem 4.3 that collects these parameters:

Theorem E.4 (Theorem 4.3 with problem parameters). Suppose that  and V are 2-homogeneous and Assumptions E.1, E.2, and E.3 hold. Fix a desired error threshold > 0. Suppose that from a starting distribution 0, a solution to the dynamics in equation 4.2 exists. Choose

 exp(-d log(1/ )poly(k, MV , MR, M, bV , BV , CR, B, L[0] - L ))

t

d2 4 poly(log(1/

), k, MV , MR, M, bV , BV , CR, B, L[0] - L

)

Then it must hold that min0tt L[t] - inf L[]  2 .

E.2 PROOF OF THEOREM E.4

Throughout the proof, it will be useful to keep track of Wt

Et [  22], the second moment of

t. We first introduce a general lemma on integrals over vector field divergences.

Lemma E.5. For any h1 : Rd+1  R, h2 : Rd+1  Rd+1 and distribution  with ()  0 as   ,

h1() · (h2()())d = -E[ h1(), h2() ]

Proof. The proof follows from integration by parts.
We note that t will satisfy the boundedness condition of Lemma E.5 during the course of our algorithm - 0 starts with this property, and Lemma E.9 proves that t will continue to have this property. We therefore freely apply Lemma E.5 in the remaining proofs. We first bound the absolute value of L [t] over the sphere by BL MRB + BV .

27

Under review as a conference paper at ICLR 2019

Lemma E.6. For any ¯  Sd-1, t  0, |L [t](¯)|  BL.

Proof. We compute

|L [t](¯)| = R d , (¯) + V (¯)

 R

d (¯) 2 + V (¯)  MRB + BV
2

Now we analyze the decrease in L[t]. Lemma E.7. Under the perturbed Wasserstein gradient flow

d dt

L[t]

=

-Et [L

[t]()]

+

E¯Ud [L

[t](¯)]

-

Et [

v[t]()

2 2

]

Proof. Applying the chain rule, we can compute

d dt L[t] =

R

d

dt

, dt

d = dt Et [L [t]()]

d

dt

+ dt

= L [t]()t()d

V dt

= - L [t]dt +  L [t]dU d - L [t]() · (v[t]()t())d

= -Et [L [t]()] + E¯Ud [L [t](¯)] - Et [

v[t]()

2 2

],

where we use Lemma E.5 with h1 = L [t] and h2 = v[t].

Now we show that the decrease in objective value is approximately the average velocity of all parameters under t plus some additional noise on the scale of . At the end, we choose  small enough so that the noise terms essentially do not matter.

Corollary E.8.

We can bound

d dt

L[t]

by

d dt

L[t]



BL(Wt2

+

1)

-

Et

[

v[t]()

2 2

]

(E.1)

Proof.

By homogeneity, and Lemma E.6, Et [L [t]()] = Et [L [t](¯)



2 2

]



BLWt2.

We

also get E¯Ud [L [t](¯)]  BL since U d is only supported on Sd. Combining these with Lemma

E.7 gives the desired statement.

Now we show that if we run the dynamics for a short time, the second moment of t will grow slowly, again at a rate that is roughly the scale of the noise .

Lemma E.9.

For all 0  t

 t, Wt2 

L[0 ]+ tBL bV -tBL

.

Proof. Let t arg maxt [0,t] Wt2. Integrating both sides of equation E.1, and rearranging, we get

t t
0  Es [ v[s]() 22]ds  L[0] - L[t] + BL (Ws2 + 1)ds
00
 L[0] - L[t ] + tBL(Wt2 + 1)

Now now

since R is nonnegative, we apply L[t plug this in and rearrange to get Wt2 

]Wt2ELbt[V0[-V]+t(t)B]BLL

Et [V (¯)



L[0 ]+t BL bV -tBL

 22]  0  t

bV Wt2 .  t.

We

28

Under review as a conference paper at ICLR 2019

Now let W 2

L[0]+t BL bv -t BL

.

By

Lemma

E.9,

0



t



t

,

Wt2



W 2.

The next statement allows us to argue that our dynamics will never increase the objective by too much.

Lemma E.10. For any t1, t2 with 0  t1  t2  t , L[t2 ] - L[t1 ]  (t2 - t1)BL(W 2 + 1).

Proof. From Corollary E.8, t  [t1, t2] we have

d dt

L[t]



BL(W

2

+

1)

Integrating from t1 to t2 gives the desired result.

The following lemma bounds the change in expectation of a 2-homogeneous function over t. At a high level, we lower bound the decrease in our loss as a function of the change in this expectation.
Lemma E.11. Let h : Rd+1  R that is 2-homogeneous, with h(¯)  M ¯  Sd and |h(¯)|  B ¯  Sd. Then 0  t  t , we have

d dt

hd  B(W 2 + 1) + M W

-

d dt

L[t]

+

BL(W

2

+

1)

1/2

(E.2)

Proof. Let Q(t) Q (t) = =

hdt. We can compute: h() dt ()d
dt h()(-t() -  · (v[t]()t()))d + 

hdU d

= -

h(¯)



2 2

t

()d

+



hdU d -

h() · (v[t]()t())d

(E.3)

Note that the first two terms are bounded by B(W 2 + 1) by the assumptions for the lemma. For the third term, we have from Lemma E.5:

h() · (v[t]()t())d = |Et [ h(), v[t]() ]|



Et [

h()

2 2

]Et

[

v[t]()

22]

(by Cauchy-Schwarz)



Et [

h(¯)

2 2



2 2

]Et

[

v[t]()

22]

(by homogeneity of h)

 MW  MW

Et [ v[t]() 22]

(since h is Lipschitz on the sphere)

-

d dt

L[t]

+

BL(W

2

+

1)

1/2

(by Corollary E.8)

Plugging this into equation E.3, we get that |Q (t)|  B(W 2 + 1) + M W

-

d dt

L[t]

+

BL(W

2

+

1)

1/2

We apply this result to bound the change in L [t] over time in terms of the change of the objective value. For clarity, we write the bound in terms of c1 that is some polynomial in the problem constants.

Lemma E.12. Define Q(t)

dt. For every ¯  Sd and 0  t  t + l  t , c1

poly(k, CR, B, M, BL) such that

t+l

|L [t](¯) - L [t+l](¯)|  CRB

Q (t) 1

t  lc1(W 2 + 1) + c1W l(L[t] - L[t+l] + lc1(W 2 + 1))1/2

(E.4) (E.5)

29

Under review as a conference paper at ICLR 2019

Proof. Recall that L [t](¯) = R( dt), (¯) + V (¯). Differentiating with respect to t,

d dt

L

[t](¯)

=

d R dt

dt , (¯)

= (¯) 2R(Q(t))Q (t)

 CRB Q (t) 2

 CRB Q (t) 1

(E.6)

Integrating and applying the same reasoning to -L [t] gives us equation E.4. Now we apply Lemma E.11 to get

kd

Q (t) 1 =

dt

i=1

idt

k
 B(W 2 + 1) + MW
i=1

-

d dt

L[t]

+

BL(W

2

+

1)

1/2

 kB(W 2 + 1) + kMW

-

d dt

L[t]

+

BL(W

2

+

1)

1/2

We plug this into equation E.6 and then integrate both sides to obtain

t+l

CRB

Q (t) 1

t

t+l
 klCRB2 (W 2 + 1) + kCRBMW t

-

d dt

L[t]

+

BL(W

2

+

1)

1/2

 klCRB2 (W 2 + 1) + kCRBMW l(L[t] - L[t+l] + lBL(W 2 + 1))1/2

Using c1 max{kCRB2 , kCRBM, BL} gives the statement in the lemma.

Now we also show that L is Lipschitz on the unit ball. For clarity, we let c2 Lemma E.13. For all ¯, ¯  Sd,
|L [](¯) - L [](¯ )|  c2 ¯ - ¯ 2

 kMRM + MV . (E.7)

Proof. Using the definition of L and triangle inequality,

|L [](¯) - L [](¯ )|  R d

(¯) - (¯ ) 2 + |V (¯) - V (¯ )|

2  ( kMRM + MV ) ¯ - ¯ 2

(by definition of M, MR, MV )

Now the remainder of the proof will proceed as follows: we show that if t is far from optimality,

either the expected velocity of  under t will be large in which case the loss decreases from Corollary E.8, or there will exist ¯ such that L [t](¯) 0. We will first show that in the latter case, the U d noise term will grow mass exponentially fast in a descent direction until we make

progress. Define Kt- {¯  Sd : L [t](¯)  - }, the - -sublevel set of L [t], and let

m(S) EUd [1(  S)] be the normalized spherical area of the set S.

Lemma E.14.

If Kt-

is nonempty, for 0  





,

log

m(Kt- + )



-2d

log

c2 

.

Proof. Let ¯  Kt- . From Lemma E.13, L [](¯ )  - +  for all ¯ with

¯

- ¯

2



 c2

.

Thus,

we have

m(Kt- +)  E¯ Ud

1[

¯

- ¯

2



 ]
c2

Now the statement follows by Lemma 2.3 of (Ball et al., 1997).

30

Under review as a conference paper at ICLR 2019

Now we show that if a descent direction exists, the added noise will find it and our function value will decrease. We start with a general lemma about the magnitude of the gradient of a 2-homogeneous function in the radial direction. Lemma E.15. Let h : Rd+1  R be a 2-homogeneous function. Then for any   Rd+1, ¯ h() = 2  2h(¯).

Proof. We have h( + ¯) = (  2 + )2h(¯). Differentiating both sides with respect to  and evaluating the derivative at 0, we get ¯ h() = 2  2h(¯), as desired.

We state the lemma claiming that our objective will decrease if L [t](¯) 0 for some ¯  Sd.

Lemma E.16. Choose

l

log(W

2/)

+

2d

log

2c2 

+1

 -

If Kt- is nonempty for some t satisfying t + l  t , then after l steps, we will have

L[t+l]



L[t ]

-

( /4

-

lc1(W 2 lc12W 2

+

1))2

+

lc1(W 2

+

1)

(E.8)

We will first show that a descent direction in L [t] will remain for the next l time steps. In the

notation of Lemma E.12, define z(s)

CRB

t +s t

Q (t) 1dt. Note that from Lemma E.12, for

all ¯  Sd we have |L [t+s](¯) - L [t ](¯)|  z(s). Thus, the following holds:

Claim E.17. For all s  l, Kt-++sz(s) is nonempty.

Proof. By assumption, ¯ with ¯  Kt- . Then L [t+s](¯)  L [t ](¯) + z(s)  - + z(s), so Kt-++sz(s) is nonempty.

Let Ts Kt-+/s2+z(s) for 0  s  l. We now argue that this set Ts does not shrink as t increases. Claim E.18. For all s > s, Ts  Ts.

Proof. From equation E.6 and the definition of z(s), |L [t+s ](¯) - L [t+s](¯)|  z(s ) - z(s). It follows that for ¯  Ts
L [t+s ](¯)  L [t+s](¯) + z(s ) - z(s)

 - /2 + z(s) - z(s) + z(s )

(by definition of Ts)

 - /2 + z(s )

which means that ¯  Ts .

Now we show that the weight of the particles in Ts grows very fast if z(k) is small.

Claim E.19. Suppose that z(l)   /4. Let T~s = {  Rd+1 : ¯  Ts}. Define N (s)

T~s  2dt+s and 

exp(-2d log

2c2 

).

Then

N

(s)



(

-

)N (s)

+

.

Proof.

From the assumption z(l) 

 4

,

it

holds

that

Ts

 Kt-+/s4

s  k.

Since Ts is defined as a

sublevel set, v[t+s](¯) points inwards on the boundary of Ts for all ¯  Ts, and by 1-homogeneity

of the gradient, the same must hold for all u  T~s.

Now consider any particle   T~s. We have that  flows to  + v[t+s]()ds at time t + s +
ds. Furthermore, since the gradient points inwards from the boundary, it also follows that u + v[t+s]()ds  T~s. Now we compute



2 2

dt

+s+ds

=

(1

-

ds)

 + v[t+s]()ds

2 2

dt

+s

+

ds

1dU d

T~s T~s

T~s

 (1 - ds)

(

2 2

+

2

v[t+s]()ds)dt+s + m(Kt-+/s2+z(s))ds

T~s

(E.9)

31

Under review as a conference paper at ICLR 2019

Now we apply Lemma E.15, using the 2-homogeneity of F and the fact that L [t+s](¯)  - /4   T~s



2 2

+

2

v[t+s]()ds =



2 2

-

4



22L [t+s](¯)ds

  22(1 +  ds)

(E.10)

Furthermore, since Kt-++sz(s) is nonempty by Claim E.17, we can apply Lemma E.14 and obtain

m(Kt-+/s2+z(s))  

(E.11)

Plugging equation E.10 and equation E.11 back into equation E.9, we get

u

2 2

dt

+s+ds

 (1 - ds)(1 + 2 ds)N (s) + ds

T~s

Since we also have that T~s+ds  T~s, it follows that

N (s + ds) =

u

2 2

dt

+s+ds

 (1 - ds)(1 +  ds)N (s) + ds

T~s+ds

and so N (s)  ( - )N (s) + .

Now we are ready to prove Lemma E.16.

Proof of Lemma E.16. If z(l) = CRB

t+l t

Q (t)

1



 4

,

then

by

rearranging

the

conclusion

of

Lemma E.12 we immediately get equation E.8.

Suppose for the sake of contradiction that z(l)   /4. From Claim E.19, it follows that N (1)  ,

and

N (l)



exp((

-)(l -1))N (1).

Thus,

in

log(W 2/)+2d log  -

2c2 

+1

time,

Wt+l



N (l)



W 2,

a contradiction. Therefore, it must be true that z(l)   /4.

The following lemma will be useful in showing that the objective will decrease fast when t is very suboptimal.

Lemma E.20. For any time t with 0  t  t , we have

d dt L[t]



BL(W 2

+

1)

-

Et [L [t]()]2 W2

(E.12)

Proof. We can first compute

Et [L [t]()] = Et [L [t](¯)  22]

1 = 2 Et [



2¯

v[t]()]

1 2

Et [



2 2

]Et

[

v[t]()

22]



1 W

2

Et [ v[t]() 22]

(via Lemma E.15) (by Cauchy-Schwarz)

Rearranging gives Et [

v[t]()

22] 

Et

[L [t]()]2 W2

,

and

plugging

this

into

equation

E.1

gives

the desired result.

Proof of Theorem E.4. Let L denote the infimum inf L[], and let  be an -approximate global

minimizer of L: L[ ]  L + . (We define  because a true minimizer of L might not exist.) Let

W

E

[



2 2

].

We

first

note

that

since

bV

W

2  L[

]  L[0], W

2  L[0]/bV

 W2.

Now we bound the suboptimality of t: since L is convex in ,

L[ ]  L[t] + E [L [t]()] - Et [L [t]()]

32

Under review as a conference paper at ICLR 2019

Rearranging gives L[t] - L[ ]  Et [L [t]()] - E [L [t]()]

 Et [L [t]()] - W 2 min

min
¯Sd-1

L

[t](¯),

0

(E.13)

Now let l

W2 -2W 2

2 log

W2 

+

2d log

4W 2c2

, which satisfies Lemma E.16 with the value of 

later specified. Suppose that there is a t with 0  t  t - 2l and t  [t, t + 2l], L[t ] - L  2 .

Then L[t ] - L[ ]  . We will argue that the objective decreases when we are suboptimal:

L[t] - L[t+2l] 

(E.14)

min

(

/8W 2

- lc1(W 2 c21W 2l

+ 1))2

- 3lc1(W 2

+

2
1), l 4W 2

-

2lBL(W 2

+ 1)

(E.15)

Using equation E.13 and W  W , we first note that

 Et [L [t ]()] - W 2 min

min
¯Sd-1

L

[t

](¯),

0

t  [t, t + l]

Thus, either min¯Sd L [t ](¯)  - 2W 2  - 2W 2 , or Et [L [t ]()]  2 . If t  [t, t + l]

such that the former holds, then the  2W 2 sub-level set Kt- is non-empty. Applying Lemma

E.16 gives

L[t

]

- L[t

+l]



(

/8W 2

- lc1(W 2 c12W 2l

+ 1))2

-

lc1(W 2

+ 1)

Furthermore, from Lemma E.10, L[t+2l] - L[t +l]  lc1(W 2 + 1) and L[t ] - L[t]  lBL(W 2 + 1), and so combining gives

L[t] - L[t+2k]



(

/8W 2

- lc1(W 2 c21W 2l

+

1))2

- 3lc1(W 2

+ 1)

(E.16)

In the second case Et [L [t ]()]  2 , t  [t, t + l]. Therefore, we can integrate equation E.12 from t to t + l in order to get

2
L[t] - L[t+l]  l 4W 2 - lBL(W 2 + 1)

Therefore, applying Lemma E.10 again gives

2
L[t] - L[t+2l]  l 4W 2 - 2lBL(W 2 + 1)

(E.17)

Thus equation E.15 follows.

Now recall that we choose  exp(-d log(1/ )poly(k, MV , MR, M, bV , BV , CR, B, L[0] - L[ ]))

For the simplicity, in the remaining computation, we will use O(·) notation to hide polynomials in

the problem parameters besides d, . We simply write  = exp(-c3d log(1/ )). Recall our choice

t

O(

d2
4

log2(1/

)).

It

suffices

to

show

that

our

objective

would

have

sufficiently

decreased

in

t steps. We first note that with c3 sufficiently large, W 2 = O(L[0]/bv) = O(1). Simplifying our

expression for l, we get that l = O( d log 1 ), so long as W 2 = o( ), which holds for sufficiently

large c3. Now let

1

(

/8W 2

- lc1(W 2 c21W 2l

+

1))2

-

3lc1(W 2

+

1)

2
2 l 4W 2 - 2lBL(W 2 + 1)

23
Again, for sufficiently large c3, the terms with  become negligible, and 1 = O( l ) = O( d log(1/ ) ). Likewise, 2 = O(d log(1/ )).

Thus, if by time t we have not encountered 2 -optimal t, then we will decrease the objective by

3
O( d log(1/

))

in

O( d

log

1)

time.

Therefore,

a

total

of

O(

d2
4

log2(1/

))

time

is

sufficient

to

obtain

accuracy.

33

Under review as a conference paper at ICLR 2019

Method

CIFAR10 CIFAR100

Weight decay annealing 5.86

26.22

Fixed weight decay

6.01

27.00

Table 1: Test error on CIFAR10 and CIFAR100 for initial  = 0.0005.

E.3 DISCRETE-TIME OPTIMIZATION
To circumvent the technical issue of existence of a solution to the continuous-time dynamics, we also note that polynomial time convergence holds for discrete-time updates. Theorem E.21. Along with Assumptions E.1, E.2, E.3 additionally assume that i and V are C and CV -Lipschitz, respectively. Let t evolve according to the following discrete-time update:
t+1 t + (-t + U d -  · (v[t]t))
There exists a choice of
 exp(-d log(1/ )poly(k, MV , MR, bV , BV , CR, B, C, CV , L[0] - L[ ]))
 poly(k, MV , MR, bV , BV , CR, B, C, CV , L[0] - L[ ]) d2
t 4 poly(k, MV , MR, bV , BV , CR, B, C, CV , L[0] - L[ ])
such that min0tt L[t] - L  .
The proof follows from a standard conversion of the continuous-time proof of Theorem E.4 to discrete time, and we omit it here for simplicity.
F ADDITIONAL EXPERIMENTS
We train a modified WideResNet architecture (Zagoruyko & Komodakis, 2016) on CIFAR10 and CIFAR100. Our theory does not entirely apply because the identity mapping prevents ResNet architectures from being homogeneous, but our experiments show that reducing weight decay can still help generalization error in this setting. Because batchnorm can cause the regularizer to have different effects (van Laarhoven, 2017), we remove batchnorm layers and train a 16 layer deep WideResNet. We again compare a network trained with weight decayed annealing to one trained without annealing. We used a fixed learning rate schedule that starts at 0.1 and decreases by a factor of 0.2 at epochs 60, 120, and 160. For CIFAR10, we use an initial weight decay of 0.0002 and decrease the weight decay by 0.2 at epoch 60, and then by 0.5 at epochs 90, 120, 140, 160. For CIFAR100, we initialize weight decay at 0.0005 and decrease it by 0.2 at epochs 60, 120, and 160. We tried different parameters for the initial weight decay and chose the ones that worked best for the model without annealing. We also tried using small weight decays at initialization, but these models failed to generalize well ­ we believe this is due to an optimization issue where the algorithm fails to find a true global minimum of the regularized loss. We believe that annealing the weight decay directs the optimization algorithm closer towards the global minima for small .
Table 1 shows the test error achieved by models with and without annealing. We see that the simple change of annealing weight decay can decrease the test error for this architecture.

34


Under review as a conference paper at ICLR 2019
INFORMATION MAXIMIZATION AUTO-ENCODING
Anonymous authors Paper under double-blind review
ABSTRACT
We propose the Information Maximization Autoencoder (IMAE), an information theoretic approach to simultaneously learn continuous and discrete representations in an unsupervised setting. Unlike the Variational Autoencoder (VAE) framework, IMAE starts from a stochastic encoder that seeks to map each input data to a hybrid discrete and continuous representation with the objective of maximizing the mutual information between the data and the representation. A decoder is included for approximating the posterior distribution of the data given their representations, where a high fidelity approximation can be achieved by leveraging our informative learned representations. We show that our objective is theoretically valid and provides a principled framework for understanding the tradeoffs among the informativeness of each representation factor, disentanglement of representations, and the decoding quality.
1 INTRODUCTION
A central tenet for designing and learning a model for data is that the resulting representation should be compact yet informative. Therefore, the goal of learning can be formulated as finding informative representations about the data under proper constraints. Generative latent variable models are a popular approach to this problem, where a model parameterized by  of the form p(x) = p(x|z)p(z)dz is used to represent the relationship between the data x and the low dimensional latent variable z. The model is optimized by fitting the generative data distribution p(x) to the training data distribution p(x), which involves maximizing the likelihood for . Typically, this model is intractable even for moderately complicated functions p(x|z) and continuous latent representations z. To remedy this issue, the variational autoencoder (VAE) framework (Kingma and Welling, 2013; Rezende et al., 2014) proposes maximizing the evidence lower bound (ELBO) of the marginal likelihood objective.
However, as was initially pointed out in (Hoffman and Johnson, 2016), maximizing ELBO also penalizes the mutual information between data and their representations. This in turn makes the representation learning harder and leads to poor reconstructions of the data as well. Many recent efforts have been focused on resolving this problem by revising ELBO. Generally speaking, these works fall into two categories. One line of work targets "disentangled representations" by encouraging the statistical independence between components of latent representations (Higgins et al., 2016; Kim and Mnih, 2018; Gao et al., 2018; Chen et al., 2018; Esmaeili et al., 2018), while the other line of work seeks to explicitly control or encourage the mutual information between the data and their representations (Mary Phuong, 2018; Burgess et al., 2018; Alemi et al., 2017; Dupont, 2018; Zhao et al., 2017). However, these approaches either result in an invalid lower bound for the VAE objective or cannot avoid minimizing the mutual information between data and their representations.
In this work, instead of starting with the generative latent variable model, we start with a stochastic encoder p(z|x) and aim at maximizing the mutual information between the data x and the representations z. In this setting, a reconstruction or generating phase can be obtained as the variational inference of the true posterior p(x|z). By explicitly targeting informative representations, the proposed model is capable of yielding better decoding quality. Moreover, as we will show in Section 3, the information maximization objective naturally induces a balance between the informativeness of each latent factor and the statistical independence between them, which gives a more principled way to learn semantically meaningful representations without invalidating the original objective or removing individual terms from the ELBO.
1

Under review as a conference paper at ICLR 2019
Another contribution of this work is a framework for simultaneously learning continuous and discrete representations, which yields a more flexible model for data that belong to different categories. Categorical data are ubiquitous in real-world tasks, and using a hybrid discrete and continuous representation to capture both categorical information and continuous variation in data is more consistent with the natural generation process. In this work, we focus on categorical data that are similar in nature, i.e., where different categories still share similar variations (features). We seek to learn semantically meaningful discrete representations while maintaining disentanglement of the continuous latent factors that capture the variations shared over different categories. We show that, compared to the VAE based approaches, our information maximization objective leads to a more natural yet powerful way for learning these hybrid representations.
2 RELATED WORK
Recently, there has been a surge of interest in learning interpretable representations. Among them, -VAE (Higgins et al., 2016) is a popular method for learning disentangled representations, which modifies ELBO by increasing the penalty on the KL divergence between the variational posterior and the factorized prior. However, by using large weight for the KL divergence term, -VAE also penalizes the mutual information between the data and the latent representations more than a standard VAE does, resulting in more severe under utilization of the latent representation space.
Several follow up works propose different approaches to address the limitations of -VAE. (Dupont, 2018; Alemi et al., 2017; Burgess et al., 2018; Mary Phuong, 2018) propose to constrain the mutual information between the representations and the data by pushing its upper bound, i.e., the KL divergence term in ELBO, towards a progressively increased target value. However, specifying and tuning this target value can itself be very challenging, which makes this method less practical. Moreover, this extra constraint results in an invalid lower bound for the VAE objective. Alternatively, (Zhao et al., 2017) drops the mutual information term in ELBO. By pushing only the aggregated posterior towards a factorial prior, they implicitly encourage independence across the dimensions of latent representations without sacrificing the informativeness of the representations. However, simply removing the mutual information term also violates the lower bound of the VAE objective.
Another relevant line of work (Gao et al., 2018; Kim and Mnih, 2018; Chen et al., 2018; Esmaeili et al., 2018) seek to learn disentangled representations by explicitly encouraging statistical independence between latent factors. They all propose to minimize the total correlation term of the latent representations, either augmented as an extra term to ELBO or obtained by reinterpreting or re-weighting the terms in the VAE objective, as a way to encourage statistical independence between the representation components. In contrast, we show that our information maximization objective inherently contains the total correlation term while simultaneously seeking to maximize the informativeness of each representation factor.
In this paper, we introduce a different perspective to the growing body of the VAE based approaches for unsupervised representation learning. Starting by seeking informative representations for our data, we follow a more intuitive way to maximize the mutual information between the data and the representations. Moreover, we augment the continuous representation with a discrete one, which gives us more flexibility to model real world data that are generated from different categories. We invoke the information maximization principle (Linsker, 1988; Bell and Sejnowski, 1995) with proper constraints implied by the objective itself to avoid degenerate solutions. The proposed objective gives a more natural yet theoretically elegant way to learn semantically meaningful representations.
3 INFORMATION MAXIMIZATION REPRESENTATION LEARNING
Given data x  Rd, we consider learning a hybrid continuous-discrete representation, denoted respectively with variables z  RK1 and y  {1, . . . , K2}, using a stochastic encoder parameterized by , i.e., p(y, z|x). We seek semantically meaningful representations in the sense that they should be informative enough about the data. A natural approach is to maximize the mutual information (Cover and Thomas, 2012) between the data and its representations, I(x; y, z). Here the mutual information between two random variables, e.g., x and z, is defined as I(x; z) = H(z)-H(z|x), where H(z) = -Ep(z) [log p(z)] is the entropy of z and H(z|x) = -Ep(x,z) [log p(z|x)] is the conditional entropy of z given x. The mutual information can therefore be interpreted as the
2

Under review as a conference paper at ICLR 2019

decrease in uncertainty of one random variable given another variable. In other words, it quantifies how much information one random variable has about the other.

In IMAE, a probabilistic decoder q(x|y, z) is adopted to approximate the true posterior p(x|y, z) which can be hard to estimate or even intractable. We want to minimize the dissimilarity between
the true posterior and the approximated one, for which we consider minimizing the KL divergence between them DKL (p(x|y, z)||q(x|y, z)). Therefore, the objective of IMAE is formulated as

I(x; y, z) - DKL (p(x|y, z)||q(x|y, z)) ,  > 0

(1)

where  is introduced for a better balance between the seeking for informative latent representations

and maintaining good posterior approximation quality. By leveraging the fact that H(x) is inde-

pendent of the optimization procedure, we can show that ptimizing (1) is equivalent to optimize the following 1.

( - 1)I(x; y, z) + Ep(x,y,z) [log q(x|y, z)] ,  > 0

(2)

where the first quantifies the informativeness of representations while the second term can be in-

terpreted as the "reconstruction error". We set  > 1 in order to achieve a balance between the

informativeness of representations and the decoding quality.

To optimize the first term, we follow the literature and assume that the conditional distribution of the representation (y, z) given x is factorial. Given this, I(x; y, z) can be decomposed as2

K1
I(x; y, z) = I(x; y) + I(x; zk) - DKL p(y, z) p(y)kK=11p(zk) .
k=1

(3)

The first two terms of the LHS quantify how much information each latent factor, i.e., y or zk, carry about the data. The last term is known as total correlation of (y, z) (Watanabe, 1960), which characterizes the dependence between the latent factors and achieves the minimum if and only if the latent factors are independent of each other. As is implied by Eq (3), the information maximization objective inherently involves a balance between maintaining the informativeness of each latent factor and the statistical independence between them. Now we proceed by first constructing tractable approximations for I(x; zk) and I(x; y) respectively.

3.1 INFORMATIVE CONTINUOUS REPRESENTATIONS

Without any constraints, the mutual information I(x; z) between a continuous latent factor and data can be trivially maximized by severely fragmenting the latent space. This can be justified by the fact that I(x; zk) = H(zk) - H(zk|x) . Hence, to maximize I(x; zk), we can simply map each data x(i) to a deterministic representation z(i) so that the conditional entropy is minimized, while maximizing the uncertainty in zk by always mapping to a distinct zk. To avoid this degenerate solution and motivate our objective, we call out the following proposition. While similar results
have likely been established in the information theory literature, we include this proposition to give
insight into our objective design.

Proposition 1. Suppose the conditional distribution p(z|x) is a factorial Gaussian distribution with mean µ(x) and covariance (x). Let (x)  RK1 denote the diagonal entries of (x), then k = 1, . . . , K1,

I(x; zk)



1 2

log

Ex k2(x) + Varx [µk(x)]

-

1 2 Ex

log k2(x)

.

(4)

The equality in (4) is attained if and only if zk is Gaussian distributed, given which we have

I(x; zk)



1 2

log

1

+

Varx [µk(x)] Ex [k2(x)]

.

(5)

The lower bound in (5) implies that, in order to maximize I(x; zk), we should increase the variance of the mean µk(x) while decreasing the variance k(x). Note here both µk(x) and k(x) are random variables. This matches the discussion above, i.e., zk is more informative about x if it
1A detailed derivation is provided in Appendix A.1 2A detailed derivation is provided in Appendix A.

3

Under review as a conference paper at ICLR 2019

has less uncertainty given x yet captures more variance in data, i.e., k(x) is small while µk(x) is dispersed within a large space.

In light of this, we can make what we described above more precise. A vanishing variance of
the conditional distribution p(zk|x) leads to a plain autoencoder that maps each data sample to a
deterministic latent point, which will fragment the latent space where each data point is associated with a delta distribution in the latent space p(zk|xi) = (zki ). On the other hand, Proposition 1 also implies that controlling the variance k(x) to be finite, I(x; zk) will be maximized by pushing µk(x) towards two extremes (±). To avoid such a trivial solution while achieving the upper bound, a natural resolution is to squeeze zk within the domain of a Gaussian distribution with finite mean and variance. By doing so, the representation zk will achieve a more reasonable trade off between enlarging the spread of µk(x) and maintaining the continuity of z.

In this paper, to do this we use a regularizing distribution r(z), which is a factorial Gaussian, and push p(zk) towards each r(zk) by minimizing the KL divergence DKL (p(zk)||r(zk)):

max L(z) := -

K1 k=1

DKL

(p

(zk

)||r(zk))

.

(6)

3.2 INFORMATIVE DISCRETE REPRESENTATIONS

Unlike the continuous representation, the mutual information I(x; y) between a discrete representation and data can be well approximated, given the fact that the cardinality of the space of y is typically low. To be more specific, given N i.i.d samples {xn}Nn=1 from some unknown distribution p(x) of the data, the empirical estimation of I(x; y) under the conditional distribution p(y|xn) follows as

1 I(x; y) = H(y) - H(y|x) = H N

N n=1

p

(y|xn

)

-1 N

Nn=1H (p(y|xn)) .

(7)

As we will now show, with a suitably large batch of samples, the empirical mutual information
I(x; y) is a good approximation to the population mutual information. This enables us to optimize I(x; y) in a theoretically justifiable way that is amenable to stochastic gradient descent with minibatches of data.
Proposition 2. Let y be a discrete random variable that belongs to some categorical class C. Assume the marginal probabilities of the true and predicted labels are bounded below, i.e. p(y), p(y)  [1/(CK2), 1] for all y  C for some constant C > 1. Then with probability exceeding 1 - 2,

I(x; y) - I(x; y)  K2 (max{log CK2 - 1, 1} + e)

log(2K2/) . 2N

(8)

where N denotes the number of samples used to establish I(x; y) according to Eq (7).

Therefore, to maximize the mutual information I(x; y), we consider the following:

max L := I(x; y).

(9)

Maximizing the the mutual information I(x; y) provides a natural way to learn discrete categorical representations. To see this, notice that I(x; y) contains two fundamental quantities, the category balance term H(y) and the category separation term H(y|x). In other words, maximizing I(x; y)
trades off uniformly assigning data over categories and seeking highly confident categorical iden-
tity for each sample x. The maximum is achieved if p(y|x) is deterministic while the marginal distribution p(y) is uniform, that is H(y|x) = 0 and H(y) = log K2.

Overall Objective Our overall objective can therefore be summarized as



max
,

L(z) + L(y) - DKL

p(y, z)||p(y)kK=11p(zk)

+ Ep(x,y,z) [log q(x|y, z)] . (10)

where L(z) and L(y) are defined in Eqs (6) and (12), respectively. As we discussed before, the first three terms associate with our information maximization objective, while the last one aims at

4

Under review as a conference paper at ICLR 2019

better approximation of the posterior p(x|y, z). A better balance between these two targets can be achieved by weighting them differently. One the other hand, the informativeness of each latent factor can be optimized through L(z) and L(y), while statistically independent latent factors can
be induced by minimizing the total correlation term DKL p(y, z)||p(y)kK=11p(zk) . Therefore,
trade-offs can be formalized among different terms so as to achieve a better balance regarding the informativeness of each latent factor, disentanglement of the representation, and better decoding quality. This motivates us to consider the following objective, with tradeoff parameters ,  > 0 :

max 
,

L(z) + LD(y) - DKL

p(y, z)||p(y)kK=11p(zk)

+ Ep(x,y,z) [log q(x|y, z)] .

(11)

Similar to VAE, the reconstruction term Ep(x,y,z) [log q(x|y, z)] is optimized using reparameterization tricks proposed by (Kingma and Welling, 2013) and (Jang et al., 2016) for continuous
representations z and discrete representation y respectively. As for the marginal distributions, we
approximate them using Monte Carlo sampling methods. Please refer to B for details.

4 EXPERIMENTAL RESULTS
We compare IMAE against a number of VAE based approaches summarized in Figure 1. We would like to demonstrate that IMAE can (i) successfully learn a hybrid of continuous (z) and discrete (y) representations, with y matching the nature categorical information ytrue well and z capturing the disentangled feature information shared across categories; (ii) outperform the VAE based models on achieving a better trade-off between the interpretability of the representations and the decoding quality. We evaluate all models on three different datasets: MNIST, Fashion MNIST (F-MNIST) and CelebA. We choose the priors r(y) and r(z) as isotropic Gaussian distribution and uniform distribution over categories correspondingly. Detailed experiment settings is provided in Appendix E. For notational convenience, we drop the subscripts  and  in this section.

LVAE = Ep(y,z|x) [q(x|y, z)] - DKL (p(z|x)||r(z)) - DKL (p(y|x)||r(y))  ELBO = Ep(y,z|x) [q(x|y, z)] - I(x; y) - DKL (p(y)||r(y)) - I(x; z) - DKL (p(z)||r(z)) 1 2345 -VAE: 1 -  ( 2 + 3 ) -  ( 4 + 5 ) InfoVAE: 1 -  3 -  5 Joint-VAE: 1 -  | 2 + 3 - Cy | -  | 4 + 5 - Cz|

Figure 1: Summarization of relevant works. -VAE modifies ELBO by increasing the penalty on the KL divergence terms. Both InfoVAE and WAE drop the mutual information terms from ELBO. JointVAE seeks to control the mutual information by pushing the their upper bounds, i.e., the associated KL divergence terms, towards progressively increased target values, Cy and Cz.
To evaluate the ability of IMAE on jointly learning the continuous and discrete representations, we conduct both qualitative and quantity experiments on MNIST and FMNIST.
4.1 INFORMATIVE CONTINUOUS LATENT REPRESENTATIONS YIELD INTERPRETABILITY
In this section, we evaluate how the informative representations can yield better interpretability. We start by qualitatively examining Proposition 1. As we can see, Figure 2 validates Proposition 1 in a way that, with roughly same amount of variance for each latent variable zk, those achieving high mutual information between the data and themselves have the means µk(x) of the conditional probability p(zk|x) disperse across data samples and the variance k(x) decrease to small values for all data samples. As a qualitative assessment of how the informative representations lead to better interpretability for continuous latent factors, we traverse the latent dimensions corresponding with the largest, medium, and smallest mutual information between themselves and the data. As shown in Figure 2, informative representations is capable of uncovering intuitive factors of variation in the data, while the non-informative one cannot. This matches the insight from recently developed

5

Under review as a conference paper at ICLR 2019

I(x, z8) = 1.7, Angle I(x, z3) = 0.9, Thickness I(x, z1) = 0, No change
Figure 2: From left to right: (1) Illustration of Proposition 1. (2) - (5) Latent traverse for IMAE on MNIST. We get the initial latent representations by feeding the encoder with randomly selected images. For each image, we then traverse the latent dimensions corresponding with the largest, medium, and smallest mutual information between themselves and the data by systematically manipulating each of them within the range [-2, 2] while keeping all other dimensions fixed.

methods (Chen et al., 2016; Gao et al., 2018) that interpretable representations can be attained by maximizing the mutual information between themselves and the data.

4.2 UNSUPERVISED LEARNING OF DISCRETE LATENT FACTOR
Before we proceed on the numerical results, we introduce an assumption on the conditional distribution p(y|x), which is crucial for learning categorical representations. A reasonable assumption for categorical representation is that the resulting distribution p(y|x) should be locally smooth such that the data that are close on their manifold should have high probability of being assigned to the same category (Agakov, 2005). This assumption is especially crucial for using deep neural network to learn discrete representations, given that the high capacity of neural network allows it to easily learn a non-smooth function p(y|x) which can abruptly change its predictions without guaranteeing similar data samples have the same categorical representation. To remedy this issue, we adopt the virtual adversarial training technique (VAT) proposed by (Miyato et al., 2016) to encourage the local smoothness of p(y|x) and consider the following 3

max LD := I(x; y) - Ep(x) max H (p(y|x); p(y|x + )) ,  > 0 .


(12)

Here the second term of RHS explicitly enforces p(y|x) being consistent within the norm ball of each data sample so as to maintain the local smoothness of the prediction model. We augment all models explored in this section with this VAT regularization, which is very important for all of them except -VAE to learn semantically meaningful discrete representation.
Figure (3a) shows that, by simply pushing the prediction p(y|x) towards a uniform distribution r(y), -VAE sacrifices the mutual information I(x; y) and hence struggles in discovering semantically meaningful discrete latent factor even with the VAT. As a comparison, InfoVAE achieves much better performance by dropping I(x; y) from ELBO. By minimizing the KL divergence DKL(p(y)||r(y)) minimizing DKL(p(y)||r(y)), InfoVAE also pushes the whole dataset to uniformly distributed over categories. Therefore, for data that is distinctive enough between categories, e.g., MNIST, minimizing this KL divergence together with the local smoothness encouraged regularization VAT can yield good performance. However, InfoVAE struggles as we move to less distinctive data, e.g., F-MNIST, for which it cannot give fairly confident category separation by only requiring local smoothness. This is demonstrated by high value of the conditional entropy H(y|x) and low values of I(x; y) and I(y; ytrue). In contrast, IMAE achieves much better performance by explicitly encouraging confident category separation via minimizing H(y|x), while using VAT to maintain local smoothness so as to prevent overfitting of neural network.

3In this paper, we set = 1 across datasets. VAT can be effectively approximated by a pair of forward and backward passes (Miyato et al., 2016).

6

Under review as a conference paper at ICLR 2019
(a) Evolution of the key quantities for discrete representations.
Figure 3: (a) Tracking the key quantities for categorical representations.
By pushing the upper bound DKL (p(y|x)||r(y)) of I(x; y) towards a progressively increasing target value Cy, JointVAE performs well on both MNIST and F-MNIST. However, we found it can easily get stuck at some bad local optima where I(x; y) is close to the maximum while the accuracy are lower than IMAE. This is an implication of overfitting, i.e., the prediction p(y|) are over confident about it's prediction. A possible explanation is that once JointVAE enter the local region of a local optima, progressively increasing the target value can only lead to oscillation within that region. Although, it might be possible to escape such local region by increasing the target value more progressively at the corresponding training steps, specifying this increasing rate can be very challenging across datasets.
4.3 BETTER TRADE-OFFS AMONG INTERPRETABILITY AND DECODING QUALITY In this section, we examine the trade-offs between the interpretability of latent representations and the decoding quality induced by different models. Table (4a) shows that IMAE achieves better overall trade-off with respect to the decoding quality, informativeness of latent representations and the statistical independence between latent factors. This is further demonstrated by Figure (4b) to (4e), where we traverse the encoding "angle" factor conditioned on the discrete representations learned by different models. As we can see, different models learn the digit type information to various degree. Being able to learn meaningful discrete representations encourages more separated mainfold structures between categories, i.e., a more interpretable embedding space, which in turn allows more flexibilities to represent the data. On the other hand, compared to JointVAE and InfoVAE, IMAE attains less overlap between the mainfolds associated with each category. We attribute this to the effect of explicitly seeking for statistical independence between the latent factors. This can serves as an evidence of statistical independence induce disentanglement, though it's not sufficient. On the other hand, an important insight from recent work, e.g., (Zhao et al., 2017; Tolstikhin et al., 2017; Makhzani and Frey, 2017), is that informative representations can give better decoding quality. This is demonstrated in Table (4a), where IMAE gives better reconstruction error as a result of being able to learn more informative and interpretable representations.
5 CONCLUSION
We have proposed IMAE, a novel approach for simultaneously learning the categorical information of data while uncovering latent continuous features shared across categories. Different from VAE,
7

Under review as a conference paper at ICLR 2019

-VAE J-VAE I-VAE IMAE

KL(x; x) 141.5 127.3 121.5 117.9

(a) Learning statistics on MNIST/F-MNIST.

I(x; y) 0.0 1.9 1.7 1.8

I(x; z) 4.4 5.6 6.8 7.2

TC(y; z) 0.0 2.3 1.5 0.8

KL(x; x) 252.5 256.0 247.1 234.9

I(x; y) 0.0 1.8 0.9 1.7

I(x; z) 4.6 4.7 6.0 8.5

TC(y; z) 0.0 2.6 0.7 0.0

(b) IMAE

(c) InfoVAE

(d) JointVAE

(e) -VAE

Figure 4: IMAE achieves better disentanglement between discrete and continuous representations. Here we set ,  = 5 for all models and choose  = 2 for IMAE. In Table 4a, TC(y; z) is referred to the total correlation of (y, z) which is introduced in Section 3. We traverse the continuous latent variable conditioned on the learnt discrete representation. The initial values of z for each category learnt by a model is selected as the sample average of the representations associated with that category. Then we traverse encoding "angle" latent factor within [-2, 2] over categories. The results for (a) - (e) are reported on the 10000 testing images.

IMAE starts with a stochastic encoder that seeks to maximize the mutual information between data and their representations, where a decoder is used to approximate the true posterior distribution of the data given the representations. This model targets at informative representations directly, which in turn naturally yields an objective that is capable of simultaneously inducing semantically meaningful representations and maintaining good decoding quality, which is further demonstrated by the numerical results.
Unsupervised joint learning of disentangled continuous and discrete representations is a challenging problem due to the lack of prior for semantic awareness and other inherent difficulties that arise in learning discrete representations. This work takes a step towards achieving this goal. A limitation of our model is that it pursues disentanglement by assuming or trying to encourage independent scalar latent factors, which may not always be sufficient for representing the real data. For example, data may exhibit category specific variation, or a subset of latent factors might be correlated. This motivates us to explore more structured disentangled representations; one possible direction is to encourage group independence. We leave this for future work.
REFERENCES
F. V. Agakov. Variational Information Maximization in Stochastic Environments. PhD thesis, University of Edinburgh, 2005.
A. A. Alemi, B. Poole, I. Fischer, J. V. Dillon, R. A. Saurous, and K. Murphy. An informationtheoretic analysis of deep latent-variable models. arXiv preprint arXiv:1711.00464, 2017.
8

Under review as a conference paper at ICLR 2019
A. J. Bell and T. J. Sejnowski. An information-maximization approach to blind separation and blind deconvolution. Neural computation, 7(6):1129­1159, 1995.
C. P. Burgess, I. Higgins, A. Pal, L. Matthey, N. Watters, G. Desjardins, and A. Lerchner. Understanding disentangling in -vae. arXiv preprint arXiv:1804.03599, 2018.
T. Q. Chen, X. Li, R. Grosse, and D. Duvenaud. Isolating sources of disentanglement in variational autoencoders. arXiv preprint arXiv:1802.04942, 2018.
X. Chen, Y. Duan, R. Houthooft, J. Schulman, I. Sutskever, and P. Abbeel. Infogan: Interpretable representation learning by information maximizing generative adversarial nets. In Advances in Neural Information Processing Systems, pages 2172­2180, 2016.
T. M. Cover and J. A. Thomas. Elements of information theory. John Wiley & Sons, 2012.
E. Dupont. Joint-vae: Learning disentangled joint continuous and discrete representations. arXiv preprint arXiv:1804.00104, 2018.
B. Esmaeili, H. Wu, S. Jain, S. Narayanaswamy, B. Paige, and J.-W. van de Meent. Hierarchical disentangled representations. arXiv preprint arXiv:1804.02086, 2018.
S. Gao, R. Brekelmans, G. V. Steeg, and A. Galstyan. Auto-encoding total correlation explanation. arXiv preprint arXiv:1802.05822, 2018.
I. Higgins, L. Matthey, A. Pal, C. Burgess, X. Glorot, M. Botvinick, S. Mohamed, and A. Lerchner. beta-vae: Learning basic visual concepts with a constrained variational framework. 2016.
M. D. Hoffman and M. J. Johnson. Elbo surgery: yet another way to carve up the variational evidence lower bound. In Workshop in Advances in Approximate Bayesian Inference, NIPS, 2016.
E. Jang, S. Gu, and B. Poole. Categorical reparameterization with gumbel-softmax. arXiv preprint arXiv:1611.01144, 2016.
H. Kim and A. Mnih. Disentangling by factorising. arXiv preprint arXiv:1802.05983, 2018.
D. P. Kingma and M. Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013.
R. Linsker. Self-organization in a perceptual network. Computer, 21(3):105­117, 1988.
A. Makhzani and B. J. Frey. Pixelgan autoencoders. In Advances in Neural Information Processing Systems, pages 1972­1982, 2017.
N. K. R. T. S. N. Mary Phuong, Max Welling. The mutual autoencoder: Controlling information in latent code representations. 2018. URL https://openreview.net/forum?id= HkbmWqxCZ.
T. Miyato, A. M. Dai, and I. Goodfellow. Virtual adversarial training for semi-supervised text classification. 2016.
D. J. Rezende, S. Mohamed, and D. Wierstra. Stochastic backpropagation and approximate inference in deep generative models. arXiv preprint arXiv:1401.4082, 2014.
I. Tolstikhin, O. Bousquet, S. Gelly, and B. Schoelkopf. Wasserstein auto-encoders. arXiv preprint arXiv:1711.01558, 2017.
R. Vershynin. High-Dimensional Probability: An Introduction with Applications in Data Science. Cambridge Series in Statistical and Probabilistic Mathematics. Cambridge University Press, 2018.
S. Watanabe. Information theoretical analysis of multivariate correlation. IBM Journal of research and development, 4(1):66­82, 1960.
S. Zhao, J. Song, and S. Ermon. Infovae: Information maximizing variational autoencoders. arXiv preprint arXiv:1706.02262, 2017.
9

Under review as a conference paper at ICLR 2019

A PROOF OF SECTION 3

Decomposition of I(x; y, z) Let b = (z, y) denote the joint random variable consisting of the continuous random variable b and discrete random variable y.
Note that I(x; y, z) = I(x; b) can be written as:

I(x; b) = - p(x) p(b|x) log p(b)dbdx + p(x) p(b|x) log p(b|x)dbdx

XZ

XZ

= - p(b) log p(b)db + p(x) p(b|x) log p(b|x)dbdx .
Z XZ
The second term in equation 13 has the form:

(13)

K1 +1

p(x) p(b|x) log p(b|x)dbdx =1

p(x) p(b|x) log p(bk|x)dbdx

XZ

k=1 X

Z

K1 +1
= H(bk|x) ,

k=1

where 1 follows by the assumption that p(b|x) is factorial.

(14)

For the first term in equation 13, we have:

p(b) log p(b)db =
Z

Z

p (b)

log

p (b) kK=1 1+1 p (bk

)

db

+

K1 +1 k=1

p(b) log p(bk)db
Z

K1 +1

= DKL p(b)||kK=11+1p(bk) -

H(bk) .

k=1

Substituting equation 14 and equation 15 into equation 13 yields the result:

(15)

K1 +1

I(x; y, z) = I(x; b) = H(bk) - DKL p(b)||kK=11+1p(bk) -

H (bk |x)

k=1

K1 +1
= I(x; bk) - DKL p(b)||kK=11+1p(bk)
k=1

K1
= I(x; y) + I(x; zk) - DKL p(y, z)||p(y)Kk=11p(zk)
k=1

. (16)

proof of proposition 1

Proof. We start with computing the expectation of zk:

E [zk] = zk p(zk|x)p(x)dxdzk = p(x) zkp(zk|x)dzkdx

Zk X

X Zk

Then the variance of zk followed as:

= p(x)µk(x)dx = Ex [µk(x)] .
X

Var [zk] = zk2 p(zk|x)p(x)dxdzk - Ex [µk(x)]2
Zk X
= p(x) zk2p(zk|x)dzkdx - Ex [µk(x)]2
X Zk
= p(x) k2(x) + µk(x)2 dx - Ex [µk(x)]2
X
= Ex k2(x) + Varx [µk(x)] .

(17) (18)

10

Under review as a conference paper at ICLR 2019

Note that I(x; zk) = H(zk) - H(zk|x) ,
for which we have the following,

(19)

H(zk|x) = - p(x) p(zk|x) log p(zk|x)dzdx

X Zk

1 =
2

p(x) log 2ek2(x) dx
X

1 =
2

log(2e) + Ex

log k2(x)

.

(20)

For the entropy of zk, we leverage the fact that H(zk) is upper bounded by the entropy of a Gaussian distributed random variable with the same mean and variance, that is

H (zk )



1 2

log 2e + log

Ex

k2(x)

+ Varx [µk(x)]

(21)

Substituting equation 20 and equation 21 into equation 19 completes the proof.

proof of proposition 2

Proof.

Let p(y)

=

1 N

N n=1

p (y |xn )

denote

the

Monte

Carlo

estimator

of

the

true

probability

p(y) = X p(x)p(y|x)dx = Ex [p(y|x)]. Note that p(y|x)  [0, 1] for all x  X , then ap-

plying the Hoeffding's inequality for bounded random variables [Theorem 2.2.6, (Vershynin, 2018)]

yields,

P (|p(y) - p(y)|  t) = P

1 N

N

p(y|xn) - Ex [p(y|x)]  t

n=1

Let  = 2 exp -2N t2 , it then follows,

 2 exp -2N t2

(22)

P |p(y) - p(y)| <

log(2/ )  1 -  2N

(23)

Given equation 23, we first establish the concentration results of the entropy Hp (y) with respect to the empirical distribution p(y). Assume For all y  C, we have p(y), p(y) bounded below by
1/(CK2) for some fixed constant C > 1. This assumption is practical since the distributions of true data and predicted data are approximately uniform and therefore p(y), p(y)  1/K2 for all y  C. Consider the function t log t, with derivative 1 + log t  [1 - log CK2, 1] for t  [1/(CK2), 1],

p (y)

|p(y) log p(y) - p(y) log p(y)| =

(1 + log t)dt

p (y)

p (y)

p (y)

 |1 + log t|dt  max{log CK2 - 1, 1}dt

p (y)

p (y)

 max{log CK2 - 1, 1}|p(y) - p(y)|

(24)

Summing over C gives

H(y) - H(y)  K2 max{log CK2 - 1, 1}|p(y) - p(y)| .

(25)

Let  = K2 , then equation 23 together with equation 25 yields

P

H(y) - H(y) < K2 max{log CK2 - 1, 1}

log(2K2/) 2N

1-

(26)

11

Under review as a conference paper at ICLR 2019

Next we are going to bound the divergence between H(y|x) and H(y|x) which are defined as,

H (y |x)

=

-

1 N

N

p(y|xn) log p(y|xn),

n=1 y

H(y|x) = -

p(y|x) log p(y|x) .

xX y

Note that h log h  [-1/e, 0] for all h  [0, 1], then again applying [Theorem 2.2.6, (Vershynin, 2018)] yields,

P

1 N

N

p(y|xn) log p(y|xn) - Ep(x) [p(y|x) log p(y|x)] < t

 2 exp -2t2e2N

n=1

(27)

Following the similar arguments as before, let  = 2 exp -2t2e2N , then

P

1 N

N

p(y|xn) log p(y|xn) - Ep(x) [p(y|x) log p(y|x)] <

e2 log(2/ )   2N

n=1

(28)

Now let  = K2 , then applying the union bound we have

|H(y|x) - H(y|x)| 

1 N

N

p(y|xn) log p(y|xn) - Ep(x) [p(y|x) log p(y|x)]

yC

n=1

 K2

e2 log(2K2/) 2N

(29)

hold with probability 1 - .

Conclude from equation 26 and equation 29, we have

I(x; y) - I(x; y)  H(y) - H(y) + H(y|x) - H(y|x)

= K2 (max{log CK2 - 1, 1} + e)

log(2K2/) . N

hold with probability at least 1 - 2.

(30)

A.1 BALANCE BETWEEN POSTERIOR INFERENCE FIDELITY AND INFORMATION
MAXIMIZATION OBJECTIVE
Notice that I(x; y, z) = H(x) + Ep(x,y,z) [log q(x|y, z)] + DKL (p(x|y, z)||q(x|y, z))
Therefore, I(x; y, z) - DKL (p(x|y, z)||q(x|y, z)) = H(x) + Ep(x,y,z) [log q(x|y, z)]
Therefore, our objective can also be motivated via max I(x; y, z) - DKL (p(x|y, z)||q(x|y, z)) ,  > 1  max ( - 1)I(x; y, z) + Ep(x,y,z) [log q(x|y, z)]
where  trade-off the informativeness of the latent representation and generation fidelity.

(31) (32) (33)

12

Under review as a conference paper at ICLR 2019

B APPROXIMATION OF THE MARGINAL DISTRIBUTION

In order to optimize the objective function (11), we need to compute the marginal probabilities

p(y), p(z) and p(zi). Computing these quantities requires the entire dataset, e.g., p(z) =

X p(z, x)dx = Ex [p(z|x)]. To scale up our method to large datasets, we would like to estimate

based

on

the

minibatch

data,

e.g.,

p (z )



1 B

B i=1

p

(z|xi

).

Now consider the first term on RHS of equation 6:

KL (p(z)||r(z)) =

Z

p (z )

log

p (z ) r(z)

dz

(34)

We will only present our approximation for Z p(z) log p(z)dz and Z p(z) log r(z)dz can be approximated in a similar fashion.

p(z) log p(z)dz =
Z

dxp(x)
X

Z

p(z|x) log p(z)dz

=

1 B

B i=1

p(z|xi) log p(z)dz
Z

1B

1B

= B

log p(zi) = B

log

i=1 i=1

dxp(x)p (zi |x)
X

1 =
B

B1 log B

B
p(zi|xj) ,

(35)

i=1 j=1

where B denotes the batch size, zi is sampled from p(z|xi) and different batches are iterated by i, j respectively.

We estimate the integral of z by sampling z  p(z|xi) and perform the Monte Carlo approximation. Although we minimize the unbiased estimator of the lower bound of the KL divergence, the term inside the logarithm is a summation of probability densities of Gaussians. In particular, we record the distribution of the variances output by our encoder and observe that the mean of the variances of the Gaussians is bounded between 0.2 and 2, which implies that the values of probability densities do not range in a large scale. Since logarithm is locally affine, we argue that our bound in (35) is tight. Other quantities involved in our objective function (11) are estimated in a similar fashion.

C CONNECTIONS TO VAE

In VAE, they assume a generative model specified by a stochastic decoder p(x|z), taking the continuous representation as an example, and seek an encoder q(z|x) as a variational approximation of the true posterior p(z|x). The model is fitted by maximizing the evidence lower bound (ELBO)
of the marginal likelihood,

Ex [log p(x)]  L(x, , ) = Eq(z|x) [log p(x|z)] - Ex [DKL (q(z|x)||r(z))] . Here the KL divergence term can be further decomposed as (Hoffman and Johnson, 2016),

(36)

Ex [DKL (q(z|x)||r(z))] = I(x; z) + Ex [DKL (q(z)||r(z))] .

(37)

That is, minimizing the KL divergence also penalizes the mutual information I(x; z), thus reduces the amount of information z has about x. This can make the inference task q(z|x) hard and lead to poor reconstructions of x as well. Many recent efforts have been focused on resolving this
problem by revising ELBO. Although approaches differ, it can be summarized as either dropping
the mutual information term in Eq (37), or encouraging statistical independence across the dimen-
sions of z by increasing the penalty on the total correlation term extracted from the KL divergence DKL (q(z)||r(z)) with respect to q(z). However, these approaches either result in an invalid lower bound for the VAE objective, or cannot avoid minimizing the mutual information I(x; z) between the representation and the data.

In contrast, IMAE starts with a stochastic encoder p(z|x) and aims at maximizing the mutual information between the data x and the representations z from the very beginning. By following
the constraints which are naturally implied by the objective in order to avoid degenerated solutions,

13

Under review as a conference paper at ICLR 2019
IMAE targets at both informative and statistical independent representations. On the other hand, in IMAE the decoder q(x|z) serves as a variational approximation to the true posterior p(x|z). As we will show in Section 4, being able to learn more interpretable representations allows IMAE to reconstruct and generate data with better quality.
D VAT STABILIZES THE LEARNING OF CATEGORICAL REPRESENTATIONS

Figure 5: Prevent over confidence predictions by encouraging local smoothness

E EXPERIMENTAL SETTINGS

Table 1: Encoder and Decoder architecture for MNIST and Fashion MNIST.

Encoder Input vectorized 28 × 28 grayscale image
FC. 500 BatchNorm ReLU FC. 2 × 500 BatchNorm ReLU FC. 20 (µz, log z) + 10 (py)

Decoder Input y  R10 and z  R10
FC. 500 ReLU
FC. 500 ReLU FC. 28 × 28 Sigmoid

We train all models using momentum, with initial learning rate being 1e-3. We decay the learning rate by 0.98 every epoch.

14


Under review as a conference paper at ICLR 2019
ISA-VAE: INDEPENDENT SUBSPACE ANALYSIS WITH VARIATIONAL AUTOENCODERS
Anonymous authors Paper under double-blind review
ABSTRACT
We provide a natural approach to encourage interpretable representations in variational autoencoders. Building on previous work on Independent Component Analysis (ICA) and Independent Subspace Analysis (ISA) we show that simply choosing an appropriate prior for the latent distribution breaks the degeneracy of the standard prior and leads to a interpretable laten representations. The proposed family of prior distributions leads to a factorized representation and its rotational asymmetry allows interpretability of individual latent dimensions. In our experiments we show that the commonly used higher weight (e.g. -VAE) of the Kullback-Leibler divergence term in the evidence lower bound (ELBO) amplifies two biases of variational inference: over-pruning and over-orthogonalization. Extensive quantitative experiments demonstrate the performance of the proposed approach for disentanglement in variational autoencoders.
1 INTRODUCTION
Recently there has been an increased interest in unsupervised learning of disentangled representations. The term disentangled usually describes two main objectives: First, a factorized latent representation, and second, interpretability of the latent factors. Schmidhuber (1992); Ridgeway (2016); Achille & Soatto (2017) Most of this recent work is inspired by the -VAE concept introduced in Higgins et al. (2016), which proposes to re-weight different parts of the evidence lower bound (ELBO) objective. In Higgins et al. (2016) a higher weight for the Kullback-Leibler divergence (KL) term is proposed, and putative mechanistic explanations for the effects of this modification are studied in Burgess et al. (2017); Chen et al. (2018). An alternative decomposition of the ELBO leads to the recent variant of -VAE called -TCVAE Chen et al. (2018), which shows the highest scores on recent disentanglement benchmarks.
These modifications of the evidence lower bound however lead to a tradeoff between disentanglement and reconstruction loss and therefore the quality of the learned model. This tradeoff is direcly encoded in the modified objective: by increasing the -weight of the KL-term, the relative weight of the reconstruction loss term is more and more decreased. Therefore, optimization of the modified ELBO will lead to latent encodings which have a lower KL-divergence from the prior, but at the same time lead to a higher reconstruction loss. Furthermore, we discuss in section 2.4 that using a higher weight for the KL-term amplifies existing biases of variational inference.
There is a foundational contradiction in many approaches to disentangling deep generative models (DGMs): the standard model employed is not identifiable as it employs a standard normal prior which then undergoes a linear transformation. Any rotation of the latent space can be absorbed into the linear transform and is therefore statistically indistinguishable. If interpretability is desired, the modelling choices are setting us up to fail.
We make the following contributions:
· We show that the current state of the art approaches employ a tradeoff between reconstruction loss and disentanglement of the latent representation.
· In section 2.3 we show that variational inference techniques are biased towards estimating an over-orthogonal components and towards over-pruning the variance of the latent distribution.
1

Under review as a conference paper at ICLR 2019

· We provide a novel description of the origin of disentanglement in -VAE and demonstrate in section 2.4 that increasing the weight of the KL term increases the over-pruning bias of variational inference.
· To mitigate these drawbacks of existing approaches, we propose a family of rotationally asymmetric distributions for the latent prior, which prevents the ambiguity of the latent encoding for arbitrary rotations of the latent space. This approach resembles independent component analysis (ICA) for variational autoencoders.
· We propose to use a prior which allows a decomposition of the latent space using independent subspace analysis (ISA) and demonstrate that this prior leads to disentangled representations even for the unmodified ELBO objective. This removes the tradeoff between disentanglement and reconstruction loss of existing approaches.
· An even higher disentanglement of the latent space can be achieved by incorporating the proposed prior distribution into the existing approaches -VAE and -TCVAE, thus exploiting their orthogonalization bias. Because the prior distribution already favours a disentangled representation, a high disentanglement score with at the same time low reconstruction loss can be achieved.

2 BACKGROUND

We briefly discuss previous work on variational inference in deep generative models and two modifications of the learning objective that have been proposed to learn a disentangled representation. We discuss characteristic biases of variational inference and how the modifications of the learning objective actually accentuate these biases.

2.1 DISENTANGLED REPRESENTATION LEARNING

Variational Autoencoder Kingma & Welling (2014) introduce a latent variable model that combines a generative model, the decoder, with an inference network, the encoder. Training is performed by optimizing the evidence lower bound (ELBO) averaged over the empirical distribution:

LELBO = Eq(z|x) [log p(x|z, )] - DKL(q(z|x) p(z)) ,

(1)

where the decoder p(x|z) and encoder q(z|x) are deep learning models with parameters  and each zl is sampled from zl  q(z|x). When choosing appropriate families of distributions, gradients through the samples zl can be estimated using the reparameterization trick. The approximate posterior q(z|x) is usually modelled as a multivariate Gaussian with diagonal covariance matrix and the prior p(z) is typically the standard normal distribution.

-VAE Higgins et al. (2016) propose to modify the evidence lower bound objective and penalize the KL-divergence of the ELBO:

L-ELBO = Eq(z|x) [log p(x|z, )] - DKL(q(z|x) p(z)) ,

(2)

where  > 1 is a free parameter that should encourage a disentangled representation. In Burgess et al. (2017) the authors provide further thoughts on the mechanism that leads to these disentangled representations. However we will show in the following that this parameter introduces a trade-off between reconstruction loss and disentanglement. Furthermore, we show in section 2.4 that this parameter amplifies biases of variational inference towards orthogonalization and pruning.

-TCVAE In Chen et al. (2018) the authors propose an alternative decomposition of the ELBO, tjat leads to the recent variant of -VAE called -TCVAE. They demonstrate that -TCVAE allows to learn representations with higher MIG score than -VAEHiggins et al. (2016), InfoGAN Chen et al. (2016) and FactorVAE Kim & Mnih (2018). The authors propose to decompose the KL-term in the ELBO objective into three parts and to weight them independently:
Eq(x) [DKL(q(z|x) p(z))] = DKL(q(z|x) q(z)p(x)) + DKL(q(z) q(zj )) + DKL(q(zj ) p(zj )) . (3)
j
The first term is the index-code mutual information, the second term is the total correlation and the third term the dimension-wise KL-divergence. Because the index-code mutual information can be

2

Under review as a conference paper at ICLR 2019

viewed as an estimator for the mutual information between p(x) and q(z), the authors propose to exclude this term when reweighting the KL-term with the  weight. In addition to the improved objective, the authors propose a quantitative evaluation score for disentanglement, the mutual information gap (MIG). They propose to first estimate the mutual information between a latent factor and an underlying generative factor of the dataset. The mutual information gap is then defined as the difference of the mutual information between the highest and second highest correlated underlying factor.

2.2 RELATED WORK
Recent work has shown an increased interest into learning of interpretable representations. In addition to the related work mentioned already, we briefly review some of the influential papers: Chen et al. (2016) present a variant of a GAN that encourages an interpretable latent representation by maximizing the mutual information between the observation and a small subset of latent variables. The approach relies on optimizing a lower bound of the intractable mutual information. Kim & Mnih (2018) propose a learning objective equivalent to -TCVAE, and train it with the density ration trick Sugiyama et al. (2012). Kumar et al. (2017) introduce a regularizer of the KL-divergence between the approximate posterior and the prior distribution. A parallel line of research proposes not to train a perfect generative model but instead to find a simpler representation of the data (Vedantam et al. (2017); Hinton et al. (2011b)). A similar strategy is followed in semi-supervised approaches that require implicit or explicit knowledge about the true underlying factors of the data Kulkarni et al. (2015); Kingma et al. (2014); Reed et al. (2014); Baydin et al. (2017); Hinton et al. (2011a); Zhu et al. (2017); Goroshin et al. (2015); Hsu et al. (2017); Denton et al. (2017)).

2.3 ORTHOGONALIZATION AND PRUNING IN VARIATIONAL INFERENCE

There have been several interpretations of the behaviour of the -VAE (Chen et al., 2018; Burgess et al., 2017). Here we provide a complementary perspective: that it enhances well known statistical biases in VI (Turner & Sahani, 2011) to produce disentangled, but not necessarily useful, representations. The form of these biases can be understood by considering the variational objective when written as an explicit lower-bound: the log-likelihood of the parameters minus the KL divergence between the approximate posterior and the true posterior

LELBO = Eq(z|x) [log p(x|z, )] - DKL(q(z|x) p(z))

(4)

From this form it is clear that VI's estimates of the parameters  will be biased away from the maximum likelihood solution (the maximizer of the first term) in a direction that reduces the KL between the approximate and true posteriors. When factorized approximating distributions are used, VI will therefore be biased towards settings of the parameters that reduce the dependence of the latent variables in the posterior. For example, this will bias learned components towards orthogonal directions in the output space as this reduces explaining away (e.g. in the factor analysis model, VI breaks the degeneracy of the maximum-likelihood solution finding the orthogonal PCA directions, see appendix). Moreover, these biases often cause components to be pruned out (in the sense that they have no effect on the observed variables) since then their posterior sits at the prior, which is typically factorized (e.g. in an over-complete factor analysis model VI prunes out components to return a complete model, see appendix). For simple linear models these effects are not pathological: indeed VI is arguably selecting from amongst the degenerate maximum likelihood solutions in a sensible way. However, for more complex models the biases are more severe: often the true posterior of the underlying model has significant dependencies (e.g. due to explaining away) and the biases can prevent the discovery of some components. For example, VAEs are known to over-prune (Burda et al., 2015; Cremer et al., 2018).

2.4 -VAE EMPHASIZES ORTHOGONALIZATION AND PRUNING

What happens to these biases in the -VAE generalization when  > 1?

L-ELBO = Eq(z|x) [log p(x|z, )] - DKL(q(z|x) p(z))

(5)

The short answer is that they grow. This can be understood by considering coordinate ascent of the modified objective. With  fixed, optimising q finds a solution that is closer to the prior distribution

3

Under review as a conference paper at ICLR 2019

p
v
p
z v
z z
(a) Tree corresponding to eq. 10

p
v
p p p p
v v v v
z zn (b) Tree visualization of a Lp-nested ISA model.

than VI due to the upweighting of the KL term in 4. With q fixed, optimization over  returns the same solution as VI (since the prior does not depend on the parameters  and so the value of  is irrelevant). However, since q is now closer to the prior than before, the KL bias in 5 will be greater. These effects are shown in the ICA example below [or in the appendix]. VI (beta = 1) learns components that are more orthogonal than the underlying ones, but  = 5 prunes out one component entirely and sets the other two to be orthogonal. This is disentangled, but arguably leads to incorrect interpretation of the data. This happens even though both methods are initialised at the true model. Arguably, the -VAE is enhancing a statistical bug in VI and leveraging this as a feature. We believe that this can be dangerous, preventing the discovery of the underlying model.

3 LATENT PRIOR DISTRIBUTIONS FOR UNSUPERVISED FACTORIZATION

In this section we describe an approach for unsupervised learning of disentangled representations. Instead of modifying the ELBO-objective, we propose to use certain families of prior distributions p(z), that lead to identifiable and interpretable models. In contrast to the standard normal distribution, the proposed priors are not rotationally invariant, and therefore allow interpretability of the latent space.

3.1 INDEPENDENT COMPONENT ANALYSIS
Independent Component Analysis (ICA) seeks to factorize a distribution into non-Gaussian factors. In order avoid the ambiguities of latent space rotations, a non-Gaussian distribution (e.g. Laplace or Student-t distribution) is used as prior for the latent variables.

Generalized Gaussian Distribution A generalized version of ICALee & Lewicki (2000); Zhang

et al. (2004); Lewicki (2002); Sinz & Bethge (2010) uses a prior from the family of exponential

power distributions of the form

p(z)  exp - ||z||pp

(6)

also called generalized Gaussian, generalized Laplacian or p-generalized normal distribution. Using p = 2/(1 + ) the parameter  is a measure of kurtosis.Box & Tiao (1973) This family of distributions generalizes the normal ( = 0) and the Laplacian ( = 1) distribution. In general we get for  > 0 leptokurtic and for  < 0 platykurtic distributions.

Obviously the normal distribution is a special instance of the class of Lp-spherically symmetric distributions, and the normal distribution is the only L2-spherically symmetric distribution with
independent marginals. Equivalently Sinz et al. (2009a) showed that this also generalizes to arbitrary
values of p. The marginals of the p-generalized normal distribution are independent, and it is the only factorial model in the class of Lp-spherically symmetric distributions.

We investigate the behaviour of Lp-spherically symmetric distributions as prior distributions for p(z) in the experiments in section 4.

3.2 INDEPENDENT SUBSPACE ANALYSIS
ICA can be generalized to include indepdence between subspaces, but dependencies within them, by using a more general prior, the family of Lp-nested symmetric distributions. Hyva¨rinen & Hoyer (2000); Hyva¨rinen & Ko¨ster (2007); Sinz et al. (2009b); Sinz & Bethge (2010)

4

Under review as a conference paper at ICLR 2019

(a) p = (b) p = 1.0 2.4
Figure 2: Leptokurtic and platykurtic priors encourage different orientations of the encoding of the X and Y coordinate in the dSprites dataset. A leptokurtic distribution (here the Laplace distribution) expects most of the probability mass around 0. Because the X and Y coordinates in dSprites are distributed in a square, the projection of the positions of the samples onto the diagonal fit better to the Laplace prior. A platykurtic distribution however fits better to the uniform distribution of x and y along the axes.

To start, let's consider functions of the form

f (z) =

|z1|p0

+

(|z2|p1

+

|z3

|p1

)

p0 p1

1
,p0

(7)

with p0, p1  R. This function is a cascade of two Lp-norms. To give a better intuition we provide a
visualization of this distribution in figure 1a, which depicts equation 10 as a tree that visualizes the nested structure of the norms. We call the class of functions which employ this structure Lp-nested.

Lp-nested Distribution Given a Lp-nested function f and a radial density  : R  R+ we define the Lp-nested symmetric distribution following Fernandez et al. (1995) as

(f (x)) (z) = f (x)n-1Sf (1) ,

(8)

where Sf (1) is the surface area of the Lp-nested sphere. This surface area can be obtained by using the gamma function:

Sf (R) = Rn-12n
iI

l1
k=1

ni,k pi

plii-1

ni pi

,

(9)

where li is the number of children of a node i, ni is the number of leaves in a subtree under the node i, and ni,k is the number of leaves in the subtree of the k-th children of node i. For further details we refer the reader to the excellent work of Sinz & Bethge (2010).

Independent Subspace Analysis The family of Lp-nested distibutions allow a generalization of ICA called independent subspace analysis (ISA). ISA uses a subclass of Lp-distributions, which are
defined by functions of the form

 f (z) = 


n1
|zi|p1
i=1

p0
p1 n

p0  pl

1 p0

+···+

|zi|pl 

 

,

i=n1 +···+nl-1 +1

(10)

and correspond to trees of depth two. The tree structure of this subclass of functions is visualizaed in
figure 1b. Sinz & Bethge (2010) showed that the subspaces v1, . . . , vl0 become independent when using the radial distribution

0(v0) =

p0v0n-1



n p0

n
s p0

exp

- v0p0 s

(11)

which we can interpret as a generalization of the Chi-distribution.

5

Under review as a conference paper at ICLR 2019
Figure 3: Comparison of the proposed ISA-VAE model with -VAE and -TCVAE. Evaluation is performed with respect to the quality of the disentanglement (MIG score, solid line) and reconstruction quality (dashed line) for different values of . The proposed models ISA-VAE and ISA-TCVAE reach a disentangled representation already for small values of  which allows better reconstructions due to the trade-off between the -weight of the KL-term and the reconstruction loss in the modified ELBO. Shaded regions depict 90% confidence intervals. Evaluated on the dSprites dataset with n = 16 for each value of . Lp-layout: p0 = 2.1, p1 = 2.2, l0 = 5, l1 = 4.
We propose to choose the latent prior p(z) from this family of distributions, which allows us to define independent subspaces in the latent space. Experiments in the following section demonstrate that priors of this form support unsupervised learning of disentangled representation even for the unmodified ELBO objective ( = 1).
4 EXPERIMENTS
In our experiments, we evaluate the influence of the proposed prior distribution on disentanglement and on the quality of the reconstruction. To provide a quantitative evaluation of the disentanglement we compute the disentanglement metric Mutual Information Gap (MIG) that was proposed in Chen et al. (2018). In this publication the authors demonstrate that -TCVAE, a modification of -VAE, enables learning of representations with higher MIG score than -VAEHiggins et al. (2016), InfoGAN Chen et al. (2016) and FactorVAE Kim & Mnih (2018). Therefore we choose -TCVAE and -VAE for comparison in our experiments.
4.1 SUPPORT OF THE PRIOR TO LEARN DISENTANGLED REPRESENTATIONS First, we investigate the ability of the prior to support unsupervised learning of disentangled representations for the unmodified ELBO-objective. We compare two DGMs after training with the unmodified ELBO-objective: the standard VAE with standard normal prior p(z), and ISA-VAE, the combination of the standard VAE with a prior p(z) from the family of Lp-nested ISA distributions. The combination of the Lp-nested ISA prior with the standard VAE approach learns reasonably disentangled representations even for the unmodified ELBO objective( = 1). Figure 4a depicts the structure of the latent representation for ISA-VAE after training. Even for the unmodified objective the latent space becomes interpretable, which we did not observe in our experiments for the standard VAE with standard normal prior.
4.2 QUANTITATIVE COMPARISONS OF THE DIFFERENT APPROACHES The proposed Lp-nested prior distribution can be easily combined with the existing approaches for unsupervised disentangled representation learning -VAE Higgins et al. (2016) and -TCVAE Chen et al. (2018). We compare the performance of the four different approaches in figure 5: -VAE and ISA--VAE, a combination of -VAE with the proposed Lp-nested ISA prior, and equivalently TCVAE and ISA--TCVAE. Clearly both approaches benefit from the proposed prior, and higher disentanglement scores can be achieved with ISA--VAE and ISA--TCVAE in comparison to the original approaches.
6

Under review as a conference paper at ICLR 2019

(a)  = 1, MIG: 0.29

(b)  = 2.5, MIG: 0.54

Figure 4: Disentangled representations with high MIG-scores that we observed in our experiments

for ISA-VAE and -ISA-TCVAE on the dSprites dataset. The ISA-VAE model is able to find a

disentangled representation even without a modification of the ELBO ( = 1.0). When combining

the ISA-model with -TCVAE, a model with a high disentanglement score of MIG = 0.54 can be

reached. This is the so far highest score reported for dSprites in the literature. a) ISA-VAE with

p0

=

2.1,

p1

=

4 3

,

l0

=

l1

=

5,

MIG-score:

0.29 b) -ISA-TCVAE for 

=

2.5 and p0

=

2.1,

p1 = 2.2, l0 = 5, l1 = 4, MIG-score: 0.54

Figure 5: Left: Comparison of the MIG score of the proposed ISA-VAE model with -VAE and -TCVAE for the individually best values of . Right: The proposed models ISA-VAE and ISATCVAE reach a disentangled representation already for small values of  which allows better reconstructions due to the trade-off between the -weight of the KL-term and the reconstruction loss in the modified ELBO. Lp-layout of the ISA model: p0 = 2.1, p1 = 2.2, l0 = 5, l1 = 4.
7

Under review as a conference paper at ICLR 2019

(a) ISA-VAE

(b) ISA-TCVAE

Figure 6: Disentanglement score (MIG) with respect to the reconstruction quality (logpx) for different layouts for the ISA subspace and different values of   [1, 4]. The number of subspaces l0 = 5 is constant throughout all experiments. The parameters p0, p1 and l0 are varied and for each configuration the four highest MIG scores for different values of  are shown. For this dataset, the confguration p0 = 2.1, p1 = 2.2 and l0 = 4 (denoted in black) is most appropriate as it achieves high MIG scores while maintaining a good reconstruction quality, both for the ISA-TCVAE and the
ISA-VAE model.

4.3 TRADE-OFF BETWEEN DISENTANGLEMENT AND RECONSTRUCTION LOSS
Since the prior facilitates learning of disentangled representations, not only a higher disentanglement score can be reached, but also higher scores are reached for smaller values of , when compared to the original approaches. This leads to a clear improvement of the trade-off between disentanglement and reconstruction loss. The improvement of this trade-off is demonstrated in figure 5, where we plot both the disentanglement score and the reconstruction loss for varying values of . ISA-VAE and ISA--TCVAE reach high values of the disentanglement score for smaller values of  which and at the same time preserves a higher quality of the reconstruction than the respective original approaches. At the same time we observe that with the proposed prior the quality of the reconstruction decreases at a smaller rate than for the original approaches.
5 CONCLUSION
We presented a structured prior for unsupervised learning of disentangled representations in deep generative models. We choose the prior from the family of Lp-nested symmetric distributions which enables defining of a hierarchy of independent subspaces in the latent space. In contrast to the standard normal prior that is often used in training of deep generative models the proposed prior is not rotationally invariant and therefore enhances the interpretability of the latent space. We demonstrate in our experiments, that a combination of the proposed prior with existing approaches for unsupervised learning of disentangled representations allows a significant improvement of the trade-off between disentanglement and reconstruction loss.
REFERENCES
Alessandro Achille and Stefano Soatto. Emergence of invariance and disentangling in deep representations. arXiv preprint arXiv:1706.01350, 2017.
Atilim Gunes Baydin, Barak A Pearlmutter, Alexey Andreyevich Radul, and Jeffrey Mark Siskind. Automatic differentiation in machine learning: a survey. Journal of machine learning research, 18(153):1­153, 2017.
8

Under review as a conference paper at ICLR 2019
George EP Box and George C Tiao. Bayesian Inference in Statistical Analysis. Addison-Wesley, Reading,Mass., 1973.
Yuri Burda, Roger Grosse, and Ruslan Salakhutdinov. Importance weighted autoencoders. arXiv preprint arXiv:1509.00519, 2015.
Christopher P Burgess, Irina Higgins, Arka Pal, Loic Matthey, Nick Watters, Guillaume Desjardins, and Alexander Lerchner. Understanding disentangling in -vae. In Learning Disentangled Representations: From Perception to Control Workshop, 2017.
Tian Qi Chen, Xuechen Li, Roger Grosse, and David Duvenaud. Isolating sources of disentanglement in variational autoencoders. arXiv preprint arXiv:1802.04942, 2018.
Xi Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya Sutskever, and Pieter Abbeel. Infogan: Interpretable representation learning by information maximizing generative adversarial nets. In Advances in neural information processing systems, pp. 2172­2180, 2016.
Chris Cremer, Xuechen Li, and David Duvenaud. Inference suboptimality in variational autoencoders. arXiv preprint arXiv:1801.03558, 2018.
Emily L Denton et al. Unsupervised learning of disentangled representations from video. In Advances in Neural Information Processing Systems, pp. 4414­4423, 2017.
Carmen Fernandez, Jacek Osiewalski, and Mark FJ Steel. Modeling and inference with -spherical distributions. Journal of the American Statistical Association, 90(432):1331­1340, 1995.
Ross Goroshin, Joan Bruna, Jonathan Tompson, David Eigen, and Yann LeCun. Unsupervised learning of spatiotemporally coherent metrics. In Proceedings of the IEEE international conference on computer vision, pp. 4086­4093, 2015.
Irina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick, Shakir Mohamed, and Alexander Lerchner. -vae: Learning basic visual concepts with a constrained variational framework. 2016.
Geoffrey E Hinton, Alex Krizhevsky, and Sida D Wang. Transforming auto-encoders. In International Conference on Artificial Neural Networks, pp. 44­51. Springer, 2011a.
Geoffrey E Hinton, Alex Krizhevsky, and Sida D Wang. Transforming auto-encoders. In International Conference on Artificial Neural Networks, pp. 44­51. Springer, 2011b.
Wei-Ning Hsu, Yu Zhang, and James Glass. Unsupervised learning of disentangled and interpretable representations from sequential data. In Advances in neural information processing systems, pp. 1878­1889, 2017.
Aapo Hyva¨rinen and Patrik Hoyer. Emergence of phase-and shift-invariant features by decomposition of natural images into independent feature subspaces. Neural computation, 12(7):1705­1720, 2000.
Aapo Hyva¨rinen and Urs Ko¨ster. Complex cell pooling and the statistics of natural images. Network: Computation in Neural Systems, 18(2):81­100, 2007.
Hyunjik Kim and Andriy Mnih. Disentangling by factorising. arXiv preprint arXiv:1802.05983, 2018.
Diederik P Kingma and Max Welling. Auto-encoding variational bayes. In International Conference on Learning Representations, 2014. URL https://openreview.net/forum?id= 33X9fd2-9FyZd.
Diederik P Kingma, Shakir Mohamed, Danilo Jimenez Rezende, and Max Welling. Semi-supervised learning with deep generative models. In Advances in Neural Information Processing Systems, pp. 3581­3589, 2014.
9

Under review as a conference paper at ICLR 2019
Tejas D Kulkarni, William F. Whitney, Pushmeet Kohli, and Josh Tenenbaum. Deep convolutional inverse graphics network. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett (eds.), Advances in Neural Information Processing Systems 28, pp. 2539­2547. Curran Associates, Inc., 2015. URL http://papers.nips.cc/paper/ 5851-deep-convolutional-inverse-graphics-network.pdf.
Abhishek Kumar, Prasanna Sattigeri, and Avinash Balakrishnan. Variational inference of disentangled latent concepts from unlabeled observations. arXiv preprint arXiv:1711.00848, 2017.
Te-won Lee and Michael Lewicki. The generalized gaussian mixture model using ica. 08 2000. Michael S Lewicki. Efficient coding of natural sounds. Nature neuroscience, 5(4):356, 2002. Scott Reed, Kihyuk Sohn, Yuting Zhang, and Honglak Lee. Learning to disentangle factors of
variation with manifold interaction. In International Conference on Machine Learning, pp. 1431­ 1439, 2014. Karl Ridgeway. A survey of inductive biases for factorial representation-learning. arXiv preprint arXiv:1612.05299, 2016. Ju¨rgen Schmidhuber. Learning factorial codes by predictability minimization. Neural Computation, 4(6):863­879, 1992. Fabian Sinz and Matthias Bethge. Lp-nested symmetric distributions. Journal of Machine Learning Research, 11(Dec):3409­3451, 2010. Fabian Sinz, Sebastian Gerwinn, and Matthias Bethge. Characterization of the p-generalized normal distribution. Journal of Multivariate Analysis, 100(5):817­820, 2009a. Fabian H Sinz, Eero P Simoncelli, and Matthias Bethge. Hierarchical modeling of local image features through Lp-nested symmetric distributions. In Advances in neural information processing systems, pp. 1696­1704, 2009b. Masashi Sugiyama, Taiji Suzuki, and Takafumi Kanamori. Density ratio estimation in machine learning. Cambridge University Press, 2012. R. E. Turner and M. Sahani. Two problems with variational expectation maximisation for time-series models. In D. Barber, T. Cemgil, and S. Chiappa (eds.), Bayesian Time series models, chapter 5, pp. 109­130. Cambridge University Press, 2011. Ramakrishna Vedantam, Ian Fischer, Jonathan Huang, and Kevin Murphy. Generative models of visually grounded imagination. arXiv preprint arXiv:1705.10762, 2017. Liqing Zhang, Andrzej Cichocki, and Shun-ichi Amari. Self-adaptive blind source separation based on activation functions adaptation. IEEE Transactions on Neural Networks, 15(2):233­244, 2004. Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A Efros. Unpaired image-to-image translation using cycle-consistent adversarial networks. arXiv preprint, 2017.
10

Supplementary Material

1 Toy examples showing biases in VI and -VI
This section provides the details of the toy examples that reveal the biases in variational methods. First we will consider the factor analysis model showing that VI breaks the degeneracy of the maximum-likelihood solution to 1) discover orthogonal weights that lie in the PCA directions, 2) prune out extra components in over-complete factor analysis models, even though there are solutions with the same likelihood that preserve all components. We also show that in these examples the -VI returns identical model fits to VI regardless of the setting of . Second, we consider an over-complete ICA model and initialize using the true model. We show that 1) VI is biased away from the true component directions towards more orthogonal directions, and 2) -VI with a modest setting of  = 5 prunes away one of the components and finds orthogonal directions for the other two. That is, it finds a disentangled representation, but one which does not reflect the underlying components.

1.1 Background

The -VAE optimizes the modified free-energy, F(q(z1:N ), ), with respect to the parameters  and the variational approximation q(z1:N ),

F(q(z1:N ), ) = Eq(z1:N )(log p(x1:N |z1:N , )) - KL(q(z1:N )||p(z1:N )).

Consider

the

case

where

M

=

1 

is

a

positive

integer,

M

 N,

we

then

have

N

F(q(z1:N ), ) =

Eq(zn)(M () log p(xn|zn, )) - KL(q(zn)||p(zn))

n=1

In this case, the -VAE can be thought of as attaching M replicated observations to each latent variable zn and then running standard variational inference on the new replicated dataset. This can equivalently be thought of as raising each likelihood p(xn|zn, ) to the power M . Now in real applications  will be set to a value that is greater than one. In this case, the effect of  is the opposite: it is to reduce the number of effective data points per latent variable to be less than one M < 1. Or equivalently we

1

raise each likelihood term to a power M that is less than one. Standard VI is then run on these modified data (e.g. via joint optimization of q and ). Although this view is mathematically straightforward, the perspective of the -VAE i) modifying the dataset, and ii) applying standard VI, is useful as it will allow us to derive optimal solutions for the variational distribution q(z) in simple cases like the factor analysis model considered next.

1.2 Factor analysis
Consider the factor analysis generative model. Let x  RL and z  RK .

for n = 1...N zn  N (z; 0, I), xn  N (x; W z, D) where D = [12, ..., D2 ]
The true posterior is a Gaussian p(zn|xn, ) = N (z; µz|x, z|x) where µz|x = z|xW D-1x and z|x = (W D-1W + I)-1.
The true log-likelihood of the parameters is

(1) (2)

N

log p(x1:N |) = log N (xn, 0, W W + D)

n=1

= - N log det(2(W W 2

1N

+ D)) - 2

xn (W W

+ D)-1xn

n=1

=

-

1 N

log det(2(W W

2

+ D)) + trace((W W

+ D)-1(µxµx + x))

Here we have defined the empirical mean and covariance of the observations

µx

=

1 N

N n=1

xn

and

x

=

1 N

Nn=1(xn - µx)(xn - µx) i.e. the sufficient

statistics.

The true likelihood is invariant under orthogonal transformations of the latent

variables: z = Rz where RR = I.

Interpreting -VI as running VI in a modified generative model (see previous

section) we have the new generative process

for n = 1...N zn  N (z; 0, I), for m = 1...M () xn,m  N (x; W z, D) where D = [12, ..., D2 ]
We now observe data and set xn,m = xn. The posterior is again Gaussian p(zn|xn, , M ()) = N (z; µ~z|x(), ~ z|x()) where
µ~z|x() = ~ -z|1x()M ()W D-1x and ~ z|x() = (M ()W D-1W + I)-1

2

Here we have taken care to explicitly reveal all of the direct dependencies on .
Mean-field variational inference, q(zn) = k qn,k(zk,d), will return a diagonal Gaussian approximation to the true posterior with the same mean and matching
diagonal precision,

q(zn|xn, , M ()) = N z; µ~z|x(), q() where q-1() = diag ~ -z|1x()

We notice that the posterior mean is a linear combination of the observations µ~z|x() = R()x where R() = ~ z|x()M ()W D-1 are recognition weights. Notice that the recognition weights and the posterior variances are the same for
all data points: they do not depend on n. The free-energy is then

F (q, , ) = Eq(z)(log p(x|z)) - KL(q(z)|p(z))

with the reconstruction term being

Eq(z)(log

p(x|z))

=

-

1 2

N

xn (D-1 - 2R

W

D-1 + R

W

D-1W R)xn

n=1

- N log det(2D) - N trace(W 2 2

D-1 q )

=-N 2

trace (D-1 - 2R W

D-1 + R W

D-1W R)(x + µxµx )

+ log det(2D) + trace(W D-1W q)

(3)

and the KL or regularization term being

KL(q(z)|p(z))

=

-NK 2

-

N 2

log det(q) +

N 2

trace(q )

+

1 2

N

xn R

Rxn

n=1

N

=- 2

K + log det(q) - trace(q) - trace(R R(x + µxµx )) .

We will now consider the objective functions and the posterior distributions in several cases to reason about the parameter estimates arising from the methods above.

1.3 Experiment 1: mean field VI applied to factor analysis yields the PCA directions
Consider the situation where we know a maximum likelihood solution of the weights WML. For simplicity we select the solution WML which has orthogonal weights in the observation space. We then rotate this solution by an amount  so that WML = R()WML. The resulting weights are no longer orthogonal (assuming the rotation is not an integer multiple of /2). We compute the loglikelihood (which will not change) and the free-energy (which will change) and

3

plot the true and approximate posterior covariance (which does not depend on

the datapoint value xn). First here are the weights are aligned with the true ones. The log-likelihood

and the free-energy is -17.82 nats.

weight rotation
0.8 data covariance original weight
0.6 rotated weight

posterior
1.00 true variational approx.
0.75

0.4 0.50

0.2 0.25

0.0 0.00

0.2 0.25

0.4 0.50

0.6 0.75

0.8 0.8 0.6 0.4 0.2 0x.10 0.2 0.4 0.6 0.8

1.00 1.00 0.75 0.50 0.25 0z.010 0.25 0.50 0.75 1.00

x2 x2 z2 z2

Second, here are the weights rotated /4 and the log-likelihood is -17.82 nats

and the free-energy -57.16 nats.

weight rotation
0.8 data covariance original weight
0.6 rotated weight

posterior
true 0.6 variational approx.

0.4 0.4

0.2 0.2

0.0 0.0

0.2 0.2

0.4 0.4

0.6 0.6

0.8 0.8 0.6 0.4 0.2 0x.10 0.2 0.4 0.6 0.8

0.6 0.4 0.2 0z.10 0.2 0.4 0.6

When varying  the plots above indicate that orthogonal settings of the weights ( = m/2 where m = 0, 1, 2, ...) lead to factorized posteriors. In these cases the KL between the approximate posterior and the true posterior is zero and the free-energy is equal to the log-likelihood. This will be the optimal free-energy for any weight setting (due to the fact that it is equal to the true log-likelihood which is maximal, and the free-energy is a lower bound of this quantity.) For intermediate values of  the posterior is correlated and the free-energy is not tight to the log likelihood. Now let's plot the free-energy and the log-likelihood as  is varied. This shows that the free-energy prefers orthogonal settings of the weights as this leads to factorized posteriors, even though the log-likelihood is insensitive to . So, variational inference recovers the same weight directions as the PCA solution.

4

evidence

43.36 43.37 43.38 43.39 43.40 43.41 43.42 43.43
0

dependence of free-energy on weight rotation

ELBO log-likelihood

/2 3 /2 2

The above shows that the bias inherent in variational methods will cause them to break the symmetry in the log-likelihood and find orthorgonal latent components. This occurs because orthoginal components result in posterior distributions that are factorized. These are then well-modelled by the variational approximation and result in a small KL between the approximate and true posteriors.

1.4 Experiment 2: mean field VI applied to over-complete factor analysis prunes out the additional latent dimensions
A similar effect occurs if we model 2D data with a 3D latent space. Many settings of the weights attain the maximum of the likelihood, including solutions which use all three latent variables. However, the optimal solution for VI is to retain two orthogonal components and to set the magnitude of the third component to zero. This solution a) has weights maximise the likelihood, and b) has a factorised posterior distribution (the pruned component having a posterior equal to its prior) that therefore incurs no cost KL(q(z)||p(z|x, )) = 0. In this way the bound becomes tight. Here's an example of this effect. We consider a model of the form:

x =  2

1 1

z1

+

 2

1 1

z2

+

 2

1 -1

z3 +

(4)

We set 2 + 2 = 1 so that all models imply the same covariance and set this to be the maximum likelihood covariance by construction. We then consider varying  from 0 to 1/2. The setting equal to 0 attains the maximum of the free-energy, even though it has the same likelihood as any other setting.

5

evidence

15 20 25 30 35
0.0

dependence of free-energy on weight magnitude

ELBO log-likelihood

0.1 0.2 0.3 0.4 magnitude of additional component

0.5

1.5 Experiment 3: The -VAE also yields the PCA components, changing  has no effect on the direction of the estimated components in the FA model

How does the setting of  change things? Here we rerun experiment 1 for

different values of .

dependence of free-energy on beta

log-likelihood

43.4

ELBO beta = 1 ELBO, beta = 1.5

43.5 ELBO, beta = 2

43.6

evidence

43.7

43.8

43.9

44.0 0

/2

3 /2 2

In this example, changing  in this example just reduces the amplitude of the fluctuations in the free-energy, but it does not change the directions found. A similar observation applies to the pruning experiment. Increasing  will increase the uncertainty in the posterior as it is like reducing the number of observations (or increasing the observation noise, from the perspective of q).

6

1.6 Summary of Factor Analysis Experiments
The behaviours introduced by the -VAE appear relatively benign, and perhaps even helpful, in the linear case: VI is breaking the degeneracy of the maximum likelihood solution in a sensible way: selecting amongst the maximum likelihood solutions to find those that have orthogonal components and removing spurious latent dimensions. This should be tempered by the fact that the  generalization recovered precisely the same solutions and so it was necessary to obtain the desired behaviour in the PCA case. Similar effects will occur in deep generative models, not least since these typically also have a Gaussian prior over latent variables, and these latents are initially linearly transformed, thereby resulting in a similar degeneracy to factor analysis. However, the behaviours above benefited from the fact that maximum-likelihood solutions could be found in which the posterior distribution over latent variables factorized. In real world examples, for example in deep generative models, this will not be case. In such cases, these same effects will cause the variational freeenergy and its -generalization to **bias the estimated parameters far away from maximum-likelihood settings, toward those settings that imply factorized Gaussian posteriors over the latent variables**.
1.7 Independent Component Analysis
We now apply VI and the  free-energy method to ICA. We're interested the properties of the variational objective and the -VI objective and so we 1. fit the data using the true generative model to investigate the biases in VI and -VI 2. do not use amortized inference, just optimizing the approximating distributions for each data point (this is possible for these small examples) The linear independent component analysis generative model we use is defined as follows. Let x  RL and z  RK .
for n = 1...N for k = 1...K zn,k  Student-t(0, , v), xn  N (W z, D) where D = [12, ..., D2 ]
We apply mean-field variational inference, q(zn) = k qn,k(zk,d), and use Gaussian distributions for each factor qn,k(zn,k) = N (zn,k; µn,k, n2,k). The free-energy is computed as follows: The reconstruction term is identical to PCA: an avergage of a quadratic form wrt to a Gaussian, which is analytic. The KL is broken down into the differential entropy of q which is also analytic and the cross-entropy with the prior which we evaluate by numerical integration (finite differences). There is a cross-entropy term for each latent variable which is one reason why the code is slow (requiring N 1D numerical integrations). The
7

gradient of the free-energy wrt the parameters W and the means and variances of the Gaussian q distributions are computed using autograd. In order to be as certain as possible that we are finding a global maximum of the free-energies, all experiments initialise at the true value of the parameters and then ensure that each gradient step improves the free-energy. Stochastic optimization or a procedure that accepted all steps regardless of the change in the objective would be faster, but they might also move us into the basis of attraction of a worse (local) optima.
1.8 Experiment 1: Learning in over-complete ICA
Now we define the dataset. We use a very sparse Student's t-distribtion with v = 3.5. For v < 4 the the kurtosis is undefined so the model is fairly simple to estimate model (it's a long way away from the degenerate factor analysis case which is recovered in the limit v  ). We use three latent components and a two dimensional observed space. The directions of the three weights are shown in blue below with data as blue circles.
10
5
0
5
10 10 5 0x1 5 10
First we run variational inference finding components (shown in red below) which are more orthogonal than the true directions. This bias is in this directions as this reduces the dependencies (explaining away) in the underlying posterior.
8

x2

x2 x2

10

5

0

5

10

10 5 0

5 10

x1

Second we run -VI with  = 5. Two components are now found that are orthogonal with one component pruned from the solution.

20

10

0

10

20 20 15 10 5 0 5 10 15 20 x1
In this case the bias is so great that the true component directions are not discovered. Instead the components are forced into the orthogonal setting regardless of the structure in the data. Summary of ICA experiment
9

The ICA example illustrates that this approach ­ of relying on a bias inherent in VI to discover meaningful components ­ will sometimes return meaningful structure (e.g. in the PCA experiments above). However it does not seem to be a sensible way of doing so in general. For example, explaining away often means that the true components will be entangled in the posterior, as is the case in the ICA example, and the variational bias will then move us away from this solution.
10


Under review as a conference paper at ICLR 2019
ON THE CONVERGENCE AND ROBUSTNESS OF BATCH NORMALIZATION
Anonymous authors Paper under double-blind review
ABSTRACT
Despite its empirical success, the theoretical underpinnings of the stability, convergence and acceleration properties of batch normalization (BN) remain elusive. In this paper, we attack this problem from a modeling approach, where we perform a thorough theoretical analysis on BN applied to a simplified model: ordinary least squares (OLS). We discover that gradient descent on OLS with BN has interesting properties, including a scaling law, convergence for arbitrary learning rates for the weights, asymptotic acceleration effects, as well as insensitivity to the choice of learning rates. We then demonstrate numerically that these findings are not specific to the OLS problem and hold qualitatively for more complex supervised learning problems. This points to a new direction towards uncovering the mathematical principles that underlies batch normalization.
1 INTRODUCTION
Batch normalization (Ioffe & Szegedy, 2015) (BN) is one of the most important techniques for training deep neural networks and has proven extremely effective in avoiding gradient blowups during back-propagation and speeding up convergence. In its original introduction (Ioffe & Szegedy, 2015), the desirable effects of BN are attributed to the so-called "reduction of covariate shift". However, it is unclear what this statement means in precise mathematical terms. To date, there lacks a comprehensive theoretical analysis of the effect of batch normalization.
In this paper, we study the convergence and stability of gradient descent with batch normalization (BNGD) via a modeling approach. More concretely, we consider a simplified supervised learning problem: ordinary least squares regression, and analyze precisely the effect of BNGD when applied to this problem. Much akin to the mathematical modeling of physical processes, the least-squares problem serves as an idealized "model" of the effect of BN for general supervised learning tasks. A key reason for this choice is that the dynamics of GD without BN (hereafter called GD for simplicity) in least-squares regression is completely understood, thus allowing us to isolate and contrast the additional effects of batch normalization.
The modeling approach proceeds in the following steps. First, we derive precise mathematical results on the convergence and stability of BNGD applied to the least-squares problem. In particular, we show that BNGD converges for any constant learning rate   (0, 1], regardless of the conditioning of the regression problem. This is in stark contrast with GD, where the condition number of the problem adversely affect stability and convergence. Many insights can be distilled from the analysis of the OLS model. For instance, we may attribute the stability of BNGD to an interesting scaling law governing  and the initial condition; This scaling law is not present in GD. The preceding analysis also implies that if we are allowed to use different learning rates for the BN rescaling variables (a) and the remaining trainable variables (), we may conclude that BNGD on our model converges for any  > 0 as long as a  (0, 1]. Furthermore, we discover an asymptotic acceleration effect of BNGD and moreover, there exist regions of  such that the performance of BNGD is insensitive to changes in , which help to explain the robustness of BNGD to the choice of learning rates. We reiterate that contrary to many previous works, all the preceding statements are precise mathematical results that we derive for our simplified model.
The last step in our modeling approach is also the most important: we need to demonstrate that these insights are not specific features of our idealized model. Indeed, they should be true characteristics, at least in an approximate sense, of BNGD for general supervised learning problems. We do this
1

Under review as a conference paper at ICLR 2019

by numerically investigating the convergence, stability and scaling behaviors of BNGD on various datasets and model architectures. We find that the key insights derived from our idealized analysis indeed correspond to practical scenarios.
1.1 RELATED WORK
Batch normalization was originally introduced in (Ioffe & Szegedy, 2015) and subsequently studied in further detail in (Ioffe, 2017). Since its introduction, it has become an important practical tool to improve stability and efficiency of training deep neural networks (He et al., 2016; Bottou et al., 2018). Initial heuristic arguments attribute the desirable features of BN to concepts such as "covariate shift", which lacks a concrete mathematical interpretation and alternative explanations have been given (Santurkar et al., 2018). Recent theoretical studies of BN includes (Ma & Klabjan, 2017), where the authors proposed a variant of BN, the diminishing batch normalization (DBN) algorithm and analyzed the convergence of the DBN algorithm, showing that it converges to a stationary point of the loss function. More recently, (Bjorck et al., 2018) demonstrated that the higher learning rates of batch normalization induce a regularizing effect.
Most relevant to the present work is (Kohler et al., 2018), where the authors also considered the convergence properties of BNGD on linear networks (similar to the least-squares problem). The authors showed that for a particularly adaptive choice of dynamic learning rate schedule, which can be seen as a fixed effective step size in our terminology (see equation (11) and section therein), BNGD converges linearly. The present research is independent and the key difference in our analysis is that we prove that the convergence occurs for constant learning rates (and in fact, arbitrarily large learning rates for , as long as 0 < a  1). This result is quite different from those in both (Kohler et al., 2018) and (Ma & Klabjan, 2017) where a specialized learning rate schedule is employed. This is an important distinction; While a decaying or dynamic learning rate is sometimes used in practice, in the case of BN it is critical to analyze the non-asymptotic, constant learning rate case, precisely because one of the key practical advantages of BN is that a bigger learning rate can be used than that in GD. Hence, it is desirable, as in the results presented in this work, to perform our analysis in this regime.
Finally, through the lens of the least-squares example, BN can be viewed as a type of overparameterization, where additional parameters, which do not increase model expressivity, are introduced to improve algorithm convergence and stability. In this sense, this is related in effect to the recent analysis of the implicit acceleration effects of over-parameterization on gradient descent (Arora et al., 2018).
1.2 ORGANIZATION
Our paper is organized as follows. In Section 2, we outline the ordinary least squares (OLS) problem and present GD and BNGD as alternative means to solve this problem. In Section 3, we demonstrate and analyze the convergence of the BNGD for the OLS model, and in particular contrast the results with the behavior of GD, which is completely known for this model. We also discuss the important insights to BNGD that these results provide us with. We then validate these findings on more general supervised learning problems in Section 4. Finally, we conclude in Section 5.

2 BACKGROUND

Consider the simple linear regression model where x  Rd is a random input column vector and y is
the corresponding output variable. Since batch normalization is applied for each feature separately,
in order to gain key insights it is sufficient to the case of y  R. A noisy linear relationship is assumed between the dependent variable y and the independent variables x, i.e. y = xT w + noise where w  Rd is the parameters. Denote the following moments:

H := E[xxT ], g := E[xy], c := E[y2].

(1)

To simplify the analysis, we assume the covariance matrix H of x is positive definite and the mean
E[x] of x is zero. The eigenvalues of H are denoted as i(H), i = 1, 2, ...d,. Particularly, the maximum and minimum eigenvalue of H is denoted by max and min respectively. The condition

2

Under review as a conference paper at ICLR 2019

number

of

H

is

defined

as



:=

.max
min

Note

that

the

positive

definiteness

of

H

allows

us

to

define

the vector norms

. H and

. H-1 by

x

2 H

= xT Hx and

x

2 H -1

= xT H-1x respectively.

2.1 ORDINARY LEAST SQUARES

The ordinary least squares (OLS) method for estimating the unknown parameters w leads to the following optimization problem

min
wRd

J0(w)

:=

1 2

Ex,y

[(y

-

xT w)2]

=

c 2

-

gT w

+

1 2

wT

H

w.

(2)

The gradient of J0 with respect to w is wJ0(w) = Hw - g, and the unique minimizer is w = u := H-1g. The gradient descent (GD) method (with step size or learning rate ) for solving the optimization problem (2) is given by the iterating sequence,

wk+1 = wk - wJ0(wk) = (I - H)wk + g,

(3)

which converges if 

<

2 max

=:

max, and the convergence rate is determined by the spectral

radius  := (I - H) = maxi{|1 - i(H)|} with

u - wk+1   u - wk .

(4)

It is well known (for example see Chapter 4 of (Saad, 2003)) that the optimal learning rate is opt =

2 max +min

,

where

the

convergence

estimate

is

related

to

the

condition

number

(H

):

u - wk+1



-1 +1

u - wk

.

(5)

2.2 BATCH NORMALIZATION

Batch normalization is a feature-wise normalization procedure typically applied to the output, which in this case is simply z = xT w. The normalization transform is defined as follows:

BN (z)

=

z-E[z]
Var[z]

=

xT w 

,

(6)

 where  := wT Hw. After this rescaling, BN (z) will be order 1, and hence in order to reintroduce

the scale, we multiply BN (z) with a rescaling parameter a (Note that the shift parameter can be set

zero since E[wT x|w] = 0). Hence, we get the BN version of the OLS problem (2):

min J(a, w)
wRd ,aR

:

=

1 2

Ex,y

y - aBN (xT w) 2

=

c 2

-

wT 

g

a

+

1 2

a2

.

(7)

The objective function J(a, w) is no longer convex. In fact, it has trivial critical points, {(a, w)|a = 0, wT g = 0}, which are saddle points of J(a, w).

We are interested in the nontrivial critical points which satisfy the relations, 
a = sign(s) uT Hu, w = su, for some s  R \ {0}.

(8)

It is easy to check that the nontrivial critical points are global minimizers, and the Hessian matrix at each critical point is degenerate. Nevertheless, the saddle points are strict (Details can be found in Appendix), which typically simplifies the analysis of gradient descent on non-convex objectives (Lee et al., 2016; Panageas & Piliouras, 2017).

Consider the gradient descent method to solve the problem (7), which we hereafter call batch normalization gradient descent (BNGD). We set the learning rates for a and w to be a and  respectively. These may be different, for reasons which will become clear in the subsequent analysis. We
thus have the following discrete-time dynamical system

ak+1 = ak + a

wkT g k

- ak

,

wk+1

=

wk

+



ak k

g

-

wkT g k2

H

wk

.

We now begin a concrete mathematical analysis of the above iteration sequence.

(9) (10)

3

Under review as a conference paper at ICLR 2019
3 MATHEMATICAL ANALYSIS OF BNGD ON OLS
In this section, we discuss several mathematical results one can derive concretely for BNGD on the OLS problem (7). First, we establish a simple but useful scaling property, which then allows us to prove a convergence result for (effectively) arbitrary constant learning rates. We also derive the asymptotic properties of the "effective" learning rate of BNGD (to be precisely defined subsequently), which shows some interesting sensitivity behavior of BNGD on the chosen learning rates. Detailed proofs of all results presented here can be found in the Appendix.
3.1 SCALING PROPERTY
In this section, we discuss a straightforward, but useful scaling property that the BNGD iterations possess. Note that the dynamical properties of the BNGD iteration are governed by a set of numbers, or a configuration {H, u, a0, w0, a, }. Definition 3.1 (Equivalent configuration). Two configurations, {H, u, a0, w0, a, } and {H , u , a0, w0, a,  }, are said to be equivalent if for iterates {wk}, {wk} following these configurations respectively, there is an invertible linear transformation T and a nonzero constant t such that wk = T wk, ak = tak for all k.
It is easy to check the system has the following scaling law. Proposition 3.2 (Scaling property). Suppose µ = 0,  = 0, r = 0, QT Q = I, then
(1) The configurations {µQT HQ, µ Qu, a0, Qw0, a, } and {H, u, a0, w0, a, } are equivalent.
(2) The configurations {H, u, a0, w0, a, } and {H, u, a0, rw0, a, r2} are equivalent.
It is worth noting that the scaling property (2) in Proposition 3.2 originates from the batchnormalization procedure and is independent of the specific structure of the loss function. Hence, it is valid for general problems where BN is used (Lemma A.9).
Despite being a simple result, the scaling property is important in determining the dynamics of BNGD, and is useful in our subsequent analysis of its convergence and stability properties. For example, one may observe that scaling property (2) implies it is sufficient to consider the case of small learning rates  when establishing stability, since an unstable iteration sequence will reach a large enough wk , after which the remaining iterations may be seen, by the scaling principle, as "restarting" the sequence with small learning rates. We shall now make use of this property to prove a convergence result for BNGD on the OLS problem.
3.2 BATCH NORMALIZATION CONVERGES FOR ARBITRARY STEP SIZE
We have the following convergence result. Theorem 3.3 (Convergence for BNGD). The iteration sequence (ak, wk) in equation (9)-(10) converges to a stationary point for any initial value (a0, w0) and any  > 0, as long as a  (0, 1]. Particularly, if a0w0T g > 0 (or a0 = 0, w0T g = 0) and  is sufficiently small, then (ak, wk) converges to a global minimizer.
Sketch of Proof. We first prove that the algorithm converges for small enough  and a  (0, 1], with any initial value (a0, w0) such that w0  1. Then, using the scaling property and the fact that wk is non-decreasing, we obtain the convergence of wk by a simple "restarting" argument outlined previously. Finally, using the positive definiteness of H, we can prove the iteration converges to either a minimizer or a saddle point.
It is important to note that BNGD converges for all step size  > 0 of wk, independent of the spectral properties of H. This is a significant advantage and is in stark contrast with GD, where the step size is limited by max(H), and the condition number of H intimately controls the stability and convergence rate.
Although (ak, wk) could converge to a saddle point, one can prove using the `strict saddle point' arguments in (Lee et al., 2016; Panageas & Piliouras, 2017) that the set of initial value for which
4

Under review as a conference paper at ICLR 2019

(ak, wk) converges to strict saddle points has Lebesgue measure 0, provided the learning rate is sufficiently small. We note that even for large learning rates, in experiments with initial values (a0, w0) drawn from typical distributions, we have not encountered convergence to saddles.

3.3 CONVERGENCE RATE, ACCELERATION AND ASYMPTOTIC SENSITIVITY

Now, let us consider the convergence rate of BNGD when it converges to a minimizer. Compared
with GD, the update coefficient before Hwk in equation (10) changed from  to a complicated term which we named as the effective step size or learning rate ^k

^k

:=



ak k

,wkT g
k2

and the recurrence relation in place of u - wk is

(11)

u

-

wwkT g
k2

k+1

=

(I

-

^k H )

u

-

wwkT g
k2 k

.

(12)

Consider the dynamics of the residual ek := u - (wkT g/k2)wk, which equals 0 if and only if wk is a global minimizer. Using the property of H-norm (see section A.1), we observe that the effective learning rate ^k determines the convergence rate of ek via

ek+1 H 

u

-

wwkT g
k2

k+1

 (I - ^kH)
H

ek

H,

(13)

where (I - ^kH) is spectral radius of the matrix I - ^kH. The inequality (13) shows that the convergence of ek is linear provided ^k  (, 2/max - ) for some positive number . It is worth noting that the convergence of the loss function value is implied by the convergence of ek
(Lemma A.19).

Next, let us discuss below an asymptotic acceleration effect of BNGD over GD. When (ak, wk) is close to a minimizer, we can approximate the iteration (9)-(10) by a linearized system. The Hessian matrix for BNGD at a minimizer (a, w) is diag(1, H/ w 2), where the matrix H is

H

=

H

-

HuuT H uT Hu

.

(14)

The matrix H is positive semi-definite and has better spectral properties than H, such as a lower

pnsoenuzdeoro-ceoingdenitvioanluensumofbHerresp=ectimmveainxly.Part,icwuhlaerrley,m a<x

and m in are the  for almost all u

maximal and minimal (see section A.1 ). This

property leads to asymptotic acceleration effects of BNGD: When ek H is small, the contraction coefficient  in (13) can be improved to a lower coefficient. More precisely, we have the following

result:

Proposition 3.4. For any positive number   (0, 1), if (ak, wk) is close to a minimizer, such that

max |ak | k2

ek

H  , then we have

ek+1

H



 (I -^k H  )+ 1-

ek

H,

(15)

where (I - ^kH) = max{|1 - ^kmin|, |1 - ^kmax|}.

Generally, we have (I - ^kH)  (I - ^kH) provided ^k > 0, and the optimal rate is

opt

:=

 -1  +1



-1 +1

=:

opt, where the inequality is strict for almost all u.

Hence, the esti-

mate (15) indicates that the optimal BNGD could have a faster convergence rate than the optimal

GD, especially when  is much smaller than  and  is small enough.

Finally, we discuss the dependence of the effective learning rate ^k (and by extension, the effective convergence rate (13) or (15)) on . This is in essence a sensitivity analysis on the performance of BNGD with respect to the choice of learning rate. The explicit dependence of ^k on  is quite
complex, but we can nevertheless give the following asymptotic estimates.

Proposition 3.5.

Suppose a

 (0, 1], a0w0T g > 0, and ||g||2



w0T g 02

gT

H w0 ,

then

(1) When  is small enough,  1, the effective step size has a same order with , i.e. there are two positive constants, C1, C2, independent on  and k, such that C1  ^k/  C2.

5

Under review as a conference paper at ICLR 2019
(2) When  is large enough,  1, the effective step size has order O(-1), i.e. there are two positive constants, C1, C2, independent on  and k, such that C1  ^k  C2.
Observe that for finite k, ^k is a differentiable function of . Therefore, the above result implies, via the mean value theorem, the existence of some 0 > 0 such that d^k/d| = 0 = 0. Consequently, there is at least some small interval of the choice of learning rates where the performance of BNGD is insensitive to this choice. In fact, empirically this is one commonly observed advantage of BNGD over GD, where the former typically allows for a variety of (large) learning rates to be used without adversely affecting performance. The same is not true for GD, where the convergence rate depends sensitively on the choice of learning rate. We will see later in Section 4 that although we only have a local insensitivity result above, the interval of this insensitivity is actually quite large in practice.
4 EXPERIMENTS
Let us first summarize our key findings and insights from the analysis of BNGD on the OLS problem.
1. A scaling law governs BNGD, where certain configurations can be deemed equivalent 2. BNGD converges for any learning rate  > 0, provided that a  (0, 1]. In particular,
different learning rates can be used for the BN variables (a) compared with the remaining trainable variables (w) 3. There exists intervals of  for which the performance of BNGD is not sensitive to the choice of 
In the subsequent sections, we first validate numerically these claims on the OLS model, and then show that these insights go beyond the simple OLS model we considered in the theoretical framework. In fact, much of the uncovered properties are observed in general applications of BNGD in deep learning.
4.1 EXPERIMENTS ON OLS
Here we test the convergence and stability of BNGD for the OLS model. Consider a diagonal matrix H = diag(h) where h = (1, ..., ) is a increasing sequence. The scaling property (Proposition 3.2) allows us to set the initial value w0 having same 2-norm with u, w0 = u = 1. Of course, one can verify that the scaling property holds strictly in this case.
Figure 1 gives examples of H with different condition numbers . We tested the loss function of BNGD, compared with the optimal GD (i.e. GD with the optimal step size opt), in a large range of step sizes a and , and with different initial values of a0. Another quantity we observe is the effective step size ^k of BN. The results are encoded by four different colors: whether ^k is close to the optimal step size opt, and whether loss of BNGD is less than the optimal GD. The results indicate that the optimal convergence rate of BNGD can be better than GD in some configurations. This acceleration phenomenon is ascribed to the pseudo-condition number of H (discard the only zero eigenvalue) being less than (H). This advantage of BNGD is significant when the (pseudo)condition number discrepancy between H and H is large. However, if this difference is small, the acceleration is imperceptible. This is consistent with our analysis in section 3.3.
Another important observation is a region such that ^ is close to opt, in other words, BNGD significantly extends the range of `optimal' step sizes. Consequently, we can choose step sizes in BNGD at greater liberty to obtain almost the same or better convergence rate than the optimal GD. However, the size of this region is inversely dependent on the initial condition a0. Hence, this suggests that small a0 at first steps may improve robustness. On the other hand, small  will weaken the performance of BN. The phenomenon suggests that improper initialization of the BN parameters weakens the power of BN. This experience is encountered in practice, such as (Cooijmans et al., 2016), where higher initial values of BN parameter are detrimental to the optimization of RNN models.
4.2 EXPERIMENTS ON PRACTICAL DEEP LEARNING PROBLEMS
We conduct experiments on deep learning applied to standard classification datasets: MNIST (LeCun et al., 1998), Fashion MNIST (Xiao et al., 2017) and CIFAR-10 (Krizhevsky & Hinton, 2009).
6

Under review as a conference paper at ICLR 2019

 = 80

a0 = 10

a0 = 1

a0 = 0.01

a0 = 0.0001

a0 = 1e - 8

yes close to optimal
no step size yes no
loss_BN < loss_GD

 = 1280

 = 327680  = 20480

loss

GD(opt)

BNGD

Figure 1: Compare of BNGD and GD on OLS model. The results are encoded by four different colors: whether ^k is close to the optimal step size opt of GD, characterized by the inequality 0.8opt < ^k < opt/0.8, and whether loss of BNGD is less than the optimal GD. Parameters: H = diag(logspace(0,log10(),100)), u is randomly chosen uniformly from the unit sphere in R100, w0 is set to Hu/ Hu . The GD and BNGD iterations are executed for k = 2000 steps with the same w0. In each image, the range of a (x-axis) is 1.99 * logspace(-10,0,41), and the range of  (y-axis) is logspace(-5,16,43).
The goal is to explore if the key findings outlined at the beginning of this section continue to hold for more general settings. For the MNIST and Fashion MNIST dataset, we use two different networks: (1) a one-layer fully connected network (784 × 10) with softmax mean-square loss; (2) a fourlayer convolution network (Conv-MaxPool-Conv-MaxPool-FC-FC) with ReLU activation function and cross-entropy loss. For the CIFAR-10 dataset, we use a five-layer convolution network (ConvMaxPool-Conv-MaxPool-FC-FC-FC). All the trainable parameters are randomly initialized by the Glorot scheme (Glorot & Bengio, 2010) before training. For all three datasets, we use a minibatch size of 100 for computing stochastic gradients. In the BNGD experiments, batch normalization is performed on all layers, the BN parameters are initialized to transform the input to zero mean/unit variance distributions, and a small regularization parameter =1e-3 is added to variance 2 + to avoid division by zero.
Scaling property Theoretically, the scaling property 3.2 holds for any layer using BN. However, it may be slightly biased by the regularization parameter . Here, we test the scaling property in practical settings. Figure 2 gives the loss of network-(2) (2CNN+2FC) at epoch=1 with different learning rate. The norm of all weights and biases are rescaled by a common factor . We observe that the scaling property remains true for relatively large . However, when  is small, the norm of weights are small. Therefore, the effect of the -regularization in 2 + becomes significant, causing the curves to be shifted.
Stability for large learning rates We use the loss value at the end of the first epoch to characterize the performance of BNGD and GD methods. Although the training of models have generally not converged at this point, it is enough to extract some relative rate information. Figure 3 shows the loss value of the networks on the three datasets. It is observed that GD and BNGD with identical learning rates for weights and BN parameters exhibit a maximum allowed learning rate, beyond which the iterations becomes unstable. On the other hand, BNGD with separate learning rates exhibits a much larger range of stability over learning rate for non-BN parameters, consistent with our theoretical results in Theorem 3.3.
Insensitivity of performance to learning rates Observe that BN accelerates convergence more significantly for deep networks, whereas for one-layer networks, the best performance of BNGD and GD are similar. Furthermore, in most cases, the range of optimal learning rates in BNGD is quite large, which is in agreement with the OLS analysis (Proposition 3.5). This phenomenon is poten-
7

Under review as a conference paper at ICLR 2019

loss accuracy

100 10-1

=0.01 =0.0316228 =0.1 =0.316228 =1 =3.16228 =10 =31.6228 =100

10-2
10-4 10-3 10-2 10-1 100 101 102 103 104 learning rate / 2

1.00
0.98 =0.01 =0.0316228 =0.1
0.96 =0.316228 =1 =3.16228
0.94 =10 =31.6228 =100
0.92
0.90 10-4 10-3 10-2 10-1 100 101 102 103 104 learning rate / 2

Figure 2: Tests of scaling property of the 2CNN+2FCnetwork on MNIST dataset. BN is performed on all layers, and = 1e - 3 is added to variance 2 + . All the trainable parameters (except the BN parameters) are randomly initialized by the Glorot scheme, and then multiplied by a same parameter .

tially crucial for understanding the acceleration of BNGD in deep neural networks. Heuristically, the "optimal" learning rates of GD in distinct layers (depending on some effective notion of "condition number") may be vastly different. Hence, GD with a shared learning rate across all layers may not achieve the best convergence rates for all layers at the same time. In this case, it is plausible that the acceleration of BNGD is a result of the decreased sensitivity of its convergence rate on the learning rate parameter over a large range of its choice.

loss loss loss

0.10 gd

bn

0.08

bn lr_a=1 bn lr_a=0.1

bn lr_a=0.01

0.06 bn lr_a=0.001

0.04

0.02

0.00 10 3 10 2 10 1 100 101 102 103 104 105 learning rate

1.0

gd

0.8

bn bn lr_a=1

bn lr_a=0.1

0.6

bn lr_a=0.01 bn lr_a=0.001

0.4

0.2

0.0 10 3 10 2 10 1 100 101 102 103 learning rate

2.25 gd

2.00

bn bn lr_a=1

1.75

bn lr_a=0.1 bn lr_a=0.01

1.50 bn lr_a=0.001

1.25

1.00

0.75

0.50

10 3 10 2 10 1 100 101 102 103 learning rate

Figure 3: Performance of BNGD and GD method on MNIST (network-(1), 1FC), Fashion MNIST (network-(2), 2CNN+2FC) and CIFAR-10 (2CNN+3FC) datasets. The performance is characterized by the loss value at ephoch=1. In the BNGD method, both the shared learning rate schemes and separated learning rate scheme (learning rate lr a for BN parameters) are given. The values are averaged over 5 independent runs.

5 CONCLUSION AND OUTLOOK
In this paper, we adopted a modeling approach to investigate the dynamical properties of batch normalization. The OLS problem is chosen as a point of reference, because of its simplicity and the availability of convergence results for gradient descent. Even in such a simple setting, we saw that BNGD exhibits interesting non-trivial behavior, including scaling laws, robust convergence properties, asymptotic acceleration, as well as the insensitivity of performance to the choice of learning rates. Although these results are derived only for the OLS model, we show via experiments that these are qualitatively valid for general scenarios encountered in deep learning, and points to a concrete way in uncovering the reasons behind the effectiveness of batch normalization.
Interesting future directions include the extension of the results for the OLS model to more general settings of BNGD, where we believe the scaling law (Proposition 3.2) should play a significant role. In addition, we have not touched upon another empirically observed advantage of batch normalization, which is better generalization errors. It will be interesting to see how far the current approach takes us in investigating such probabilistic aspects of BNGD.
8

Under review as a conference paper at ICLR 2019
REFERENCES
Sanjeev Arora, Nadav Cohen, and Elad Hazan. On the optimization of deep networks: Implicit acceleration by overparameterization. In Jennifer Dy and Andreas Krause (eds.), Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pp. 244­253, Stockholmsmssan, Stockholm Sweden, 10­15 Jul 2018. PMLR.
J. Bjorck, C. Gomes, and B. Selman. Understanding Batch Normalization. ArXiv e-prints, May 2018.
Le´on Bottou, Frank E Curtis, and Jorge Nocedal. Optimization methods for large-scale machine learning. SIAM Review, 60(2):223­311, 2018.
Tim Cooijmans, Nicolas Ballas, Ce´sar Laurent, and Aaron C. Courville. Recurrent batch normalization. CoRR, abs/1603.09025, 2016. URL http://arxiv.org/abs/1603.09025.
Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural networks. In Proceedings of the thirteenth international conference on artificial intelligence and statistics, pp. 249­256, 2010.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2016.
Sergey Ioffe. Batch renormalization: Towards reducing minibatch dependence in batch-normalized models. CoRR, abs/1702.03275, 2017. URL http://arxiv.org/abs/1702.03275.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In International Conference on Machine Learning, pp. 448­456, 2015.
J. Kohler, H. Daneshmand, A. Lucchi, M. Zhou, K. Neymeyr, and T. Hofmann. Towards a Theoretical Understanding of Batch Normalization. ArXiv e-prints, May 2018.
Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. Technical report, Citeseer, 2009.
Yann LeCun, Le´on Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278­2324, 1998.
J. D. Lee, M. Simchowitz, M. I. Jordan, and B. Recht. Gradient Descent Converges to Minimizers. ArXiv e-prints, February 2016.
Yintai Ma and Diego Klabjan. Convergence analysis of batch normalization for deep neural nets. CoRR, 1705.08011, 2017. URL http://arxiv.org/abs/1705.08011.
Ioannis Panageas and Georgios Piliouras. Gradient Descent Only Converges to Minimizers: NonIsolated Critical Points and Invariant Regions. In Christos H. Papadimitriou (ed.), 8th Innovations in Theoretical Computer Science Conference (ITCS 2017), volume 67 of Leibniz International Proceedings in Informatics (LIPIcs), pp. 2:1­2:12, Dagstuhl, Germany, 2017. Schloss Dagstuhl­ Leibniz-Zentrum fuer Informatik. ISBN 978-3-95977-029-3. doi: 10.4230/LIPIcs.ITCS.2017.2.
Yousef Saad. Iterative methods for sparse linear systems, volume 82. siam, 2003.
S. Santurkar, D. Tsipras, A. Ilyas, and A. Madry. How Does Batch Normalization Help Optimization? (No, It Is Not About Internal Covariate Shift). ArXiv e-prints, May 2018.
Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms, 2017.
9

Under review as a conference paper at ICLR 2019

A PROOF OF THEOREMS

A.1 GRADIENTS AND HESSIAN MATRIX

The objective function in problem (7) has an equivalent form:

J(a, w)

=

1 2

(u

-

a 

w)T

H

(u

-

a 

w)

=

1 2

u

2 H

-

wT 

g

a

+

1 2

a2

,

where u = H-1g.

The gradients are:

J a

=

-

1 

(wT

H

u

-

a 

wT

H

w)

=

-

1 

wT

g

+ a,

J w

=

-

a 

(H

u

-

a 

H

w)

+

a 3

(wT

H

u

-

a 

wT

H

w)H

w

=

-

a 

g

+

a 3

(wT

g

)H

w.

(16)
(17) (18)

The Hessian matrix is

2J a2 2J
wa

2J
aw 2J w2

=

1 A12 A1T2 A22

where

A22

=

a 3

(wT

g)

H

+

1 wT g

(Hw)gT + g(Hw)T

-

3 2

(H w)(H w)T

,

A12

=

-

1 

g

-

1 2

(wT g)Hw

.

(19)
(20) (21)

The objective function J(a, w) has trivial critical points, {(a, w)|a = 0, wT g = 0}. It is obvious that a is the minimizer of J(a, w), but (a, w) is not a local minimizer of J(a, w) unless g = 0, hence (a, w) are saddle points of J(a, w). The Hessian matrix at those saddle points has at least a negative eigenvalue, hence the saddle points are strict.

On the other hand, the nontrivial critical points satisfies the relations, 
a = ± uT Hu, w //u,

(22)

where the sign of a depends on the direction of u, w, i.e. sign(a) = sign(uT w). It is easy to check that the nontrivial critical points are global minimizers. The Hessian matrix at those minimizers is diag(1, H/ w 2) where the matrix H is

H

=

H

-

HuuT H uT Hu

(23)

which is positive semi-definite and has a zero eigenvalue corresponding to the eigenvector u, i.e. Hu = 0.

Lemma A.1. of H and H

If H is positive definite and H is satisfy the following inequalities:

defined

as

H

=

H

-

HuuT H uT Hu

,

then

the

eigenvalues

0 = 1(H) < 1(H)  2(H)  2(H)  ...  d(H)  d(H).

(24)

Here i(H) means the i-th smallest eigenvalue of H.

Proof. (1) According to the definition, we have Hu = 0, and for any x  Rd,

xT Hx = xT Hx -

(xT Hu)2 uT Hu

 [0, xT Hx],

(25)

which implies H is semi-positive definite, and i(H)  1(H) = 0. Furthermore, we have the following equality:

xT Hx = min
tR

x - tu

2 H

.

(26)

(2) We will prove i(H)  i(H) for all i, 1  i  d. In fact, using the Min-Max Theorem, we have

i (H  )

=

min max
dimV =i xV

xT Hx x2



min max
dimV =i xV

xT Hx x2

=

i (H ).

10

Under review as a conference paper at ICLR 2019

(3) We will prove i(H)  i-1(H) for all i, 2  i  d. In fact, using the Max-Min Theorem, we have

i (H  )

=

max min
dimV =n-i+1 xV

xT Hx x2

=

max
dimV =n-i+1,uV

min min
xV tR

x-tu

2 H

x2

 max min min
dimV =n-i+1,uV xV tR

x-tu

2 H

x-tu 2

= max

min

dimV =n-i+1 yspan{V,u}

y y

2
H 2

,

y

=

x

-

tu



max min
dimV =n-(i-1)+1 yV

yT Hy y2

=

i-1 (H ),

where we have used the fact that x  u, x - tu 2 = x 2 + t2 u 2  x 2.

There are several corollaries related to the spectral property of H. We first give some definitions. Since H is positive semi-definite, we can define the H-seminorm.

Definition A.2. The H-seminorm of a vector x is defined as x H := xT Hx. x H = 0 if and only if x is parallel to u.

Definition A.3.

The pseudo-condition number of H is defined as (H) :=

.d (H  )
2 (H  )

Definition A.4. For any real number , the pseudo-spectral radius of the matrix I - H is defined

as

(I

-

H  )

:=

max
2id

|1

-

i (H  )|.

The following corollaries are direct consequences of Lemma A.1, hence we omit the proofs.

Corollary A.5. The pseudo-condition number of H is less than or equal to the condition number of H :

 (H  )

:=

d (H  ) 2 (H  )



d (H ) 1 (H )

=:

(H ),

(27)

where the equality holds up if and only if u  span{v1, vd}, vi is the eigenvector of H corresponding to eigenvalue i(H).
Corollary A.6. For any vector x  Rd and any real number , we have (I - H)x H  (I - H) x H .

Corollary A.7. For any positive number  > 0, we have

(I - H)  (I - H),

(28)

where the inequality is strict if uT vi = 0 for i = 1, d.

It is obvious that the inequality in (27) and (28) is strict for almost all u.

A.2 SCALING PROPERTY

The dynamical system defined in equation (9)-(10) is completely determined by a set of configurations {H, u, a0, w0, a, }. It is easy to check the system has the following scaling property:
Lemma A.8 (Scaling property). Suppose µ = 0,  = 0, r = 0, QT Q = I, then

(1) The configurations {µQT HQ, µ Qu, a0, Qw0, a, } and {H, u, a0, w0, a, } are equivalent.

(2) The configurations {H, u, a0, w0, a, } and {H, u, a0, rw0, a, r2} are equivalent.

The scaling property is valid for general loss functions provided batch normalization is used. Consider a general problem

min J0(w) := Ex,y[f (y, xT w)],
wRd

(29)

and its BN version

min J (a, w) : = Ex,y f y, aBN (xT w) .
wRd ,aR

(30)

11

Under review as a conference paper at ICLR 2019

Then the gradient descent method gives the following iteration,

ak+1

=

ak

+

a

,wkT h~
k

wk+1

=

wk

+



ak k

h~

-

wkT h~ k2

H

wk

,

where h~ = h(akwk/k), and h is the gradient of original problem:

h(w) := Ex,y[xf2(y, xT w)].

(31) (32)
(33)

It is easy to check the general BNGD has the following property: Lemma A.9 (General scaling property). Suppose r = 0, then the configurations {w0, , } and {rw0, r2, } are equivalent. Here the sign * means other parameters.

A.3 PROOF OF THEOREM 3.3

Recall the BNGD iterations

ak+1 = ak + a

wkT g k

- ak

,

wk+1

=

wk

+



ak k

g

-

wkT g k2

H

wk

.

The scaling property simplify our analysis by allowing us to set, for example, u = 1 and w0 = 1. In the rest of this section, we only set u = 1.
For the step size of a, it is easy to check that ak tends to infinity with a > 2 and initial value a0 = 1, w0 = u. Hence we only consider 0 < a < 2, which make the iteration of ak bounded by some constant Ca.
Lemma A.10 (Boundedness of ak). If the step size 0 < a < 2, then the sequence ak is bounded for any  > 0 and any initial value (a0, w0).

Proof.

Define k

:=

wkT g k

,

which

is

bounded

by

|k |



 uT

Hu

=:

C,

then

ak+1 = (1 - a)ak + ak = (1 - a)k+1a0 + (1 - a)ka0 + ... + (1 - a)ak-1 + ak.

Since |1 - a| < 1, we have |ak+1|  |a0| + 2C

k i=0

|1

-

a|i



|a0|

+

2C

1 1-|1-a

|

.

According to the iterations (34), we have

u

-

wwkT g
k2

k+1

=

I

-



ak k

HwkT g
k2

u

-

wwkT g
k2

k

.

Define

ek

:=

u

-

wkT g k2

wk

,

qk

:= uT Hu -

(wkT g)2 k2

=

^k

:=



ak k

,wkT g
k2

ek

2 H



0,

and using the property

wT g k2

= argmin
t

u - tw

H , and the property of H-norm, we have

qk+1 

u

-

wwkT g
k2

k+1

2
=
H

(I - ^kH)ek

2 H

 (I - ^kH)2qk.

Therefore we have the following lemma to make sure the iteration converge:

(34)
(35) (36) (37)
(38)

12

Under review as a conference paper at ICLR 2019

Lemma A.11. Let 0 < a < 2. If there are two positive numbers - and ^+, and the effective step size ^k satisfies

0<

- wk 2

 ^k

 ^+

<

2 max

(39)

for all k large enough, then the iterations (34) converge to a minimizer.

Proof. Without loss of generality, we assume

- wk 2

<

1 max

and the inequality (39) is satisfied for

all k  0. We will prove wk converges and the direction of wk converges to the direction of u.

(1) Since wk is always increasing, we only need to prove it is bounded. We have,

wk+1 2 = =  

wk

2

+

2

a2k k2

H ek

2

k

w0 2 + 2

a2i i2

H ei

2

i=0

k

w0 2 + 2max

qai2
i2

i

i=0

k

w + 2
0

2 maxCa2 min

i=0

.qi
wi 2

(40) (41) (42) (43)

The by a

inequality in constant Ca.

last lines Next, we

are based on will prove

the fact
 qi i=0 wi

that 2<

Hei 2  , which

max ei implies

2 H

,

wk

and are

|ak| are bounded bounded.

According to the estimate (38), we have

qk+1  max{|1 - ^+i|2, |1 -
i

- i wk 2

|2

}qk



max{1

-

+, 1

-

- min wk 2

}qk

,

where 1 - + = maxi{|1 - ^+i|2}  (0, 1). Using the definition of qk, we have

(44) (45)

qk - qk+1



min{+

qw0 2,-min}

wk 2

k

=:

C qk wk 2

 0.

(46)

Since qk is bounded in [0, uT Hu], summing both side of the inequality, we get the bound of the

infinite series
k

qk wk

2



uT Hu C

< .

(2) Since

wk

is bounded, we denote ^- :=

- w

2,

and

define



:=

max{|1 - ^±i|}
i



(0, 1),

then the inequality (38) implies qk+1  2qk. As a consequence, qk tends to zero, which implies

the direction of wk converges to the direction of u.

(3) The convergence of ak is a consequence of wk converging.

Since ak

is bounded,

we assume |ak|

<

C~a

 uT

H u,

C~a



1,

and define 0

:=

.1
2C~a max

The

following lemma gives the convergence for small step size.

Lemma A.12. (0, 1], / w0 2

If <

the initial values (a0, w0) satisfies a0w0T g > 0, and step size 0, then the sequence (ak, wk) converges to a global minimizer.

satisfies

a



Remark

1:

If

we

set

a0

=

0,

then

we

have

w1

=

w0, a1

=

a

w0T g 0

,

hence

a1w1T g

>

0

provided

w0T g = 0.

Remark 2: For the case of a  (1, 2), if the initial value satisfies an additional condition 0 <

|a0|



a

|w0T g| 0

,

then

we

have

(ak ,

wk )

converges

to

a

global

minimizer

as

well.

13

Under review as a conference paper at ICLR 2019

Proof. Without loss of generality, we only consider the case of a0 > 0, w0T g > 0, w0  1.

(1) We will prove ak > 0, wkT g > 0 for all k. Denote yk := wkT g,  =

g 4

.

On the one hand, if ak > 0, 0 < yk < 2, then

yk+1



yk

+



ak k

g2 2



yk .

(47)

On the other hand, when ak > 0, yk > 0,  < 0, we have

yk+1





ak g k

2

+ yk

1

-



ak k2

ak+1  min{ak, yk/k}.

gT Hg



1 2

yk

,

(48) (49)

As a consequence, we have ak > 0, yk  y := min{y0, } for all k by induction.

(2) We will prove the effective step size ^k satisfying the condition in Lemma A.11.

Since ak is bounded,  < 0, we have

^k

:=



ak k

wkT g k2



C~a max min wk 2

 C~a =: ^+

<

,1
2max

and

(50)

qk+1  (1 - ^kmin)2qk  (1 - ^kmin)qk < qk.

(51)

which implies

wkT+1 g k+1



wkT g k



.w0T g
0

Furthermore, we have ak



min{a0,

w0T g 0

},

and

there

is

a

positive constant - > 0 such that

^k

  ak
max wk

wkT g 2 k



.-
wk 2

(52)

(3) Employing the Lemma A.11, we conclude that (ak, wk) converges to a global minimizer.
Lemma A.13. If step size satisfies a  (0, 1], / w0 2 < 0, then the sequence (ak, wk) converges.

Proof. Thanks to Lemma A.12, we only need to consider the case of akwkT g  0 for all k, and we will prove the iteration converges to a saddle point in this case. Since the case of ak = 0 or wkT g = 0 is trivial, we assume akwkT g < 0 below. More specifically , we will prove |ak+1| < r|ak| for some constant r  (0, 1), which implies convergence to a saddle point. (1) If ak and ak+1 have same sign, hence different sign with wkT g, then we have |ak+1| = |1 - a ak| - a|wkT g|/k  |1 - a ak|. (2) If ak and ak+1 have different signs, then we have

 |wkT g|
|ak k |

1 k2

Consequently, we get

g

2

-

wkT g k2

gT

H

wk

 2max < 1.

(53)

|ak+1 | |ak |

=

a

|wkT g| |ak k |

- (1 - a)



2amax

- (1 - a)

<

a



1.

(54)

(3) Setting r := max(|1 - a|, 2amax - (1 - a)), we finish the proof.

To simplify our proofs for Theorem 3.3, we give two lemmas which are obvious but useful.

Lemma A.14.

If positive series fk, hk satisfy fk+1

 rfk + hk, r



(0,

1)

and

lim
k

hk

= 0, then

lim
k

fk

=

0.

Proof. It is obvious, because the series bk defined by bk+1 = rbk + hk, b0 > 0, tends to zeros.

14

Under review as a conference paper at ICLR 2019

Figure 4: The geometric meaning of the separation property

Lemma A.15 (Separation property). For 0 small enough, the set S := {w|y2q < 0, w  1}

is composed by two separated parts: S1 and S2, dist(S1, S2) > 0, where in the set S1 one has

y2 < 1, q > 2, and in S2 one has q < 2, y2 > 1 for some 1 > 0, 2 > 0. Here y := wT g, q :=

uT Hu -

(wT Hu)2 wT Hw

= uT Hu -

y2 wT H

w

.

Proof. The proof is based on H being positive. The geometric meaning is illustrated in Figure 4.

Corollary A.16. If lim
k

wk+1 - wk

= 0, and klim(wkT g)2qk = 0, then either klim(wkT g)2 = 0

or

lim
k

qk

=

0.

Proof. 0 > 0

Denote yk := small enough

swuckThgt.haAtctchoersdeinpgaratotetdhepasretspaorfattihoenspetroSpe:r=ty{(wL|eym2qma<A.01,5)w, weca1n},cSh1osaenda

S2, have dist(S1, S2) > 0.

Because yk2qk tends to zero, we have wk belongs to S for k large enough, for instance k > k1. On the other hand, because wk+1 - wk tends to zero, we have wk+1 - wk < dist(S1, S2) for k large enough, for instance k > k2. Then consider k > k3 := max(k1, k2), we have all wk belongs to the same part S1 or S2.

If wk  S1, (qk > 2), for all k > k3, then we have klim(wkT g)2 = 0.

On

the

other

hand,

if

wk



S2,

(yk2

>

1),

for

all

k

>

k3,

then

we

have

lim
k

qk

=

0.

Theorem A.17. Let a  (0, 1] and  > 0. The sequence (ak, wk) converges for any initial value (a0, w0).

Proof. We will prove wk converges, then prove (ak, wk) converges as well.

(1) We will prove that wk is bounded and hence converges.

In fact, according to the Lemma A.13, once wk 2  /0 for some k, the rest of the iteration will converge, hence wk is bounded.

(2) We will prove lim
k

wk+1 - wk

= 0, and klim(wkT g)2qk = 0.

The convergence of wk implies k ak2qk is summable. As a consequence,

lim
k

a2k pk

=

0,

lim
k

ak ek

=

0,

(55)

and lim
k

wk+1 - wk

= 0. In fact, we have

wk+1 - wk

2

=

2 a2k
2

H ek

2



a qmax2
2min

2 kk



0.

(56)

15

Under review as a conference paper at ICLR 2019

Consider the iteration of series |ak - wkT g/k|,

a -  a - + -k+1

wkT+1 g k+1

k+1

wkT+1 g k

wkT+1 g k

wkT+1 g k+1



(1

-

a)

ak

-

wkT g k



(1

-

a)

ak

-

wkT g k



(1

-

a)

ak

-

wkT g k

+

 |akgT Hek|
k2

+

||wkT+1 g |
(kk+1) k+1

-

k |

+  +  a eg H akek H
k2

|wkT+1g| max (kk+1) k

kk

+ 2C akek H .

H

(57)

The constant C

in (57) can be chosen as C

=

.max u H
min w0 2

Since

akek H tends to zero, we can

use

Lemma

A.14

to

get

lim
k

|ak

-

wkT g/k|

= 0.

Combine the equation (55), then we have

klim(wkT g)2pk = 0.

(3) According

to

the Corollary

A.16,

we have

either

lim
k

yk2

=

0,

or

lim
k

qk

=

0.

In

the

former

case, the iteration of (ak, wk) converges to a saddle point. However, in the latter case, (ak, wk)

converges to a global minimizer. In both cases we have (ak, wk) converges.

A.4 CONVERGENCE RATE

In

the

last

section,

we

encountered

the

following

estimate

for

ek

=

u

-

wwkT g
k2

k

ek+1 H  (I - ^kH) ek H .

(58)

We can improve the convergence rate of the above, at least asymptotically, if H has better spectral

property. This is the content of Proposition 3.4 and the following lemma is enough to prove it.

Lemma A.18. The following inequality holds,

(1 - k) ek+1 H  (I - ^kH) + k ek H ,

(59)

where

k

:=

max |ak | k2

ek

H.

Proof. The case of wkT g = 0 is trivial, hence we assume wkT g = 0 in the following proof. Rewrite the iteration on wk as the following equality,

u

-

wwkT g
k2

k+1

=

(I

-

^k H )ek

=

(I

-

^k H  )ek

-

^k

1 - (wkT g)2
uT Huk2

H u.

Then we will use the properties of H-seminorm to prove our argument.

(60)

(1) Estimate the H-seminorm on the right hand of equation (60).

right H 

(I - ^kH)ek

H + |^k|

1 - (wkT g)2
uT Huk2

Hu H

 (I - ^kH)

ek

+H 

max |^k | uT Hu

ek

2 H

=

(I

-

^k

H



)

 |wkT g| uT Huk

ek

+H

max|akwkT g| uT Huk3

ek

2 H

=  |wkT g|
uT Huk

(I - ^kH) + k

ek H .

(61) (62) (63) (64)

(2) Estimate the H-seminorm on the left hand of equation (60). Using the H-norm on the iteration

of wk, we have

k+1 =

wk

+



ak k

H ek

H



k

-  max|ak|
k

ek H .

(65)

Consequently, we have

left

H

=

 |wkT g| k+1 uT Huk k

ek+1

H



 |wkT g| uT Huk

(1

-

k )

ek+1

H.

(66)

(3) Combining (1) and (2), we finish the proof.

16

Under review as a conference paper at ICLR 2019

Now, we turn to the convergence of the loss function which can be rewritten as Jk =

1 2

e~k

2 H

with

e~k

=

u

-

ak k

wk .

There

is

an

useful

equality

between

e~k

2 H

and

ek

2 H

:

e~k

2 H

=

ek

2 H

+

ak

-

wkT g k

2
.

(67)

Recalling the inequality (57) and the boundedness of ak, we have a constant C0 such that

a -k+1

wkT+1 g k+1



|1

-

a|

ak

-

wkT g k

+ C0 ek H ,

(68)

which indicates that we can use the convergence of ek to estimate the convergence of the loss value Jk. In fact we have the following lemma.

Lemma A.19. If ek H  Ck for some constant C and   (0, 1), a  (0, 1], then we have

e~k

2 H



C 2 2k

+

C1(1 - a)k + C2kk

2
,

(69)

where  = max(, 1 - a), C1 = |a0 - w0T g/0| and C2 = CC0.

Proof. According to the inequality (68), we have

k-1

ak

-

wkT g k

 C1(1 - a)k + C2

(1 - a)ik-i  C1(1 - a)k + C2kk.

i=0

Put it in the equality (67), then we finish the proof.

(70)

A.5 ESTIMATING THE EFFECTIVE STEP SIZE

Accorded to Lemma A.12, the effective step size ^k has same order with

 wk

2

provided a0w0T g

>

0, /||w0|| < 0. In fact, we have

:=C1
wk 2

a0w0T g



0 max wk

2

 ^k





uT

H

u

Ca  min wk

2

=:

.C2
wk 2

(71)

Hence, to prove the Proposition 3.5, we only need to estimate the norm of wk.

proof of Proposition 3.5. According to the BNGD iteration, we have (see the proof of Lemma A.11)

wk+1 2 

k

w0 2 + 2max

a2i i2

qi

.

i=0

(72)

(1) When

 w0 2

<

0 (0 is defined in Lemma A.12), the sequence qk satisfies qk+1



(1 -

^kmin)qk. Hence the norm of wk is bounded by



wk 2 

w0

2

+

Ca

0 w0T g

(qi - qi+1)  w0 2 + C,

i=0

(73)

for some constant C. As a consequence,

C~1 :=

w0

C1  2 (1+C 0 )

 ^k



C2  w0 2

=:

C~2.

(74)

(2) When  is large enough, the increment of the norm wk at the first step is large as well. In fact,

we have

w1 2 -

w0

2

=

2 a02
02

He0 2 = C32.

(75)

Since ||g||2 1 such that

 w0T g
02 w1 2

gT <

Hw0, we have 0, then we can

a1w1T g use the

> a1w0T g argument

> in

0. (1)

Choose on (a1,

 to be larger than some value w1). More precisely, there are

two constants, C1, C2, such that

C1  w1 2

 ^k



.C2
w1 2

(76)

Plugging the equation (75) into it, we have

  ^    .C112
w0 2+C312

C1 2 w0 2+C32

k

C2 2 w0 2+C32

C2 C3

(77)

17

Under review as a conference paper at ICLR 2019

B MODIFIED BNGD

Through our analysis, we discovered that a modification of the BNGD, which we call MBNGD, becomes much easier to analyze and possesses better convergence properties. Note that the results in the main paper do not depend on the results in this section.

The modification is simply to enforce ak

=

wkT g k

at every iteration, which yields the modified

iterations:

wk+1

=

wk

+

 wkT g
k2

g

-

wkT g k2

H

wk

.

(78)

In a sense, one can view the above as a limiting version of BNGD where the BN rescaling variable a

is adjusted every step to the optimal value based on the current value of the weights w. The MBNGD

iterations is governed by the variables: H, u, a0, w0, , where the scaling properties (Lemma 3.2, omit the parameter a now) remains. More importantly, we find the iteration will converge to a
saddle point if and only if it exactly meets the saddle point at a finite step. More precisely, we have

the convergence theorem:

Theorem B.1 (Convergence for modified BNGD). The iteration sequence wk in equation (78) con-
verges for any initial value w0 and any step size  > 0. It converges to a global minimizer almost sure, in the sense that the set of initial values such that wk converges to a saddle point is of Lebesgue measure zero. Furthermore, It converges to a saddle point if and only if wkT g = 0 for some k.

Particularly,

if



<

2 w0 max 

2
u

2 , w0T g

=

0,

then

wk

converges

to

a

global

minimizer.

In the following, we assume u = 1.

B.1 PROOF

Lemma B.2. If w0T g = 0 and

 w0

2

<

0

:=

2 max

,

then

the

sequence

wk

converges to a global

minimizer.

Proof. Similar to the proof of Lemma A.12, but here the effective step size is always nonnegative which is defined as

^k := 

wkT g k2

2

 wk 2



 w0 2

=: ^+

<

.2
max

(79)

The inequality (38) immediately gives qk+1

 qk, which implies

(wkT+1 g )2 k2+1



(wkT g)2 k2



.(w0T g)2
02

As

a consequence, the effective step size has a lower bound

^k

  (w0T g)2

1

02 max wk

2

=:

.-
wk 2

Employing the Lemma A.11, we conclude that wk converges to a global minimizer.

(80)

Lemma B.3. If wkT g = 0 for all k, then

 k=0

|wkT

g|

=

k=0(wkT g)2 = .

Proof. Without loss of generality, we assume

w0

 1, denote yk := wkT g and set  =

g 4

.

From

the iteration of wk, we have

yk+1 = yk +

yk k2

g

2

-

yk k2

g

T

H

wk

.

If 0 < |wkT g| < 2, then we have the inequality:

(81)

then

g

2

-

yk k2

gT

H

wk



g

2

-

g wk

|yk |



1 2

g 2,

(82)

|yk+1| 

1+ g 2
max wk 2 2

|yk| > |yk| > 0.

(83)

As a consequence, limk wkT g = 0 is not possible unless wkT g = 0 for some k, which implies the results we want.

18

Under review as a conference paper at ICLR 2019

Theorem B.4. The iteration sequence wk in equation (78) converges for any initial value w0 and any step size  > 0. Furthermore, wk will converge to a global minimizer unless wkT g = 0 for some k.

Proof. Obviously, if wkT g = 0 for some k = k0, then wk = wk0 for all k  k0, hence wk converges to wk0 . Without losing generality, we consider wkT g = 0 for all k and w0  1 below.

(1) Firstly, we will prove that wk is bounded and hence converges.

In fact, according to the Lemma B.2, once wk 2  /0 for some k, the rest of the iteration will converge, hence wk is bounded.

(2) Secondly, we will prove wk converges to a vector parallel to u.

Denote yk and then

:k==0wykTk2gq,kziks

:=

.wkT g
2

The

convergence

of

wk

indicates

summable as well. Therefore we have

that

 k=0

zk2qk

is

summable,

wk+1 - wk

2

=

2 (wkT g)2
4

g

-

wkT g k2

H

wk

2  max2zk2qk,

(84)

and the above tends to zero, i.e. limk wk+1 - wk = 0.

According to the separation property (Lemma A.15), we can chose a 0 > 0 small enough such that the separated parts of the set S := {w|y2q < 0, w  1}, S1 and S2, have dist(S1, S2) > 0.

Because yk2qk tends to zero, we have wk belongs to S for k large enough, for instance k > k1. On the other hand, because wk+1 - wk tends to zero, we have wk+1 - wk < dist(S1, S2) for k large enough, for instance k > k2. Then consider k > k3 := max(k1, k2), we have all wk belongs to the same part S1 or S2.

However, Therefore

Lemma B.3 wk  S2 (yk2

says > 1)

 k=0

yk2

for all k

= >

, hence wk  S1 (qk > 2) k3. Consequently, we can claim

for all that

k > k3 is not true.

 k=0

qk

is

summable

and wk converges to a vector parallel to u.

B.2 EXPERIMENT

Here we test the convergence and stability of MBNGD for OLS model. Consider the diagonal matrix H = diag(h), where h = (1, ..., ) is an increasing sequence. The scaling property allows us to set the initial value w0 having same 2-norm with u, w0 = u = 1.

Figure 5 gives an example of a 5-dimensional H with condition number  = 2000. The GD and

MBNGD iteration are executed k = 5000 times where u and w0 are randomly chosen from the unit

sphere. The values of effective step size, loss

ek

2 H

and error

ek

are plotted. Furthermore, to

explore the performance of GD and MBNGD, the mean values over 300 random tests are given. It

is worth to note that, the geometric mean (G-mean) is more reliable than the arithmetic mean (A-

mean), where the geometric mean of x can be defined as exp(E(ln x)). Here the reliability means

that the G-mean converges quickly when the number of tests increase, however the A-mean does not

converge as quickly. In this example, the optimal convergence rate of MBNGD is observably better than GD. This acceleration phenomenon is ascribed to the pseudo-condition number of (H) being less than (H). However, if the difference between (pseudo-)condition number of H and H

is small, the acceleration is imperceptible.

Another important observation is that the BN significantly extends the range of `optimal' step size, which is embodied by the effective step size ^k having a large constant C in ^ = O(C-1). This means we can chose step size in BN at a large interval to get almost same or better convergence rate
than that of the best choice for GD.

Figure 6 gives an example of 100-dimension H with condition number  = 2000. Similar results as those in the 5-dimensional case are obtained. However, the best optimal convergence rate of MBNGD here has not noticeably improved compared with GD with the optimal learning rate, which is due to the fact that large d decrease the difference between eigenvalues of H and H.

Additional tests indicate that:

19

Under review as a conference paper at ICLR 2019
Figure 5: Plot of 300 random initial tests. H = diag(logspace(0,log10(2000),5)).
Figure 6: Plot of 500 random initial tests. H = diag(linspace(1,2000,100)). (1) larger dimensions leads to larger intervals of `optimal' step size, (Figure 7) (2) the effect of condition number on the `optimal' interval is small (Figure 8).
Figure 7: H = diag(linspace(1,2000,d)).
Figure 8: H = diag(linspace(1,cond,100)).
20


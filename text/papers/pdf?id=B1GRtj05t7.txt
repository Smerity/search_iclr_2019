Under review as a conference paper at ICLR 2019
REINFORCEMENT LEARNING: FROM TEMPORAL TO
SPATIAL VALUE DECOMPOSITION
Anonymous authors Paper under double-blind review
ABSTRACT
Value function, lying in the heart of reinforcement learning (RL), is defined as expected total reward to capture the long-term dependency between current state/action and delayed reward. Following this definition, it's natural to derive a temporal decomposition for value function as a sum of discounted reward on different time steps. Most existing RL algorithms estimate value function based on this temporal decomposition. Apart of traditional temporal decomposition, in this paper, we explore a novel spatial decomposition for value function. In our formulation, value function Q(s, a) is a weighted sum of reward R(s ), and the corresponding weight is defined as the discounted visiting frequency d(s, a, s ). Here, d(s, a, s ) is the key part that captures long-term dependency between current state/action (s/a) with future state s . This formulation inspires us to study d(s, a, s ) in depth, with function approximation as usual. Under the function approximation setting, the estimation of d(s, a, s ) can benefit from enhanced generalization between similar future state s , which thus results in a better estimation for Q(s, a). Accordingly, we propose spatial monte-carlo (SMC) method, and apply the idea of SMC to actor-critic algorithms with m-step advantage estimation and GAE. Extensive experiments on a couple of public environment have demonstrated that our algorithms yields satisfactory performance empirically.
1 INTRODUCTION
Model-free reinforcement learning (RL) has risen as a promising approach for solving arbitrary goal-directed sequential decision-making problems with only high-level reward signals but no explicit supervision. Since its recent extension to utilize large scale and deep neural networks to approximate policies and value functions, it has demonstrated a great success in solving a variety of difficult problems (Mnih et al., 2015; Schulman et al., 2015a; Silver et al., 2016; Lillicrap et al., 2015). In terms of specific learning algorithms, all model-free reinforcement learning methods can be categorized into three major classes, includeing value-based (Watkins & Dayan, 1992), policybased (Williams, 1992; Kakade, 2002; Schulman et al., 2015a), and actor-critic methods (Konda & Tsitsiklis, 2000; Peters & Schaal, 2008; Mnih et al., 2016). Among all these various methods, one of the essential parts is to estimate the Q-value, which represents the expected total reward. In practical value-based (e.g., DQN (Mnih et al., 2015), A3C (Mnih et al., 2016)) and actor-critic methods (e.g., DDPG (Lillicrap et al., 2015), TRPO (Schulman et al., 2015a), PPO (Schulman et al., 2017)), Q-value is often estimated by temporal different (TD) technique, which is however biased. In comparison, policy gradient methods (e.g., REINFORCE) compute an unbiased Q-value by using monte carlo (MC) methods.
Despite of diverse estimation methods, various RL algorithms formulate the value function based on the same idea of temporal decomposition, which follows the natural definition of value function and define the value function as a sum of reward over different time steps. For instance, TD methods formulate value function as a sum of expected reward in current time step and expected discount value function in next time step, and MC methods define value function as a sum of expected discount reward on all time steps. In this way, the reward is associated with time steps and shares the similar temporal decomposition of value over all RL algorithms.
1

Under review as a conference paper at ICLR 2019

Apart of existing temporal decomposition, we explore a new direction and introduce the spatial decomposition for value function in this paper. From the perspective of spatial decomposition, the value function Q(s, a) can be defined as a weighted sum of reward R(s ) over all state s in the state space, where the weight is specified by the discounted visiting frequency d(s, a, s ). Essentially, d(s, a, s ) is determined by the interaction between the policy and the dynamic environment and yields the capability of capturing long-term dependency between the action and delayed reward/state. This formulation inspires us to take deep understanding on spatial decomposition as well as obtain effective function approximation on d(s, a, s ) . An effective function approximation on d(s, a, s ) is quite crucial since it can enhance the generalization of the learned d(s, a, s ) when facing rarely-observed states.
However, directly learning d(s, a, s ) will lead to an intractable algorithm, since we need to sum over the whole state space. In order to derive an tractable algorithm, we convert the Q-value estimation to depend on sampled states s by introducing Gamma function (s, a, s ), which equals the average discounted visiting frequency of state s over all the trajectories in which s ever appears. Accordingly, the Q-value can be estimated by summing over all the states that appear in sampled trajectories based on  function. Following the above principles, we introduce spatial MC and apply the idea of spatial MC to actor-critic algorithms with m-step advantage estimation and GAE. Extensive experiments have demonstrated that such spatial decomposition is quite promising for Q-value estimation.
To summarize, our contributions are as follows:
1. We introduce a new perspective of value function estimation through spatial decomposition, other than existing temporal decomposition. In this way, the Q-value function is computed by summing over the state space instead of the time step. To our best knowledge, this work is the first attempt to investigate spatial value decomposition for reinforcement learning.
2. We identify a key role of such novel spatial view, i.e., (s, a, s ), which reflects the discounted state visiting frequency based on interaction between the environment and the agent. Moreover, an effective function approximation on (s, a, s ) can enable generalization of the learned  function.
3. To verify our idea, we demonstrate its application to policy gradient methods. We conduct experiments on both MuJoCo and Atari domain and show that our idea is quite promising.

2 Q-FUNCTION: TEMPORAL V.S. SPATIAL DECOMPOSITION

2.1 PRELIMINARIES

One of the key problem of reinforcement learning is to learn the policy towards delayed reward. That is, the action we take now may have some delayed consequence, so we cannot take action greedily according to current reward. To represent delay reward, value function is introduced to capture the long-term dependency between reward and action, by backing up future reward into current state(or state-action pair).

Specifically, value function is defined as the expected future reward starting from current state s or

state-action pair (s, a) and then following policy . By definition, there are several formulation for

value function.



V (s) = E

lrt+l st = s,  ,

(1)

l=0



Q(s, a) = E

lrt+l st = s, at = a,  ,

(2)

l=0

where   [0, 1] is a discount rate ( = 1 is allowed only in episodic tasks). When applying value function in actor-critic based methods, estimating Q-function over limited number of sampled trajectories may introduce high variance. One way to reduce the variance is replacing original Qfunction with an advantage function, which is defined as

A(s, a) = Q(s, a) - V (s)

(3)

2

Under review as a conference paper at ICLR 2019

Besides value function, another less used notation in RL is d(s) (Sutton et al., 2000), which repre-

sents the discounted weighting of states encountered starting at s0 and then following :


d(s) = kP r{sk = s|s0, }.

(4)

k=0

Here d(s) characterizes the statistics of encountered states in discounted setting under policy .

2.2 TEMPORAL EXPANSION FOR Q-FUNCTION

Following the definition as shown in Eq. 2, Q-function can be naturally expanded in a temporal way. The roll-out in time step is common in all RL algorithms, as shown in Figure 1. For instance, MonteCarlo (MC) methods count all time step in each episode, while temporal different (TD) methods count one step and then perform estimation on the next state. The formulation of MC and TD methods are shown in Eq. 5 and Eq. 6 respectively:

Q(s, a) = E rt + rt+1 + 2rt+2 + ... st = s, at = a,  ,

(5)

Q(s, a) = E rt + Q(st+1, at+1) st = s, at = a,  .

(6)

Note that MC is an unbiased estimation and suffers from high variance, while TD is biased with lower variance. Both MC and TD leverage the temporal structure of sampled trajectories  with using different discount value t on each time step (t = 0, 1, 2, ...), where the discount factor ( < 1) is used to ignore delayed effect of reward in the temporal structure. Since each reward is associated with time step in such temporal decomposition, the Q-function estimation is computed
by a sum of discounted reward in time line.

(, )

sum 1 2 3 4 k

Figure 1: Temporal value decomposition

2.3 SPATIAL EXPANSION FOR Q-FUNCTION

In this paper, we highlight another expansion of Q-function from a spatial perspective. Inspired by d(s) in Eq. 4, we introduce a new notation d(s, a, s ), which represents the discounted visiting
frequency of state s starting from s, taking action a and then following policy .



d(s, a, s ) = E

k-11{st+k = s |st = s, at = a, } ,

(7)

k=1

Based on this definition, we can rewrite the expectation of Q-value from spatial view:

Q(s, a) = d(s, a, s )R(s ),

(8)

s S

where R(s ) represents a scalar reward at state s .1 From Eq. 8, we can see that the sum is over the whole state space, i.e., we are counting in state space instead of temporal space. In this spatial

1Here we assume that reward function only depends on a single state. If reward function depends on previous state and action, then we can reformulate the MDP so that the state includes previous state and action.

3

Under review as a conference paper at ICLR 2019

formulation, the reward is associated with state. As shown in Figure 2, the Q-function is a weighted sum over rewards with respect to various states, where the weight is d(s, a, s ). In this new formulation, we care about which state leads to reward, instead of which time step leads to reward.
By using spatial decomposition, the value function is apart into the reward R(s ) and the state discounted visiting frequency d(s, a, s ). And, d(s, a, s ) can capture the long-term dependency between reward and action in RL, it become a critical problem to make function approximation of d(s, a, s ) to increase the generalization of learned policy.

(, )

sum d(, , )

1
2 3
···
k

State Space

Figure 2: Spatial value decomposition.

2.4 FUNCTION APPROXIMATION IN SPATIAL DECOMPOSITION

When learning a function approximation of d(s, a, s ), i.e., d, it remains challenging to derive Q-function directly from d, since the Q-function, defined by summing over the whole state space (even unseen state s' may also have a non-zero d(s, a, s ) and R(s )), is not tractable.

To solve this problem, it is natural to sum over all sampled state. In order to do so, we introduce Gamma function (s, a, s ), which is defined as follows:



(s, a, s ) = Es 

k-11{st+k = s |st = s, at = a, ,  } .

(9)

k=1

(s, a, s ) represents the estimation of discounted visiting frequency of states that appears in sam-

pled trajectories. By do so, we do not need to traverse in the whole state space and we can estimate

the state visiting frequency using sampled trajectories. We show in Appendix that the relation between d(s, a, s ) and (s, a, s ) is

d(s, a, s ) = E 1{s   } (s, a, s ).

(10)

Then we can use (s, a, s ) for the estimation of Q-value, by converting Eq. 8 to the following:

Q(s, a) = E

(s, a, s )R(s ) .

s 

(11)

The full derivation is shown in Appendix. As shown in Figure 3, (s, a, s ) can be estimated using sampled trajectories between state s and s .

(, )

11 21 12
1 2

···

31 22




Figure 3: Estimation of (s, a, s ) on sampled trajectories. 4

Under review as a conference paper at ICLR 2019

3 SPATIAL MONTE-CARLO METHODS WITH FUNCTION APPROXIMATION

Based on the definition of spatial decomposition, we introduce a new spatial monte-carlo (SMC) methods. We denote the parameter of Gamma function by , then the spatial monte-carlo value can be estimated by sampling n trajectories {1, 2, ..., n}:

Q^(s, a) = 1 n

^(s, a, s ; )R^(s ).

n

i=1 s i

Here (s, a, s ; ) is trainable function which is learned using on-policy data.

(12)

3.1 GENERALIZATION VIA FUNCTION APPROXIMATION
In this subsection, we want to discuss why we can benefit from the function approximation for (s, a, s ; ). In fact, without function approximation, SMC will achieve exactly the same estimation results with MC, as shown in Appendix B, since the difference between SMC and MC is the sum order of sampled rewards, which means SMC organizes the sampled rewards in a different view. On the other hand, with function approximation, the estimation result could be quite different. Particularly, assume that we have two similar states s and s with quite different number of observations in the sampled trajectories, e.g., s appears 100 times while s appears only 1 times. Under tabular setting, we can have a more accurate estimation (s, a, s ) for state s since we have more samples for s , while the estimation (s, a, s ) will be less accurate for state s since we have less samples for s . But, with function approximation, we can learn the spatial structure between similar states, since the general assumption behind function approximation is that, similar state s may have similar estimation of (s, a, s ). If this assumption holds well in our problem, then the function approximation enables generalization across similar s . In our example, we can generalize the more accurate (s, a, s ) estimation of state s to state s , and resulting in a better (s, a, s ) estimation for state s . Such generalization property of (s, a, s ) can help reduce variance for Q-value estimation, thus can help estimate Q-function better.
However, function approximation might introduce some bias. If the assumption above doesn't hold well, then we may have a bad estimation for the rarely observed state s . When SMC is combined with m-step TD method, we can trade-off the variance and bias by tuning the value of m. We demonstrate the effectiveness of our model by empirical experiments in section 4.

3.2 ALGORITHMS
While MC control so far has not been the state-of-the-art value-based algorithm, MC estimation is still widely used in actor-critic algorithms, especially when combined with m-step advantage estimation (Mnih et al., 2016) or GAE (Schulman et al., 2015b). Most of results in previous works (Schulman et al., 2015b; 2017; Wu et al., 2017) suggest that m-step TD methods are superior to direct MC estimation. The reason is that TD estimation suffers from much lower variance than MC estimation with the cost of introducing bias. To compare with the state-of-the-art actor-critic algorithms such as PPO (Schulman et al., 2017) which often adopts advantage estimator, we propose two actor-critic based algorithms which combine SMC methods with m-step advantage estimator and GAE, respectively. Our algorithms are built based on PPO. The pseudo codes of the two proposed algorithms are shown in Appendix.

SMC-based m-step Advantage Estimation The formulation of m-step advantage estimation is:

m-1

A(MmC) (s, a) = E

krt+k + mV (st+m) - V (s) st = s, at = a,  .

k=0

(13)

In this formulation, MC estimation is used in the first m steps. By substituting MC estimation with SMC estimation, we get the combined formulation:

A(SmM)C (s, a) = E

(s, a, s ; )R(s )+mV (st+m)-V (s) st = s, at = a,  .

s {st+1,...,st+m}

(14)

5

Under review as a conference paper at ICLR 2019

In this new formulation, m controls the scale of spatial expansion and V (s) represents the state-value function.

SMC-based GAE GAE is defined as an exponentially-weighted average of all m-step advantage estimators. In this subsection, we apply SMC to GAE. For notation simplicity, we denote A(MmC) (st, at) by AM(mC) and generalized advantage estimator GMC (st, at) by GMC . Then the formulation of GAE is:

G^MC = (1 - )(A^(M1)C + A^M(2)C + 2A^(M3)C + ...).

(15)

By replacing A(MmC) with A(SmM)C , we get the formulation of SMC-based GAE. G^SMC = (1 - )(A^S(1M) C + A^S(2M) C + 2A^(S3M) C + ...)

(16)

4 EXPERIMENT

In this section, we verify our proposed algorithms on a various of environments. We test the effectiveness of SMC-based m-step advantage estimator and SMC-based GAE. For the comparison between MC-based and SMC-based m-step advantage estimation, we conduct experiments on CartPole, Mujoco and Atari Pong game. For the comparison between MC-based and SMC-based GAE, we conduct experiments on Mujuco. We also conduct analysis for the value of m. Due to the limit of space, we show the results of Atari Pong game in Appendix.
Our algorithms are built based on PPO (Schulman et al., 2017). We call our first algorithm PPOSMC, which represents PPO with our proposed SMC estimation, and the second algorithm PPOGAE-SMC, which represents PPO with SMC-based GAE. We build policy network and value network following common practices (Schulman et al., 2015a; 2017). For CartPole and MuJoCo tasks, Gamma function is represented by two MLP hidden layers of 64 units and relu nonlinearities. For Atari Pong task, Gamma function shares all the parameters with policy network except the last layer. Our code is implemented based on OpenAI baselines (Dhariwal et al., 2017).

4.1 CARTPOLE

We conduct experiment in CartPole-v0 environment. In this environment, the dimension of observation and action are 4 and 2 respectively. The agent gets reward 1 for every step taken. The episode terminates when one of the condition meets: 1) pole angle is more than ±12 degree; 2) cart position is more than ±2.4; 3) episode length is greater than 200. Therefore, the maximum total reward is 200. We set m = 200 in this task, which means the agent leverages spatial structure in all sampled states. We use learning rate 0.01 for the Gamma-function learning. At each policy training iteration, we update the Gamma-function for 5 epochs.

The comparative results between PPO and PPO-SMC are shown in Figure 4. We find that PPO-SMC converges much faster and has much lower variance than PPO. In the contrast, PPO is unstable and suffer from high variance. This example demonstrate that the SMC estimation is much better than MC estimation.

Return

200.0 197.5 195.0 192.5 190.0 187.5 185.0 182.5 180.0 0.0

CartPole
PPO-SMC PPO
0.1 Tim0e.2step x0.31e6 0.4 0.5

Figure 4: The results are averaged across 10 experiments with different random seeds.

6

Under review as a conference paper at ICLR 2019

4.2 MUJOCO
We also conduct the robotic locomotion experiments using the MuJoCo simulator (Todorov et al., 2012). We set m = 32 for both PPO and PPO-SMC. For each policy update, we train the Gamma function with a learning rate 0.1 for 5 epochs.
The results are shown in Figure 5. In the Swimmer and HalfCheetah environments, PPO-SMC outperforms PPO, which illustrates that PPO-SMC can benefit from the generalization from Gamma function approximation and reduce variance for the value estimation. In the Walker2d environment, PPO-SMC does not demonstrate advantage over PPO. One possible reason is that the Gamma function learns not so well that introduces high bias in the value estimation.

Return Return Return

300 250 200 150 100 50
0 0.0

Swimmer
PPPPPPOOO--SSMQC--mm3322
0.2 Tim0e.4step x0.61e6 0.8 1.0

3000 2000 1000
0 0.0

HalfCheetah
PPPPPPOOO--SSMQC--mm3322
0.2 Tim0e.4step x0.61e6 0.8 1.0

3000 2500 2000 1500 1000 500
0 0.0

Walker2d
PPPPPPOOO--SSMQC-m-m3322
0.2 Tim0e.s4tep x0.16e6 0.8 1.0

Figure 5: Performance comparisons on three standard MuJoCo environments trained for 1 million time steps. The shaded region denotes the standard deviation over 5 random seeds.

GAE is one of the state-of-the-art algorithms that used to trade off variance and bias of policy gradient. Here we compare the performance between MC-based and SMC-based GAE. Since GAE uses exponentially-weighted averaging for all m-step advantage estimators, it pays less attention to the advantage estimators with larger m, which means that the benefit of spatial MC will be decayed when m becomes larger. Therefore, we use m = 16 in these experiments. We set the hyperparameter  in GAE to 0.95 which is well tuned in Schulman et al. (2015b). The results are shown in Figure 6. We find that PPO-GAE-SMC ourperforms PPO-GAE in all environments that we test. Even though PPO-SMC shows similar performance to PPO in Walker2d environment as shown in Figure 5, PPO-GAE-SMC can benefit a lot from SMC. This is because the Gamma function can be learned well and introduce less bias when the value of m becomes smaller.

Return Return Return

4000 3000 2000 1000
0
0.0

HalfCheetah
PPO-SQ-m16PPPOP-OG-ASEM-SCM-mC-1m616 PPOPPOP-OGAE
0.2 Tim0e.4step x0.61e6 0.8 1.0

3000 2500 2000 1500 1000 500
0 0.0

Hopper
PPO-SQ-m16PPO-GAE-SMC-m16 PPOPPO-GAE
0.2 Tim0e.4step x0.61e6 0.8 1.0

4000 3500 3000 2500 2000 1500 1000 500
0 0.0

Walker2d
PPO-SQ-m16PPO-GAE-SMC-m16 PPOPPO-GAE
0.2 Tim0e.4step x0.61e6 0.8 1.0

Figure 6

4.3 EFFECTS OF HYPER-PARAMETER
In this section, we conduct analysis for the effect of hypar-parameter m. The value of m controls the scale of SMC: when m becomes larger, the algorithm can get more benefits from SMC to reduce variance for value estimation with the cost of introducing bias from Gamma function. The result is shown in Figure 7. In the Hopper environment, we get better performance when increasing the value of m. While in the HalfCheetah and Walker2d environments, larger m will introduce much bias from Gamma function for value estimation, thus the performances drop when the value of m becomes larger. We find that the algorithm can perform consistently good when we set m = 16.
7

Under review as a conference paper at ICLR 2019

Return Return Return

4000 3000 2000 1000
0 1000 0.0

HalfCheetah
PPO-SQ-m8PPO-GAE-SMC-m8 PPO-SQ-m16PPO-GAE-SMC-m16 PPO-SQ-m24PPO-GAE-SMC-m24 PPO-SQ-m32PPO-GAE-SMC-m32
0.2 Tim0e.4step 0x.61e6 0.8 1.0

3000 2500 2000 1500 1000 500
0 0.0

Hopper
PPO-SQ-m8PPO-GAE-SMC-m8 PPO-SQ-m16PPO-GAE-SMC-m16 PPO-SQ-m24PPO-GAE-SMC-m24 PPO-SQ-m32PPO-GAE-SMC-m32
0.2 Tim0e.4step x0.61e6 0.8 1.0

4000 3500 3000 2500 2000 1500 1000 500
0 0.0

Walker2d
PPO-SQ-m8PPO-GAE-SMC-m8 PPO-SQ-m16PPO-GAE-SMC-m16 PPO-SQ-m24PPO-GAE-SMC-m24 PPO-SQ-m32PPO-GAE-SMC-m32
0.2 Tim0e.4step x0.61e6 0.8 1.0

Figure 7

5 RELATED WORKS
Baram et al. (2016) propose semi-aggregated markov decision process (SAMDP) model, with the purpose of better understanding of complex behaviours by identifying spatio-temporal abstractions. SAMDP is built in a transformed state-space that encodes the dynamics of the problem. It combines the advantages of skill-identifying MLP (SMDP) and state-aggregated MLP (AMLP) to form a spatio-temporal abstractions, which make the complex behaviours of RL agents more interpretable. Our spatial decomposition for value function is very different from their work. SAMDP focuses on the combination of state and skill abstractions, while our paper focuses on a spatial perspective on value function estimation, which has not yet been studied before.
Russell & Zimdars (2003) discuss Q-decomposition problem, wherein a complex agent is built from simpler sub-agents. In their setting, each sub-agent has its own reward function and runs its own reinforcement learning process. It supplies to a central arbitrator that selects an action maximizing the sum of Q-values from all the sub-agents. This form of agent decomposition allows the local Q-functions to be expressed by much-reduced state and action spaces. Our spatial decomposition method differs from their method in several aspects. Firstly, their Q-decomposition in state or action space falls into the category of multi-agent systems, which needs corporation or ensemble decision. Our proposed spatial Q-decomposition in this paper is a more general formulation that suitable for different RL settings. Secondly, our formulation aims to decouple dynamic system of the environment from the Q-value, which is not discussed in their paper.
Our method is also related to works about discount problem, even though there have been very limited works that study the discount problem. Franc¸ois-Lavet et al. (2015) discuss how to dynamically discount in deep reinforcement learning. Different from their work, our proposed method provides another perspective to revisit the role of discount value in reinforcement learning in a spatial way. We formulate the discount problem to a learning problem in spatial structure.
6 CONCLUSION
In this paper, we propose a novel spatial value decomposition for reinforcement learning, which splits the value function Q(s, a) into two parts: R(s ) that represents reward and d(s, a, s ) that captures discounted state visiting frequency. Such spatial decomposition further inspires us to study the d(s, a, s ) with function approximation. In particular, we introduce a novel RL algorithm via learning d(s, a, s ), which introduces bias but reduces variance. Extensive experiments demonstrate that we can benefit from learning d(s, a, s ). Further analysis implies that, the assumption behind function approximation, i.e. similar input (s, a, s ) pair has similar output d(s, a, s ), does hold well in our problem.
We believe spatial value decomposition is a quite interesting direction. In the future, there is still much more stuff to explore. For example, by splitting R(s ) that represents reward, and d(s, a, s ) that capture discounted state visiting frequency, we can study how to learn smooth d(s, a, s ) even when reward is highly non-smooth. Another interesting direction is how to use the spatial value decomposition into the RL problem with extremely sparse reward. While existing RL can learn nothing from zero-reward trajectories, our new algorithm enables us to learn d(s, a, s ) through zero-reward trajectories, which will further improve sample efficiency.
8

Under review as a conference paper at ICLR 2019
ACKNOWLEDGMENTS
Use unnumbered third level headings for the acknowledgments. All acknowledgments, including those to funding agencies, go at the end of the paper.
REFERENCES
Nir Baram, Tom Zahavy, and Shie Mannor. Spatio-temporal abstractions in reinforcement learning through neural encoding. 2016.
Prafulla Dhariwal, Christopher Hesse, Oleg Klimov, Alex Nichol, Matthias Plappert, Alec Radford, John Schulman, Szymon Sidor, and Yuhuai Wu. Openai baselines. https://github.com/ openai/baselines, 2017.
Vincent Franc¸ois-Lavet, Raphael Fonteneau, and Damien Ernst. How to discount deep reinforcement learning: Towards new dynamic strategies. arXiv preprint arXiv:1512.02011, 2015.
Sham M Kakade. A natural policy gradient. In Advances in neural information processing systems, pp. 1531­1538, 2002.
Vijay R Konda and John N Tsitsiklis. Actor-critic algorithms. In Advances in neural information processing systems, pp. 1008­1014, 2000.
Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. arXiv preprint arXiv:1509.02971, 2015.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control through deep reinforcement learning. Nature, 518(7540):529­533, 2015.
Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement learning. In International conference on machine learning, pp. 1928­1937, 2016.
Jan Peters and Stefan Schaal. Natural actor-critic. Neurocomputing, 71(7-9):1180­1190, 2008.
Stuart J Russell and Andrew Zimdars. Q-decomposition for reinforcement learning agents. In Proceedings of the 20th International Conference on Machine Learning (ICML-03), pp. 656­663, 2003.
John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region policy optimization. In International Conference on Machine Learning, pp. 1889­1897, 2015a.
John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel. Highdimensional continuous control using generalized advantage estimation. arXiv preprint arXiv:1506.02438, 2015b.
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.
David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering the game of go with deep neural networks and tree search. Nature, 529(7587):484­489, 2016.
Richard S Sutton, David A McAllester, Satinder P Singh, and Yishay Mansour. Policy gradient methods for reinforcement learning with function approximation. In Advances in neural information processing systems, pp. 1057­1063, 2000.
Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control. In Intelligent Robots and Systems (IROS), 2012 IEEE/RSJ International Conference on, pp. 5026­ 5033. IEEE, 2012.
Christopher JCH Watkins and Peter Dayan. Q-learning. Machine learning, 8(3-4):279­292, 1992.
9

Under review as a conference paper at ICLR 2019 Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement
learning. Machine learning, 8(3-4):229­256, 1992. Yuhuai Wu, Elman Mansimov, Roger B Grosse, Shun Liao, and Jimmy Ba. Scalable trust-region
method for deep reinforcement learning using kronecker-factored approximation. In Advances in neural information processing systems, pp. 5285­5294, 2017.
10

Under review as a conference paper at ICLR 2019

A DERIVATION FOR SPATIAL VALUE DECOMPOSITION

Expectation of Q-value in temporal view:

Q(s, a) = E [ k-1rt+k|st = s, at = a, ]
k=1

(17)

To get the expectation of Q-value from spatial view, firstly, we use d(s, a, s ) to represent the discounted weighting of states encountered starting at st = s, at = a and then following policy .



d(s, a, s ) = E

k-11{st+k = s |st = s, at = a, }

k=1

Based on this definition, we can rewrite the expectation of Q-value in spatial view:

(18)

Q(s, a) = d(s, a, s )R(s )

(19)

s S

In this spatial formulation, we need to traverse all states, which is intractable in actual implemen-

tation. Here we introduce Gamma function (s, a, s ), which represents the expectation of the

discounted weighting of states s under the distribution of sampled trajectories. The expectation of

Gamma function is:



(s, a, s ) = Es 

k-11{st+k = s |st = s, at = a, ,  } .

(20)

k=1

Based on the Gamma function, we get:



d(s, a, s ) = E

k-11{st+k = s |st = s, at = a, }

k=1



= E

1{s   }k-11{st+k = s |st = s, at = a, ,  }

k=1



+ E

1{s   }k-11{st+k = s |st = s, at = a, ,  }

k=1



= E

1{s   }k-11{st+k = s |st = s, at = a, ,  }

(21)

k=1



= E 1{s   } Es 

1{s   }k-11{st+k = s |st = s, at = a, ,  }

k=1



= E 1{s   } Es 

k-11{st+k = s |st = s, at = a, ,  }

k=1
= E 1{s   } (s, a, s )

By substituting d(s, a, s ) using Eq. 21, we get:

Q(s, a) = d(s, a, s )R(s )

s S

= E 1{s   } (s, a, s )R(s )
s S

= E

1{s  i}(s, a, s )R(s )

s S

= E

(s, a, s )R(s )

s 

Afterward, we can estimate the Q-value using (s, a, s ) based on the sampled trajectories.

(22)

11

Under review as a conference paper at ICLR 2019

B EQUIVALENCE BETWEEN TEMPORAL AND SPATIAL FORMULATION
Temporal Q-value expectation:



Q(s, a) = E

 k-1 rt+k

k=1

Spatial Q-value expectation:



= E

 k-1 R(st+k )

k=1

Q(s, a) = d(s, a, s )R(s )

s S

= E = E = E = E = E

(s, a, s )R^(s )

s 



Es 

k-11{st+k = s |st = s, at = a, ,  } R(s )

s 

k=1



Es 

k-1R(s )1{st+k = s |st = s, at = a, ,  }

s 

k=1



Es 

k-1R(st+k)1{st+k = s |st = s, at = a, ,  }

s 

k=1


 k-1 R(st+k )

k=1

(23) (24)

C ADDITIONAL EXPERIMENTS

C.1 ATARI GAME

We also conduct experiments on Atari domain. Due to the limit of time and resource, we only choose the commonly used Pong environment. We show the training result in Figure 8. Empirically, we find that our spatial formulation can help to reduce variance of training process and make the training policy more robust.

Return

Pong
20

10

0

10
20 0

PPPPPPPPOOOO--SSMQC-m-m3322
1 Ti2meste3p x 14e6 5 6

Figure 8: The curves are averaged on 3 different random seeds.

C.2 EXTENSION TO OTHER RL ALGORITHMS
We extend our proposed method to other RL algorithms (e.g. TRPO) and find that TRPO can also benefit a lot from this spatial decomposition. We believe that our proposed method can be leveraged by other RL algorithms.
12

Under review as a conference paper at ICLR 2019

Return Return

HalfCheetah

1000 750

TTRRPPOO-S-SMQC--mm1166 TTRRPPOO

500

250

0

250

500

750
0.0 0.2 Tim0e.4step x0.61e6 0.8

1.0

2500 2000 1500 1000 500
0 0.0

Hopper
TRPO-SMC-m16 TRPO
0.2 Tim0e.4step x0.61e6 0.8 1.0

Figure 9

D ALGORITHMS

Algorithm 1 SMC-based m-step advantage estimation for Policy Optimization

1: Initialize policy parameter 0, value-function parameter 0, and Gamma-function parameter 0

2: for k = 0, 1, 2, ... do

3: Simulate current policy k until M timesteps are obtained.

4:

Compute Gamma function loss: L =

T -1 t=1

s {st+1,...,st+m}(k (st, at, s ) -

^(st, at, s ))2

5: Update distance function parameter using Adam optimizer: k+1 = k + L

6: Compute A^(SmM)C (st, at) at all timesteps. 7: Compute k+1 and k+1.

8: end for

Algorithm 2 SMC-based GAE for Policy Optimization

1: Initialize policy parameter 0, value-function parameter 0, and Gamma-function parameter 0

2: for k = 0, 1, 2, ... do

3: Simulate current policy k until M timesteps are obtained.

4:

Compute Gamma function loss: L =

T -1 t=1

s {st+1,...,st+m}(k (st, at, s ) -

^(st, at, s ))2

5: Update distance function parameter using Adam optimizer: k+1 = k + L

6: Compute G^SMC (st, at) at all timesteps.

7: Compute k+1 and k+1.

8: end for

13


Under review as a conference paper at ICLR 2019
A THEORETICAL FRAMEWORK FOR DEEP LOCALLY CONNECTED RELU NETWORK
Anonymous authors Paper under double-blind review
ABSTRACT
Understanding theoretical properties of deep and locally connected nonlinear network, such as deep convolutional neural network (DCNN), is still a hard problem despite its empirical success. In this paper, we propose a novel theoretical framework for such networks with ReLU nonlinearity. The framework explicitly formulates data distribution, favors disentangled representations and is compatible with common regularization techniques such as Batch Norm. The framework is built upon teacher-student setting, by expanding the student forward/backward propagation onto the teacher's computational graph. The resulting model does not impose unrealistic assumptions (e.g., Gaussian inputs, independence of activation, etc). Our framework could help facilitate theoretical analysis of many practical issues, e.g. overfitting, generalization, disentangled representations in deep networks.
1 INTRODUCTION
Deep Convolutional Neural Network (DCNN) has achieved a huge empirical success in multiple disciplines (e.g., computer vision (Krizhevsky et al., 2012; Simonyan & Zisserman, 2014; He et al., 2016), Computer Go (Silver et al., 2016; 2017; Tian & Zhu, 2016), and so on). On the other hand, its theoretical properties remain an open problem and an active research topic.
Learning deep models are often treated as non-convex optimization in a high-dimensional space. From this perspective, many properties in deep models have been analyzed: landscapes of loss functions (Choromanska et al., 2015b; Li et al., 2017; Mei et al., 2016), saddle points (Du et al., 2017; Dauphin et al., 2014), relationships between local minima and global minimum (Kawaguchi, 2016; Hardt & Ma, 2017; Safran & Shamir, 2017), trajectories of gradient descent (Goodfellow et al., 2014), path between local minima (Venturi et al., 2018), etc.
However, two components are missing: such a modeling does not consider specific network structure and input data distribution, both of which are critical factors in practice. Empirically, deep models work particular well for certain forms of data (e.g., images); theoretically, for certain data distribution, popular methods like gradient descent is shown to be unable to recover the network parameters (Brutzkus & Globerson, 2017).
Along this direction, previous theoretical works assume specific data distributions like spherical Gaussian and focus on shallow nonlinear networks (Tian, 2017; Brutzkus & Globerson, 2017; Du et al., 2018). These assumptions yield nice forms of gradient to enable analysis of many properties such as global convergence, which makes it nontrivial to extend to deep nonlinear neural networks that yield strong empirical performance.
In this paper, we propose a novel theoretical framework for deep locally connected ReLU network that is applicable to general data distributions. Specifically, we embrace a teacher-student setting: the teacher generates classification label via a hidden computational graph, and the student updates the weights to fit the labels with gradient descent. Then starting from gradient descent rule, we marginalize out the input data conditioned on the graph variables of the teacher at each layer, and arrive at a reformulation that (1) captures data distribution as explicit terms and leads to more interpretable model, (2) compatible with existing state-of-the-art regularization techniques such as Batch Normalization (Ioffe & Szegedy, 2015), and (3) favors disentangled representation when data distributions have factorizable structures. To our best knowledge, our work is the first theoretical framework to achieve these properties for deep and locally connected nonlinear networks.
1

Under review as a conference paper at ICLR 2019

(a) (b)



(c)

(d)

Locally connected Neural Network

image

Backpropagation

Figure 1: Problem Setting. (a) Locally connected network, (b) the receptive fields of each node. (c) notations used in backpropagation. (d) nodes with same receptive fields are grouped (Eqn. 56).

Previous works have also proposed framework to explain deep networks, e.g., renormalization group for restricted Boltzmann machines (Mehta & Schwab, 2014), spin-glass models (Amit et al., 1985; Choromanska et al., 2015a), transient chaos models (Poole et al., 2016), differential equations (Su et al., 2014; Saxe et al., 2013). In comparison, our framework (1) imposes mild assumptions rather than unrealistic ones (e.g., independence of activations), (2) explicitly deals with back-propagation which is the dominant approach used for training in practice, (3) considers spatial locality of neurons, an important component in practical deep models, and (4) models data distribution explicitly.
The paper is organized as follows: Sec. 2 introduces a novel approach to model locally connected networks. Sec. 3 introduces the teacher-student setting and label generation, followed by the proposed reformulation. Sec. 4 gives one novel finding that Batch Norm is a projection onto the orthogonal complementary space of neuron activations, and the reformulation is compatible with it. Sec. 5 shows a few applications of the framework, e.g., why nonlinearity is helpful, how factorization of the data distribution leads to disentangled representation and other issues.

2 BASIC FORMULATION

2.1 GENERAL SETTING

In this paper, we consider multi-layer (deep) network with ReLU nonlinearity. We consider supervised setting, in which we have a dataset {(x, y)}, where x is the input image and y is its label
computed from x in a deterministic manner. Sec. 3 describes how x maps to y in details.

We consider a neuron (or node) j. Denote fj as its activation after nonlinearity and gj as the (input) gradient it receives after filtered by ReLU's gating. Note that both fj and gj are deterministic functions of the input x and label y. Since y is a deterministic function of x, we can write fj = fj(x) and gj = gj(x). Note that all analysis still holds with bias terms. We omit them for brevity.
The activation fj and gradient gk can be written as (note that fj is the binary gating function):

fj(x) = fj(x)

wjkfk(x), gk(x) = fk(x)

wjkgj (x)

kch(j)

jpa(k)

(1)

And the weight update for gradient descent is:

wjk = Ex [fk(x)gj(x)]

(2)

Here is the expectation is with respect to a training dataset (or a batch), depending on whether GD or SGD has been used. We also use fjraw and gjraw as the counterpart of fj and gj before nonlinearity.

2.2 LOCALLY CONNECTED NETWORK
Locally connected networks have extra structures, which leads to our reformulation. As shown in Fig. 1, each node j only covers one part of the input images (i.e., receptive field). We use Greek letters {, , . . . , } to represent receptive fields. For a region , x is the content in that region. j   means node j covers the region of  and n is the number of nodes that cover the same region (e.g., multi-channel case). The image content is x(j), abbreviated as xj if no ambiguity. The parent j's receptive field covers its children's. Finally,  represents the entire image.

2

Under review as a conference paper at ICLR 2019

By definition, the activation fj of node j is only dependent on the region xj, rather than the entire image x. This means that fj(x) = fj(xj) and fj(xj) = fj(xj) k wjkfk(xk). However, the gradient gj is determined by the entire image x, and its label y, i.e., gj = gj(x, y). Note that here we assume that the label y is a deterministic (but unknown) function of x. Therefore, for gradient
we just write gj = gj(x).

2.3 MARGINALIZED GRADIENT

Given the structure of locally connected network, the gradient gj has some nice structures. From
Eqn. 2 we knows that wjk = Ex [fk(x)gj(x)] = Exk fk(xk)Ex-k|xk [gj(x)] . Define x-k = x\xk as the input image x except for xk. Then we can define the marginalized gradient:

gj (xk) = Ex-k|xk [gj (x)]

(3)

as the marginalization (average) of x-k, while keep xk fixed. With this notation, we can write wjk = Exk [fk(xk)gj (xk)].

On the other hand, the gradient which back-propagates to a node k can be written as

gk(x) = fk(x)

wjkgj (x) = fk(xk) wjkgj (x)

jpa(k)

j

(4)

where fk is the derivative of activation function of node k (for ReLU it is just a gating function). If we take expectation with respect to x-k|xk on both side, we get

gk(xk) = fk(xk)gkraw(xk) = fk(xk)

wjkgj (xk)

jpa(k)

(5)

Note that all marginalized gradients gj(xk) are independently computed by marginalizing with respect to all regions that are outside the receptive field xk. Interestingly, there is a relationship between these gradients that respects the locality structure:
Theorem 1 (Recursive Property of marginalized gradient). gj(xk) = Exj,-k|xk [gj(xj)]

This shows that the marginal gradient has recursive structure: we can first compute gj(xj) for top node j, then by marginalizing over the region within xj but outside xk, we get its projection gj(xk) on child k, then by Eqn. 5 we collect all projections from all the parents of node k, to get gk(xk). This procedure can be repeated until we arrive at the leaf nodes.

3 TEACHER-STUDENT SETTING

In order to analyze the behavior of neural network under backpropagation (BP), one needs to make assumptions about how the input x is generated and how the label y is related to the input x. Previous works assume Gaussian inputs and shallow networks, which yields analytic solution to gradient (Tian, 2017; Du et al., 2018), but might not align well with the practical data distribution.

3.1 THE SUMMARIZATION LADDER
We consider a multi-layered deterministic function as the teacher (not necessary a neural network). For each region x, there is a latent discrete summarization variable z that captures the information of the input x. Furthermore, we assume z = z(x) = z({z}ch()), i.e., the summarizaion only relies on the ones in the immediate lower layer. In particular, the top level summarization is the label of the image, y = z, where  represents the region of the entire image. We call a particular assignment of z, z = a, an event. Finally, m is how many values z can take.
During training, all summarization functions Z = {z} are unknown except for the label y.

3.2 FUNCTION EXPANSION ON SUMMARIZATION

Let's consider the following quantity. For each neural node j, we want to compute the expected gradient given a particular factor z, where  = rf(j) (the reception field of node j):

gj (z)  EXj|z [gj (Xj )] = gj (xj )P(xj |z)dxj

(6)

3

Under review as a conference paper at ICLR 2019

F, G~, D
W P

Dimension
m-by-n n -by-n m-by-m

Description
Activation fj(z), gradient g~j(z) and gating prob fj(z) at group . Weight matrix that links group  and  Prob P(z|z) of events at group  and 

Table 1: Matrix Notation. See Eqn. 56.

And g~j (z) = gj (z)P(z). Similarly, fj (z) = EXj|z [fj (Xj )] and fj (z) = EXj|z fj (Xj ) .
Note that P(xj|z) is the frequency count of xj for z. If z captures all information of xj, then P(xj|z) is a delta function. Throughout the paper, we use frequentist interpretation of probabilities.
Intuitively, if we have gj(z = a) > 0 and gj(z = a) < 0, then the node j learns about the hidden event z = a. For multi-class classification, the top level nodes (just below the softmax layer) already embrace such correlations (here j is the class label):

gj(y = j) > 0, gj(y = j) < 0,

(7)

where we know z = y is the top level factor. A natural question now arises:

Does gradient descent automatically push gj(z) to be correlated with the factor z?

If this is true, then gradient descent on deep models is essentially a weak-supervised approach that automatically learns the intermediate events at different levels. Giving a complete answer of this question is very difficult and is beyond the scope of this paper. Here, we aim to build a theoretical framework that enables such analysis. We start with the relationship between neighboring layers:
Theorem 2 (Reformulation). Denote  = rf(j) and  = rf(k). k is a child of j. If the following conditions hold:

· Focus of knowledge. P(xk|z, z) = P(xk|z). · Broadness of knowledge. P(xj|z, z) = P(xj|z). · Decorrelation. Given z, (gkraw(·) and fk(·)) and (fkraw(·) and fk(·)) are uncorrelated. Then the following iterative equations hold:

fj(z) = fj(z)

wjkEz|z [fk(z)] , gk(z) = fk(z)

wjkEz|z [gj (z)]

kch(j)

jpa(k)

(8)

One key property of this formulation is that, it incorporates data distributions P(z, z) into the gradient descent rules. This is important since running BP on different dataset is now formulated into the same framework with different probability, i.e., frequency counts of events. By studying which family of distribution leads to the desired property, we could understand BP better.
For completeness, we also need to define boundary conditions. In the lowest level L, we could treat each input pixel (or a group of pixels) as a single event. Therefore, fk(z) = I [k = z]. On the top level, as we have discussed, Eqn. 7 applies and gj(z) = a1I [j = z] - a2I [j = z].
The following theorem shows that the reformulation is exact if z has all information of the region.
Theorem 3. If P(xj|z) is a delta function for all , then all conditions in Thm. 2 hold.
In general, P(xj|z) is a distribution encoding how much information gets lost if we only know the factor z. When we climb up the ladder, we lose more and more information while keeping the critical part for the classification. This is consistent with empirical observations (Bau et al., 2017), in which the low-level features in DCNN are generic, and high-level features are more class-specific.

3.3 MATRIX FORMULATION
Eqn. 8 can be hard to deal with. If we group the nodes with the same reception field at the same level together (Fig. 1(d)), we have the matrix form ( is element-wise multiplication):

4

Under review as a conference paper at ICLR 2019

(a) Sub-layers in BN layer

subspace (b) Projected Gradient

Figure 2: Batch Normalization (BN) as a projection. (a) Three sublayers in BN (zero-mean, unit-
variance, affine). (b) The gradient gf that is propagated down is a projection of input gradient g onto the orthogonal complementary space spanned by {f , 1}.

Theorem 4 (Matrix Representation of Reformulation).

F = D 

PFW, G~ = D 

PT G~WT,

ch()

pa()

W = (PF)T G~ (9)

See Tbl. 2 for the notation. For this dynamics, we want F = In , i.e., the top n neurons faithfully represents the classification labels. Therefore, the top level gradient is G = In - F. On the
other side, for each region  at the bottom layer, we have F = In , i.e., the input contains all the preliminary factors. For all regions  in the top-most and bottom-most layers, we have n = m.

4 BATCH NORMALIZATION UNDER REFORMULATION
Our reformulation naturally incorporates empirical regularization technique like Batch Normalization (BN) (Ioffe & Szegedy, 2015).

4.1 BATCH NORMALIZATION AS A PROJECTION

We start with a novel finding of Batch Norm: the back-propagated gradient through Batch Norm layer at a node j is a projection onto the orthogonal complementary subspace spanned by all one vectors and the current activations of node j.

Denote pre-batchnorm activations as f (i) = fj(xi) (i = 1 . . . N ). In Batch Norm, f (i) is whitened to be f~(i), then linearly transformed to yield the output f¯(i):

f^(i) = f (i) - µ, f~(i) = f^(i)/, f¯(i) = c1f~(i) + c0

(10)

where

µ

=

1 N

i f (i)

and

2

=

1 N

i(f (i) - µ)2 and c1, c0 are learnable parameters.

The original Batch Norm paper derives complicated and unintuitive weight update rules. With vector notation, the update has a compact form with clear geometric meaning.

Theorem 5 (Backpropagation of Batch Norm). For a top-down gradient g, BN layer gives the

following gradient update (Pf,1 is the orthogonal complementary projection of subspace {f , 1}):

gf

=

J BN (f )g

=

c1 

Pf,1g,

gc = S(f )T g

(11)

Intuitively, the back-propagated gradient JBN (f )g is zero-mean and perpendicular to the input activation f of BN layer, as illustrated in Fig. 2. Unlike (Kohler et al., 2018) that analyzes BN in an approximate manner, in Thm. 5 we do not impose any assumptions.

4.2 BATCH NORM UNDER THE REFORMULATION

The analysis of Batch Norm is compatible with the reformulation and we arrive at similar backprop-

agation rule, by noticing that Ex [fj(x)] = Ez [fj(z)]: µ = Ez [fj ] , 2 = Ez (fj (z) - µ)2 ,

J BN (f )

=

c1 

Pf,1

(12)

Note that we still have the projection property, but under the new inner product fj, gj z =

Ez [fj(z)gj(z)] and norm

f

z =

f, f

1/2 z

.

5

Under review as a conference paper at ICLR 2019

(a) (b)

(c)

(d) 0
0

0 0

Figure 3: disentangled representation. (a) Nodes are grouped according to regions. (b) An example
of one parent region  (2 nodes) and two child regions 1 and 2 (5 nodes each). We assume factorization property of data distribution P . (c) disentangled activations, (d) Separable weights.

5 EXAMPLE APPLICATIONS OF PROPOSED THEORETICAL FRAMEWORK

With the help of the theoretical framework, we now can analyze interesting structures of gradient descent in deep models, when the data distribution P(z, z) satisfies specific conditions. Here we give two concrete examples: the role played by nonlinearity and in which condition disentangled representation can be achieved. Besides, from the theoretical framework, we also give general comments on multiple issues (e.g., overfitting, GD versus SGD) in deep learning.

5.1 NONLINEAR VERSUS LINEAR

In the formulation, m is the number of possible events within a region , which is often exponential with respect to the size sz() of the region. The following analysis shows that a linear model cannot handle it, even with exponential number of nodes n, while a nonlinear one with ReLU can.
Definition 1 (Convex Hull of a Set). We define the convex hull Conv(P ) of m points P  Rn to be Conv(P ) = P a, a  n-1 , where n-1 = {a  Rn, ai  0, i ai = 1}. A row pj is called vertex if pj / Conv(P \pj).
Definition 2. A matrix P of size m-by-n is called k-vert, or vert(P ) = k  m, if its k rows are vertices of the convex hull generated by its rows. P is called all-vert if k = m.

Theorem 6 (Expressibility of ReLU Nonlinearity). Assuming m = n = O(exp(sz())), where sz() is the size of receptive field of . If each P is all-vert, then: ( is top-level receptive field)

min LossReLU(W ) = 0, min LossLinear(W ) = O(exp(sz()))
WW

(13)

Note that here Loss(W ) 

F - I

2 F

.

This shows the power of nonlinearity, which guarantees

full rank of output, even if the matrices involved in the multiplication are low-rank. The following

theorem shows that for intermediate layers whose input is not identity, the all-vert property remains.

Theorem 7. (1) If F is full row rank, then vert(P F ) = vert(P ). (2) P F is all-vert iff P is all-vert.

This means that if all P are all-vert and its input F is full-rank, then with the same construction of Thm. 6, F can be made identity. In particular, if we sample W randomly, then with probability 1, all F are full-rank, in particular the top-level input F1. Therefore, using top-level W1 alone would be sufficient to yield zero generalization error, as shown in the previous works that random
projection could work well.

5.2 DISENTANGLED REPRESENTATION
The analysis in Sec. 5.1 assumes that n = m, which means that we have sufficient nodes, one neuron for one event, to convey the information forward to the classification level. In practice, this is never the case. When n m = O(exp(sz())) and the network needs to represent the information in a proper way so that it can be sent to the top level. Ideally, if the factor z can be written down as a list of binary factors: z = z[1], z[2], . . . , z[j] , the output of a node j could represent z[j], so that all m events can be represented concisely with n nodes.

6

Under review as a conference paper at ICLR 2019

To come up with a complete theory for disentangled representation in deep nonlinear network is far from trivial and beyond the scope of this paper. In the following, we make an initial attempt by constructing factorizable P so that disentangled representation is possible in the forward pass. First we need to formally define what is disentangled representation:
Definition 3. The activation F is disentangled, if its j-th column F,:j = 1  . . .f[j] . . .1, where each f[j] and 1 is a 2-by-1 vector.
Definition 4. The gradient G~ is disentangled, if its j-th column G~,:j = p[1]  . . .g~[j]. . . p[n], where p[j] = [P([j] = 0), P([j] = 1)]T and g~[j] is a 2-by-1 vector.
Intuitively, this means that each node j represents the binary factor z[j]. A follow-up question is whether such disentangled properties carries over layers in the forward pass. It turns out that the disentangled structure carries if the data distribution and weights have compatible structures:
Definition 5. The weights W is separable with respect to a disjoint set {Si}, if W = diag W[S1, 1], W[S2, 2], . . . , W[Sn, n] .
Theorem 8 (Disentangled Forward). If for each   ch(), P can be written as a tensor product P = i P[i][Si] where {Si} are -dependent disjointed set, W is separable with respect to {Si}, F is disentangled, then F is also disentangled (with/without ReLU /Batch Norm).
If the bottom activations are disentangled, by induction, all activations will be disentangled. The next question is whether gradient descent preserves such a structure. The answer is also conditionally yes:
Theorem 9 (Separable Weight Update). If P = i P[i][Si], F and G~ are both disentangled, 1T G~ = 0, then the gradient update W is separable with respect to {Si}.
Therefore, with disentangled F and G~ and centered gradient 1T G~ = 0, the separable structure is conserved over gradient descent, given the initial W(0) is separable. Note that centered gradient is guaranteed if we insert Batch Norm (Eqn. 12) after linear layers. And the activation F remains disentangled if the weights are separable.
The hard part is whether G~ remains disentangled during backpropagation, if {G~}pa() are all disentangled. If so, then the disentangled representation is self-sustainable under gradient descent. This is a non-trivial problem and generally requires structures of data distribution. We put some discussion in the Appendix and leave this topic for future work.

5.3 EXPLANATION OF COMMON BEHAVIORS IN DEEP LEARNING

In the proposed formulation, the input x in Eqn. 8 is integrated out, and the data distribution is now encoded into the probabilistic distribution P(z, z), and their marginals. A change of such distribution means the input distribution has changed. For the first time, we can now analyze many
practical factors and behaviors in the DL training that is traditionally not included in the formulation.

Over-fitting. Given finite number of training samples, there is always error in estimated factor-factor distribution P~(z, z) and factor-observation distribution P~(x|z). In some cases, a slight change of distribution would drastically change the optimal weights for prediction, which is overfitting.

Here is one example. Suppose there are two different kinds of events at two disjoint reception fields: z and z. The class label is z, which equals z but is not related to z. Therefore, we have:

P~(z = 1|z = 1) = 1, P~(z = 1|z = 0) = 0

(14)

Although z is unrelated to the class label z, with finite samples z could show spurious correlation:

P~(z = 1|z = 1) = 0.5 + , P~(z = 1|z = 0) = 0.5 -

(15)

On the other hand, as shown in Fig. 4, P(x|z) contains a lot of detailed structures and is almost impossible to separate in the finite sample case, while P(x|z) could be well separated for z = 0/1. Therefore, for node j with rf(j) = , fj(z)  constant (input almost indistinguishable):

wj = Ez [fj (z)g0(z)]  0

(16)

7

Under review as a conference paper at ICLR 2019

Input image Class label Intermediate stage label

is the discriminative factor.
Figure 4: Overfitting Example

is a noisy factor.

where g0(z) = Ez|z [g0(z)] =

1 -1

z = 1 z = 0

, which is a strong gradient signal backpropa-

gated from the top softmax level, since z is strongly correlated with z. For node k with rf(k) = , an easy separation of the input (e.g., random initialization) yields distinctive fk(z). Therefore,

wk = Ez [fj (z )g0(z )] > 0

(17)

where g0(z ) = Ez|z [g0(z)] =

2 -2

z = 1 z = 0

, a weak signal because of z

is (almost)

unrelated to the label. Therefore, we see that the weight wj that links to meaningful receptive

field z does not receive strong gradient, while the weight wk that links to irrelevant (but spurious)

receptive field z receives strong gradient. This will lead to overfitting.

With more data, over-fitting is alleviated since (1) P~(z|z) becomes more accurate and  0; (2) P~(x|z) starts to show statistical difference for z = 0/1 and thus fj(z) shows distinctiveness.

Note that there exists a second explanation: we could argue that z is a true but weak factor that contributes to the label, while z is a fictitious discriminative factor, since the appearance difference between z = 0 and z = 1 (i.e., P~(x|z) for  = 0/1) could be purely due to noise and thus should be neglected. With finite number of samples, these two cases are essentially indistinguish-
able. Models with different induction bias might prefer one to the other, yielding drastically different
generalization error. For neural network, SGD prefers the second explanation but if under the pres-
sure of training, it may also explore the first one by pushing gradient down to distinguish subtle
difference in the input. This may explain why the same neural networks can fit random-labeled data,
and generalize well for real data (Zhang et al., 2016).

Gradient Descent: Stochastic or not? Previous works (Keskar et al., 2017) show that empirically stochastic gradient decent (SGD) with small batch size tends to converge to "flat" minima and offers better generalizable solution than those uses larger batches to compute the gradient.

From our framework, SGD update with small batch size is equivalent to using a perturbed/noisy version of P(z, z) at each iteration. Such an approach naturally reduces aforementioned over-fitting issues, which is due to hyper-sensitivity of data distribution and makes the final weight solution invariant to changes in P(z, z), yielding a "flat" solution.

6 CONCLUSION AND FUTURE WORK
In this paper, we propose a novel theoretical framework for deep (multi-layered) nonlinear network with ReLU activation and local receptive fields. The framework utilizes the specific structure of neural networks, and formulates input data distributions explicitly. Compared to modeling deep models as non-convex problems, our framework reveals more structures of the network; compared to recent works that also take data distribution into considerations, our theoretical framework can model deep networks without imposing idealistic analytic distribution of data like Gaussian inputs or independent activations. Besides, we also analyze regularization techniques like Batch Norm, depicts its underlying geometrical intuition, and shows that BN is compatible with our framework.
Using this novel framework, we have made an initial attempt to analyze many important and practical issues in deep models, and provides a novel perspective on overfitting, generalization, disentangled representation, etc. We emphasize that in this work, we barely touch the surface of these core issues in deep learning. As a future work, we aim to explore them in a deeper and more thorough manner, by using the powerful theoretical framework proposed in this paper.

8

Under review as a conference paper at ICLR 2019
REFERENCES
Daniel J Amit, Hanoch Gutfreund, and Haim Sompolinsky. Spin-glass models of neural networks. Physical Review A, 32(2):1007, 1985.
David Bau, Bolei Zhou, Aditya Khosla, Aude Oliva, and Antonio Torralba. Network dissection: Quantifying interpretability of deep visual representations. CVPR, 2017.
Alon Brutzkus and Amir Globerson. Globally optimal gradient descent for a convnet with gaussian inputs. ICML, 2017.
Anna Choromanska, Mikael Henaff, Michael Mathieu, Ge´rard Ben Arous, and Yann LeCun. The loss surfaces of multilayer networks. In Artificial Intelligence and Statistics, pp. 192­204, 2015a.
Anna Choromanska, Yann LeCun, and Ge´rard Ben Arous. Open problem: The landscape of the loss surfaces of multilayer networks. In Conference on Learning Theory, pp. 1756­1760, 2015b.
Yann N Dauphin, Razvan Pascanu, Caglar Gulcehre, Kyunghyun Cho, Surya Ganguli, and Yoshua Bengio. Identifying and attacking the saddle point problem in high-dimensional non-convex optimization. In Advances in neural information processing systems, pp. 2933­2941, 2014.
Simon S Du, Chi Jin, Jason D Lee, Michael I Jordan, Aarti Singh, and Barnabas Poczos. Gradient descent can take exponential time to escape saddle points. In Advances in Neural Information Processing Systems, pp. 1067­1077, 2017.
Simon S Du, Jason D Lee, Yuandong Tian, Barnabas Poczos, and Aarti Singh. Gradient descent learns one-hidden-layer cnn: Don't be afraid of spurious local minima. ICML, 2018.
Ian J Goodfellow, Oriol Vinyals, and Andrew M Saxe. Qualitatively characterizing neural network optimization problems. arXiv preprint arXiv:1412.6544, 2014.
Moritz Hardt and Tengyu Ma. Identity matters in deep learning. ICLR, 2017.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770­778, 2016.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. ICML, 2015.
Kenji Kawaguchi. Deep learning without poor local minima. In Advances in Neural Information Processing Systems, pp. 586­594, 2016.
Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak Peter Tang. On large-batch training for deep learning: Generalization gap and sharp minima. ICLR, 2017.
Jonas Kohler, Hadi Daneshmand, Aurelien Lucchi, Ming Zhou, Klaus Neymeyr, and Thomas Hofmann. Towards a theoretical understanding of batch normalization. arXiv preprint arXiv:1805.10694, 2018.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. In Advances in neural information processing systems, pp. 1097­1105, 2012.
Hao Li, Zheng Xu, Gavin Taylor, and Tom Goldstein. Visualizing the loss landscape of neural nets. arXiv preprint arXiv:1712.09913, 2017.
Pankaj Mehta and David J Schwab. An exact mapping between the variational renormalization group and deep learning. arXiv preprint arXiv:1410.3831, 2014.
Song Mei, Yu Bai, and Andrea Montanari. The landscape of empirical risk for non-convex losses. arXiv preprint arXiv:1607.06534, 2016.
Ben Poole, Subhaneil Lahiri, Maithra Raghu, Jascha Sohl-Dickstein, and Surya Ganguli. Exponential expressivity in deep neural networks through transient chaos. In Advances in neural information processing systems, pp. 3360­3368, 2016.
9

Under review as a conference paper at ICLR 2019
Itay Safran and Ohad Shamir. Spurious local minima are common in two-layer relu neural networks. CoRR, abs/1712.08968, 2017. URL http://arxiv.org/abs/1712.08968.
Andrew M Saxe, James L McClelland, and Surya Ganguli. Exact solutions to the nonlinear dynamics of learning in deep linear neural networks. arXiv preprint arXiv:1312.6120, 2013.
David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering the game of go with deep neural networks and tree search. nature, 529(7587):484, 2016.
David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez, Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, et al. Mastering the game of go without human knowledge. Nature, 550(7676):354, 2017.
Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014.
Weijie Su, Stephen Boyd, and Emmanuel Candes. A differential equation for modeling nesterov's accelerated gradient method: Theory and insights. In Advances in Neural Information Processing Systems, pp. 2510­2518, 2014.
Yuandong Tian. An analytical formula of population gradient for two-layered relu network and its applications in convergence and critical point analysis. ICML, 2017.
Yuandong Tian and Yan Zhu. Better computer go player with neural network and long-term prediction. ICLR, 2016.
Luca Venturi, Afonso Bandeira, and Joan Bruna. Neural networks with finite intrinsic dimension have no spurious valleys. arXiv preprint arXiv:1802.06384, 2018.
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep learning requires rethinking generalization. ICLR, 2016.
10

Under review as a conference paper at ICLR 2019
7 APPENDIX



Figure 5: Notation used in Thm. 1.

7.1 HIERARCHICAL CONDITIONING

Theorem 1 (Recursive Property of marginalized gradient). gj (xk) = Exj,-k|xk [gj (xj )]

Proof. We have:

gj (xk) = Ex-k|xk [gj (x)] = Ex-j ,xj,-k|xk [gj (x)] = Ex-j |xj,-k,xk Exj,-k|xk [gj (x)] = Ex-j |xj Exj,-k|xk [gj (x)] = Exj,-k|xk Ex-j |xj [gj (x)] = Exj,-k|xk [gj (xj )]

(18)

7.2 NETWORK THEOREM

Theorem 2 (Reformulation). Denote  = rf(j) and  = rf(k). k is a child of j. If the following two conditions hold:

· Focus of knowledge. P(xk|z, z) = P(xk|z).

· Broadness of knowledge. P(xj|z, z) = P(xj|z). · Decorrelation. Given z, (gkraw(·) and fk(·)) and (fkraw(·) and fk(·)) are uncorrelated

Then the following two conditions holds:

fj(z) = fj(z)

wjkEz|z [fk(z )]

kch(j)

(19a)

gk(z) = fk(z)

wjkEz|z [gj (z)]

jpa(k)

(19b)

Proof. For Eqn. 19a, we have: fjraw(z) =

fjraw(x)P(x|z)dx

(20)

= fjraw(xj )P(xj |z)dxj



=

wjkfk(xk) P(xj |z)dxj

kch(j)

(21) (22)

11

Under review as a conference paper at ICLR 2019

And for each of the entry, we have:

fk(xk)P(xj|z)dxj = fk(xk)P(xk|z)dxk

For P(xk|z), using focus of knowledge, we have:
P(xk|z) = P(xk, z|z)
z
= P(xk|z, z)P(z|z)
z
= P(xk|z)P(z|z)
z
Therefore, following Eqn. 23, we have:

fk(xk)P(xk|z)dxk = fk(xk) P(xk|z)P(z|z)dxk
z

=
z

fk (xk )P(xk |z )dxk

= fk(z)P(z|z)
z
= Ez|z [fk(z )]

Putting it back to Eqn. 22 and we have:

P(z |z )

fjraw(z) =

wjkEz|z [fk(z )]

kch(j)

For Eqn. 19b, similarly we have:

gkraw(z) = gkraw(x)P(x|z)dx

= wjkgj(x)P(x|z)dx
jpa(k)
Notice that we have: P(x|z) = P(xj|z)P(x-j|xj, z) = P(xj|z)P(x-j|xj)
since xj covers xk which determines z. Therefore, for each item we have:
gj(x)P(x|z)dx = gj(x)P(xj|z)P(x-j|xj)dx

= gj(x)P(x-j|xj)dx-j P(xj|z)dxj

= gj(xj)P(xj|z)dxj

Then we use the broadness of knowledge:
P(xj|z) = P(xj, z|z)
z
= P(xj|z, z)P(z|z)
z
= P(xj|z)P(z|z)
z

12

(23)
(24) (25) (26)
(27) (28) (29) (30) (31)
(32) (33)
(34) (35) (36) (37)
(38) (39) (40)

Under review as a conference paper at ICLR 2019

Following Eqn. 37, we now have:

gj(x)P(x|z)dx = gj(xj) P(xj|z)P(z|z)dxj
z

=
z

gj (xj )P(xj |z )dxj

= gj(z)P(z|z)
z
= Ez|z [gj (z)]

P(z |z )

Putting it back to Eqn. 33 and we have:

gkraw(z ) =

wjkEz|z [gj (z)]

jpa(k)

(41) (42) (43) (44)
(45)

Using Eqn. 5:

gk(z) = gk(xk)P(xk|z)dxk

(46)

= fk(xk)gkraw(xk)P(xk|z)dxk = EXk|z [fk(Xk)gkraw(Xk)] The un-correlation between gkraw(·) and fk(·) means that EXk|z [fkgkraw] = EXk|z [fk] · EXk|z [gkraw] Similarly for fj(z).

(47) (48)
(49)

7.3 EXACTNESS OF REFORMULATION
Theorem 3. If P(xj|z) is a delta function for all , then the conditions of Thm. 2 hold and the reformulation becomes exact.

Proof. The fact that P(xj|z) is a delta function means that there exists a function j so that:

P(xj|z) = (xj - j(z))

(50)

That is, z contains all information of xj (or x). Therefore,

· Broadness of knowledge. z contains strictly more information than z for   ch(), therefore P(xj|z, z) = P(xj|z).
· Focus of knowledge. z captures all information of zk, so P(xk|z, z) = P(xk|z).
· Decorrelation. For any h1(xj) and h2(xj) we have

EXj|z [h1h2] = h1(xj )h2(xj )P(xj |z)dxj

(51)

= h1(xj)h2(xj)(xj - j(z))dxj = h1(j(z))h2(j(z)) = h1(xj)P(xj|z)dxj h2(xj)P(xj|z)dxj = EXj |z [h1] EXj |z [h2]

(52) (53) (54) (55)

13

Under review as a conference paper at ICLR 2019

F, D
G, G~ W P , Pb 

Dimension
m-by-n
m-by-n n -by-n m-by-m m-by-m

Description
Activation fj(z) and gating prob fj(z) in group . Gradient gj(z) and unnormalized gradient g~j(z) in group . Weight matrix that links group  and . Prob P(z|z), P(z|z) of events between group  and . Diagonal matrix encoding prior prob P(z).

Table 2: Matrix Notation. See Eqn. 56.

7.4 MATRIX FORM

Theorem 4 (Matrix Representation of Reformulation).

F = D 

PFW, G~ = D 

PT G~WT,

ch()

pa()

W = (PF)T G~ (56)

Proof. We first consider one certain group  and , which uses x and x as the receptive field. For this pair, we can write Eqn. 19 in the following matrix form:

Fraw = P F W F = Fraw  D
Graw = Pb T G WT G = Graw  D

(57a) (57b)
(57c) (57d)

Using (Pb)T = PT and G~ = G, we could simplify Eqn. 57 as follows: F = P F W  D G~ = (P)T G~WT  D

(58a) (58b)

Therefore, using the fact that jpa(k) = pa() j (where  = rf(k)) and kch(j) = ch() k (where  = rf(j)), and group all nodes that share the receptive field together, we
have:

F = D 

P F W

ch()

G~ = D 

PT G~WT

pa()

For the gradient update rule, from Eqn. 2 notice that:

wjk = Ex [fk(x)gj(x)]

= fk(x)gj(x)P(x)dx

= fk(x)gj(x) P(x|z)P(z)dx
z

=
z
We assume decorrelation so we have:

fk (x)gj (x)P(x|z )P(z )dx

wjk = EX|z [fk(x)] gj (z)P(z)
z
= EXk|z [fk(xk)] g~j (z)
z

(59a) (59b)
(60) (61) (62) (63)
(64) (65)

14

Under review as a conference paper at ICLR 2019

For EXk|z [fk(xk)], again we use focus of knowledge: EXk|z [fk(xk)] = fk(xk)P(xk|z)dxk

=
z

fk(xk)P(xk|z, z)P(z|z)dxk

=
z

fk (xk )P(xk |z )P(z |z )dxk

= fk(z)P(z|z)
z

Put them together and we have:

wjk =

fk(z)g~j (z)P(z|z) = Ez,z [fk(z)gj (z)]

z z

Write it in concise matrix form and we get: W = (PF)T G~

(66) (67) (68) (69)
(70)
(71)

7.5 BATCH NORM AS A PROJECTION

Theorem 5 (Backpropagation of Batch Norm). For a top-down gradient g, BN layer gives the following gradient update (Pf,1 is the orthogonal complementary projection of subspace {f , 1}):

gf

=

J BN (f )g

=

c1 

Pf,1g,

gc = S(f )T g

(72)

Proof. We denote pre-batchnorm activations as f (i) = fj(xi) (i = 1 . . . N ). In Batch Norm, f (i) is whitened to be f~(i), then linearly transformed to yield the output f¯(i):

f^(i) = f (i) - µ, f~(i) = f^(i)/, f¯(i) = c1f~(i) + c0

(73)

where

µ

=

1 N

i f (i)

and

2

=

1 N

i(f (i) - µ)2 and c1, c0 are learnable parameters.

While in the original batch norm paper, the weight update rules are super complicated and unintuitive (listed here for a reference):

Figure 6: Original BN rule.
It turns out that with vector notation, the update equations have a compact vector form with clear geometric meaning.
15

Under review as a conference paper at ICLR 2019

(a) Sub-layers in BN layer

subspace (b) Projected Gradient

Figure 7: Analysis of Batch Normalization. (a) A Batch Normalization layer could be decomposed
into three sublayers (zero-mean, unit-variance, affine). (b) The down gradient gf is a projection of input gradient g onto the orthogonal complementary space spanned by {f , 1}.

To achieve that, we first write down the vector form of forward pass of batch normalization:

^f = P1f , ~f = ^f / ^f uni, ¯f = c1~f + c01 = S(f )c

(74)

where f , ^f , ~f and ¯f are vectors of size N , P1

I-

11T N

is the projection matrix that centers the

data,  =

f

uni =

1 N

f

2 and c  [c1, c0]T are the parameters in Batch Normalization and

S(f )  [~f , 1] is the standardized data. Note that S(f )T S(f ) = N · I2 (I2 is 2-by-2 identity matrix)

and thus S(x) is an column-orthogonal N -by-2 matrix. If we put everything together, then we have:

BN (f ) = c1

P1f P1f uni

+ c01

(75)

Using this notation, we can compute the Jacobian of batch normalization layer. Specifically, for any vector f , we have:

d

f f

df

1 =
f

I-

ffT f2

=

1 f

Pf

(76)

where Pf projects a vector into the orthogonal complementary space of f . Therefore we have:

J BN (f )

=

d¯f df

=

d~f c1 df

=

d^f c1 df

d~f d^f

=

c1 

P~fc,1

(77)

where P~f,1

=I-

S(f )S(f )T N

is a symmetric projection matrix that projects the input gradient to the

orthogonal complement space spanned by x~ and 1 (Fig. 2(b)). Note that the space spanned by ~f and

1 is also the space spanned by f and 1, since ~f = (f - µ1)/ can be represented linearly by f and

1. Therefore P~f,1 = Pf,1.

An interesting property is that since BN (f ) returns a vector in the subspace of f and 1, for the N -by-N Jacobian matrix of Batch Normalization, we have:

J BN (f )BN (f ) = J BN (f )1 = J BN (f )f = 0

(78)

Following the backpropagation rule, we get the following gradient update for batch normalization. If g = L/¯f is the gradient from top, then

gc = S(x)T g, gf = J BN (f )g

(79)

Therefore, any gradient (vector of size N ) that is back-propagated to the input of BN layer will be automatically orthogonal to that activation (which is also a vector of size N ).

7.5.1 CONSERVED QUANTITY IN BATCH NORMALIZATION FOR RELU ACTIVATION

One can find an interesting quantity, by multiplying gj(x) on both side of the forward equation in Eqn. 1 and taking expectation:


Ex [gj fj ] = Ex 

wjkfkgj  =

wj k wj k

kch(j)

kch(j)

(80)

16

Under review as a conference paper at ICLR 2019

Using the language of differential equation, we know that:

t
Ex gj(t )fj(t ) dt = Ej (t) - Ej (0)
0

(81)

where

Ej

=

1 2

kch(j) wj2k =

1 2

Wj·

2. If we place Batch Normalization layer just after ReLU

activation and linear layer, by BN property, since Ex [gjfj]  0 for all iterations, the row energy

Ej(t) of weight matrix W of the linear layer is conserved over time. This might be part of the reason

why BN helps stabilize the training. Otherwise energy might "leak" from one layer to nearby layers.

7.6 NONLINEAR VERSUS LINEAR

Theorem 6 (Expressibility of ReLU Nonlinearity). Assuming m = n = O(exp(sz())), where sz() is the size of receptive field of . If each P is all-vert, then: ( is top-level receptive field)

min LossReLU(W ) = 0, min LossLinear(W ) = O(exp(sz()))
WW

(82)

Here we define Loss(W ) 

F - I

2 F

.

Proof. We prove that in the case of nonlinearity, there exists a weight so that the activation F = I for all . We prove by induction. The base case is trivial since we already know that F = I for all leaf regions.
Suppose F = I for any   ch(). Since P is all-vert, every row is a vertex of the convex hull, which means that for i-th row pi, there exists a weight wi and bi so that wiT pi + bi = 1/|ch()| > 0 and wiT pj + bi < 0 for j = i. Put these weights and biases together into W and we have

Fraw =

P F W =

P W



(83)

All diagonal elements of Fraw are 1 while all off-diagonal elements are negative. Therefore, af-

ter ReLU, F = I. Applying induction, we get F = I and G = I - F = 0. Therefore,

LossReLU(W ) =

G

2 F

= 0.

In the linear case, we know that rank(F)   rank(PFW)   rank(F), which is on
the order of the size sz() of 's receptive field (Note that the constant relies on the overlap between receptive fields). However, at the top-level, m = n = O(exp(sz())), i.e., the information contained in  is exponential with respect to the size of the receptive field. By Eckart­Young­Mirsky
theorem, we know that there is a lower bound for low-rank approximation. Therefore, the loss for linear network Losslinear is at least on the order of m0, i.e., Losslinear = O(m). Note that this also works if we have BN layer in-between, since BN does a linear transform in the forward pass.

Theorem 7. The following two are correct:

(1) If F is row full rank, then vert(P F ) = vert(P ).

(2) P F is all-vert iff P is all-vert.

Proof. For (1), note that each row of P F is pTi F . If F is row full rank, then F has pseudo-inverse F so that F F = I. Therefore, if pi is not a vertex:

pi = aj pj ,
j=i

aj = 1, aj  0,
j

(84)

then piT F is also not a vertex and vice versa. Therefore, vert(P F ) = vert(P ). (2) follows from (1).

7.7 DISENTANGLED REPRESENTATION
We first start with two lemmas. Both of them have simple proofs. Lemma 1. Distribution representations have the following property:

17

Under review as a conference paper at ICLR 2019

(1) If F(i) is disentangled, F = i wiF(i) is also disentangled.
(2) If F is disentangled and h is any per-column element-wise function, then h(F) is disentangled.
(3) If F(i) are disentangled, hi are per-column element-wise function, then h1(F(1))  h2(F(2)) . . .  hn(F(n)) is disentangled.

Proof. (1) follows from properties of tensor product. For (2) and (3), note that the j-th column of F is F,:j = 1. . . fj . . .1, therefore hj (F,:j ) = 1. . . hj (fj ) . . .1, and hj1(F(1,:)j )h2j (F(2,:)j ) =
1  . . . h1j (fj(1))  hj2(fj(2)) . . .  1.

Given one child   ch(), denote

PSj = P[j][Sj ] = P(z[Sj ]|z[j]) wSj = W[Sj , j] p[j] = [P([j] = 0), P([j] = 1)]T

(85) (86) (87)

We have PSj 1 = 1 and 1T p[j] = 1. Note here for simplicity, 1 represents all-one vectors of any length, determined by the context.

Since F and G are disentangled, their j-th column can be written as: F,:j = 1  . . .  fj  . . .  1 G~,:j = p[1]  . . .  g~j  . . .  p[n]

(88) (89)

For simplicity, in the following proofs, we just show the case that n = 2, n = 3, z =
z[1], z[2] and S = {S1, S2} = {{1, 2}, {3}}. We write f1,2 = [f1  1, 1  f2] as a 2-column matrix. The general case is similar and we omit here for brevity.
Theorem 8 (disentangled Forward). If for each   ch(), P can be written as a tensor product P = i P[i][Si] where {Si} are -dependent disjointed set, W is separable with respect to {Si}, F is disentangled, then F is also disentangled (with/without ReLU /Batch Norm).

Proof. For a certain   ch(), we first compute the quantity PF: PF = (P1,2  P3) [f1,2  1, 1  f3] = [P1,2f1,2  1, 1  P3f3]

(90)

Therefore, the forward information sent from  to  is:

Fraw

= PFW = [P1,2f1,2  1, 1  P3f3]

w1,2 0

0 w3

(91)

= [P1,2f1,2w1,2  1, 1  P3f3w3]

(92)

Note that both P1,2f1,2w1,2 and P3f3w3 are 2-by-1 vectors. Therefore, for each   ch(), Fraw is disentangled. By Lemma 1, both Fraw = ch() Fraw and the nonlinear response F are
disentangled. By Eqn. 12, the forward pass of Batch Norm is a per-column element-wise function,
so BN also preserves disentangledness.

Theorem 9 (Separable Weight Update). If P = i P[i][Si], both F and G~ are disentangled, 1T G~ = 0, then the gradient update W is separable with respect to {Si}.

Proof. Following Eqn. 59 and Eqn. 90, we have:

W = (PF)T G~

=

(P1,2f1,2)T  1T 1T  (P3f3)T

g~1  p[2], p[1]  g~2

=

(P1,2f1,2)T g~1

(P1,2f1,2)T p[1]  1T g~2

1T g~1  (P3f3)T p[2]

(P3f3)T g~2

(93) (94)
(95)

18

Under review as a conference paper at ICLR 2019

Since 1T G~ = 0, we have for any j, 1T G~,:j = 0 and thus 1T g~j = 0. Therefore, W = diag (P1,2f1,2)T g~1, (P3f3)T g~2
which is separable with respect to S. In particular: w1,2 = (P1,2f1,2)T g~1, w3 = (P3f3)T g~2

(96) (97)

7.7.1 DISCUSSION ABOUT BACKPROPAGATION OF DISENTANGLED GRADIENT

One problem remains. If {G~}pa() are all disentangled, whether G~ is disentangled? We can try computing the following quality:

G~raw = PT G~WT

(98)

= P1T,2  P3T g~1  p[2], p[1]  g~2

w1T,2 0 0 w3T

(99)

= P1T,2g~1  p[3], p[1,2]  P3T g~2

w1T,2 0 0 w3T

(100)

= P1T,2g~1w1T,2  p[3], p[1,2]  P3T g~2w3T

(101)

Note that here we use the following equality from total probability rule:

P3T p[2] = p[3], P1T,2p[1] = p[1,2]

(102)

where p[1,2] is a 4-by-1 vector:

 P(z[1] = 0, z[2] = 0) 

p[1,2]

=

 

P(z[1] = 0, z[2] = 1) P(z[1] = 1, z[2] = 0)

 

P(z[1] = 1, z[2] = 1)

(103)

Note that the ordering of these joint probability corresponds to the column order of P1,2.

Now with this example, we see that the backward case ( Eqn. 102) is very different from the forward case (Eqn. 92), in which G~raw is no longer disentangled. Indeed, P1T,2g~1w1T,2 is a 2-column matrix and p[1,2] is not a rank-1 tensor anymore. Intuitively this makes sense, if two low-level attributes have very similar behaviors, there is no way to distinguish the two via backpropagation.

Note that we also cannot assume independence: p[1,2] = p[1]  p[2] since the independence property is in general not carried from layer to layer.

For general cases, G~raw takes the following form:


n

G~ raw 

=

PST1

g~1

wT
S1



p[Sj ],

j=2

n

p[S1 ]



PT
S2

g~2

wT
S2



p[Sj ],

j=3

 . . .
(104)

One hope here is that if we consider pa() G~raw, the summation over parent  could lead to a
better structure, even for individual , PST1 g~1wST1 is not 1-order tensor. For example, if Sj = Sj, then for the first column in S1, due to 1T g~j = 0, we know that:

PT,S1 g~1wT,S1[1] =

c(v+,S1 - v-,S1 )

pa()

pa()

(105)

where v+,S1 = P(z[S1]|z[1] = 1) and v-,S1 = P(z[S1]|z[1] = 0) and P,Si =

v-,S1 v+,S1

. If

each   pa() is informative in a diverse way, and |S1| is relatively small (e.g., 4), then v+,S1 -

v-,S1 = 0 and spans the probability space of dimension 2|S1| - 1. Then we can always find c

(or equivalently, weights) so that Eqn. 105 becomes rank-1 tensor (or disentangled). Besides, the

gating D, which is disentangled as it is an element-wise function of F, will also play a role in

regularizing G~.

We will leave this part to future work.

19


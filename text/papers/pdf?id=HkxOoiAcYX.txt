Under review as a conference paper at ICLR 2019
ESTIMATING INFORMATION FLOW IN DNNS
Anonymous authors Paper under double-blind review
ABSTRACT
We study the evolution of internal representations during deep neural network (DNN) training, aiming to demystify the compression aspect of the information bottleneck theory. The theory suggests that DNN training comprises a rapid fitting phase followed by a slower compression phase, in which the mutual information I(X; T ) between the input X and internal representations T decreases. Several papers observe compression of estimated mutual information on different DNN models, but the true I(X; T ) over these networks is provably either constant (discrete X) or infinite (continuous X). This work explains the discrepancy between theory and experiments, and clarifies what was actually measured by these past works. To this end, we introduce an auxiliary (noisy) DNN framework for which I(X; T ) is a meaningful quantity that depends on the network's parameters. This noisy framework is shown to be a good proxy for the original (deterministic) DNN both in terms of performance and the learned representations. We then develop a rigorous estimator for I(X; T ) in noisy DNNs and observe compression in various models. By relating I(X; T ) in the noisy DNN to an information-theoretic communication problem, we show that compression is driven by the progressive clustering of hidden representations of inputs from the same class. Several methods to directly monitor clustering of hidden representations, both in noisy and deterministic DNNs, are used to show that meaningful clusters form in the T space. Finally, we return to the estimator of I(X; T ) employed in past works, and demonstrate that while it fails to capture the true (vacuous) mutual information, it does serve as a measure for clustering. This clarifies the past observations of compression and isolates the geometric clustering of hidden representations as the true phenomenon of interest.
1 INTRODUCTION
Recent work by Shwartz-Ziv & Tishby (2017) uses the Information Bottleneck framework (Tishby et al., 1999; Tishby & Zaslavsky, 2015) to study the dynamics of DNN learning. The framework considers the mutual information pair I(X; T ), I(Y ; T ) between the input X or the label Y and the network's hidden layers T . Plotting the evolution of these quantities during training, Shwartz-Ziv & Tishby (2017) made two interesting observations: (1) while I(Y ; T ) remains mostly constant as the layer index increases, I(X; T ) decreases, suggesting that layers gradually shed irrelevant information about X; and (2) after an initial fitting phase, there is a long compression phase during which I(X; T ) slowly decreases. It was suggested that this compression is responsible for the generalization performance of DNNs. A follow-up paper (Saxe et al., 2018) contends that compression is not inherent to DNN training, claiming double-sided saturating nonlinearities yield compression while single-sided/non-saturating ones do not necessarily compress. Shwartz-Ziv & Tishby (2017) and Saxe et al. (2018) present many plots of I(X; T ), I(Y ; T ) evolution across training epochs. These plots, however, are inadvertently misleading: they show dynamically changing I(X; T ) when this mutual information is provably either infinite or a constant independent of the DNN's parameters. Recall that the mutual information I(X; T ) is a functional of the joint distribution of (X, T )  PX,T = PX PT |X , and that, in standard DNNs, T is a deterministic function of X. Hence, if PX is continuous, then so is T , and thus I(X; T ) =  (cf. (Polyanskiy & Wu, 2012-2017, Theorem 2.4)). If PX is discrete (e.g., when the features are discrete or if X adheres to an empirical distribution over the dataset), then the mutual information is a finite constant that does not depend on the parameters of the DNN. Specifically, for deterministic DNNs, the mapping from a discrete X to T is injective for strictly monotone nonlinearities such as tanh or sigmoid, except for a measure-zero set of weights. In other words, deterministic DNNs
1

Under review as a conference paper at ICLR 2019

I(X; Bin(T_l))

8 4 0
100

bin size = 0.0001

Layer 1 Layer 2 Layer 3 Layer 4 Layer 5
101

102 Epoch

103

104

bin size = 0.001

bin size = 0.01

bin size = 0.1

Figure 1: I X; Bin(T ) vs. epochs for different bin sizes and the model in Shwartz-Ziv & Tishby

(2017). The curves converge to ln(212)  8.3 for small bins, per the 12-bit uniformly distributed X.

can encode all information about a discrete X in arbitrarily fine variations of T , causing no loss of information and implying I(X; T ) = H(X), even if deeper layers have fewer neurons.

The compression observed in Shwartz-Ziv & Tishby (2017) and Saxe et al. (2018) therefore cannot be due to changes in mutual information. This discrepancy between theory and experiments originates from a theoretically unjustified discretization of neuron values in their approximation of I(X; T ). To clarify, the quantity computed and plotted in these works is I(X; Bin(T )), where Bin is a per-neuron discretization of each hidden activity of T into a user-selected number of bins. This I X; Bin(T ) is highly sensitive to the selection of bin size (as illustrated in Fig. 1) and does not track I(X; T ) for any choice of bin size.1 Nonetheless, compression results based on I X; Bin(T ) are observed by Shwartz-Ziv & Tishby (2017) and Saxe et al. (2018) in many interesting cases.

To understand this curious phenomenon we first develop a rigorous framework for tracking the flow of information in DNNs. In particular, to ensure I(X; T ) is meaningful for studying the learned representations, we need to make the map X  T a stochastic parameterized channel whose parameters are the DNN's weights and biases. We identify several desirable criteria that such a stochastic DNN framework should fulfill for it to provide meaningful insights into commonly used practical systems. (1) The stochasticity should be intrinsic to the operation of the DNN, so that the characteristics of mutual information measures are related to the learned internal representations, and not to an arbitrary user-defined parameter. (2) The stochasticity should relate the mutual information to the deterministic binned version I X; Bin(T ) , since this is the object whose compression was observed; this requires the injected noise to be isotropic over the domain of T analogously to the per-neuron binning operation. And most importantly, (3) the network trained under this stochastic model should be closely related to those trained in practice.

We propose a stochastic DNN framework in which independent and identically distributed (i.i.d.) Gaussian noise is added to the output of each of the DNN's neurons. This makes the map from X to T stochastic, ensures the data processing inequality (DPI) is satisfied, and makes I(X; T ) reflect the true operating conditions of the DNN, following Point (1). Since the noise is centered and isotropic, Point (2) holds. As for Point (3), Section 2 experimentally shows the DNN's learned representations and performance are not meaningfully affected by the addition of noise, for variances 2 not too large. Furthermore, randomness during training has long been used to improve neural network performance, e.g., to escape poor local optima (Hinton et al., 1984), improve generalization performance (Srivastava et al., 2014), encourage learning of disentangled representations (Achille & Soatto, 2018), and ensure gradient flow with hard-saturating nonlinearities (Gulcehre et al., 2016).

Under the stochastic model, I(X; T ) has no exact analytic expression and is impossible to approx-

imate numerically. In Section 3 we therefore propose a sampling technique that decomposes the

estimation of I(X; T ) into several instances of a simpler differential entropy estimation problem:

estimating h(S + Z) given n samples of the d-dimensional random vector S and knowing the

distribution of Z  N (0, 2Id). We analyze this problem theoretically and show that any differential entropy estimator over the noisy DNN requires at least exponentially many samples in the dimension

d. as

OLev(leorgagnin)dg/4th/eenxp,liwcihticmhosdieglninifigcoafntSly+ouZtp, ewrfeortmhesnthperocpoonsveeargneenwceersatitme aotfogr ethnaetraclo-pnuvreprgoeses

differential entropy estimators when applied to the noisy DNN framework.

1Another approach taken in Saxe et al. (2018) considers I(X; T + Z) (instead of I X; Bin(T ) ), where Z is an independent Gaussian with a user-defined variance. This approach has two issues: (i) the values as a function of may violate the data processing inequality, and (ii) they do not reflect the operation of the actual DNN, which was trained without noise. We focus on I X; Bin(T ) because it was commonly used in Shwartz-Ziv & Tishby (2017) and Saxe et al. (2018), and since both methods have a similar effect of blurring T .

2

Under review as a conference paper at ICLR 2019

We find that I(X; T ) exhibits compression in many cases during training of small DNN classifiers. To explain compression in an insightful yet rigorous manner, Section 4 relates I(X; T ) to the well-understood notion of data transmission over additive white Gaussian noise (AWGN) channels. Namely, I(X; T ) is the aggregate information transmitted over the channel PT |X with input X drawn from a constellation defined by the data samples and the noisy DNN parameters. As training progresses, the representations of inputs from the same class tend to cluster together and become increasingly indistinguishable at the channel's output, thereby decreasing I(X; T ). Furthermore, these clusters tighten as one moves into deeper layers, providing evidence that the DNN's layered structure progressively improves the representation of X to increase its relevance for Y .

Finally, we examine clustering in deterministic DNNs. We identify methods for measuring clustering that are valid for both noisy and deterministic DNNs, and show that clusters of inputs in learned representations typically form in both cases. We complete the circle back to I X; Bin(T ) by clarifying why this binned mutual information measures clustering. This explains what previous works were actually observing: not compression of mutual information, but increased clustering by hidden representations. The geometric clustering of hidden representations is thus the fundamental phenomenon of interest, and we aim to test its connection to generalization performance, theoretically and experimentally, in future work.

2 PRELIMINARY DEFINITIONS

Noisy DNNs: For integers k  , let [k : ] i  Z k 

i with

L

and use [ ] when k = 1. + 1 layers {T } [0:L], with

Consider input T0 =

aXnaonisTdy-o1DutNpuNtW(k)TT-1-+1b(k)



S(k)
W(k)T-1 +b(k)

T(k) S(k)

T(k)

TL. The th hidden layer,  [L - 1], is described by T = f (T -1) + Z , where f : Rd -1  Rd is a deterministic

Z(k)  N (0, 2) Z(k)  N (0, 2)

function of the previous layer and Z  N 0, 2Id ; no Figure 2: kth noisy neuron in layer

noise is injected to the output, i.e., TL = fL(TL-1). We set with nonlinearity ; W(k) and b (k)

S f (T -1) and use  for the probability density function (PDF) of Z . The functions {f } [L] can represent any type

are the kth row/entry of the weight matrix and the bias, respectively.

of layer (fully connected, convolutional, max-pooling, etc.).

Fig. 2 shows a neuron in the th layer of a noisy DNN.

To explore the relation between noisy and deterministic DNNs

under conditions representative of current machine learning practices, we trained four-layer convolutional neural networks

Model

# Errors

(CNNs) on MNIST (LeCun et al., 1999). The CNNs used different levels of internal noise, including no noise, and one used dropout in place of additive noise. We measured their performance on the validation set and characterized the cosine similarities between their internal representations. Full details of the CNN architecture and training procedure are in

Deterministic Noisy ( = 0.05) Noisy ( = 0.1) Noisy ( = 0.2) Noisy ( = 0.5) Dropout (p = 0.2)

50 ± 4.6 50 ± 5.0 51 ± 6.9 86 ± 9.8 2200 ± 520 39 ± 3.9

Supplement 9.3. The results in Table 1 show small amounts of internal additive noise (  0.1) have a minimal impact on classification performance, while dropout strongly improves it. The histograms in Fig. 3 show that the noisy (for small ) and dropout models learn internal representations similar to

Table 1: Total MNIST validation errors for different models, showing mean ± standard deviation over eight initial random seeds.

the representations learned by the deterministic model. In this high-dimensional space, unrelated

representations would create cosine similarity histograms with zero mean and standard deviation

between 0.02­0.3, so the observed values are quite large. As expected, dissimilarity increases as the

noise increases, and similarity is lower for the internal layers (2 and 3).

Mutual Information: Noisy DNNs induce a stochastic map from X to the rest of the network,

described by the conditional distribution PT1,...,TL|X . The corresponding PDF2 is pT1,...,TL|X=x. Its

marginals are denoted by keeping only the relevant variables in the subscript. Let X {xi}i[m] be

the input dataset, and P^X be its empirical distribution, described by the probability mass function

(PMF) p^X (x)

=

1 m

i[m] 1{xi=x}, for x  X . Since data sets typically contain no repetitions,

we

assume

p^X (x)

=

1 m

,

x



X.

The

input

and

the

hidden

layers

are

jointly

distributed

according

2PT1,...,TL|X=x is absolutely continuous with respect to (w.r.t.) the Lebesgue measure for all x  X .

3

Under review as a conference paper at ICLR 2019

# occurrences

600 Layer 1 (conv) Layer 2 (conv) Layer 3 (full) Layer 4 (full)
400 200
00.0 0.5 1.0 0.0 0.5 1.0 0.0 0.5 1.0 0.0 0.5 1.0 Cosine similarity to noiseless model

Model
noisy ( = 0.05) noisy ( = 0.1) noisy ( = 0.2) noisy ( = 0.5) dropout (p = 0.2)

Figure 3: Histograms of cosine similarities between internal representations of deterministic, noisy, and dropout MNIST CNN models. To encourage comparable internal representations, all models were initialized with the same random weights and accessed the training data in the same order.

to3 PX,T1,...,TL P^X PT1,...,TL|X , under which X - T1 - . . . - TL-1 - TL forms a Markov chain. For each  [L - 1], we study the mutual information (Supplement 7 explains this factorization)

I(X; T )

dPX,T log
X ×Rd

dPX,T dPX × PT

=

h(pT

)

-

1 m

h(pT |X=xi ),

i[m]

(1)

where log(·) is with respect to the natural base. Although PT and PT |X are readily sampled from using the DNN's forward pass, these distributions are too complicated (due to the composition of

Gaussian noises and nonlinearities) to analytically compute I(X; T ) or even to evaluate their densities

at the sampled points. Therefore, we must estimate I(X; T ) directly from the available samples.

3 MUTUAL INFORMATION ESTIMATION OVER NOISY DNNS

Expanding I(X; T ) as in (1), our goal is to estimate h(pT ) and h(pT |X=x), x  X : a problem that we show is hard in high dimensions. We develop the sample propagation (SP) estimator, which exploits the ability to propagate samples up the DNN layers and the known noise distribution. Using i.i.d. samples of S and S |X = x we form a mixture distribution whose differential entropy can be computed numerically. This mixture's entropy is then shown to converge to the desired true entropy.

3.1 THE SAMPLE-PROPAGATION DIFFERENTIAL ENTROPY ESTIMATOR

In what follows, we denote the empirical PMF associated with a set A = {ai}i[n]  Rd by p^A.

Unconditional Entropy: Since T = S + Z , where S and Z are independent, we have

pT = pS  . To estimate h(pT ), let {x^j}j[n] be n i.i.d. samples from PX . Feed each x^j into the DNN and collect the outputs it produces at the ( - 1)-th layer. The function f is then applied on

each collected output to obtain S

s ,1, s ,2, . . . , s ,n , which is a set of n i.i.d. samples from

pS . We estimate h(pT ) by h(p^S  ), which is the differential entropy of a Gaussian mixture with

centers s ,j, j  [n]. The term h(p^S  ) is referred to as the SP estimator of h(pT ) = h(pS  ).

Conditional Entropies: Fix i  [m] and consider the estimation of h(pT |X=xi ). Note that

pT xi

i|nXto=xthi e=DpNSN|Xn=ixtiimes,scinoclleecZt

is independent outputs from T

of (X, T -1). To sample from -1 corresponding to different

pS |X noise

=rexai ,liwzaetifoenesd,

and apply pS |X=xi .

f on each. The obtained Each h(pT |X=xi ) = h(pS

samples |X=xi 

S (i) ) is

s(i,1) , s(i,2) , . . . estimated by the

S, sP(i,en)sitimaarteori.ih.d.p^Sa(ci)cordin.g4

to

Mutual Information Estimator: Combining the above described pieces, we estimate I(X; T ) by

I(X; T ) = h(p^S



)

-

1 m

h
i[m]

p^S(i)  

.

(2)

3.2 THEORETICAL GUARANTEES AND COMPUTING THE ESTIMATOR
The above sampling procedure unifies the estimation of h(pT ) and h(pT |X=x) xX into a single new differential entropy estimation problem: estimate h(pS  ) based on i.i.d. samples Sn (Si)i[n] from pS and knowledge of . The SP estimator solution approximates h(pS  ) by h^SP(Sn, ) h(p^Sn  ), where p^Sn is the empirical PMF induced by Sn. Before analyzing the

3We set X  Unif(X ) to conform with past works (Shwartz-Ziv & Tishby, 2017; Saxe et al., 2018).

4For

=

1,

we

have

h(T1 |X )

=

h(Z1)

=

d1 2

log(2e2)

because

its

previous

layer

is

X

(fixed).

4

Under review as a conference paper at ICLR 2019

p(x) Mutual information Mutual information

15 (a)
10
5

Epoch 250 Epoch 2500

(b)

1.5 (c)
1
0.5

2 (d)
1.5
1
0.5

= 0.01 = 0.02 = 0.05 = 0.10 = 0.20 = 0.50

0 -1.2 -1 -0.8 -0.6 -0.4
x

0 10 0

10 2

10 4

10 6

0 0

1

2

3

4

Epoch

Weight

Figure 4: Single-layer tanh network: (a) the density pT (k) at epochs k = 250, 2500; (b) pT (k) and (c) I X; T (k) as a function of k; and (d) mutual information as a function of weight w with bias -2w.

performance of h^SP, we note that this estimation problem is statistically difficult in the sense that any good estimator of h(pS  ) based on Sn and  requires exponentially many samples in d (Theorem 1 from Supplement 10). Nonetheless, the following theorem shows that the SP estimator absolute-error risk converges at a satisfactory rate (Theorem 3 from Supplement 10 states this with all constants explicit, and Theorem 4 gives the results for ReLU).

Theorem 1 [-1, 1]d. We

Fix  have:

> 0, d  1, and let suppSFd E h(pS 

Fd )

be the class of - h^SP(Sn, )

=d-Odim(elongsino)nda/l4P/DFns.supported

inside

Evaluating the SP estimator requires computing the differential entropy of a (known) Gaussian mixture. Although it cannot be computed in closed form, efficient numerical computation is possible via Monte Carlo (MC) integration (Robert, 2004). For a fixed sample set Sn = sn, let G  p^sn   be distributed according to the Gaussian mixture of interest. Then,

h^SP(Sn, ) = h(G) = -E log (p^sn  )(G) ,

(3)

which we compute by generating nMC i.i.d. samples from p^sn and approximating the expectation by an empirical average. This gives an unbiased proxy of h^SP(sn, ), whose mean squared error is O (n · nMC)-1 (Supplement 10).

4 COMPRESSION AND CLUSTERING: A MINIMAL EXAMPLE

Before presenting our empirical results, we connect compression to clustering using an informationtheoretic perspective. Consider a single noisy neuron with a one-dimensional input X. Let T (k) = S(k) + Z be the neuron's output at epoch k, where S(k) (wkX + bk), for a strictly monotone nonlinearity , and Z  N (0, 2). Invariance of mutual information to invertible operations implies

I X; T (k) = I (wkX + bk); (wkX + bk) + Z = I S(k); S(k) + Z .

(4)

From an information-theoretic perspective, I S(k); S(k) + Z is the aggregate information transmit-
ted over an AWGN channel with input constellation Sk (wkx + bk) | x  X . In other words, I S(k); S(k) + Z is a measure of how distinguishable the symbols of Sk are when composed with Gaussian noise (roughly equals log of the number of resolvable clusters under noise level ). Since the distribution of T (k) = S(k) + Z is a Gaussian mixture with means s  Sk, the closer two constellation points s and s are, the more overlapping the Gaussians around them will be. Hence reducing point spacing in Sk (by changing wk and bk) directly reduces I X; T (k) .

Let  = tanh and  = 0.05, and set X = X-1  X1, with X-1 = {-3, -1, 1} and X1 = {3}, labeled -1 and 1, respectively. We train the neuron using mean squared loss and gradient descent

(GD) with a fixed learning rate of 0.01 to best illustrate the behavior of I X; T (k) . The Gaussian

mixture pT (k) is plotted across epochs k in Fig. 4(a)-(b). The learned bias is approximately -2.3w, ensuring that the tanh transition region correctly divides the two classes. Initially w = 0, so all four

Gaussians from X-1

ienvepnTt(u0a)lalyreresu-cpoenrivmerpgoisnegd.asAtshkeyinecarcehasmese,etthtehGe ataunsshiabnosuinnditairayll.yTdhiviserigser,ewfleitchtethdeinthtrheee

mutual information trend in Fig. 4(c), with the dips in I X; T (k) around k = 103 and k = 104

corresponding to the second and third Gaussians respectively merging into the first. Thus, there is

a direct connection between clustering and compression. Fig. 4(d) shows the mutual information

for different noise levels  as a function of weight w, with bias -2w. For small  (as above) the

X-1 Gaussians are distinct and merge in two stages as w grows. For larger , however, the X-1 Gaussians are indistinguishable for any w, making I(X; T ) only increase as the two classes gradually

separate. A similar example for a two-neuron network with leaky-ReLU nonlinearities is provided in

the Supplement 8.

5

Under review as a conference paper at ICLR 2019
(a)
(b)
(c) Figure 5: (a) Evolution of I(X; T ) and training/test losses across training epochs for the SZT model with  = 0.005 and tanh nonlinearities. The scatter plots show the values of Layer 5 (d5 = 3) at the arrow-marked epochs on the mutual information plot. The bottom plot shows H Bin(T ) across epochs for bin size B = 10. (b) Same setup as in (a) but with regularization that encourages orthonormal weight matrices. (c) SZT model with  = 0.01 and linear activations.
5 EMPIRICAL RESULTS
We now show the observations from our minimal examples also hold for two larger networks: (1) the small, fully connected network (FCN) studied in (Shwartz-Ziv & Tishby, 2017; Saxe et al., 2018), which we call the SZT model; and (2) a convolutional network for MNIST classification, called MNIST CNN. We present selected results; additional details and experiments are found in the supplement. SZT model: Consider the data and model of Shwartz-Ziv & Tishby (2017) for binary classification of 12-dimensional inputs using a fully connected 12­10­7­5­4­3­2 architecture. The FCN was tested with tanh and ReLU nonlinearities as well as a linear model. Fig. 5(a) presents results for the SZT model with tanh nonlinearity and  = 0.005 (test classification accuracy 99%), showing the relationship across training epochs between estimated I(X; T ), train/test losses and the distribution of neuron values in 5 layers (layers 0 (d0 = 12) and 6 (d7 = 2) are not shown). The rise and fall of mutual information corresponds to how spread out or clustered the representation in each layer are. For example, I(X; T5) grows until epoch 28, when the Gaussians move away from each other along
6

Under review as a conference paper at ICLR 2019
(a) (b) Figure 6: (a) Histogram of within- and between-class pairwise distances for SZT model with tanh non-linearities and additive noise  = 0.005. (b) Same as (a) but training with weight normalization. a curve (see scatter plots on the right). Around epoch 80 they start clustering and I(X; T5) drops. At the end of training, the saturating tanh nonlinearities push the Gaussians to two furthest corners of the cube, reducing I(X; T5) even more. To confirm that clustering (via saturation) was central to the compression observed in Fig. 5(a), we also trained the model using the regularization from (Cisse et al., 2017) (test classification accuracy 96%), which encourages orthonormal weight matrices. The results are shown in Fig. 5(b). Apart from minor initial fluctuations, the bulk of compression is gone. The scatter plots show that the vast majority of neurons do not saturate and no clustering is observed at the later stages of training. Saturation is not the only mechanism that can cause clustering and consequently reduce I(X; T ). For example, in Fig. 5(c) we illustrate the clustering behavior in a linear SZT model (test classification accuracy 89%). As seen from the scatter plots, due to the formation of several clusters and projection to a lower dimensional space, I(X; T ) drops even without the nonlinearities. To provide another perspective on clustering that is sensitive to class membership, we compute histograms of pairwise distances between representations of samples, distinguishing within-class distances from between-class distances. Fig. 6 shows histograms for the SZT models from Figs. 5(a) and (b). As training progresses, the formation of clusters is clearly seen (layer 3 and beyond) for the unnormalized SZT model in Fig. 5(a). In the normalized model (Fig. 5(b)), however, no tight clustering is apparent, supporting the connection between clustering and compression. Once clustering is identified as the source of compression, we focus on it as the point of interest. To measure clustering, the discrete entropy of Bin(T ) is considered, where the number of equal-sized bins, B, is a tuning parameter. Note that Bin(T ) partitions the dynamic range (e.g., [-1, 1]d for a tanh layer) into Bd cells or bins. When hidden representations are spread out, many bins will be non-empty, each assigned with a positive probability mass. On the other hand, for clustered representations, the distribution is concentrated on a small number of bins, each with relatively high probability. Recalling that discrete entropy is maximized by the uniform distribution, we see why reduction in H Bin(T ) measures clustering. To illustrate this measure, we compute H Bin(T ) for each of the SZT models using bin size B = 10 (bottom plots in Fig. 5(a), (b) and (c)). We can see a clear correspondence between H Bin(T ) and I(X; T ), indicating that although H Bin(T ) does not capture the exact value of I(X; T ), it follows this mutual information in measuring clustering. This is particularly important when moving back to deterministic DNNs, where I(X; T ) is no longer an informative measure, being either a constant or infinity, for discrete or continuous X, respectively. Fig. 1 shows H Bin(T ) for the deterministic SZT model ( = 0). The bin size is a free parameter, and depending on its value, H Bin(T ) reveals different clustering granularities. Moreover, since in deterministic networks T = f (X), for a deterministic map f , we have H Bin(T ) X = 0, and therefore I X; Bin(T ) = H Bin(T ) . Thus, the plots from (Shwartz-Ziv & Tishby, 2017), (Saxe et al., 2018) and our Figs. 1 and 5(a), (b) and (c) all show the entropy of the binned T .
7

Under review as a conference paper at ICLR 2019

MNIST CNN: We now examine a model that is more representative of current machine learning practice: the

9.21

MNIST CNN trained with dropout from Section 2. Fig. 7 portrays the near-injective behavior of this model. Even when only two bins are used to compute H Bin(T ) , it takes values that are approximately ln(10000) = 9.210, for all layers and training epochs, despite that the two convolutional layers use max-pooling. This binning merges two samples in the validation set, so the input

H(Bin(T ))

9.20 9.19 0

50 epoch

Input Layer 1 (conv) Layer 2 (conv) Layer 3 (full) Layer 4 (full)
100

has H Bin(T ) = 9.209. While Fig. 7 does not show Figure 7: H Bin(T ) for the MNIST

compression at the level of entire layers, computing CNN, computed using two bins: [-1, 0]

H Bin(T (k)) for individual units k in layer 3 reveals and (0, 1]. The tiny range of the y axis

a gradual decrease over epochs 1­128. To quantify shows the near injectivity of the model.

this trend, we computed linear regressions predicting

H Bin(T (k)) from the epoch index, for all units k in layer 3. Then we found the mean and

standard deviation of the slope of the linear predictions. If most slopes are negative, then compression

occurs during training at the level of individual units. For a range of bin sizes from 10-4­10-1

the least negative mean slope was -0.002 nats/epoch with a maximum standard deviation of 0.001,

showing that most units undergo compression.

1000000

Input

Layer 1 (conv) Layer 2 (conv) Layer 3 (full) Layer 4 (full)

Epoch 128 Epoch 32 Epoch 1 Epoch 0

0 1000000

0 1000000

# occurrences

0 1000000

between within

0 0 20 40 0 20 40 0 20 40 0 20 40 0 20 40 pairwise distance
Figure 8: Histograms of within-class and between-class pairwise distances from the MNIST CNN.

In Fig. 8 we show histograms of pairwise distances between MNIST validation set samples in the input (pixel) space and in the four layers of the CNN. The histograms were computed for epochs 0, 1, 32, and 128, where epoch 0 is the initial random weights and epoch 128 is the final weights. The histogram for the input shows that the mode of within-class pairwise distances is lower than the mode of between-class pairwise distances, but that there is substantial overlap. Layers 1 and 2, which are convolutional and therefore do not contain any units that receive the full input, do little to reduce this overlap, suggesting that the features learned in these layers are somewhat generic. In contrast, even after one epoch of training, layers 3 and 4, which are fully connected, separate the distribution of within-class distances from the distribution of between-class distances.

6 CONCLUSIONS
In this work we reexamined the compression aspect of the Information Bottleneck theory (ShwartzZiv & Tishby, 2017), noting that fluctuations of I(X; T ) in deterministic networks with strictly monotone nonlinearities are theoretically impossible. Setting out to discover the source of compression observed in past works, we: (i) created a rigorous framework for studying and accurately estimating information-theoretic quantities in DNNs whose weights are fixed; (ii) identified clustering of the learned representations as the phenomenon underlying compression; and (iii) demonstrated that the compression-related experiments from past works were in fact measuring this clustering through the lens of the binned mutual information. In the end, although binning-based measures do not accurately estimate mutual information, they are simple to compute and prove useful for tracking changes in clustering, which is the true effect of interest in deterministic DNNs. We believe that further study of geometric phenomena driven by DNN training is warranted to better understand the learned representations and to potentially establish connections with generalization.

8

Under review as a conference paper at ICLR 2019
REFERENCES
A. Achille and S. Soatto. Information dropout: Learning optimal representations through noisy computation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2018. 10.1109/TPAMI.2017.2784440.
M. Cisse, P. Bojanowski, E. Grave, Y. Dauphin, and N. Usunier. Parseval networks: Improving robustness to adversarial examples. In Proceedings of the International Conference on Machine Learning (ICML), 2017.
C. Gulcehre, M. Moczulski, M. Denil, and Y. Bengio. Noisy activation functions. In Proceedings of the International Conference on Machine Learning (ICML), pp. 3059­3068, Jun. 2016.
G. E. Hinton, T. J. Sejnowski, and D. H. Ackley. Boltzmann machines: Constraint satisfaction networks that learn. Technical Report CMU-CS84-119, Carnegie-Mellon University, 1984.
Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recognition. Proc. IEEE, 86(11):2278­2324, Nov. 1999.
Yury Polyanskiy and Yihong Wu. Lecture notes on information theory. 2012-2017. URL http: //people.lids.mit.edu/yp/homepage/data/itlectures_v5.pdf.
Christian P Robert. Monte Carlo Methods. Wiley Online Library, 2004. A. M. Saxe, Y. Bansal, J. Dapello, M. Advani, A. Kolchinsky, B. D. Tracey, and D. D. Cox. On the
information bottleneck theory of deep learning. In Proceedings of the International Conference on Learning Representations (ICLR), 2018. R. Shwartz-Ziv and N. Tishby. Opening the black box of deep neural networks via information. arXiv:1703.00810 [cs.LG], 2017. N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhutdinov. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learning Research, 15: 1929­1958, 2014. N. Tishby and N. Zaslavsky. Deep learning and the information bottleneck principle. In Proceedings of the Information Theory Workshop (ITW), pp. 1­5, Jerusalem, Israel, Apr.-May 2015. N. Tishby, F. C. Pereira, and W. Bialek. The information bottleneck method. In Proceedings of the Allerton Conference on Communication, Control and Computing, pp. 368­377, Monticello, Illinois, US, Sep. 1999.
9

Under review as a conference paper at ICLR 2019

SUPPLEMENT TO ESTIMATING INFORMATION FLOW IN DNNS
Anonymous authors Paper under double-blind review

7 MUTUAL INFORMATION

Let (A, B) be a pair of random variables with values in the product set A × B and a joint distribution PA,B (whose marginals are denoted by PA and PB). The mutual information between A and B is:

I(A; B)

dPA,B log
A×B

dPA,B dPA × PB

,

(5)

where PA ×

dPA,B
PdBPA. ×WPBe

is the Radon-Nikodym are mostly interested in

derivative of PA,B the scenario where

with respect A is discrete

to the with a

product measure probability mass

function (PMF) pA, and given A = a  A, B is continuous with probability density function (PDF)

pB|A=a pB|A(·|a). In this case, (5) simplifies to

I(A; B) = pA(a) pB|A(b|a) log

aA

B

pB|A(b|a) pB (b)

db.

(6)

Defining the differential entropy of a continuous random variable C with PDF pC supported in C as1

h(C) = h(pC ) = - pC (c) log pC (c)dc,
C
the mutual information from (6) can also be expressed as

(7)

I(A; B) = h(pB) - pA(a)h(pB|A=a).

(8)

aA

The subtracted term above is the conditional differential entropy of B given A, denoted by h(B|A).

8 TWO-NEURON LEAKY-RELU NETWORK EXAMPLE
To expand upon Section 4, we provide here a second example to illustrate the relation between clustering and compression of mutual information. In particular, this example also shows that as opposed to the claim from (Saxe et al., 2018), non-saturating nonlinearities can achieve compression. Consider the non-saturating Leaky-ReLU nonlinearity R(x) max(x, x/10). Let X = X0  X1/4, with X0 = {1, 2, 3, 4} and X1/4 = {5, 6, 7, 8}, and labels 0 and 1/4, respectively. We train the network via GD with learning rate 0.001 and mean squared loss. Initialization (shown in Fig. 9(a)) was chosen to best illustrate the connection between the Gaussians' motion and mutual information. The network converges to a solution where w1 < 0 and b1 is such that the elements in X1/4 cluster. The output of the first layer is then negated using w2 < 0 and the bias ensures that the elements in X0 are clustered without spreading out the elements in X1/4. Figs. 9(b) show the Gaussian motion at the output of the first layer and the resulting clustering. For the second layer (Fig. 9(c)), the clustered bundle X1/4 is gradually raised by growing b2, such that its elements successively split as they cross the origin; further tightening of the bundle is due to shrinking |w2|. Fig. 9(d) shows the mutual information of the first (blue) and second (red) layers. The merging of the elements in X1/4 after their initial divergence is clearly reflected in the mutual information. Likewise, the spreading of the bundle, and successive splitting and coalescing of the elements in X1/4 are visible in the spikes in the red mutual information curve. The figure also shows how the bounds on I X; T (k) precisely track its evolution.
1Throughout this work we interchanging use h(C) and h(pC ) for the differential entropy of C  pC .

1

Under review as a conference paper at ICLR 2019

0.5 (a)
0
-0.5
-1 10 0 10 1 10 2 10 3 10 4 10 5
Epoch

(b) (c)

Mutual information

2.5 (d) 2 1.5
1 0.5
0 10 0 10 1 10 2 10 3 10 4 10 5
Epoch

Figure 9: Two-layer leaky ReLU network: (a) network parameters as a function of epoch, (b,c) the corresponding PDFs pT1(k) and pT2(k), and (d) the mutual information for both layers.

9 EXPERIMENTAL DETAILS
9.1 SZT MODEL
In this section we provide additional experimental details and results for the SZT model discussed in Section 5 of the main paper. To regularize the network weights, we followed (Cisse et al., 2017) and adopted their approach for enforcing an orthonormality constraint. Specifically, we first update the weights {W } [L] using the standard gradient descent step, and then perform a secondary update to set
W  W -  W WT - Id W , where the regularization parameter  controls the strength of the orthonormality constraint. The value of  was was selected from the set {1.0 × 10-5, 2.0 × 10-5, 3.0 × 10-5, 4.0 × 10-5, 5.0 × 10-5, 6.0 × 10-5, 7.0 × 10-5} and the optimal value was found to be equal to 5.0 × 10-5 for both the tanh and ReLU. In Fig. 10 we present additional experimental results that provide further insight into the clustering and compression phenomena for both tanh and ReLU nonlinearities. Fig. 10(a) shows what happens when the additive noise has a high variance. In this case, although saturation still occurs (see the histograms on top of Fig. 10(a)) and the Gaussians still cluster together (see the scatter plots on the right for the epoch 54 and epoch 8990), compression overall is very mild. The effect of increasing the noise parameter was explained in Section 4 of the main text (see, in particular, Fig. 4(d) therein). Comparing Fig. 10(a) to Fig. 5(a) of the main text, for which  = 0.005 was used and compression was observed, further highlights the effect of large . Recall that smaller  values correspond to narrow Gaussians, while larger  values correspond to wider Gaussians. When  is small, even Gaussians that belong to the same cluster are distinguishable so long as they are not too close. When clusters tighten, the in-class movement brings these Gaussians closer together, effectively merging them, and causing a reduction in mutual information (compression). One the other hand, for large , the in-class movement is blurred at the outset (before clusters tighten). Thus, the only effect on mutual information is the separation between the clusters: as these blobs move away from each other, mutual information rises. Based on the above observation, we can conclude that while the two notions of "clustering Gaussians" and "compression/decrease in mutual information" are strongly related in the low-beta regime, once the noise becomes large, these phenomena decouple, i.e., the network may cluster inputs and neurons may saturate, but this will not be reflected in a decrease of mutual information. Finally, we present results for ReLU activation without weight normalization (Fig. 10(b)) and with orthonormal weight regularization (Fig. 10(c)). We see that both these networks exhibit almost no compression. For Fig. 10(c), the lack of compression is attributed to regularization of the weight matrices, as explained in Section 5 of the main text. For Fig. 10(b), the reduction in compression can be explained by the fact that although ReLU forces saturation of the neurons at the origin (which promotes clustering), since the positive axes remain unconstrained, the Gaussians can move off towards infinity without bound. This is visible from the histograms in the top row of Fig. 10(b), where, for example, in layer 5 the neurons can take arbitrarily large positive values (note that the bin corresponding to the value 5 accumulates all the values from 5 to infinity). Therefore, the clustering at the origin and the potential drop in mutual information is counterbalanced by the spread of Gaussians along the positive axes and the potential increase of mutual information it causes. Eventually, this leads to the approximately constant profile of the mutual information plot in Fig. 10(b).

2

Under review as a conference paper at ICLR 2019
(a)
(b)
(c) Figure 10: SZT model with (a) tanh nonlinearity and additive noise  = 0.01 without weight normalization, (b) ReLU nonlinearity and  = 0.01 without weight normalization, (c) ReLU nonlinearity and  = 0.01 with weight normalization. Test classification accuracy is 97%, 96%, and 97%, respectively. The behavior of the weight-normalized ReLU in Fig. 10(c) is similar to Fig. 10(b), although now the growth of the network weights is bounded and the saturation around origin is reduced. For example, for layers 4 and 5 we can see an upward trend in the mutual information, which is then flattened at the end of training. This occurs since more Gaussians are moving away from the origin, although their motion remains bounded (see the histograms on the top and the scatter plots on the right), thus decreasing the clustering density, leading to the rise in the mutual information profile. Once the Gaussians are prevented from moving any further along the positive axes, a slight compression occurs and the mutual information flattens.
3

Under review as a conference paper at ICLR 2019
9.2 SPIRAL MODEL In this section we present results for another synthetic example. We generated data in the form of spiral as in Fig. 11. The network architecture was similar to SZT model, except that the size of each layer was set to 3. Fig. 12 shows MI estimates I(X; T ) computed using SP estimator and the discrete entropy estimates H Bin(T ) for weight un-normalized Fig. 12 (a) and normalized models Fig. 12 (b) and using additive noise  = 0.005. Similar as in the main paper, the results in the figure illustrate a connection between clustering and compression.
Figure 11: Generated spiral data for binary classification problem. Finally, in Fig. 13 we also show an estimate of H Bin(T ) for the case of deterministic DNN trained on spiral data. For the particular choice of the bin size, the result of the estimated entropy reveal a certain level of clustering granularity. 9.3 MNIST CNN In this section, we describe in detail the architecture of the MNIST CNN models used in Sections 2 and 5 in the main paper. The MNIST CNNs were trained using PyTorch (Paszke et al., 2017) version 0.3.0.post4. The CNNs use the following fairly standard architecture with two convolutional layers, two fully connected layers, and batch normalization.
1. 2-d convolutional layer with 1 input channel, 16 output channels, 5x5 kernels, and input padding of 2 pixels
2. Batch normalization 3. Tanh() activation function 4. Zero-mean additive Gaussian noise with variance 2 or dropout with a dropout probability
of 0.2 5. 2x2 max-pooling 6. 2-d convolutional layer with 16 input channels, 32 output channels, 5x5 kernels, and input
padding of 2 pixels 7. Batch normalization 8. Tanh() activation function 9. Zero-mean additive Gaussian noise with variance 2 or dropout with a dropout probability
of 0.2 10. 2x2 max-pooling 11. Fully connected layer with 1586 (32x7x7) inputs and 128 outputs 12. Batch normalization 13. Tanh() activation function
4

Under review as a conference paper at ICLR 2019
(a)
(b) Figure 12: (a) Evolution of I(X; T ) and training/test losses across training epochs for Spiral dataset with  = 0.005 and tanh nonlinearities. The scatter plots on the right are the values of Layer 5 (d5 = 3) at the arrow-marked epochs on the mutual information plot. The bottom plot shows the entropy estimate H Bin(T ) across epochs for bin size B = 10. (b) Same setup as in (a) but with a regularization that encourages orthonormal weight matrices. Figure 13: H Bin(T ) estimate for deterministic net using spiral data. Bin size was set to B = 0.001.
14. Zero-mean additive Gaussian noise with variance 2 or dropout with a dropout probability of 0.2
15. Fully connected layer with 128 inputs and 10 outputs 5

Under review as a conference paper at ICLR 2019

All convolutional using the default

iannidtiafulilzlyatcioonn,nwechteicdhladyreawrsshwaveeigwhetsigfhrtosmanUdnbifia[-se1s/, anmd t,h1e/wemig]h, tws iatrheminitthiaelifzaend-

in to a neuron in the layer. Training uses cross-entropy loss, and is performed using stochastic

gradient descent with no momentum, 128 training epochs, and 32-sample minibatches. The initial

learning rate is 5 × 10-3, and it is reduced following a geometric schedule such that the learning

rate in the final epoch is 5 × 10-4. To improve the test set performance of our models, we applied

data augmentation to the training set by translating, rotating, and shear-transforming each training

example each time it was selected. Translations in the x- and y-directions were drawn uniformly from

{-2, -1, 0, 1, 2}, rotations were drawn from Unif(-10, 10), and shear transforms were drawn

from Unif(-10, 10).

To obtain more reliable performance results, we train eight different models and report the mean number of errors and standard deviation of the number of errors on the MNIST validation set. To ensure that the internal representations of different models are comparable, which is necessary for the use of the cosine similarity measure between internal representations, for each noise condition (deterministic, noisy with  = 0.05, noisy with  = 0.1, noisy with  = 0.2, noisy with  = 0.5, and dropout with p = 0.2), we use a common random seed (different for the eight replications, of course) so the models have the same initial weights and access the training data in the same order (use the same minibatches).

At test time, all models are fully deterministic: the additive noise blocks and dropout layers are replaced by identities. Thus, in the figures and text in the main paper, "Layer 1" is the output of step 5 (2x2 max-pooling), "Layer 2" is the output of step 10 (2x2 max-pooling), "Layer 3" is the output of step 13 (Tanh() activation function), and "Layer 4" is the output of step 15 (fully connected layer with 10 outputs).

10 SAMPLE PROPAGATION ESTIMATOR - THEORETIC GUARANTEES

NOTE: The content of this section is included in a separate paper focused on the theoretical estimation problem that is to be submitted to an upcoming information-theoretic conference. Relevant portions are included here for clarity and the reviewers' convenience. If accepted, the final version of this supplement will replace this section with appropriate citations to the companion paper. Both conditional and unconditional entropy estimators reduce to the problem of estimating h(pS  ) using i.i.d. samples Sn (Si)i[n] from S  pS while knowing . In this section we state performance guarantees for the SP estimator. These results are cited from our work (Anonymized, 2018), where this estimation problem is thoroughly studied. The interested reader is referred to (Anonymized, 2018) for proofs of the subsequently stated results.

10.0.1 PRELIMINARY DEFINITIONS

Let Fd be the set of distributions P with supp(P )  [-1, 1]d.2 The minimax absolute-error risk over Fd is

Rd(n, )

inf sup ESn h(P  ) - h^(Sn, ) ,
h^ P Fd

(9)

where h^ is an estimator of h(P  ) based on the empirical data Sn = (S1, . . . , Sn) of i.i.d. samples from P and the noise parameter 2. In (9), by P   we mean either: (i) (P  )(x) =

Rdu:pp((uu))>(0xp(-u)u)(dxu-=u)(,pifP)is(xd)i,scwrehteenwPithisPcMoFntpin. uTohuiss

with density p; or (ii) convolved distribution

(P  )(x) = can be defined

generally in a way that the two instances above as special cases using measure-theoretic concepts

(see (Anonymized, 2018)). Regardless of the nature of P , however, we stress that P   is always a

continuous distribution since it corresponds to the random variable S + Z, where Z is an isotropic

Gcoanusstsainatnf.acTtohres)safomrpwlehiccohmepstliemxiattyionndw(it,hin)

is an

defined as the additive gap 

smallest number of is possible. Namely,

samples

(up

to

nd(, ) min n Rd(n, )   .

(10)

2Any support included in a compact subset of Rd would do. We focus on the case of supp(P )  [-1, 1]d due to its correspondence to a noisy DNN with tanh nonlinearities.

6

Under review as a conference paper at ICLR 2019

We also consider the class of distributions with subgaussian marginals; these will correspond to mutual information estimation over noisy DNNs with ReLU nonlinearities. A subgaussian random variable is defined as follows. Definition 1 (Subgaussian Random Variable) A random variable X is subgaussian if it satisfies either of the following equivalent properties

1. Tail condition: K1 > 0,

P |X| > t

 exp

1

-

t2 K12

, for all t  0;

2.

Moment condition: K2 > 0,

(E|X

|p

)

1 p



K2p,

for all p  1;

3. Super-exponential moment: K3 > 0, E exp

X2 K32

 e,

where Ki, for i = 1, 2, 3, differ by at most an absolute constant. Furthermore, the subgaussian

norm X 2 of a subgaussian random variable X is defined as the smallest K2 in property 2, i.e.,

X 2

supp1

p-

1 2

(E|X

|p

)

1 p

.

Now, let Fd(S,KG) be the class of distributions P of a d-dimensional random variable S =

S(1), . . . , S(d) whose coordinates are subgaussian with S(i) 2  K, for all i  [d]. The

risk and the sample complexity defined with respect to the nonparametric class Fd(S,KG) are denoted

by Rd,K (n, ) and nd,K (, ), respectively. Clearly, for any S  P with supp(P )  [-1, 1]d

we have Rexdp(lani,ned)

S  in

(tRih)ed,fKo2l(lnow, i1n),gafonrerdmanalldrk(i, ,the[)dc]o, nansniddd,Ketrh(eed,rseuf)obfrgoearuFasdslliannitFyNd(rS,e1Gqa)un.idrAems>ean0ct ,oiswnnsheaeqtnuureeavnlelcyreKswatiesfio1eb.dtaAbinys

our noisy DNN framework.

Remark 1 (Generality counts for distributions

of Subgaussian induced by noisy

CDlNasNssFwd(iS,tKGh)vaanridouNsonisoynlRineeLaUritDieNs.NSsp)ecTifihceaclllay,ssitFcda(S,pKGtu) raecs-

the following important cases:

1. Distributions with bounded support (corresponding to noisy DNN with bounded activisions).

2. Discrete distributions over a finite set, which is a special case of bounded support.

3. Distributions of the random variable S = f (T -1) in a noisy ReLU DNN, so long as the input X to the network is itself subgaussian. To see this recall that linear combinations

of independent subgaussian random variables is also subgaussian. Furthermore, for any

(scalar) random variable A, we have that ReLU(A) = max{0, A}  |A|, almost surely.

Now, since each layer in a noisy ReLU DNN is nothing but a coordinate-wise ReLU applied

to a linear transformation of the previous layer plus a Gaussian noise, one may upper bound

E S(i) p

1
p , for a d -dimensional hidden layer S

and i  [d ], as in Item (2) of Definition

1, provided that the input X is coordinate-wise subgaussian. The constant K2 will depend on the network's weights and biases, the depth of the hidden layer, the subgaussian norm

poaf rthtiecuilnapr,ustatXisfied2

and the noise variance. This input subgaussianity assumption is, by the distribution of X considered herein, i.e., by X  Unif(X ).

in

10.1 SAMPLE COMPLEXITY IS EXPONENTIAL IN DIMENSION We start with two converse claims establishing that the sample complexity is exponential in d. The first claim states that there exists a class of distributions P , for which the estimation of h(P  ) cannot be done with fewer than exponentially many samples in d, when d is sufficiently large. Theorem 1 (Asymptotic (in d) Exponential Sample-Complexity) For any  > 0 there exist () > 0 (monotonically decreasing in ) and a class of distributions in Fd, such that for any d sufficiently large and  > 0 sufficiently small, the sample complexity of estimating h(p  ) within

7

Under review as a conference paper at ICLR 2019

an additive gap  > 0 over that class grows as 

2()d +(1,)d d

, where limd (1,d) = 0, for all

 > 0. In particular, nd(, ) = 

2()d

in this regime.

+(1,)d d

The fact that the exponent () is monotonically decreasing in  suggests that larger values of  are favorable for estimation. Theorem 1 shows that an exponential sample complexity is inevitable when d is large. As a complementary result, the next theorem gives a sample complexity lower bound valid in any dimension but only for small enough noise variances. Nonetheless, the result is valid for orders of  considered in this work.

Theorem 2 (Asymptotic (in ) Exponential Sample-Complexity) Fix d  1. There exists a class of distributions in Fd such that for any ,  > 0 sufficiently small, the sample complexity of

estimating h(S + Z) within an additive gap  > 0 over that class grows as 

2d , where
+(2,)d d

lim0 (2,d) = 0, for all d  1. In particular, nd(, ) = 

2d in this regime.
+(2,)d d

Remark 2 We state Theorem 2 asymptotically in  for the sake of simplicity, but for any d it is possible to follow the constants through the proof to determine a value c such that Theorem 2 holds for all  < c. For example for d = 1, a careful analysis gives that Theorem 2 holds for all  < 0.08, which is satisfied by most of the experiments run in this paper. This threshold on  changes very slowly with increasing d due to the rapid decay of the PDF of the normal distribution.

10.2 MINIMAX RISK CONVERGENCE RATE OF THE SAMPLE PROPAGATION ESTIMATOR

We next focus on analyzing the performance of the SP estimator. For any fixed Sn = sn, denote the empirical PMF associated with sn by p^sn . The SP estimator of h(T ) = h(pS  ) is

h^SP(sn) h(p^sn  ).

(11)

The estimator h^SP(sn) also depends on , but we omit this from our notation. The following theorem

shows that the expected absolute error of h^SP decays like O

Polylog(n) n

for all dimensions d. We

provide explicit constants (in terms of  and d), which present an exponential dependence on the

dimension, in accordance to the results of Theorems 1 and 2.

Theorem 3 (SP Estimator Absolute-Error Risk for Bounded Support) Fix  > 0, d  1 and any > 0. The absolute-error risk of the SP estimator (11) over the class Fd, for all n sufficiently large, is bounded as

sup ESn h(P  ) - h^SP(Sn)

P Fd



d



1

2(4



2

)

d 4

log  n

2 + 2 (2 +

(



2

)

d 2

) log n

 2 + 2

(2 +

) log n

d 2

1

n

+

c2,d

+

2c,dd(1 2

+

2)

+

8d(d

+

24 4

+

d4)

2 n

,

(12)

where c,d

d 2

log(22)

+

d 2

.

In

particular,

sup ESn h(P  ) - h^SP(Sn) = O,d
P Fd

Polylong(n)

,

(13)

and the right-hand sides (RHSs) of (12) and (13) are, respectively, explicit and implicit upper bounds on the minimax absolute-error risk Rd(n, ).

8

Under review as a conference paper at ICLR 2019

Remark 3 (Comparison to General-Purpose Estimators) Note that one could always sample  and add up these noise samples to Sn to obtain a sample set from P  . These samples can be used to get a proxy of h(P  ) via a kNN- or a KDE-based differential entropy estimator. However, P   violated the boundedness away from zero assumption that most of the convergence rate results in the literature rely on (Levit, 1978; Hall, 1984; Joe, 1989; Hall & Morton, 1993; Tsybakov & der Meulen, 1996; Haje & Golubev, 2009; K et al., 2012; Singh & Póczos, 2016; Kandasamy et al., 2015). The only result we are aware of that analyses a differential entropy estimator (namely, the kNN-based estimator from (A. Kraskov & Grassberger, 2004)) without assuming the density is bounded from below (Jiao et al., 2017) relies on the density being supported inside [0, 1]d, satisfying periodic boundary conditions and having a Hölder smoothness parameter s  (0, 2]. The convolved density P   satisfies neither of these three conditions. Furthermore, because the SP estimator is constructed to exploit the particular structure of our estimation setup it achieves a fast convergence

rate of

Polylog(n) n

. The risk associated with unstructured differential entropy estimators typically

converges as the slower O

n-

s s+d

. This highlights the advantage of ad-hoc estimation as opposed

to general-purpose estimation.

Theorem 1 provides convergence rates when estimating differential entropy (or mutual information) over DNNs with bounded activation functions, such as tanh or sigmoid. To account for networks with unbounded nonlinearities, such as ReLU networks, the following theorem gives a more general result of estimation over the nonparametric class Fd(S,KG) of d-dimensional distributions with subgaussian marginals.

Theorem 4 (SP Estimator Absolute-Error Risk for Subgaussians) Fix  > 0 and d  1. The absolute-error risk of the SP estimator (11) over the class Fd(S,KG), for all n sufficiently large, is bounded as

sup ESn h(P  ) - h^SP(Sn)  log
P Fd(S,KG)

 n

8(K + )2 (e - 1)2

d 4

2(K (e -

+ )2 1)2

log

n

+

c,d + 2dK2

2+

2

c,d + 2dK2 2

2d + 2dK2

8 16d2K4 + 4d(2 + d) + 4

d
4 1 n

ed n

,

(14)

where c,d

d 2

log(22).

In

particular,

sup ESn h(P  ) - h^SP(Sn) = O,d
P Fd(S,KG)

Polylog(n) n

,

(15)

and the RHSs of (14) and (15) are, respectively, explicit and implicit upper bounds on the minimax absolute-error risk Rd,K (n, ).

As mentioned in Remark 1, the class Fd(S,KG) is rather general, and, in particular, includes Fd whenever K  1. This means that Theorem 4 also provides an upper bound on the minimax risk under the setup of Theorem 1. Nonetheless, we chose to separately state Theorem 1 since the derivation under the bounded support assumption enables extracting slightly better constants (which is important for our applications - see Section 5). We do highlight, however, that the expressions from (12) and (14) with K = 1 not only have the same convergence rates, but their constants are also very close.

Remark 4 (Near Minimax Rate-Optimality)

A

convergence

rate

faster

than

1 n

cannot

be

at-

tained for parameter estimation under the absolute-error loss. This follows from, e.g., Proposition

1 of (Chen, 1997), which establishes this convergence rate as a lower bound for the parametric

estimation problem given n i.i.d. samples. Consequently, the convergence rate of O,d

Polylog(n) n

established in Theorems 1 and 4 for the SP estimator is near minimax rate-optimal (i.e., up to

logarithmic factors).

9

Under review as a conference paper at ICLR 2019

Remark 5 (Mutual Information Estimation) Denoting the upper bound on the estimation error from Theorem 1 or 4 by n(, d), we see that the error of the mutual information estimator from (2) is bounded as by 2n(, d), which vanishes as n  .
10.2.1 SAMPLE PROPAGATION ESTIMATOR BIAS
The results of the previous subsection are of minimax flavor. That is, they state worst-case convergence rates of the SP estimation over a certain nonparametric class of distributions. In practice, the true distribution may very well not be one that attains these worst-case rates, and convergence may be faster. However, while variance of h^SP(Sn) can be empirically evaluated using bootstrapping, there is no empirical test for the bias. Even if multiple estimations of h(P  ) via h^SP(Sn) consistently produce similar values, this does not necessarily suggest that these values are close to the true h(P  ). To have a guideline to the least number of samples needed to avoid biased estimation, we present the following lower bound on supP Fd ESn h(P  ) - h^SP(Sn) .

Theorem 5 (SP Estimator Bias Lower Bound) Fix d  1 and  > 0, and let 

1-

1 - 2Q

1 2

d
,1

, where Q is the Q-function.3 Set k

1

Q-1

1 2

1
1-(1- ) d

, where

Q-1 is the inverse of the Q-function. By the choice of , clearly k  2, and the bias of the SP

estimator over the class Fd is bounded as

sup ESn h(P  ) - h^SP(Sn)  log
P Fd

kd(1- ) n

- Hb( ).

(16)

Consequently, the bias cannot be less than a given  > 0 so long as n  kd(1- ) · e-(+Hb( )).

Since Hb( ) shrinks with , for sufficiently small values the lower bound from (16) essentially shows

that the SP estimator will not have negligible bias unless n > kd(1- ) is satisfied. The condition

> 1-

1 - 2Q

1 2

d is non-restrictive in any relevant regime of  and d. For instance, for

typical  values we work with - around 0.1 - this lower bound is at most 0.0057 for all dimensions

up to at least d = 104. Setting, e.g., = 0.01 (for which Hb(0.01)  0.056), the corresponding k equals 3 for d  11 and 2 for 12  d  104. Thus, with these parameters, the number of

estimation samples n should be at least 20.99d, for any conceivably relevant dimension, in order to

have negligible bias.

10.3 COMPUTING THE SAMPLE PROPAGATION ESTIMATOR

Evaluating the mutual information estimator from (2) requires computing the differential entropy of a Gaussian mixture. Although it cannot be computed in closed form, this section presents a method for approximate computation via MC integration (Robert, 2004). To simplify the presentation, we present the method for an arbitrary Gaussian mixture without referring to the notation of the estimation setup.

Let g(t)

1 n

i[n] (t - µi) be a d-dimensional, n-mode Gaussian mixture, with {µi}i[n]  Rd

and  as the PDF of N (0, 2Id). Let C  Unif{µi}i[n] be independent of Z   and note that

V C + Z  g.

We use Monte Carlo (MC) integration (Robert, 2004) to compute the h(g). First note that

h(g)

=

-E

log

g(V

)

=

-

1 n

E

log g(µi + Z) C = µi

=

-

1 n

E log g(µi + Z), (17)

i[n]

i[n]

3The Q-function is defined as Q(x)

1 2

 x

e-

t2 2

dt.

10

Under review as a conference paper at ICLR 2019

where the last step follows by the independence of Z and C. Let Zj(i) i[n] be n × nMC i.i.d.
j [nMC ]
samples from . For each i  [n], we estimate the i-th summand on the RHS of (17) by

which produces

I^M(iC)

1 nMC

log g
j [nMC ]

µi + Zj(i)

,

h^ MC

1 n

I^M(iC)

i[n]

(18a) (18b)

as our estimate of h(g). Define the mean squared error (MSE) of h^MC as

MSE h^MC

E h^MC - h(g) 2 .

(19)

We have the following bounds on the MSE for tanh and ReLU networks.

Theorem 6 (MSE Bounds for MC Estimator)

(i) Assume C  [-1, 1]d almost surely (i.e., tanh network), then

MSE

h^ MC



n

·

1 nMC

2d(2 + 2

2) .

(20)

(ii)

Assume MC MSE

EC h^ MC

2 2

< 

 n·

(e.g., ReLU network with bounded 2nd

 1 9d2 + 8(2 +  d)MC + 3(11

nMC

2

moments), then 
d + 1) MC .

(21)

The bounds on the MSE scale only linearly with the dimension d, making 2 in the denominator often the dominating factor experimentally.

Remark 6 (Comparison to Generic Entropy Estimation) We briefly present empirical results illustrating the convergence of the SP estimator and comparing it to two current state-of-the-art methods: the KDE-based estimator of (Kandasamy et al., 2015) and the kNN-based estimator often known as the Kozachenko­Leonenko (KL) nearest neighbor estimator (Kozachenko & Leonenko, 1987; Jiao et al., 2017). In this example, the distribution P of S is set to be a mixture of Gaussians truncated to have support in [-1, 1]d. Before truncation, the mixture consists of 2d Gaussian components with means at the 2d corners of [-1, 1]d. The entropy of P  , i.e., h(S + Z), where Z  N (0, 2Id), is estimated and various values of  are examined. Fig. 14 shows the estimation error results as a function of the number of samples n for dimensions d = 5 and d = 10. The kernel width for the KDE estimate was chosen via cross-validation, varying with both d and n; the kNN estimator and h^SP(Sn) require no tuning parameters. We found that the KDE estimate is highly sensitive to the choice of kernel width, the curves shown correspond to optimized values and are highly unstable to any change in kernel width. Note that both the kNN and the KDE estimators converge slowly, at a rate that degrades with increased d. This rate is worse than that of h^SP, which also lower bounds the true entropy (as according to our theory - see (Anonymized, 2018, Equation (60))).

REFERENCES
H. Stögbauer A. Kraskov and P. Grassberger. Estimating mutual information. Phys. rev. E, 69(6): 066138, June 2004.
Anonymized. Differential entropy estimation under Gaussian convolutions. Submitted to IEEE Transactions on Information Theory, 2018.
J. Chen. A general lower bound of minimax risk for absolute-error loss. Canadian Journal of Statistics, 25(4):545­558, Dec. 1997.

11

Under review as a conference paper at ICLR 2019
Figure 14: Estimation results for the SP estimator compared to state-of-the-art kNN-based and KDE-based differential entropy estimators. The differential entropy of S + Z is estimated, where S is a truncated d-dimensional mixture of 2d Gaussians and Z  N (0, 2Id). Results are shown as a function of n, for d = 5, 10 and  = 0.1, 0.5. The SP estimator presents faster convergence rates, improved stability and better scalability with dimension compared to the two competing methods. M. Cisse, P. Bojanowski, E. Grave, Y. Dauphin, and N. Usunier. Parseval networks: Improving
robustness to adversarial examples. In Proceedings of the International Conference on Machine Learning (ICML), 2017. H. F. El Haje and Y. Golubev. On entropy estimation by m-spacing method. Journal of Mathematical Sciences, 163(3):290­309, Dec. 2009. P. Hall. Limit theorems for sums of general functions of m-spacings. Mathematical Proceedings of the Cambridge Philosophical Society, 96(3):517­532, Nov. 1984. P. Hall and S. C. Morton. On the estimation of entropy. Annals of the Institute of Statistical Mathematics, 45(1):69­88, Mar. 1993. J. Jiao, W. Gao, and Y. Han. The nearest neighbor information estimator is adaptively near minimax rate-optimal. arXiv:1711.08824 [stat.ML], 2017. H. Joe. Estimation of entropy and other functionals of a multivariate density. Annals of the Institute of Statistical Mathematics, 41(4):683­697, Dec. 1989. Sricharan K, R. Raich, and A. O. Hero. Estimation of nonlinear functionals of densities with confidence. IEEE Trans. Inf. Theory, 58(7):4135­4159, Jul. 2012. K. Kandasamy, A. Krishnamurthy, B. Poczos, L. Wasserman, and J. M. Robins. Nonparametric von Mises estimators for entropies, divergences and mutual informations. In Advances in Neural Information Processing Systems (NIPS), pp. 397­405, 2015. L. F. Kozachenko and N. N. Leonenko. Sample estimate of the entropy of a random vector. Problemy Peredachi Informatsii, 23(2):9­16, 1987. B. Y. Levit. Asymptotically efficient estimation of nonlinear functionals. Problemy Peredachi Informatsii, 14(3):65­72, 1978.
12

Under review as a conference paper at ICLR 2019 A. Paszke, S. Gross, S. Chintala, G. Chanan, E. Yang, Z. DeVito, Z. Lin, A. Desmaison, L. Antiga,
and A. Lerer. Automatic differentiation in PyTorch. In NIPS Autodiff Workshop, 2017. Christian P Robert. Monte Carlo Methods. Wiley Online Library, 2004. A. M. Saxe, Y. Bansal, J. Dapello, M. Advani, A. Kolchinsky, B. D. Tracey, and D. D. Cox. On the
information bottleneck theory of deep learning. In Proceedings of the International Conference on Learning Representations (ICLR), 2018. S. Singh and B. Póczos. Finite-sample analysis of fixed-k nearest neighbor density functional estimators. In Advances in Neural Information Processing Systems, pp. 1217­1225, 2016. A. B. Tsybakov and E. C. Van der Meulen. Root-n consistent estimators of entropy for densities with unbounded support. Scandinavian Journal of Statistics, pp. 75­83, Mar. 1996.
13


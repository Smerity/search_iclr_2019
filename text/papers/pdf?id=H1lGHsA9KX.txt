Under review as a conference paper at ICLR 2019

A RESIZABLE MINI-BATCH GRADIENT DESCENT BASED ON A MULTI-ARMED BANDIT
Anonymous authors Paper under double-blind review

ABSTRACT
Determining the appropriate batch size for mini-batch gradient descent is always time consuming as it often relies on grid search. This paper considers a resizable mini-batch gradient descent (RMGD) algorithm based on a multi-armed bandit for achieving best performance in grid search by selecting an appropriate batch size at each epoch with a probability defined as a function of its previous success/failure. This probability encourages exploration of different batch size and then later exploitation of batch size with history of success. At each epoch, the RMGD samples a batch size from its probability distribution, then uses the selected batch size for mini-batch gradient descent. After obtaining the validation loss at each epoch, the probability distribution is updated to incorporate the effectiveness of the sampled batch size. The RMGD essentially assists the learning process to explore the possible domain of the batch size and exploit successful batch size. Experimental results show that the RMGD achieves performance better than the best performing single batch size. Furthermore, it, obviously, attains this performance in a shorter amount of time than grid search. It is surprising that the RMGD achieves better performance than grid search.

1 INTRODUCTION

Gradient descent (GD) is a common optimization algorithm for finding the minimum of the expected loss. It takes iterative steps proportional to the negative gradient of the loss function at each iteration. It is based on the observation that if the multi-variable loss functions f (w) is differentiable at point w, then f (w) decreases fastest in the direction of the negative gradient of f at w, i.e., -f (w). The model parameters are updated iteratively in GD as follows:

wt+1 = wt - tgt,

gt = wf (wt)

where wt, gt, and t are the model parameters, gradients of f with respect to w, and learning rate at time t respectively. For small enough t, f (wt)  f (wt+1) and ultimately the sequence of wt will move down toward a local minimum. For a convex loss function, GD is guaranteed to converge
to a global minimum with an appropriate learning rate.

There are various issues to consider in gradient-based optimization. First, GD can be extremely slow and impractical for large dataset: gradients of all the data have to be evaluated for each iteration. With larger data size, the convergence rate, the computational cost and memory become critical, and special care is required to minimize these factors. Second, for non-convex function which is often encountered in deep learning, GD can get stuck in a local minimum without the hope of escaping. Third, stochastic gradient descent (SGD), which is based on the gradient of a single training sample, has large gradient variance, and it requires a large number of iterations. This ultimately translates to slow convergence. Mini-batch gradient descent (MGD), which is based on the gradient over a small batch of training data, trades off between the robustness of SGD and the stability of GD. There are three advantages for using MGD over GD and SGD: 1) The batching allows both the efficiency of memory usage and implementations; 2) The model update frequency is higher than GD which allows for a more robust convergence avoiding local minimum; 3) MGD requires less iteration per epoch and provides a more stable update than SGD. For these reasons, MGD has been a popular algorithm for machine learning. However, selecting an appropriate batch size is difficult. Various studies suggest that there is a close link between performance and batch size used in MGD Breuel (2015); Keskar et al. (2016); Wilson & Martinez (2003).

1

Under review as a conference paper at ICLR 2019
There are various guidelines for selecting a batch size but have not been completely practical Bengio (2012). Grid search is a popular method but it comes at the expense of search time. There are a small number of adaptive MGD algorithms to replace grid search Byrd et al. (2012); De et al. (2016); Friedlander & Schmidt (2012). These algorithms increase the batch size gradually according to their own criterion. However, these algorithms are based on convex loss function and hard to be applied to deep learning. For non-convex optimization, it is difficult to determine the optimal batch size for best performance.
This paper considers a resizable mini-batch gradient descent (RMGD) algorithm based on a multi-armed bandit for achieving best performance in grid search by selecting an appropriate batch size at each epoch with a probability defined as a function of its previous success/failure. At each epoch, RMGD samples a batch size from its probability distribution, then uses the selected batch size for mini-batch gradient descent. After obtaining the validation loss at each epoch, the probability distribution is updated to incorporate the effectiveness of the sampled batch size. The benefit of RMGD is that it avoids the need for cumbersome grid search to achieve best performance and that it is simple enough to apply to any optimization algorithm using MGD. The detailed algorithm of RMGD are described in Section 4, and experimental results are presented in Section 5.
2 RELATED WORKS
There are only a few published results on the topic of batch size. It was empirically shown that SGD converged faster than GD on a large speech recognition database Wilson & Martinez (2003). It was determined that the range of learning rate resulting in low test errors was considerably getting smaller as the batch size increased on convolutional neural networks and that small batch size yielded the best test error, while large batch size could not yield comparable low error rate Breuel (2015). It was observed that larger batch size are more liable to converge to a sharp local minimum thus leading to poor generalization Keskar et al. (2016). It was found that the learning rate and the batch size controlled the trade-off between the depth and width of the minima in MGD Jastrzkebski et al. (2017).
A small number of adaptive MGD algorithms have been proposed. Byrd et al. (2012) introduced a methodology for using varying sample size in MGD. A relatively small batch size is chosen at the start, then the algorithm chooses a larger batch size when the optimization step does not produce improvement in the target objective function. They assumed that using a small batch size allowed rapid progress in the early stages, while a larger batch size yielded high accuracy. However, this assumption did not corresponded with later researches that reported the degradation of performance with large batch size Breuel (2015); Keskar et al. (2016); Mishkin et al. (2017). Another similar adaptive algorithm, which increases the batch size gradually as the iteration proceeded, was done by Friedlander & Schmidt (2012). The algorithm uses relatively few samples to approximate the gradient, and gradually increase the number of samples with a constant learning rate. It was observed that increasing the batch size is more effective than decaying the learning rate for reducing the number of iterations Smith et al. (2017). However, these increasing batch size algorithm lacks flexibility since it is a unidirectional. Balles et al. (2017) proposed a dynamic batch size adaptation algorithm. It estimates the variance of the stochastic gradients and adapts the batch size to decrease the variance. However, this algorithm needs to find the gradient variance and its computation depends on the number of model parameters.
3 SETUP
Let B = {bk}kK=1 be a batch size set and  = {k}Kk=1 be a probability distribution of batch size where bk, k, and K are the kth batch size, the probability of bk to be selected, and number of batch sizes respectively. This paper considers algorithm for multi-armed bandit over B according to Algorithm 1. Let w  W be the model parameters at epoch  , and w~t be the temporal parameters at sub iteration t. Let J : W  R be the training loss function and let g = J(w) be the gradients of training loss function with respect to the model parameters.  is the learning rate at epoch  . Let : W  R be the validation loss function, and yk  {0, 1} be the cost of choosing the batch
2

Under review as a conference paper at ICLR 2019

Figure 1: An overall framework of considered resizable mini-batch gradient descent algorithm (RMGD). The RMGD samples a batch size from a probability distribution, and parameters are updated by mini-batch gradient using the selected batch size. Then the probability distribution is updated by checking the validation loss.
size bk. In here, yk = 0 if the validation loss decreases by the selected batch size bk (well-updating) and yk = 1 otherwise (misupdating). The aim of the algorithm is to have low misupdating. For the cost function yk, hinge loss can be a variation of 0-1 loss. However, there is no difference in regret bound between 0-1 loss and hinge loss in this setting and it is experimentally confirmed that there is little performance gap between the two losses. Therefore, this paper uses the 0-1 loss, which is simple and basic.

4 RESIZABLE MINI-BATCH GRADIENT DESCENT
The resizable mini-batch gradient descent (RMGD) sets the batch sizes as multi arms, and at each epoch it samples one of the batch sizes from probability distribution. Then, it suffers a cost of selecting this batch size. Using the cost, probability distribution is updated.

4.1 ALGORITHMS
The overall framework of the RMGD algorithm is shown in Figure 1. The RMGD consists of two components: batch size selector and parameter optimizer. The selector samples a batch size from probability distribution and updates the distribution. The optimizer is usual mini-batch gradient.

Selector samples a batch size bk  B from the probability distribution  at each epoch  where k is selected index. Here bk is associated with probability k. The selected batch size bk is applied to optimizer for MGD at each epoch, and the selector gets cost yk from optimizer. Then, the selector updates probabilities,

for i = k , for i = k ,
i,

~i = i e-yk /i ~i = i
i +1 = ~i/ ~j
j

where   (0, 1) is positive hyperparameter. When  = 0,  = {1/K, . . . , 1/K}.

Optimizer updates the model parameters w. For each epoch, temporal parameters w~0 is set to w , and MGD iterates T = m/bk 1 times using the selected batch size bk where m is the total
1 x is the least integer that is greater than or equal to x

3

Under review as a conference paper at ICLR 2019

number of training samples:

w~t+1 = w~t -  gt, gt = J (w~t).
After T iterations at epoch  , the model parameters is updated as w+1 = w~T . Then, the optimizer obtains validation loss , and outputs cost as follows:

yk =

0 1

if (w+1) < otherwise

(w )

.

Algorithm 1 Resizable Mini-batch Gradient Descent
Input: B = {bk}kK=1 : Set of batch sizes 0 = {1/K, . . . , 1/K} : Prior probability distribution
Procedure:
1: Initialize model parameters w0 2: for epoch  = 0, 1, 2, . . . 3: Select batch size bk  B from  4: Set temporal parameters w~0 = w 5: for t = 0, 1, . . . , T - 1 where T = m/bk 6: Compute gradient gt = J(w~t) 7: Update w~t+1 = w~t -  gt 8: end for 9: Update w+1 = w~T 10: Observe validation loss (w+1) 11: if (w+1) < (w ) 12: Get cost yk = 0 13: else 14: Get cost yk = 1 15: end if 16: for i = 1, 2, . . . , K 17: if i = k 18: Set temporal probability ~i = i e-yk /i 19: else 20: Set temporal probability ~i = i 21: end if 22: end for 23: Update i  [K], i +1 = ~i/ j ~j 24: end for
The RMGD samples an appropriate batch size from a probability distribution at each epoch. This probability distribution encourages exploration of different batch size and then later exploits batch size with history of success, which means decreasing validation loss. Figure 2 shows an example of training progress of RMGD. The figure represents the probability distribution with respect to epoch. The white dot represents the selected batch size at each epoch. In the early stage of training, commonly, all batch sizes tend to decrease validation loss:  is uniform. Thus, all batch size have equal probability of being sampled (exploration). In the later stages of training, the probability distribution varies based on success and failure. Thus, better performing batch size gets higher probability to be sampled (exploitation). In this case, 256 is the best performing batch size.

4.2 REGRET BOUND

The regret bound of the RMGD follows the regret bound derived in Shalev-Shwartz et al. (2012). The goal of this algorithm is to have low regret for not selecting the best performing batch size such that

TT

RegretT (S) = E

yk

- min
i

yi

 =1

 =1

4

Under review as a conference paper at ICLR 2019

Figure 2: The probability distribution vs epoch using the RMGD. (top) The early stages of the training. (bottom) The later stages of the training. The white dot represents the selected batch size at each epoch. In the early stages of the training, RMGD updates the probabilities to search various batch sizes (exploration), and in the later stages, RMGD increases the probability of successful batch size (exploitation).

where the expectation is over the algorithm's randomness of batch size selection. The regret of the

RMGD is bounded,

E

T
yk

T
- min
i

yi



log K 

+ KT

.

 =1

 =1

In particular, setting  = log(K)/(KT ), the regret is bounded by 2 K log(K)T , which is sublinear with T . The detailed derivation of regret bound is described in the appendix A.

5 EXPERIMENTS
This section describes various experimental results on MNIST, CIFAR10, and CIFAR100 dataset. In the experiments, simple convolutional neural networks (CNN) is used for MNIST and `All-CNNC' Springenberg et al. (2014) is used for CIFAR10 and CIFAR100. The details of the dataset and experimental settings are presented in the appendix B.
5.1 MNIST DATASET
The validity of the RMGD was assessed by performing image classification on the MNIST dataset using AdamOptimizer and AdagradOptimizer as optimizer. The experiments were repeated 100 times for each algorithm and each optimizer, then the results were analyzed for significance. Figure 3 shows the probability distribution and the selected batch size with respect to epoch during training for the RMGD. The white dot represents the batch size selected at each epoch. The top figure is the case that small batch size (32) performs better. After epoch 50, batch size 32 gets high probability and is selected more than others. It means that batch size 32 has less misupdating in this case. The gradually increasing batch size algorithm may not perform well in this case. The middle figure is the case that large batch size (512) performs better. After epoch 60, batch size 512 gets high probability and selected more than others. The bottom figure shows that the best performing batch size varies with epoch. During epoch from 40 to 55, batch size of 256 performs best, and best performing batch size switches to 128 during epoch from 60 to 70, then better performing batch size backs to 256 after epoch 80. In the results, any batch size can be a successful batch size in the later stages without any particular order. The RMGD is more flexible for such situation than the MGD or directional adaptive MGD such as gradually increasing batch size algorithm.
5

Under review as a conference paper at ICLR 2019
Figure 3: The probability distribution and selected batch size. The white dot is selected batch size at epoch. (top) The case that small batch size performs better. (middle) The case that large batch size performs better. (bottom) The case that best performing batch size varies.
Figure 4: The results of test accuracy for the MNIST dataset. The number in parenthesis next to MGD represents the batch size used in the MGD. The error bar is standard error. (left) The test accuracy of 100 times repeated experiments with AdamOptimizer. (right) The test accuracy of 100 times repeated experiments with AdagradOptimizer. In both cases, the RMGD outperforms all fixed MGD algorithms.
Figure 4 shows the test accuracy of each algorithm. The error bar is standard error. The number in parenthesis next to MGD represents the batch size used in the MGD. The left figure is the test accuracy with AdamOptimizer. The right figure is the test accuracy with AdagradOptimizer. Among the MGD algorithms, relatively small batch sizes (16 - 64) lead to higher performance than large batch sizes (128 - 512) and batch size 64 achieves the best performance in grid search. These results correspond with other studies Breuel (2015); Keskar et al. (2016); Mishkin et al. (2017). The RMGD outperforms all fixed MGD algorithm in both case. Although the performance of RMGD is not significantly increased compared to the MGD, the purpose of this algorithm is not to improve performance, but to ensure that the best performance is achieved without performing a grid search on the batch size. Rather, the improved performance of the RMGD is a surprising result. Therefore, the RMGD is said to be valid. Table 1 and 2 present iterations and real time for training, mean, maximum, and minimum of test accuracies for each algorithm with AdamOptimizer and AdagradOptimizer respectively. The MGD (total) is the summation of the iterations and real time of whole MGDs for grid search. The RMGD outperforms best performing MGD and is, also, faster than best performing MGD. Furthermore, it is 8 times faster than grid search in both cases. In the results, the RMGD is effective regardless of the optimizer.
5.2 CIFAR10 AND CIFAR100 DATASET The CIFAR10 and CIFAR100 dataset were, also, used to assess effectiveness of the RMGD. The experiments were repeated 25 times and 10 times respectively. In these experiments, all images are whitened and contrast normalized before being input to the network. Figure 5 shows the test
6

Under review as a conference paper at ICLR 2019

Table 1: Iterations and real time for training, and test accuracy of MNIST classification with

AdamOptimizer. The 'total' is the sum of the average values from MGD 16 to 512, which means

the whole grid search is performed.

Algorithms

Iterations

Real time (sec)

Test accuracy (%)

Mean ± SD

Max Min

MGD (16)

343,800

1,221.54 ± 36.00 99.327 ± 0.064 99.480 99.140

MGD (32)

171,900

697.82 ± 19.70 99.322 ± 0.060 99.500 99.150

MGD (64)

86,000

379.14 ± 11.32 99.328 ± 0.058 99.460 99.170

MGD (128)

43,000

262.33 ± 2.34 99.314 ± 0.056 99.440 99.170

MGD (256)

21,500

208.13 ± 2.20 99.295 ± 0.059 99.470 99.170

MGD (512)

10,800

180.06 ± 0.37 99.254 ± 0.054 99.430 99.110

MGD (total)

677,000

2,949.02

RMGD

68,309 ± 8,900 333.73 ± 25.38 99.342 ± 0.064 99.480 99.110

Table 2: Iterations and real time for training, and test accuracy of MNIST classification with Ada-

gradOptimizer.

Algorithms

Iterations

Real time (sec)

Test accuracy (%)

Mean ± SD

Max Min

MGD (16)

343,800

1,160.87 ± 22.34 99.268 ± 0.090 99.430 98.920

MGD (32)

171,900

640.68 ± 15.53 99.270 ± 0.070 99.410 99.050

MGD (64)

86,000

367.40 ± 12.63 99.277 ± 0.077 99.440 99.110

MGD (128)

43,000

262.48 ± 1.37 99.269 ± 0.069 99.410 99.080

MGD (256)

21,500

195.60 ± 2.00 99.240 ± 0.072 99.390 99.030

MGD (512)

10,800

170.31 ± 1.41 99.198 ± 0.085 99.390 98.810

MGD (total)

677,000

2,797.34

RMGD

68,159 ± 8,447 323.33 ± 23.57 99.286 ± 0.088 99.490 98.900

accuracy for each algorithm. The left figure represents the test accuracy on CIFAR10. In contrast to the MNIST results, relatively large batch sizes (128 - 256) lead to higher performance than small batch sizes (16 - 64) and batch size 256 achieves the best performance in grid search. The right figure represents the test accuracy on CIFAR100 and batch size 128 achieves the best performance in grid search. The results on MNIST, CIFAR10 and CIFAR100 indicate that it is difficult to know which batch size is optimal before performing a grid search. Meanwhile, the RMGD has again exceeded the best performance of fixed MGD.
Table 3 and 4 present the detailed results on CIFAR10 and CIFAR100 dataset. The RMGD is a little slower than single best performing MGD (256 for CIFAR10 and 128 for CIFAR100), however, it was much faster than grid search -about 4.6 times on CIFAR10 and 5.0 times on CIFAR100 faster. Therefore, this results, also, show the effectiveness of the RMGD.
It is difficult to compare the RMGD with other adaptive batch size algorithm, e.g. coupling adaptive batch sizes (CABS) Balles et al. (2017), directly since the underlying goals are different. While the goal of the RMGD is to reduce the validation loss in terms of generalization performance, the CABS determines the batch size to balance between the gradient variance and computation. However, it is obvious that the RMGD is simpler and easier to implement than any other adaptive algorithm cited in this paper, and comparing the test accuracy between the RMGD and the CABS on the CIFAR10 and CIFAR100 using the same experimental settings with 'All-CNN-C' shows that the performance of the RMGD is higher than that of the CABS (CIFAR10: 87.862 ± 0.142, CIFAR100: 60.782 ± 0.421). And again, the purpose of this algorithm is not to outperform other algorithms, but to guarantee that the best performance is reached without grid search.
CONCLUSION
Selecting batch size affects the model quality and training efficiency, and determining the appropriate batch size is time consuming and requires considerable resources as it often relies on grid search. The focus of this paper is to design a simple robust algorithm that is theoretically sound
7

Under review as a conference paper at ICLR 2019

Figure 5: The results of test accuracy for the CIFAR10 and CIFAR100 dataset. The error bar is standard error. (left) The test accuracy of 25 times repeated experiments on CIFAR10. (right) The test accuracy of 10 times repeated experiments on CIFAR100. In both cases, the RMGD outperforms all fixed MGD algorithms.

Table 3: Iterations and real time for training, and test accuracy on CIFAR10.

Algorithms

Iterations

Real time (sec)

Test accuracy (%)

Mean ± SD Max Min

MGD (16)

1,072,050

10,085.26 ± 216.48 87.778 ± 0.207 88.290 87.480

MGD (32)

536,200

7,643.93 ± 459.95 87.851 ± 0.160 88.250 87.630

MGD (64)

268,100

6,160.16 ± 68.54 87.853 ± 0.202 88.330 87.450

MGD (128)

134,050

5,675.15 ± 181.80 87.873 ± 0.234 88.210 87.090

MGD (256)

67,200

5,466.79 ± 402.20 87.897 ± 0.293 88.260 87.170

MGD (total)

2,077,600

35,031.29

RMGD 463,629 ± 48,692 7,592.43 ± 403.65 88.004 ± 0.167 88.380 87.780

Table 4: Iterations and real time for training, and test accuracy on CIFAR100.

Algorithms

Iterations

Real time (sec)

Test accuracy (%)

Mean ± SD Max Min

MGD (16)

1,072,050

12,097.47 ± 57.47 60.247 ± 0.690 61.940 59.620

MGD (32)

536,200

8,058.14 ± 39.87 60.475 ± 0.721 61.750 59.290

MGD (64)

268,100

6,400.21 ± 12.78 60.628 ± 0.795 61.950 59.170

MGD (128)

134,050

5,598.85 ± 38.18 60.954 ± 0.834 62.120 59.530

MGD (256)

67,200

5,245.88 ± 40.67 60.504 ± 0.553 61.560 59.830

MGD (total)

2,077,600

37,400.55

RMGD 425,416 ± 44,392 7,503.09 ± 281.79 61.203 ± 0.502 62.050 60.310

and applicable in many situations. This paper considers a resizable mini-batch gradient descent (RMGD) algorithm based on a multi-armed bandit for achieving best performance in grid search by selecting an appropriate batch size at each epoch with a probability defined as a function of its previous success/failure. This probability encourages exploration of different batch size and then later exploitation of batch size with history of success. The goal of this algorithm is not to achieve state-of-the-art accuracy but rather to select appropriate batch size which leads low misupdating and performs better. The RMGD essentially assists the learning process to explore the possible domain of the batch size and exploit successful batch size. The benefit of RMGD is that it avoids the need for cumbersome grid search to achieve best performance and that it is simple enough to apply to various field of machine learning including deep learning using MGD. Experimental results show that the RMGD achieves the best grid search performance on various dataset, networks, and optimizers. Furthermore, it, obviously, attains this performance in a shorter amount of time than the grid search. In conclusion, the RMGD is effective and flexible mini-batch gradient descent algorithm.
8

Under review as a conference paper at ICLR 2019
REFERENCES
Lukas Balles, Javier Romero, and Philipp Hennig. Coupling adaptive batch sizes with learning rates. In Proceedings of the Thirty-Third Conference on Uncertainty in Artificial Intelligence, UAI 2017, Sydney, Australia, August 11-15, 2017, 2017. URL http://auai.org/uai2017/ proceedings/papers/141.pdf.
Yoshua Bengio. Practical recommendations for gradient-based training of deep architectures. In Neural networks: Tricks of the trade, pp. 437­478. Springer, 2012.
James Bergstra and Yoshua Bengio. Random search for hyper-parameter optimization. Journal of Machine Learning Research, 13(Feb):281­305, 2012.
Thomas M Breuel. The effects of hyperparameters on sgd training of neural networks. arXiv preprint arXiv:1508.02788, 2015.
Richard H Byrd, Gillian M Chin, Jorge Nocedal, and Yuchen Wu. Sample size selection in optimization methods for machine learning. Mathematical programming, 134(1):127­155, 2012.
Soham De, Abhay Yadav, David Jacobs, and Tom Goldstein. Big batch sgd: Automated inference using adaptive batch sizes. arXiv preprint arXiv:1610.05792, 2016.
Michael P Friedlander and Mark Schmidt. Hybrid deterministic-stochastic methods for data fitting. SIAM Journal on Scientific Computing, 34(3):A1380­A1405, 2012.
Stanislaw Jastrzkebski, Zachary Kenton, Devansh Arpit, Nicolas Ballas, Asja Fischer, Yoshua Bengio, and Amos Storkey. Three factors influencing minima in sgd. arXiv preprint arXiv:1711.04623, 2017.
Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak Peter Tang. On large-batch training for deep learning: Generalization gap and sharp minima. arXiv preprint arXiv:1609.04836, 2016.
Dmytro Mishkin, Nikolay Sergievskiy, and Jiri Matas. Systematic evaluation of convolution neural network advances on the imagenet. Computer Vision and Image Understanding, 161:11­19, 2017.
Shai Shalev-Shwartz et al. Online learning and online convex optimization. Foundations and Trends R in Machine Learning, 4(2):107­194, 2012.
Samuel L Smith, Pieter-Jan Kindermans, and Quoc V Le. Don't decay the learning rate, increase the batch size. arXiv preprint arXiv:1711.00489, 2017.
Jost Tobias Springenberg, Alexey Dosovitskiy, Thomas Brox, and Martin Riedmiller. Striving for simplicity: The all convolutional net. arXiv preprint arXiv:1412.6806, 2014.
D Randall Wilson and Tony R Martinez. The general inefficiency of batch training for gradient descent learning. Neural Networks, 16(10):1429­1451, 2003.
9

Under review as a conference paper at ICLR 2019

APPENDIX

A REGRET BOUND

In the RMGD algorithm, there are K batch sizes as multi arms with the probability distribution   S, and at each epoch the algorithm should select one of the batch sizes bk . Then it receives a cost of selecting this arm, yk  {0, 1} by testing the validation loss . The vector y  {0, 1}K represents the selecting cost for each batch size. The goal of this algorithm is to have low regret for
not selecting the best performing batch size.

TT

RegretT (S) = E

yk

- min
i

yi

 =1

 =1

where the expectation is over the algorithm's randomness of batch size selection. To analyze the
regret bound of the RMGD, online mirror descent (OMD) with estimated gradients algorithm is
applied. For OMD setting, let S be the probability simplex and the selecting loss functions be f () = , y . The algorithm select a batch size with probability P[bk ] = k and therefore f ( ) is the expected cost of the selected batch size at epoch  . The gradient of the selecting loss function is y . However, only one element yk is known at each epoch. To estimate gradient, random vector z is defined as follows:

zi =

yi /i if i = k 0 otherwise

and expectation of z satisfies,

K
E[z |z-1, . . . , z0] = P[bk ]zi
i=1

=

k

yk k

= yk .

The probability distribution  is updated by the rule of the normalized exponentiated gradient
(normalized-EG) algorithm described in Algorithm 1. The selecting loss function is linear and it is satisfied that , i we have zi  -1. Then,

T

 - , z

 log(K) +  

T

K
i (zi )2

 =1

 =1 i=1

where   S is a fixed vector which minimizes the cumulative selecting loss,

T
 = arg min f ().
S  =1

Since f is convex and z is estimated gradients for all  ,

E

T
(f ( ) - f ())

 log(K) +  T 

E

K
i (zi )2 .

 =1

 =1 i=1

The last term is bounded as follows:
K
E i (zi )2
i=1

KK

= P[k = j] i (zi )2

j=1

i=1

K
= (j )2(yj /j )2
j=1

K
= (yj )2  K.
j=1

10

Under review as a conference paper at ICLR 2019

Therefore, the regret of the RMGD is bounded,

E

T
yk

T
- min
i

yi



log K 

+ KT

.

 =1

 =1

In particular, setting  = log(K)/(KT ), the regret is bounded by 2 K log(K)T , which is sublinear with T .

B EXPERIMENTAL SETTINGS
DATASET
MNIST is a dataset of handwritten digits that is commonly used for image classification. Each sample is a black and white image and 28 × 28 in size. The MNIST is split into three parts: 55,000 samples for training, 5,000 samples for validation, and 10,000 samples for test.
CIFAR10 consists of 60,000 32 × 32 color images in 10 classes (airplane, automobile, bird, cat, deer, dog, frog, horse, ship, and truck), with 6,000 images per class. The CIFAR10 is split into three parts: 45,000 samples for training, 5,000 samples for validation, and 10,000 samples for test.
CIFAR100 consists of 60,000 32 × 32 color images in 100 classes. The CIFAR100 is split into three parts: 45,000 samples for training, 5,000 samples for validation, and 10,000 samples for test.
SETTINGS
The simple CNN consists of two convolution layers with 5 × 5 filter and 1 × 1 stride, two max pooling layers with 2 × 2 kernel and 2 × 2 stride, single fully-connected layer, and softmax classifier. Description of the 'All-CNN-C' is provided in Table 5. For MNIST, AdamOptimizer with  = 10-4 and AdagradOptimizer with  = 0.1 are used as optimizer, and B = {16, 32, 64, 128, 256, 512}. The model is trained for a total of 100 epochs. For CIFAR10 and CIFAR100, MomentumOptimizer with fixed momentum of 0.9 is used as optimizer. The learning rate k is scaled up proportionately to the batch size (k = 0.05  bk/256) and decayed by a schedule S = [200, 250, 300] in which k is multiplied by a fixed multiplier of 0.1 after 200, 250, and 300 epochs respectively. The model is trained for a total of 350 epochs. Dropout is applied to the input image as well as after each convolution layer with stride 2. The dropout probabilities are 20% for dropping out inputs and 50% otherwise. The model is regularized with weight decay  = 0.001, and B = {16, 32, 62, 128, 256}. For all experiments, rectified linear unit (ReLU) is used as activation function. For RMGD,  is set to log(6)/(6  100)  0.055 for MNIST and log(5)/(5  350)  0.030 for CIFAR10 and CIFAR100.

11

Under review as a conference paper at ICLR 2019

Table 5: Architecture of the All-CNN-C for CIFAR10 and CIFAR100

Layer

Layer description

input

Input 32 × 32 RGB image

conv1

3 × 3 conv. 96 ReLU, stride 1, dropout 0.2

conv2

3 × 3 conv. 96 ReLU, stride 1

conv3

3 × 3 conv. 96 ReLU, stride 2

conv4

3 × 3 conv. 192 ReLU, stride 1, dropout 0.5

conv5

3 × 3 conv. 192 ReLU, stride 1

conv6

3 × 3 conv. 192 ReLU, stride 2

conv7

3 × 3 conv. 192 ReLU, stride 1, dropout 0.5

conv8

1 × 1 conv. 192 ReLU, stride 1

conv9

1 × 1 conv. 10 or 100 ReLU, stride 1

pool averaging over 6 × 6 spatial dimensions

softmax

10-way or 100-way softmax

12


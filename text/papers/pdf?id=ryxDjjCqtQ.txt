Under review as a conference paper at ICLR 2019
DECONFOUNDING REINFORCEMENT LEARNING
Anonymous authors Paper under double-blind review
ABSTRACT
In this paper, we propose a general formulation to cope with a family of reinforcement learning tasks in which confounder (i.e., a factor affecting both actions and rewards) exists in dynamic environments. Based on the proposed approach, we extend two representatives of reinforcement learning algorithms: Q-learning and Actor-Critic Methods, to their deconfounding variants. Due to lack of datasets in this direction, a benchmark is developed for deconfounding reinforcement learning algorithms by revising OpenAI Gym and MNIST. We demonstrate that the proposed algorithms are superior to traditional reinforcement learning algorithms in confounding environments. To the best of our knowledge, this is the first time that confounders are taken into consideration for addressing full reinforcement learning problems.
1 INTRODUCTION
In recent years, reinforcement learning has made great progress, spawning a large number of successful applications especially in terms of games (Silver et al., 2016; Mnih et al., 2013; OpenAI, 2018). To the best of our knowledge, however, little work has been done in the direction in which confounding bias exists in dynamic environments. Confounding is a causal concept that is described in the language of causality instead of probability and statistics (Pearl, 2009). Confounding bias occurs when a variable influences both who is selected for the treatment and the outcome of the experiment (Pearl & Mackenzie, 2018), which naturally corresponds to the action and the reward in reinforcement learning, respectively. As a matter of fact, confounders have been extensively studied in epidemiology, sociology, and economics. Take for example the widespread kidney stones in which the size of the kidney stone is a confounding factor affecting both the treatment and the recovery (Peters et al., 2017; Pearl, 2009), whether deconfounding the size of the kidney stone or not entirely determines how to choose a more effective treatment. Similarly, in reinforcement learning, if unobserved potential confounders exist, they would affect both actions and rewards when an agent interacts with environments and eventually influence the policy to be optimized.
It is widely acknowledged that one should draw a causal graph before one can achieve any causal conclusion (Pearl, 2009; Pearl & Mackenzie, 2018). Throughout the paper, we assume that, given causal assumptions, we first estimate a model from the observational data we collected from real environments or simulators, and then optimize a policy on the basis of the learned model. This assumption is quite useful in real-world reinforcement learning applications, because in most circumstances, except the data we observed, we either know nothing about real environments or are allowed to do nothing in real environments probably for the sake of ethics, laws or cost. For instance, in healthcare, we can only collect historical medical data such as Electronic Health Records to evaluate the policy rather than directly experiment with patients' lives without evidence that the proposed treatment strategy is better than the current practice (Gottesman et al., 2018); in economics, considering the cost in terms of time and money, it is not practical to study the optimal strategy by actually buying and selling stocks in the market. Hence, in other words, we focus on the observational setting in this paper.
In order to adjust for confounders, we present a general formulation for addressing this class of reinforcement learning problems, namely deconfounding reinforcement learning. More specifically, given several common confounding assumptions, we first estimate the corresponding causal model from the observational data, and then deconfound the confounders using the causal language developed by (Pearl, 2009), and finally optimize the policy based on the deconfounding model we calculated. On the basis of the proposed formulation, we extend two popular reinforcement learning
1

Under review as a conference paper at ICLR 2019

algorithms: Q-learning and Actor-Critic methods to their corresponding deconfounding variants. Due to lack of datasets in this respect, we revise the classic control toolkit in OpenAI Gym (Brockman et al., 2016), making it a benchmark for comparison of deconfounding reinforcement learning algorithms. In addition, we also devise a confounding version of the MNIST dataset (LeCun et al., 1998) to verify the performance of our causal model. Finally, we conduct extensive experiments to demonstrate the superiority of the proposed formulation in deconfounding environments, in comparison to traditional reinforcement learning algorithms.
To sum up, our contributions in this paper are as follows:
1. We propose a general formulation to address a family of reinforcement learning problems in confounding environments, namely deconfounding reinforcement learning;
2. We present deconfounding variants of two popular reinforcement learning algorithms: deconfounding Q-learning and deconfounding Actor-Critic methods;
3. We develop a benchmark for deconfounding reinforcement learning by revising the toolkit for classic control in OpenAI Gym (Brockman et al., 2016) and by devising a confounding version of the MNIST dataset (LeCun et al., 1998);
4. We perform a comprehensive comparison of deconfounding reinforcement learning algorithms with their vanilla versions, showing that the proposed approach has an advantage in confounding environments.
5. To the best of our knowledge, this is the first time that we are attempting to build a bridge between confounding and the full reinforcement learning problem. This is one of few research papers aiming at understanding the connections between causal inference and full reinforcement learning.

2 BACKGROUND

In this section, we briefly review confounding in causal inference. We recommend Pearls excellent monograph for further reading (Pearl, 2009; Pearl & Mackenzie, 2018).

2.1 SIMPSON'S PARADOX

Let us begin with one of the most famous paradoxes in statistics: Simpson's Paradox. Consider the previously mentioned kidney stones, a classic example of Simpson's paradox (Peters et al., 2017). We collect electronic patient records to investigate the effectiveness of two treatments against kidney stones, where although the overall probability of recovery is higher for patients who took treatment b, treatment a performs better than treatment b on both patients with small kidney stones and with large kidney stones. More precisely, we have

p(R = 1|T = b) > p(R = 1|T = a); but p(R = 1|T = b, Z = 0) < p(R = 1|T = a, Z = 0), p(R = 1|T = b, Z = 1) < p(R = 1|T = a, Z = 1);

(1)

where Z is the size of the stone, T the treatment, and R the recovery (all binary). How do we cope with this inversion of conclusion? Which treatment do you prefer if you had kidney stones? Does treatment b cause recovery? The answers to these questions depend on the causal relationship between treatment, recovery, and the size of the kidney stone.

Z TR Figure 1: Causal diagram for kidney stones.

2

Under review as a conference paper at ICLR 2019

2.2 CONFOUNDING
An intuitive explanation for this kidney stone example of Simpson's paradox is that larger stones are more severe than small stones and are much more likely to be treated with treatment a, resulting in that treatment a looks worse than treatment b. Therefore, it is straightforward to assume that the true underlying causal diagram of the kidney stone example is shown in Figure 1, where confounding occurs because the size of kidney stones influences both treatment and recovery. Here, the size of kidney stones is called confounder. The term "confounding" originally meant "mixing" in English, which describes that the true causal effect T  R is "mixed" with the spurious correlation between T and R induced by the fork T  Z  R (Pearl & Mackenzie, 2018). In other words, we will not be able to disentangle the true effect of T on R from the spurious effect if we do not have data on Z. Conversely, if we have measurements of Z, it is easy to deconfound the true and spurious effects by adjusting for Z that averages the effect of T on R in each subgroup of Z (i.e., different size groups in the case of kidney stones).

2.3 DO-OPERATOR AND BACK-DOOR CRITERION

From the viewpoint of causal inference, we can also use the language of intervention, namely do-
operator, to formulate confounding. In fact, in the example of kidney stones, what we are interested in is how these two treatments compare when we force all patients to take treatment a or b, rather than
which treatment has a higher recovery rate given only the observational patient records. Mathematically, we focus on the true effect p(R = 1|do(T = a)) (i.e., intervention distribution where patients are forced to take treatment a) instead of the spurious effect p(R = 1|T = a) (i.e., observational distribution where patients are observed to take treatment a). Therefore, as described previously, confounding can be naturally formulated by the discrepancy between p(R|T ) and p(R|do(T )).

Generally speaking, do-operator can be calculated in two ways: Randomized Controlled Trials (RCTs) (Fisher, 1935) and Back-Door Criterion (Pearl, 2009). RCTs is the so-called golden standard but rather limited due to many impractical factors (e.g., safety, laws, ethics, physically infeasibility, etc.). Back-door criterion requires a known causal diagram, which applies to our case, in which causal assumptions are provided in advance. According to Back-door criterion, in the kidney stone example, we can immediately attain

1 p(R = 1|do(T = a)) = p(R = 1|T = a, Z = z)p(Z = z).
z=0

(2)

3 DECONFOUNDING REINFORCEMENT LEARNING

3.1 CAUSAL ASSUMPTIONS
Without loss of generality, as shown in Figure 2, we assume there exists a common confounder in the sequential model, which is time-independent for each individual or for each procedure. This assumption is so general that it would apply to various reinforcement learning tasks across domains. For example, in personalized medicine or precision medicine, socio-economic status can affect both the medication strategy a patient has access to, and the patients general health (Louizos et al., 2017). Therefore socio-economic status acts as confounder between the medication and health outcomes, in which case socio-economic status is time-independent for each patient during the course of treatment. In agriculture, soil fertility may serve as one of confounders affecting both the application of fertilizer and the yield of each plot (Pearl & Mackenzie, 2018). In this circumstance, soil fertility is stable and thought of as a time-independent factor within a period of time (e.g., several months, the growth circle of crops, etc.). In the example of stock markets, apart from socio-economic status as mentioned above, government policy may also act as one of confounders, all of which can be seen time-independent during a reasonable period of time.

3.2 THE MODEL
Given the causal assumption, we first fit a generative model to a sequence of observational data: observations, actions, and rewards, where actions and rewards are confounded by one or several

3

Under review as a conference paper at ICLR 2019

x1 x2 x3
z1 z2 z3
a1 a2
r2 r3
U
u
Figure 2: The model for deconfounding reinforcement learning. Solid nodes denote observed variables and open nodes represent unobserved variables. Black dashed lines denote the prior over the possibly true policy learned from the observational data, and red and blue dashed lines denote the variational approximation q(at|xt) and q(rt+1|xt, at), respectively.

unknown factors. Formally, Let x = (x1, . . . , xT ), a = (a1, . . . , aT -1), r = (r2, . . . , rT +1), z = (z1, . . . , zT ) be the sequence of observations, actions, rewards, and corresponding latent states, respectively. The confounder is denoted by u, and it is worth noting that here u may stand for
more than one confounder in which multiple confounders are seen as a whole represented by u. We assume that xt  RDx , at  RDa , rt  RDr , zt  RDz , and u  RDu , where Dz  Dx. The generative model for deconfounding reinforcement learning is then given by:

Dz p(zi) = N (zij|0, 1);

Du p(u) = N (zij|0, 1);

pppp((((azrxtttt+|||zzz1ttt|-,)zu1it==,,)aa1=Ntt-, uN1())x(t==|aµ^tNNt|µ,^^t((,t2rz^)tt+|t2;µ^)1t|;,µ^µ^^t,tt2µ^^)=tt2;

i=1

f1(zt) ^t2 = f2(zt);

= )
;

f3(zt, u) ^t2 µ^t = f5(zt,

= f4(zt, u); at, u) ^t2 =

µ^t = f7(zt-1, at-1) ^t2 =

f6(zt, at, u); f8(zt-1, at-1

).

(3) (4) (5) (6)

Note that we parametrize each probability distribution as a Gaussian with its mean and variance modeled by nonlinear functions fk and each fk is parametrized by a neural network with its own parameters k for k = 1, . . . , 8. Note that Equation (4) is not necessary in our model, denoted by black dashed lines in Figure 2, but it is potentially useful acting as a prior policy because it is learned
from the observational data containing, for example, the real treatment strategies by doctors.

3.3 LEARNING

Since the nonlinear functions parametrized by neural networks make inference intractable, we will
learn the parameters of the model k by employing variational inference along with an inference model, a neural network which approximates the intractable posterior (Rezende et al., 2014; Kingma
& Welling, 2013; Krishnan et al., 2015). More specifically, using the variational principle, we posit an approximate posterior distribution q(z|x) to obtain the following lower bound on the marginal likelihood:

log p(x)  E [log p(x|z)] - KL (q(z|x)||p(z)) ,
q (z |x)

(7)

where the inequality is by Jensen's inequality and  is the parameter of the inference model q(z|x). Note that in this general case x stands for observational variables and z for latent variables.

4

Under review as a conference paper at ICLR 2019

3.3.1 VARIATIONAL LOWER BOUND

Directly applying the lower bound in Inequality (7) to our model, we obtain

log p(x, a, r)  E [log p(x, a, r|z, u)] - KL (q(z, u|x, a, r)||p(z, u))
q (z ,u|x,a,r)
= L(x, a, r; , ).

(8)

Using the Markov property of our model, the full distribution can be factorized in the following way:

[ T

][ T

]

p(x, a, r, z, u) = p(u)p(z1) p(xt|zt)p(at|zt, u)p(rt+1|zt, at, u)

p(zt|zt-1, at-1) .

t=1 t=2
(9)

In addition, for simplicity's sake, we also have the factorization assumption for the posterior approximation:

T q(z, u|x, a, r) = q(u|x, a, r)q(z1|x, a, r) q(zt|zt-1, x, a, r).
t=2

(10)

Combining Equation (8), (9) and (10) yields:

log p(x, a, r) L(x, a, r; , )

T

= E [log p(xt|zt) + log p(at|zt, u) + log p(rt+1|zt, at, u)]

t=1

zt q (zt |zt-1 ,x,a,r) uq(u|x,a,r)

- KL (q(u|x, a, r)||p(u)) - KL (q(z1|x, a, r)||p(z1))

T - E [KL (q(zt|zt-1, x, a, r)||p(zt|zt-1, at-1))] ,
t=2 zt-1q(zt-1|zt-2,x,a,r)

(11)

where we omit subscripts  and , and a more detailed derivation can be found in Appendix A. Obviously, Equation (11) is differentiable with respect to the parameters of the model  and . Using the reparametrization trick (Kingma & Welling, 2013), we can directly apply backpropagation to update the parameters.

3.3.2 INFERENCE MODEL

From the factorization form in Equation (10), we can see that there are two types of inference mod-

els: q(u|x, a, r) and q(z|x, a, r). Similar to the generative model in Section 3.2, we also parametrize

both of them as Gaussian:

q(u|x, a, q(z|x, a,

r) r)

= =

N N

((zu||µ^µ^tt,,^^t2t2));;

µ^t = f9(x, a, r) ^t2 = f10(x, a, r); µ^t = f11(x, a, r) ^t2 = f12(x, a, r).

(12) (13)

In fact, as shown in Equation (10), q(z|x, a, r) can be further factorized as the product of
q(zt|zt-1, x, a, r) for t = 1, . . . , T . Taking a closer look at this term, based on the Markov property of our model, we have zt x1, . . . , xt-1, a1, . . . , at-2, r2, . . . , rt|zt-1, and then the term can be
simplified as follows,

|=

q(zt|zt-1, x, a, r) = q(zt|zt-1, at-1, xt, at, rt+1, xt+1, . . . , xT , at+1, . . . , aT , rt+2, . . . , rT ). (14)

Equation (14) tells us that zt depends on zt-1 and all the current and future observed data (x, a, r). Meanwhile, the conditional independence above means that zt-1 contains all the historical data. Therefore, it is natural to calculate zt based on the whole sequence of data, which is exactly what recurrent neural networks (RNNs) do. Inspired by (Krishnan et al., 2015; 2017), we similarly choose
a bi-directional LSTM (Zaremba & Sutskever, 2014) to parameterize f11 and f12 in Equation (13). Considering Equation (12) has the same structure as Equation (13), f9 and f10 are parameterized by a bi-directional LSTM as well. More details about the architecture can be found in Appendix B.

5

Under review as a conference paper at ICLR 2019

Note that for the task of sample predictions, that is, at any time step t, given a new xt, we require to know at and rt+1 before inferring the distribution over zt. Hence, we need to introduce two auxil-

iary distributions, denoted by red and blue dashed lines in Figure 2, to help conduct counterfactual

reasoning

qq(i((.aret.t+,|xs1at|)mxt=p,laeNtp) r(=eµdNi=cti(µo^µnt,=on2µ^u=tn,s^e2te2n)=x^tµ)^t2.t)T=o

be more precise, we have f13(xt) ^t2 = f14(xt); µ^t = f15(xt, at) ^t2 = f16(xt

,

at

),

(15) (16)

where f13, f14, f15, and f16 are also parameterized by neural networks.

3.4 DECONFOUNDING RL ALGORITHMS

Now we have all the building blocks for deconfounding reinforcement learning algorithms. Once our

model is learned from the observational data, it can be directly used as a dynamic environment like

those in OpenAI Gym (Brockman et al., 2016). We can exploit the learned model to generate rollouts

for policy optimization. In our model, the key difference between traditional and deconfounding

reinforcement learning lies in the reward function. To be more precise, assuming that an agent

standing at state zt = z performs an action at = a, unlike p(rt|zt = z, at = a) in traditional

reinforcement learning, our deconfounding version based on do-operator as depicted in Section 2.3

is given by



p(rt|zt = z, do(at = a)) = p(rt|zt = z, at = a, u)q(u)du,

(17)

where q(u) is the approximate posterior q(u|x, a, r) which we compute through the inference network presented in Section 3.3.2. Identification in our case is an immediate result of Pearls back-door criterion mentioned in Section 2.3. Note that the state is fixed while calculating the reward and, therefore, it is unnecessarily taken into account when the back-door criterion is applied. In practice, Equation (17) is approximated using the Monte Carlo method as follows:

p(rt|zt

=

z, do(at

=

a))

=

1 N

N p(rt|zt

=

z, at

=

a, ui)

ui  q(u|x, a, r),

i=1

where N is the number of samples from the approximate posterior of u.

(18)

On the basis of our deconfounding reward function, it is straightforward to extend traditional reinforcement learning algorithms to their corresponding deconfounding version. In this paper, we select two representatives of them: Q-learning (Watkins & Dayan, 1992) and Actor-Critic method (Sutton et al., 1998).

Deconfounding Q-Learning Q-learning is one of the most successful approaches in reinforce-

ment learning. Let Q(z, a; ) be an approximate action-value function with parameters . The

parameters  are learned by iteratively minimizing a sequence of loss functions, where the loss

function at step time t is defined as (

)2

LQ(t) = E

rt+1

+

maxQ(zt+1,
at+1

at+1;

t-1)

-

Q(zt,

at;

t)

,

(19)

where rt+1  p(rt+1|zt, at) in vanilla Q-learning whilst rt+1  p(rt+1|zt, do(at)) (Equation (18)) in deconfounding Q-learning.

Deconfounding Actor-Critic Methods In contrast to value-based Q-learning, the actor-critic

method is a policy-based method directly parameterizing the policy (a|z; ), which aims to re-

duce the variance of the estimate of the policy gradient by subtracting a learned function of the state

b(z), known as a baseline, from the return. The learned value function V (z; ) is commonly used

as the baseline. Taking into consideration that the return is the estimate of Q(z, a; Q) and b(z) is the estimate of V (z; V ), the gradient of the actor-critic loss function at step time t is given by

J () = E [(Q(zt, at; Q) - V (zt; V ))  ln (at|zt; )] ,

(20)

where Q(zt, at; Q) - V (zt; V ) used to be seen as an estimate of the advantage of action at in state zt. In practice, Q(zt, at; Q) is usually replaced with one-step return, that is, rt+1 + Q(zt+1, at+1; Q), which brings us back to the same situation described in deconfounding Qlearning. Similarly, the vanilla and deconfounding versions of actor-critic method correspond to

rt+1  p(rt+1|zt, at) and rt+1  p(rt+1|zt, do(at)), respectively.

6

Under review as a conference paper at ICLR 2019
4 EXPERIMENTAL RESULTS
It is widely acknowledged that evaluating approaches dealing with confounding is always challenging due to lack of groundtruth and benchmark datasets. Besides, little work has been done before in deconfounding reinforcement learning, which renders evaluating such algorithms in this respect much harder. Therefore, we first develop benchmark datasets for evaluation of our algorithms, primarily by revising the MNIST dataset (LeCun et al., 1998) and two environments in OpenAI Gym (Brockman et al., 2016): CartPole and Pendulum. We then evaluate our model as well as compare two proposed deconfounding algorithms to their corresponding vanilla versions on the benchmark datasets we developed.
4.1 IMPLEMENTATION DETAILS
We used Tensorflow (Abadi et al., 2016) for the implementation of our model and deconfounding reinforcement learning algorithms. Optimization was done with Adam (Kingma & Ba, 2014). Unless stated otherwise, the setting of all the hyperparameters and architectures of the neural networks we adopted in this paper can be found in Appendix C.
To verify how good the learned model is, we performed two types of tasks: reconstruction and counterfactual reasoning. The reconstructions were performed by feeding the input sequence into the learned inference network, and then sampling from the resulting posterior distribution according to Equation (13), and finally feeding those samples into the generative network described in Equation (3). The counterfactual reasoning, that is, predicting xt+1 given unseen xt, were executed through four steps: 1) Given an unseen xt, we estimate at and rt+1 based on Equation (15) and (16); 2) Once we have xt, at, and rt+1, it is easy to estimate zt from Equation (13); 3) Using the estimated zt and at, we can directly compute zt+1 from Equation (6); (4) The final step is to reconstruct xt+1 from zt+1 according to Equation (3). Repeating the four steps, we can counterfactually reason out a sequence of data.
To evaluate the confounder u, there are also two scenarios. The easy one is that, given a sequence of observational data (x, a, r), it is obvious to estimate u from Equation (12). The more challenging one is to calculate u given only xt at any time step. Following the same steps used in the task of counterfactual reasoning, we first compute at and rt+1 based on Equation (15) and (16), and then estimate u through Equation (12).
4.2 CONFOUNDING DATASETS
4.2.1 CONFOUNDING MNIST
Motivated by the Healing MNIST dataset mimicking the healthcare data under harsh conditions (e.g., noisy laboratory measurements, surgeries and drugs affected by patient age and gender, etc.) (Krishnan et al., 2015), we developed a confounding MNIST dataset in which a binary confounder is introduced. More specifically, we select 100 different digits (one hundred 4's and one hundred 5's) to create a synthetic dataset where rotations are encoded as the actions a (-45  a  45). First, 20% bit-flip noise are added to each digit image , and then, based on some policy, rotations are performed on each noisy image for five time steps, which produces a large number of 5-step sequences of rotated noisy images. To each generated sequence, exactly one sequence of three consecutive squares (2 × 2 in pixel) is superimposed with the top-left corner of the images in a random starting location. We treat such generated sequences of images as the observations x. The training/validation/test set respectively comprises 140000/28000/28000 sequences of length five. Unless stated otherwise, we keep the same setting in the following two datasets. The definition of reward is as follows. We assume that our goal is to rotate the digit to the canonical view (i.e., the position of 12 o'clock on the clock face, namely upright position) wherever it lies initially. Therefore, the reward is defined as the minus degree between the upright position and the position the digit rotates to. For example, if the digit rotates to the position of 3 o'clock or 9 o'clock, then both rewards are -90.
Now we are arriving at the key stage: how to define the confounder u. Let us first imagine that each sequence of observations represents a sequence of an individual's observed symptoms, the degree of the rotation represents the effectiveness of the treatment, and the reward means to what extent
7

Under review as a conference paper at ICLR 2019
Figure 3: Reconstruction and counterfactual reasoning on the confounding MNIST dataset. Top row: results of a model without confounder; Bottom row: results of our model with confounder as shown in Figure 2.
the patient recovers. Under this circumstance, we assume that the confounder is socio-economic status as mentioned in Section 3.1, and further assume that here the socio-economic status is binary, meaning the rich and the poor, where the rich access more effective treatments (22.5  |a|  45) and recover more quickly (analogy to quicker rotation to the canonical view) whilst the poor receive less effective treatments (0  |a| < 22.5) and recover more slowly. Note that for each sequence, like the unchanged socio-economic status for each patient during the course of treatment, the confounder is fixed, meaning that the range of actions is fixed for generating that sequence.
4.2.2 CONFOUNDING PENDULUM To further verify our deconfounding actor-critic method in Section 4.5, we develop a confounding Pendulum dynamic environment by revising the original Pendulum in OpenAI Gym (Brockman et al., 2016). More precisely, we process the screen images of Pendulum in the same way as we do in the confounding MNIST dataset, and treat such generated images as the observations. Similar to Section 4.2.1, we also introduce a binary confounder making the actions divided into two categories 1  |a|  2 and 0  |a| < 1. All the others are left unchanged.
4.2.3 CONFOUNDING CARTPOLE Similarly, to verify our deconfounding Q-learning algorithm in Section 4.5, for simplicity, we pay our attention only to the confounding CartPole with discrete actions. We first use the same method as in Section 4.2.2 to deal with screen images. In order to increase the difficulty of the task, we extend the original discrete action space (i.e., {0, 1} representing left and right) to a larger space (i.e., {-1.0, -0.9, . . . , 0, 0.1, . . . , 1.0}) with 21 actions. The confounder we imposed partitions the action space into two parts: {-0.5, -0.4, . . . , 0, 0.1, . . . , 0.5} and the rest. In order to force the confounder to affect the reward, we define the reward as the minus cosine of the pole angle instead of the fixed value of 1 in the original environment.
4.3 PERFORMANCE ANALYSIS OF THE DECONFOUNDING MODEL Figure 3 presents some samples of reconstruction and counterfactual reasoning on the confounding MNIST dataset. The bottom row is based on our model shown in Figure 2, whilst the top row comes from the same model without the confounder. It is evident that the results generated by our deconfounding model is superior to those produced by the model not taking into account the confounder. Likewise, some samples of reconstruction and counterfactual reasoning on the confounding Pendulum dataset are shown in Figure 4, where the two models are exactly the same as those used in
8

Under review as a conference paper at ICLR 2019
Figure 4: Reconstruction and counterfactual reasoning on the confounding Pendulum dataset. Top row: results of a model without confounder; Bottom row: results of our model with confounder as shown in Figure 2.
(a) (b)
Figure 5: (a) Plot of 128 data points sampling from the posterior approximate of u on the confounding MNIST dataset; (b) Plot of 128 data points sampling from the posterior approximate of u on the confounding CartPole dataset;
Figure 3. In this case, however, it is more straightforward to see the reason why the model without the confounder does not work on the confounding dataset. From the first row, apparently we can see that there are always two pointers appearing in the generated images, because the latent confounder makes the model confusing about the direction the pointer will go. In contrast, such confusion is removed from the second row, in which our deconfounding model can learn the latent confounder and clearly tells the pointer where to go. 4.4 VISUALIZATION OF THE CONFOUNDER As shown in Figure 5, by visualizing the 2-dimensional confounder u, we can discern that although the prior distribution of the confounder is assumed to be a unit Gaussian distribution, the model still can learn two obvious clusters from the data because it is originally a binary variable. It demonstrates that our model has an advantage in learning confounders even if the assumed prior over them were not that accurate. 4.5 COMPARISON OF DECONFOUNDING RL ALGORITHMS In this section, we will evaluate the proposed deconfounding Q-learning algorithm and deconfounding Actor-Critic method by comparing them to their vanilla versions. For simplicity's sake, we conduct experiments of Q-learning algorithms on the confounding CartPole dataset which involves discrete actions, while assessing Actor-Critic methods on the confounding Pendulum dataset with continuous actions. In both circumstances, our deconfounding algorithms perform significantly better than their vanilla ones in confounding environments.
9

Under review as a conference paper at ICLR 2019
Figure 6: Left: Comparison of original and deconfounding Q-learning algorithms on the confounding CartPole dataset; Right: Comparison of original and deconfounding Actor-Critic methods on the confounding Pendulum dataset.
5 RELATED WORK
Krishnan et al. (2015; 2017) used deep neural networks to model nonlinear state space models and leveraged a structured variational approximation parameterized by recurrent neural networks to mimic the posterior distribution. Levine (2018) reformulated reinforcement learning and control problems to probabilistic inference, which allows us to bear a large pool of approximate inference methods, and flexibly extend the model. Raghu et al. (2017a;b) exploited continuous state-space models and deep reinforcement learning to deduce treatment policies for septic patients from observational data. Gottesman et al. (2018) discussed some issues of evaluating reinforcement learning algorithms in observational health setting. However, all the work mentioned above did not take into account confounders in their models. Louizos et al. (2017) attempted to learn individual-level causal effects from observational data using variational auto-encoder to estimate the unknown confounder given a causal graph in then nontemporal setting. Paxton et al. (2013) developed predictive models based on electronic medical records without using causal inference. Saria et al. (2010) proposed a nonparametric Bayesian method to analyze clinical temporal data. Soleimani et al. (2017) represented the treatment response curves using linear time-invariant dynamical systems which provides a flexible approach to modeling response over time. Although the latter two work modeled the sequential data, they both do not exploit reinforcement learning or causal inference. Bareinboim et al. (2015) considered the problem of bandits with unobserved confounders, which is one quite simple reinforcement learning setting without state transitions. Sen et al. (2016) and Ramoly et al. (2017) further studied contextual bandits with latent confounders. Forney et al. (2017) circumvented some problems caused by unobserved confounders in Multi-Armed Bandit by counterfactual-based decision-making. Zhang & Bareinboim (2017) leveraged causal inference to tackle the problem of transferring knowledge across bandit agents. However, all these methods are based on the bandit problem, a simplified version of reinforcement learning, instead of the full reinforcement learning problem. In fact, as far as we are concerned, this is the first attempt to build a bridge between confounding and the full reinforcement learning problem, and this is also one of few research papers aiming at understanding the connections between causal inference and full reinforcement learning.
6 CONCLUSION AND FUTURE WORK
To address the confounding issue in reinforcement learning, we introduced a general formulation, namely deconfounding reinforcement learning. On the basis of the proposed formulation, we presented deconfounding variants of Q-learning and actor-critic methods and showed their superior performance on three confounding datasets that we created by revising OpenAI Gym and MNIST. In the future, we will collaborate with hospitals and apply our approach to real-world medical datasets. We also hope that our work will stimulate further investigation of connections between causal inference and reinforcement learning.
10

Under review as a conference paper at ICLR 2019
REFERENCES
Mart´in Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, et al. Tensorflow: a system for largescale machine learning. In OSDI, volume 16, pp. 265­283, 2016.
Elias Bareinboim, Andrew Forney, and Judea Pearl. Bandits with unobserved confounders: A causal approach. In Advances in Neural Information Processing Systems, pp. 1342­1350, 2015.
Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba. Openai gym, 2016.
Ronald Aylmer Fisher. The design of experiments. 1935.
Andrew Forney, Judea Pearl, and Elias Bareinboim. Counterfactual data-fusion for online reinforcement learners. In International Conference on Machine Learning, pp. 1156­1164, 2017.
Omer Gottesman, Fredrik Johansson, Joshua Meier, Jack Dent, Donghun Lee, Srivatsan Srinivasan, Linying Zhang, Yi Ding, David Wihl, Xuefeng Peng, et al. Evaluating reinforcement learning algorithms in observational health settings. arXiv preprint arXiv:1805.12298, 2018.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.
Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013.
Rahul G Krishnan, Uri Shalit, and David Sontag. Deep kalman filters. arXiv preprint arXiv:1511.05121, 2015.
Rahul G Krishnan, Uri Shalit, and David Sontag. Structured inference networks for nonlinear state space models. In AAAI, pp. 2101­2109, 2017.
Yann LeCun, Le´on Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278­2324, 1998.
Sergey Levine. Reinforcement learning and control as probabilistic inference: Tutorial and review. arXiv preprint arXiv:1805.00909, 2018.
Christos Louizos, Uri Shalit, Joris M Mooij, David Sontag, Richard Zemel, and Max Welling. Causal effect inference with deep latent-variable models. In Advances in Neural Information Processing Systems, pp. 6446­6456, 2017.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin Riedmiller. Playing atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602, 2013.
OpenAI. Openai five. https://blog.openai.com/openai-five/, 2018.
Chris Paxton, Alexandru Niculescu-Mizil, and Suchi Saria. Developing predictive models using electronic medical records: challenges and pitfalls. In AMIA Annual Symposium Proceedings, volume 2013, pp. 1109. American Medical Informatics Association, 2013.
Judea Pearl. Causality. Cambridge university press, 2009.
Judea Pearl and Dana Mackenzie. The Book of Why. Allen Lane, 2018.
Jonas Peters, Dominik Janzing, and Bernhard Scho¨lkopf. Elements of causal inference: foundations and learning algorithms. MIT press, 2017.
Aniruddh Raghu, Matthieu Komorowski, Imran Ahmed, Leo Celi, Peter Szolovits, and Marzyeh Ghassemi. Deep reinforcement learning for sepsis treatment. arXiv preprint arXiv:1711.09602, 2017a.
11

Under review as a conference paper at ICLR 2019
Aniruddh Raghu, Matthieu Komorowski, Leo Anthony Celi, Peter Szolovits, and Marzyeh Ghassemi. Continuous state-space models for optimal sepsis treatment-a deep reinforcement learning approach. arXiv preprint arXiv:1705.08422, 2017b.
Nathan Ramoly, Amel Bouzeghoub, and Beatrice Finance. A causal multi-armed bandit approach for domestic robots failure avoidance. In International Conference on Neural Information Processing, pp. 90­99. Springer, 2017.
Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and approximate inference in deep generative models. arXiv preprint arXiv:1401.4082, 2014.
Suchi Saria, Daphne Koller, and Anna Penn. Learning individual and population level traits from clinical temporal data. In Proceedings of Neural Information Processing Systems, pp. 1­9. Citeseer, 2010.
Rajat Sen, Karthikeyan Shanmugam, Murat Kocaoglu, Alexandros G Dimakis, and Sanjay Shakkottai. Contextual bandits with latent confounders: An nmf approach. arXiv preprint arXiv:1606.00119, 2016.
David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering the game of go with deep neural networks and tree search. nature, 529(7587):484, 2016.
Hossein Soleimani, Adarsh Subbaswamy, and Suchi Saria. Treatment-response models for counterfactual reasoning with continuous-time, continuous-valued interventions. arXiv preprint arXiv:1704.02038, 2017.
Richard S Sutton, Andrew G Barto, et al. Reinforcement learning: An introduction. MIT press, 1998.
Christopher JCH Watkins and Peter Dayan. Q-learning. Machine learning, 8(3-4):279­292, 1992. Wojciech Zaremba and Ilya Sutskever. Learning to execute. arXiv preprint arXiv:1410.4615, 2014. Junzhe Zhang and Elias Bareinboim. Transfer learning in multi-armed bandit: a causal approach. In
Proceedings of the 16th Conference on Autonomous Agents and MultiAgent Systems, pp. 1778­ 1780. International Foundation for Autonomous Agents and Multiagent Systems, 2017.
12

Under review as a conference paper at ICLR 2019
Appendices
A VARIATIONAL LOWER BOUND

log

p

(x, 

a,

r)

= log

p(x, a, r, z, u)dzdu

  u z



u

z

q(z,

u|x, a,

r)

log

p(x, a, r, z, q(z, u|x, a,

u) dzdu
r)

=

u


z


q(u|x, a,

r)q(z|x, a,

r)

log

qp(u(u)pp|x(z,(1ax),,[ra),qrTt,=(zz1,|pux(),xat,|zrt))dpz(daut|zt,

(factorization assumption)

u)p(rt+1|zt,

at,

] u)

[T
t=2

p(zt

|zt-1,

] at-1)

= q(u|x, a, r)q(z|x, a, r) log u z

q(u|x, a, r)q(z|x, a, r)

dzdu

= q(u|x, a, r)q(z1|x, a, r) · · · q(zT |zT -1, x, a, r)

u
log

z
p(u)p(z1)

T
t=1

p(xt|zt

)p(at|zt

,

q(u|x, a, r)q(z1|x,

u)p(rt+1|zt,

at,

u)

T
t=2

a, r) · · · q(zT |zT -1, x, a,

p(zt|zt-1, r)

at-1)

dzdu

T  



= · · · q(u|x, a, r)q(z1|x, a, r) · · · q(zT |zT -1, x, a, r) log (p(xt|zt)p(at|zt, u)p(rt+1|zt, at, u)) dzdu

t=1

u


z1

 zT

+

u

z1

···

zT

q(u|x, a, r)q(z1|x, a, r)

·

· · q(zT |zT -1, x, a, r) log

p(u) q(u|x, a, r) dzdu

+

u

···
z1

zT

q(u|x, a, r)q(z1|x, a, r)

·

· · q(zT |zT -1, x, a, r) log

p(z1) q(z1|x, a,

r)

dzdu

+

T
t=2


u


z1

·

·


·
zT

q(u|x, a, r)q(z1|x, a, r) ·

·

·

q(zT |zT -1, x, a, r)

log

p(zt|zt-1, at-1) q(zt|zt-1, x, a, r)

dzdu

T  

= q(u|x, a, r)q(zt|zt-1, x, a, r) log (p(xt|zt)p(at|zt, u)p(rt+1|zt, at, u)) dztdu

t=1 u zt

p(u)

+ u q(u|x, a, r) log q(u|x, a, r) du

+

z1

q(z1|x,

a,

r)

log

p(z1) q(z1|x, a,

r)

dz1

+

T
t=2


zt-1


zt

q(zt|zt-1,

x, a,

r)

log

p(zt|zt-1, at-1) q(zt|zt-1, x, a, r)

dztdzt-1

T

= E [log p(xt|zt) + log p(at|zt, u) + log p(rt+1|zt, at, u)]

t=1

zt q (zt |zt-1 ,x,a,r) uq(u|x,a,r)

- KL (q(u|x, a, r)||p(u))

- KL (q(z1|x, a, r)||p(z1))

T - E [KL (q(zt|zt-1, x, a, r)||p(zt|zt-1, at-1))] .
t=2 zt-1q(zt-1|zt-2,x,a,r)

13

Under review as a conference paper at ICLR 2019

B BI-DIRECTIONAL LSTM
In our inference model, we use a similar architecture of bi-directional LSTM to that in (Krishnan et al., 2017). Apart from different inputs, the main difference is to introduce at-1 to computing the combined hidden feature as shown in the following formula.
0 z^1 z^2 z^3

function

(µ1, 1)

(µ2, 2)

(µ3, 3)

Backward LSTM

h1right

h2right

h3right

Forward LSTM

h1lef t

h2lef t

h3lef t

x1, a1, r2

x2, a2, r3

x3, a3, r4

hcombined

=

1 4

( tanh(Wz

· zt-1

+ bz)

+ tanh(Wa

· at-1

+ ba)

+ hlteft

+

) htright

µt = Wµ · hcombined + bµ

t2 = softplus (W2 · hcombined + b2 )

C EXPERIMENTAL SETTINGS

Hyperparameter
learning rate dimension of zt dimension of xt dimension of at dimension of rt dimension of u dimension of LSTM unit
batch size number of steps number of epoch

Value
0.0001 50 784 1 1 2 100 128 5 400

Table 1: Hyperparameters of our model for deconfounding reinforcement learning

14

Under review as a conference paper at ICLR 2019

Function
f1, f2 f3, f4 f5, f6 f7, f8 f9, f10 f11, f12 f13, f14 f15, f16

Architecture
FC512  Conv7,32  Conv14,16  Conv28,1  FC784  {sigmoid, softplus} {FC100, FC100}  FC200  FC200  FC1  {tanh, softplus}
{FC100, FC100, FC100}  FC100  FC100  FC1  {sigmoid, softplus} {FC100, FC100}  FC100  FC100  FC50  {None, softplus}
{[Conv5,16  Conv5,32  Conv5,32  FC100], FC100, FC100}  FC100  FC100  LSTM100,5  FC1  {None, }softplus {[Conv5,16  Conv5,32  Conv5,32  FC100], FC100, FC100}  FC100  FC100  LSTM100,5  FC50  {None, }softplus
Conv5,16  Conv5,32  Conv5,32  FC1  {tanh, softplus} {[Conv5,16  Conv5,32  Conv5,32  FC100], FC100}  FC100  FC1  {sigmoid, softplus}

Table 2: Architectures of our model for deconfounding reinforcement learning. Here FCk stands for a fully-connected layer with k units, Convk,n for a convolution layer with n filters of size k × k, LSTMn,t for a LSTM layer rolling out for t steps with latent size of n, {·} for the parallel operators, and [·] for the sequential operators.

15


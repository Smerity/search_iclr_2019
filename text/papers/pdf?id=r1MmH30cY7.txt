Under review as a conference paper at ICLR 2019
CONFIDENCE REGULARIZED SELF-TRAINING
Anonymous authors Paper under double-blind review
ABSTRACT
Recent advances in domain adaptation show that self-training with deep networks presents a powerful means for unsupervised domain adaptation. Specifically, these methods often involve an iterative process of predicting on target domain and then taking the confident predictions as pseudo-labels for retraining. Basic self-training treats selected pseudo-labels equally as one-hot vectors, and the selection process is often modeled to be progressive via self-paced learning. While one-hot vector is a natural choice to model multiclass targets, such encoding scheme does not consider the difference between selected samples. As a result, the approach sometimes generates overconfident false pseudo-labels that lead to the convergence to deviated solutions with propagated errors, especially when there is a large domain gap. To address this problem, we generalize self-training as an expectation maximization (EM) problem which treats pseudo-labels as latent variables, and solves maximum marginal likelihood estimation (MMLE) by maximizing its lower bound. We then propose confidence regularized self-training, where we introduce multiple confidence regularizers along with their solutions. These regularizers mainly consider the control of pseudo-label confidence from two aspects: model regularization and label refinement. Experiments on both semantic segmentation and image classification show that self-training with different confidence regularizers comprehensively outperform their non-regularized counterparts.
1 INTRODUCTION
Transferring knowledge learned by deep neural networks from label-rich domains to a new target domains is a challenging problem. Such domain change occurs in many practical applications. An example is the recent use of game rendering engines to simulate driving images with readily available labels to train semantic segmentation models Richter et al. (2016). In reality, however, the existence of cross-domain differences often leads to significantly decreased model performance. Unsupervised domain adaptation (UDA) seeks to address this problem by adapting the source model to target domain. To this end, a predominant stream of adversarial learning based UDA methods have been proposed to reduce the discrepancy between the source and target domain features Hoffman et al. (2016); Tzeng et al. (2017); Chen et al. (2017); Long et al. (2017); Saito et al. (2018); Hoffman et al. (2018); Tsai et al. (2018); Zhang et al. (2018); Sankaranarayanan et al. (2018); Pinheiro (2018).
More recently, self-training emerged as a promising alternative towards domain adaptation Lee (2013); Inoue et al. (2018); Zou et al. (2018). Instead of seeking to reduce domain gaps by confusing the discriminator with a separate adversarial loss, self-training with deep networks implicitly align cross-domain features by unifying both domain alignment and the learning task itself under the same loss Zou et al. (2018). The algorithms predict labels on unlabeled samples and then incorporate confident predictions as pseudo-labels for subsequent model training. Basic self-training methods trusts all the selected pseudo-labels as "ground truth" by encoding as one-hot vectors for model retraining. In reality, however, 100% accuracy cannot always be guaranteed even with highly conservative selection criteria. This is particularly true when there exists a relatively large domain gap. As a result, a drawback of basic self-training methods is the ignorance of overconfidence on certain wrong pseudo-labels. And these overconfident noisy pseudo-labels can be the major reason of error propagation and convergence to undesired results.
Towards addressing the above problem, we propose a generalized self-training framework with probabilistic interpretations. Our contributions in this paper are summarized as follows:
1

Under review as a conference paper at ICLR 2019
Confidence regularization for self-training: On top of Zou et al. (2018), we first introduce a softlabel self-training framework, by reformulating basic self-training as a constrained optimization problem and relaxing its latent variables to a feasible set. We then introduce confidence regularizers to the framework by considering from two main aspects: network output confidence and pseudolabel assignment confidence. In particular, we propose five different regularizers, and systematically evaluate various self-training frameworks with these regularizers. These regularizers attempt to avoid the sparsity of either the network output probabilities or the pseudo-label variable by encouraging their smoothness. We also present the optimization solutions to the self-training frameworks and their convergence analysis.
Self-training as MMLE: We provide a probabilistic interpretation of our self-training framework, by formulating it as a maximum marginal likelihood estimation for annotated source data and unlabeled target data. The pseudo-labels can be regarded as the latent variables of the marginal likelihood, and the learning target of self-training exactly seeks to maximize a lower bound of the data marginal likelihood. In addition, both self-training with self-paced learning and the proposed confidence regularized self-training can be seen as generalized MMLEs with regularizations.
2 RELATED WORKS
Self training: The idea of self-training is not new Nigam & Ghani (2000); Amini & Gallinari (2002). Early ideas of self-training has been investigated in semi-supervised learning Grandvalet & Bengio (2005) and self-paced learning Tang et al. (2012). In addition, a detailed taxonomy and empirical study of different self-training techniques is presented in Triguero et al. (2015). However, a subtle difference between these methods and self-training with deep networks is that the latter involves the learning of deep embeddings which renders greater flexibility towards domain alignment than classifier-level self-training. Within this context, Lee (2013) theoretically shows that training with pseudo-labels is equivalent to entropy regularization which encourages low-density feature separation on unlabeled data. More recently, Zou et al. (2018) introduced class-balanced self-training which achieved state-of-the-art performance on domain adaptation for semantic segmentation.
Domain adaptation: Unsupervised domain adaptation has recently gained considerable interests. For UDA with deep networks, a major principle is to let the network learn domain invariant embeddings by minimizing the cross-domain difference of feature distributions with certain criteria. Examples of these methods include maximum mean discrepancy (MMD) Tzeng et al. (2014); Long et al. (2015a), deep Correlation Alignment (CORAL) Sun & Saenko (2016), adversarial learning at input-level Hoffman et al. (2018); Zhu et al. (2017); Huang et al. (2018), feature level Ganin & Lempitsky (2015); Hoffman et al. (2016); Tzeng et al. (2017); Saito et al. (2018); Hoffman et al. (2018), output space level Tsai et al. (2018), and a variety of follow up works Chen et al. (2017); Long et al. (2017); Zhang et al. (2018); Sankaranarayanan et al. (2018); Pinheiro (2018) etc. Despite the underlying difference, there exists an interesting connection between some of these methods with conditional forms1 and self-training as the they can be broadly considered as EM algorithms, and such conditional formulation has been widely proved to greatly benefit the adaptation performance.
Semi-supervised learning (SSL): There exist a natural strong connection between domain adaptation and semi-supervised learning with their problem definitions. A series of teacher-student based approaches have been recently proposed to address both SSL Laine & Aila (2016); Tarvainen & Valpola (2017); Luo et al. (2018) and UDA problems French et al. (2018).
Noisy lablel learning: Self-training can also be broadly regarded as noisy label learning due to the uncertainty and potential mistakes on pseudo-labels. As a result, this paper shows intrinsic connections to techniques such as softmax with temperature Hinton et al. (2015), label smoothing, robust losses, robust model Szegedy et al. (2016), and network output confidence regularization Pereyra et al. (2017), bootstrapping for noisy labels Reed et al. (2014); Sukhbaatar & Fergus (2014); Natarajan et al. (2013).
1E.g., class-wise adversarial learning, or discriminators taking network predictions as input.
2

Under review as a conference paper at ICLR 2019

3 SELF-TRAINING WITH SELF-PACED LEARNING
For an unsupervised domain adaptation problem, we have access to labeled source images xs and corresponding labels ys from a set of labeled source images {Xs, Ys}, as well as an unlabeled target images xt drawn from unlabeled target images Xt. ys is in the form of one-hot vector. The unknown label for a target image as y^t from the unobserved label set Y^t. The network weights are denoted as W . K is the total number of classes.

3.1 BASIC SELF-TRAINING

We begin with basic self-training, which can be defined as follows:

KK

min
Y^T ,W

Lbst(Y^T

,

W

)

=

-

xs XS

k=1

ys(k)

log(p(k|xs;

W

))

-

xt XT

k=1

y^t(k)

log(p(k|xt;

W ))

s.t. y^t  {0, 1}K , t

(1)

where y^t is constrained to be a one-hot vector. To alleviate the problem of over-confidence on wrong pseudo-labels, we introduce self-training with soft pseudo-labels, defined as follows:

KK

min
Y^T ,W

Lcst(Y^T

,

W

)

=

-

xs XS

k=1

ys(k)

log(p(k|xs;

W ))

-

xt XT

k=1

y^t(k)

log(p(k|xt;

W ))

K
s.t. y^t  {(y^t(1), ..., y^t(K)) : y^t(k)  0, y^t(k) = 1}, t

k=1

(2)

The above formulation is similar the original self-training framework, except with a relaxed convex

feasible set which is a probability simplex {y^t = (y^t(1), ..., y^t(K))|yt(k)  0,

K k=1

y^t(k)

=

1},

instead

of a non-convex set {0, 1}K. Accordingly, we propose the following alternating block coordinate

descent, to solve (2):

a) Pseudo-label optimization Fix W and solve the following degenerated problem:

K

min
Y^T

-

xt XT

k=1

y^t(k)

log(p(k|xt;

W

)),

K
s.t.y^t  {(y^t(1), ..., y^t(K)) : yt(k)  0, y^t(k) = 1}, t

k=1

b) Model retraining Fix Y^T and solve the following degenerated problem:

(3)

KK

min -
W

ys(k) log(p(k|xs; W )) -

y^t(k) log(p(k|xt; W ))

xsXS k=1

xtXT k=1

(4)

The optimization 3 in step a) aims to estimate each label y^t for xt. We call this problem as a pseudolabel optimization. For each xt, the label optimization is a standard linear programming and the global minimum is y^t = (y^t(1), ..., y^t(K)) where y^t(k) = 1{k == arg maxk{p(k|xt; W ))}. And the optimization 4 in b) is a standard cross-entropy loss minimization that can be solved by gradient
descent. We call it model retraining.

3.2 CONTINUOUS SELF-PACED SELF-TRAINING
Self-paced self-training adapts model by progressively learning from relative reliable pseudo-labels. The continuous self-paced self-training is a continuous self-training optimization with a pace regu-

3

Under review as a conference paper at ICLR 2019

larizer RSP (Y^T ) = -y^t(k)t and a modified feasible set. min Lspst(Y^T , W ) = Lbst(Y^T , W ) + RSP (Y^T )
Y^T ,W

=-

K
ys(k) log(p(k|xs; W )) -

K
[y^t(k) log(p(k|xt; W )) + y^t(k)t]

xsXS k=1

xtXT k=1

K
s.t. y^t  {(y^t(1), ..., y^t(K)) : yt(k)  0, y^t(k) = 1}  {0}, t

k=1
(5)

Similar to 1, we can use alternating optimization to solve this problem. While the model retraining remains unchanged, the pseudo-label optimization problem changes as follows.

K

min
Y^T

-

xt XT

y^t(k)[log
k=1

p(k|xt;

W)

+

t]

K
s.t. y^t  {(y^t(1), ..., y^t(K)) : yt(k)  0, y^t(k) = 1}  {0}, t

(6)

k=1

Now the feasible set is the union of probability simplex and a zero vector set {0}. Based on these

two feasible sets, we can split the above optimization problem into two subproblems. The global

minimum of 6 is the minimum of these two subproblem minimums. The global optimizer is given

as follows.

y^t(k) =

1, if k = arg max{p(k|xt; W )} and - log p(k|xt; W ) < t
k

0, otherwise

(7)

For each xt, t is a parameter controlling if xt is selected for model retraining. If y^t = 0, (xt, y^t) is not selected for training.

3.3 ADAPTIVE POLICIES FOR t
t are critical parameters controlling the pseudo-label selection for model retraining. We would like to adaptively update t since we expect the selected pseudo-labels to be improved progressively. For example, we can initial p = 15% and gradually increase 5% for every model retraining iteration.
Although there are plentiful parameters t to be decided which seems to be very difficult, we can design simple yet efficient rules for t determination according to just only one single parameter, similar to Zou et al. (2018). In general, we define a hyper-parameter p indicating how many portion we want to select from all the pseudo-labels for model retraining. Given a model W , all t can be obtained based on this portion p and the following t determination policy. One p rules all t.
In model retraining of self-training, we want to select the samples with the smallest losses for model retraining. These samples have the top prediction confidence and are assumed to be accurate. Based on this principle, The following two rules are introduced.
Global threshold policy The basic policy is global threshold policy, where t is identical for t. The value is determined by the loss value thresholding for portion p samples with the smallest losses among all target data. The detailed algorithm can be seen in Algorithm 1. We call the self-paced self-training with this policy as ST.
Class-balanced threshold policy Since the pseudo-labels selected by global threshold policy can be quite imbalanced, a class-balanced threshold policy is introduced. Here for the samples predicted as the same class, t is identical. For each class k, the corresponding t is determined by the loss value thresholding for portion p samples with the smallest losses among all target data predicted as class k. The detailed algorithm can be seen in Algorithm 2. We call the self-paced self-training with this policy as UCBST.
Remark In our class-balanced threshold, the pseudo-label is obtained by taking argmax of softmax probability. The class-balanced policy in Zou et al. (2018) is different. It's a reweighed classbalanced policy since its label is decided by a reweighted softmax. We call our class-balanced self-training as UCBST since the softmax probability for each sample is uniformly weighted.

4

Under review as a conference paper at ICLR 2019

4 CONFIDENCE REGULARIZED SELF-TRAINING

All the above self-training approaches give full confidence to predicated and selected labels by assigning a one-hot vector to them and encourage the network to output softmax probability as sparse as possible. However, we argue that the pseudo-labels are noisy and a selected label class cannot be assigned by 100% confidence. Also the sparse output softmax probability leads to model overfitting Pereyra et al. (2017); Szegedy et al. (2016). To attenuate the above two overconfidence, we introduce confidence regularization into the self-paced self-training, leading to confidence regularized selftraining as follows.

min Lcst(Y^T , W ) = Lbst(Y^T , W ) + Rsp(Y^T ) + Rc(Y^T , W )
Y^T ,W
K
= - ys(k) log(p(k|xs; W ))
xsXS k=1
K
- { [y^t(k) log(p(k|xt; W )) + y^t(k)t] - rc(xt, y^t, W )}
xtXT k=1
K
s.t. y^t  {(y^t(1), ..., y^t(K)) : yt(k)  0, y^t(k) = 1}  {0}, t
k=1

(8)

The confidence regularizer is RC (XT , Y^T , W ) = xtXT rc(xt, y^t, W ).   0 is the weight coefficient for confidence regularization. In the following, we explore a family of confidence regularizers
for label refinement and model regularization. The rule of thumb in designing confidence regularizer
is to reduce sparsity and encourage smoothness of either output probability vectors or pseudo-label probability vectors. L2 norm, negative entropy and KL-divergence between interested distribution and uniform distribution are convenient tools for fulfilling this goal. The confidence regularizers are proposed based on them. Note that global minimum of L2 norm on probability is achieved by uniform distribution, thus minimization for L2 norm in probability can smooth the distribution. Also L2, negative entropy and KL-divergence are convex operators with respect to probability vectors.

4.1 MODEL CONFIDENCE REGULARIZATION

The model confidence regularizers rc(xt, W ) only depends on xt and W rather than y^t. Compared to self-paced self-training, they only changes the model retraining to 9, and does not effect the
pseudo-label optimization.

KK

min -
W

ys(k) log(p(k|xs; W )) -

[ y^t(k) log(p(k|xt; W )) - rc(xt, W )]

xsXS k=1

xtXT k=1

(9)

Three model confidence regularizers, L2, negative entropy and KLD on output probability, are explored. Table 4.1 gives their equations along with the gradients with respect to logits. In Pereyra et al. (2017); Szegedy et al. (2016) the negative entropy and KLD model regularization are investigated in supervised learning. Pereyra et al. (2017) points out KLD model regularization is equivalent to uniform label smoothing Pereyra et al. (2017). Moreover, the L2 regularization are popular regularization for weights decay. However, its validity on output confidence is underdeveloped in both supervised and semi-supervised settings.

L2
Negative entropy
KL-Div.

Regularizer

K

L22(p(xt; W )) = p(k|xt; W )2

k=1

K

-H(p(xt; W )) = p(k|xt; W ) log p(k|xt; W )

k=1

K

DKL(u||p(xt; W )) =-

1 K

log

p(k|xt;

W)

k=1

Gradient w.r.t. logit zi
K
2 pk2 (xt; W )(ki - pi(xt; W )), ki = 1{k = i}
k=1
p(i|xt; W )[log p(i|xt; W ) + H(p(xt; W ))]

p(i|xt;

W

)

-

1 K

Table 1: Model confidence regularizers and gradients w.r.t. to logits

5

Under review as a conference paper at ICLR 2019

No reg.
L2
Neg. entropy

Regularizer 0

L22(y^t) = K (y^t(k))2
k=1

K
-H(y^t) =

y^t(k)logo y^t(k)

k=1

Valid pseudo-label

subproblem solution

y^t(i) =

1, i = arg max{p(k|xt; W )}
k

0, otherwise

Sort {p(k|xt; W ), k = 1, ..., K} in ascending order as

{p(kq|xt; W ), q = 1, ..., K}; define Ckq : |{kc : kc  kq}|,

and

^vt

:

{1

+

kc kq

log(

p(kq p(kc

|xt |xt

;W ;W

) )

)

1 2

,

q

=

1, ..., K}

sets the first positive number in ^vt as kpos

 0,

if kq < kpos;



y^t(kq )

=

   

1

+

kc kpos

log(

p(kq p(kc

|xt ;W |xt ;W

) )

)

1 2

,

  

Ckpos



 if kq  kpos

1

y^t(i) =

p(i|xt; W ) 
K1

p(k|xt; W ) 

k=1

Table 2: Pseudo-label confidence regularizers along with optimizers

4.2 PSEUDO-LABEL CONFIDENCE REGULARIZATION
While pseudo-labels Y^T are fixed in model retraining, pseudo-label regularization only change the pseudo-label optimization to 10 compared to self-paced self-training.

K

min-
y^t

k=1

y^t(k)

log(p(k

|xt

;

W

))

+

rc

(xt

,

y^t

,

W

)

-

y^t(k)

t

K
s.t. y^t  {(y^t(1), ..., y^t(K)) : yt(k)  0, y^t(k) = 1}  {0}, t

k=1

(10)

Solving the above optimization for only the probability simplex feasible set gives the refined soft pseudo-labels. Then the pseudo-label are selected according to the following criterion.

K
-y^t(k) log p(k|xt; W ) + rc(xt, y^t, W ) < t
k=1

(11)

Specifically, L2 and negative entropy on pseudo-label probability are explored for pseudo-label confidence regularization. Their equations are given in 4.2. For the soft pseudo-labels optimization,

Karush-Kuhn-Tucker conditions (KKT) conditions give the necessary and sufficient conditions of

optimal solutions Boyd & Vandenberghe (2004). Since these regularizers are convex operators on

probability vectors, solving the KKT system equations leads to the global solutions shown in Table

4.2. Note that we define a new operator logo(x) as follows. logo(x) =

log(x) 0

if if

x x

> =

0 0

.

Negative entropy label refinement and softmax with temperature Set p(i|xt; W ) as softmax

probability and zi is logit.

When

1 

is

an

positive

integer,

we

connect

the

solution

of

negative

entropy pseudo-label refinement with softmax with temperature Hinton et al. (2015) as follows.

[ ]ezi K

1 

y^t(i) =

1
p(k|xt; W )  =
K1
p(k|xt; W ) 

ezq
q=1
[K ezk K

=
1
]

(ezi

)

1 

=

K1

(ezk ) 

e zi 
eK zk 

k=1

k=1

ezq

k=1

k=1

q=1

(12)

6

Under review as a conference paper at ICLR 2019

5 CONFIDENCE REGULARIZD SELF-TRAINING AND REGULARIZED MMLE

First, let's show basic self-training is Expectation-Maximization (EM) for maximization marginal
likelihood estimation (MMLE). Suppose we have pairs of samples (XS, YS) in source domain and (XT , YT ) in target domain of which data distribution is different from source. We model the conditional probability as p(y|x; W ). If YS, YT exist in both domains, cross-entropy loss minimization
is equivalent to maximum likelihood estimation Bishop & Nasrabadi (2007). For a semi-supervised
setting where YT are missing, we will show the basic self-training optimization 1 with cross-entropy loss is exactly an EM for MMLE by maximizing the lower bound. At first, let's define Y~T as latent variables. y~t can only be a one-hot vector and Dy~t is the feasible set. For each t, we define
q(A|A = {y~t(k) = 1}), k = 1, ...K as an arbitrary distribution. log L(XS, YS, XT ; W ) is the marginal log-likelihood. Its lower bound is obtained as follows.

log L(XS, YS, XT ; W ) = log

p(xs, ys)

p(xt, y~t)

xs XS

xXT y~tDy~t

= log

p(xs)p(ys|xs; W )

[p(xt)

p(y~t|xt; W )]

xs XS

xt XT

y~t Dy~t

=

xs XS

[log

p(xs)

+

K k=1

ys(k)

log

p(k|xs;

W )]

+

xt XT

[log

p(xt)

+

log

y~t Dy~t

K
yt(k)q(y~t(k)
k=1

=

1)

p(k|xt q(y~t(k)

;W) = 1)

]

=

xs XS

[log

p(xs)

+

K k=1

ys(k)

log

p(k|xs;

W )]

+

xt XT

[log

p(xt)

+

log

K k=1

q(y~t(k)

=

1) p(k|xt; W ) ] q(y~t(k) = 1)



xs XS

[log

p(xs)

+

K k=1

ys(k)

log

p(k|xs;

W )]

+

xt XT

[log

p(xt)

+

K k=1

q(y~t(k)

=

1)

log

p(k|xt; W ) q(y~t(k) = 1)

]

KK
 ys(k) log p(k|xs; W ) + q(y~t(k) = 1) log p(k|xt; W )

xsXS k=1

xtXT k=1

(13)

The first inequality is derived by Jensen' inequality. By setting y^t(k) = q(A|A = {y~t(k) = 1}), k = 1, ...K, t, the lower bound maximization problem is exactly equivalent to basic self-training minimization with cross-entropy loss 1 problem while we can set p(xs) and p(xt) as constants. Alternating Optimization leads to the EM optimization. The E-step is lower bound maximization in terms of y^t(k) fixing W , which is equivalent to pseudo-label optimization 3. The M-step is maximization in terms of W fixing y^t(k) which is equivalent to model retraining 4. Alternately maximizing the lower bound leads to marginal likelihood maximization.
For MMLE problem, the E-step always gives y^t = q(A) as one-hot vector. Since we know the latent variable q(A) is estimated and cannot be given full confidence in M-step, we can introduce confidence regularization on either q(A) or the model p(y|x; W ) into the MMLE which leads to regularized MMLE (RMMLE). Similar to 13, it's easy to see confidence regularized self-training is still an EM for regularized MMLE.

6 EXPERIMENTS
6.1 DOMAIN ADAPTATION FOR IMAGE CLASSIFICATION
First, we evaluate proposed confidence regularized self-training for image classification in the domain adaptation VisDA-17 benchmark Peng et al. (2018). It is a challenging task of adapting from synthetic to real images. The source domain contains 152, 409 synthetic 2D images from 12 object classes rendered from 3D models. We choose the validation set as our target domain which contains 55, 400 real images and the classes are shared. We choose ResNet-101 as our base model He et al. (2016) that is pretrained on ImageNet Deng et al. (2009). The pre-trained top fc-layer is modified to a two fc-layers as the input to final softmax layer. Table 3 gives the results of proposed methods and other methods for comparison. The metric is per-class accuracy and mean accuracy Note that MR and LR indicates model regularization and label regularization. In the experiments of this paper,

7

Under review as a conference paper at ICLR 2019

Method Aero Bike Bus Car Horse Knife Motor Person Plant Skateboard Train Truck Mean
Source Only 55.1 53.3 61.9 59.1 80.6 17.9 79.7 31.2 81.0 26.5 73.5 8.5 52.4 MMD 87.1 63.0 76.5 42.0 90.3 42.9 85.9 53.1 49.7 36.3 85.8 20.7 61.1 DANN 81.9 77.7 82.8 44.3 81.2 29.5 65.1 28.6 51.9 54.6 82.8 7.8 57.4 ENT 80.3 75.5 75.8 48.3 77.9 27.3 69.7 40.2 46.5 46.6 79.3 16.0 57.0 MCD 87.0 60.9 83.7 64.0 88.9 79.6 84.7 76.9 88.6 40.3 83.0 25.8 71.9 ADR 87.8 79.5 83.7 65.3 92.3 61.8 88.9 73.2 87.8 60.0 85.5 32.3 74.8
Source only 68.7 36.7 61.3 70.4 67.9 5.9 82.6 25.5 75.6 29.4 83.8 10.9 51.6 ST 95.8 61.9 80.3 84.9 93.1 8.7 95.7 14.7 91.9 25.3 82.0 5.6 61.6
UCBST 83.5 75.8 55.6 47.1 82.2 70.9 84.3 76.0 81.5 82.4 68.2 72.1 73.3 MRL2 87.5 79.5 56.0 51.2 81.6 77.3 83.7 78.0 82.8 86.7 65.5 71.3 75.1 MRNE 87.5 74.7 59.9 54.2 83.5 68.4 83.7 75.0 84.1 86.3 69.0 67.1 74.4 MRKLD 85.5 74.1 66.4 56.6 77.8 85.5 86.3 78.9 86.0 79.9 68.7 64.7 75.9 LRL2 88.2 79.6 61.5 51.8 83.7 77.6 82.0 78.5 84.3 86.5 69.1 66.6 75.8 LRNE 86.2 78.7 59.2 53.5 84.6 74.2 83.5 77.4 83.3 84.5 69.2 66.6 75.1

Table 3: Experimental results for VisDA17-val setting. MMD:Long et al. (2015b), DANN:Ganin et al. (2016), ENT: Grandvalet & Bengio (2005), MCD:Saito et al. (2017b), ADR:Saito et al. (2017a),

Method

Base Net Road SW Build Wall Fence Pole TL TS Veg. Terrain Sky PR Rider Car Truck Bus Train Motor Bike mIoU

Source only

DRN-26 42.7 26.3 51.7 5.5 6.8 13.8 23.6 6.9 75.5 11.5 36.8 49.3 0.9 46.7 3.4 5.0 0.0 5.0 1.4 21.7

CyCADA

79.1 33.1 77.9 23.4 17.3 32.1 33.3 31.8 81.5 26.7 69.0 62.8 14.7 74.5 20.9 25.6 6.9 18.8 20.4 39.5

Source only DenseNet 67.3 23.1 69.4 13.9 14.4 21.6 19.2 12.4 78.7 24.5 74.8 49.3 3.7 54.1 8.7 5.3 2.6 6.2 1.9 29.0

IIA 85.8 37.5 80.2 23.3 16.1 23.0 14.5 9.8 79.2 36.5 76.4 53.4 7.4 82.8 19.1 15.7 2.8 13.4 1.7 35.7

Source only DRN-105 36.4 14.2 67.4 16.4 12.0 20.1 8.7 0.7 69.8 13.3 56.9 37.0 0.4 53.6 10.6 3.2 0.2 0.9 0.0 22.2

MCD

90.3 31.0 78.5 19.7 17.3 28.6 30.9 16.1 83.7 30.0 69.1 58.5 19.6 81.5 23.8 30.0 5.7 25.7 14.3 39.7

Source only DeepLab-v2 75.8 16.8 77.2 12.5 21.0 25.5 30.1 20.1 81.3 24.6 70.3 53.8 26.4 49.9 17.2 25.9 6.5 25.3 36.0 36.6

MAA

86.5 36.0 79.9 23.4 23.3 23.9 35.2 14.8 83.4 33.3 75.6 58.5 27.6 73.7 32.5 35.4 3.9 30.1 28.1 42.4

Source only DeepLab-v2 -

-

-

-

-

----

-

-- - - - - -

-

- 29.2

FCAN

- - - - - - - - - - - - - - - - - - - 46.6

Source only ResNet-38 70.0 23.7 67.8 15.4 18.1 40.2 41.9 25.3 78.8 11.7 31.4 62.9 29.8 60.1 21.5 26.8 7.7 28.1 12.0 35.4

ST 90.1 56.8 77.9 28.5 23.0 41.5 45.2 39.6 84.8 26.4 49.2 59.0 27.4 82.3 39.7 45.6 20.9 34.8 46.2 41.5

CBST

86.8 46.7 76.9 26.3 24.8 42.0 46.0 38.6 80.7 15.7 48.0 57.3 27.9 78.2 24.5 49.6 17.7 25.5 45.1 45.2

CBST-SP

88.0 56.2 77.0 27.4 22.4 40.7 47.3 40.9 82.4 21.6 60.3 50.2 20.4 83.8 35.0 51.0 15.2 20.6 37.0 46.2

CBST-SP+MST

89.6 58.9 78.5 33.0 22.3 41.4 48.2 39.2 83.6 24.3 65.4 49.3 20.2 83.3 39.0 48.6 12.5 20.3 35.3 47.0

UCBST

ResNet-38 80.5 38.5 72.9 27.9 15.2 38.7 44.9 35.9 64.9 11.0 48.5 55.7 26.7 80.3 22.7 50.9 18.8 23.3 42.6 42.1

MRL2

87.9 43.9 77.4 26.5 27.0 41.8 44.8 32.0 81.1 17.1 52.0 61.0 29.8 84.2 28.9 51.9 15.9 29.2 40.1 45.9

MRNE

87.8 46.9 77.9 24.8 27.6 41.7 45.7 33.8 79.9 16.8 53.8 60.5 30.3 83.4 30.6 51.1 15.3 26.1 40.9 46.0

MRKLD

85.7 43.0 78.4 22.5 28.7 42.0 45.6 34.1 80.5 12.0 46.0 59.8 29.2 80.1 28.1 50.7 23.1 24.7 39.1 44.9

LRNE

82.5 31.5 75.4 25.8 23.3 41.0 42 35.7 80.4 13.6 48.1 64.1 32.3 75.7 29.6 41.5 11.0 27.9 23.6 42.4

UCBST-SP ResNet-38 89.0 51.8 77.0 25.5 24.6 40.9 45.2 41.0 83.9 21.9 57.5 62.4 30.8 83.3 36.0 46.5 11.1 30.5 46.2 47.6

MRL2

87.2 48.4 77.2 30.5 28.5 41.1 45.9 41.1 83.0 23.3 60.7 63.5 27.5 85.1 31.9 52.2 38.2 28.2 43.6 49.3

MRNE

89.9 50.7 79.7 27.7 22.5 39.4 46.1 43.2 84.5 20.5 67.7 58.0 24.8 83.9 38.5 44.4 22.5 37.0 47.8 48.9

MRKLD

88.4 51.9 77.7 29.7 21.4 39.2 47.1 48.2 81.4 22.2 52.0 65.8 32.3 83.4 40.3 46.0 31.3 37.0 41.1 49.3

LRNE

90.2 52.8 79.5 27.2 22.8 38.4 46.1 46.1 84.2 21.4 71.8 51.3 18.7 83.3 38.6 38.0 17.0 35.4 48.3 47.9

MRL2-MST

89.0 50.4 78.4 32.0 28.7 42.8 47.1 41.2 83.6 24.0 61.9 63.9 27.7 85.5 33.3 51.1 39.3 28.5 43.9 50.1

Table 4: Experimental results for GTA5  Cityscapes.

CyCADA: Hoffman et al. (2018),

IIA: Murez et al. (2018), MCD: Saito et al. (2017b), MAA: Tsai et al. (2018), FCAN: Zhang et al.

(2018), ST/CBST/CBST-SP: Zou et al. (2018)

MRL2, MRNE, MRKLD and LRL2, LRNE indicates the UCBST (or UCBST-SP in segmentation) with L2, negative entropy, KLD model regularization, L2, or negative entropy label regularization. We conduct a grid search for regularization weights, The weights for MRL2, MRNE, MRKLD, LRL2, LRNE is 0.0025, 0.0025, 0.05, 0.05, 0.2 respectively. It can be seen UCBST outperforms ST with global threshold policy and has more balanced predication model. All the confidence regularizers boost UCBST about 1.1% - 2.6%, which demonstrates the need of confidence regularization in self-training. Compared to other state-of-the-art, confidence regularized self-training achieves better or competitive performance.
6.2 DOMAIN ADAPTATION FOR SEMANTIC SEGMENTATION
We also evaluate proposed confidence regularized self-training in the standard benchmark GTA5 Richter et al. (2016)  Cityscapes dataset Cordts et al. (2016). GTA5 includes annotated 24,966 1, 052 × 1, 914 images captured from the video game GTA5. The validation set of Cityscapes is treated as target domain, containing 500 1, 024 × 2048 real images. Wide ResNet-38 Wu et al. (2016) is adopted as base model. We test confidence regularization in both UCBST and UCBSTSP. In UCBST-SP we follow the CBST-SP in Zou et al. (2018) to use the spatial priors in source domain. We utilize a few class mining strategy the same as Zou et al. (2018). All the results are given in Table 4. Both UCBST and UCBST-SP outperform ST. UCBST is slightly worse than CBST while UCBST-SP is better than CBST-SP. All the confidence regularization boost the base UCBST or UCBST-SP. The UCBST-SP-MRL2 with multi-scale testing achieves mIoU 50.1 that outperforms other state-of-the-art by a large margin.
8

Under review as a conference paper at ICLR 2019
7 CONCLUSIONS
On top of Zou et al. (2018), we first introduce a continuous self-training framework, by reformulating basic self-training as a constrained optimization problem and relaxing feasible set of latent variables. We then introduce a family of confidence regularizers to the framework by considering from two main aspects: model confidence regularization and pseudo-label confidence regularization. We connect negative entropy pseudo-label refinement with softmax with temperature. Systematical experiments demonstrates the confidence regularization's validity in self-training. Theoretically we show self-training is an EM to solve Maximum Marginal Likelihood Estimation by maximizing its lower bound. Proposed confidence regularized self-training can be seen as generalized MMLEs with regularizations.
REFERENCES
Massih-Reza Amini and Patrick Gallinari. Semi-supervised logistic regression. In ECAI, 2002.
Christopher M Bishop and Nasser M Nasrabadi. Pattern recognition and machine learning. Journal of Electronic Imaging, 16(4):049901­049901, 2007.
Stephen Boyd and Lieven Vandenberghe. Convex optimization. Cambridge university press, 2004.
Yi-Hsin Chen, Wei-Yu Chen, Yu-Ting Chen, Bo-Cheng Tsai, Yu-Chiang Frank Wang, and Min Sun. No more discrimination: Cross city adaptation of road scene segmenters. In ICCV, 2017.
Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe Franke, Stefan Roth, and Bernt Schiele. The cityscapes dataset for semantic urban scene understanding. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 3213­3223, 2016.
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on, pp. 248­255. Ieee, 2009.
Geoffrey French, Michal Mackiewicz, and Mark Fisher. Self-ensembling for visual domain adaptation. ICLR, 2018.
Yaroslav Ganin and Victor Lempitsky. Unsupervised domain adaptation by backpropagation. In ICML, 2015.
Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, Franc¸ois Laviolette, Mario Marchand, and Victor Lempitsky. Domain-adversarial training of neural networks. The Journal of Machine Learning Research, 17(1):2096­2030, 2016.
Yves Grandvalet and Yoshua Bengio. Semi-supervised learning by entropy minimization. In NIPS, 2005.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770­778, 2016.
Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531, 2015.
Judy Hoffman, Dequan Wang, Fisher Yu, and Trevor Darrell. Fcns in the wild: Pixel-level adversarial and constraint-based adaptation. arXiv preprint arXiv:1612.02649, 2016.
Judy Hoffman, Eric Tzeng, Taesung Park, Jun-Yan Zhu, Phillip Isola, Kate Saenko, Alexei A Efros, and Trevor Darrell. Cycada: Cycle-consistent adversarial domain adaptation. In ICML, 2018.
Xun Huang, Ming-Yu Liu, Serge Belongie, and Jan Kautz. Multimodal unsupervised image-toimage translation. In ECCV, 2018.
9

Under review as a conference paper at ICLR 2019
Naoto Inoue, Ryosuke Furuta, Toshihiko Yamasaki, and Kiyoharu Aizawa. Cross-domain weaklysupervised object detection through progressive domain adaptation. In CVPR, 2018.
Samuli Laine and Timo Aila. Temporal ensembling for semi-supervised learning. In ICLR, 2016.
Dong-Hyun Lee. Pseudo-label: The simple and efficient semi-supervised learning method for deep neural networks. In ICML Workshop on Challenges in Representation Learning, 2013.
Mingsheng Long, Yue Cao, Jianmin Wang, and Michael Jordan. Learning transferable features with deep adaptation networks. In ICML, 2015a.
Mingsheng Long, Yue Cao, Jianmin Wang, and Michael I Jordan. Learning transferable features with deep adaptation networks. In ICML, 2015b.
Mingsheng Long, Zhangjie Cao, Jianmin Wang, and Michael I Jordan. Conditional adversarial domain adaptation. arXiv preprint arXiv:1705.10667, 2017.
Yucen Luo, Jun Zhu, Mengxi Li, Yong Ren, and Bo Zhang. Smooth neighbors on teacher graphs for semi-supervised learning. In CVPR, 2018.
Zak Murez, Soheil Kolouri, David Kriegman, Ravi Ramamoorthi, and Kyungnam Kim. Image to image translation for domain adaptation. In CVPR, 2018.
Nagarajan Natarajan, Inderjit S Dhillon, Pradeep K Ravikumar, and Ambuj Tewari. Learning with noisy labels. In NIPS, 2013.
Kamal Nigam and Rayid Ghani. Analyzing the effectiveness and applicability of co-training. In Proceedings of The Ninth Int. Conf. on Information and Knowledge Management. ACM, 2000.
Xingchao Peng, Ben Usman, Neela Kaushik, Dequan Wang, Judy Hoffman, Kate Saenko, Xavier Roynard, Jean-Emmanuel Deschaud, Francois Goulette, Tyler L Hayes, et al. Visda: A syntheticto-real benchmark for visual domain adaptation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, pp. 2021­2026, 2018.
Gabriel Pereyra, George Tucker, Jan Chorowski, Lukasz Kaiser, and Geoffrey Hinton. Regularizing neural networks by penalizing confident output distributions. In ICLR Workshop, 2017.
Pedro O Pinheiro. Unsupervised domain adaptation with similarity learning. CVPR, 2018.
Scott Reed, Honglak Lee, Dragomir Anguelov, Christian Szegedy, Dumitru Erhan, and Andrew Rabinovich. Training deep neural networks on noisy labels with bootstrapping. arXiv preprint arXiv:1412.6596, 2014.
Stephan R Richter, Vibhav Vineet, Stefan Roth, and Vladlen Koltun. Playing for data: Ground truth from computer games. In ECCV, 2016.
Kuniaki Saito, Yoshitaka Ushiku, Tatsuya Harada, and Kate Saenko. Adversarial dropout regularization. arXiv preprint arXiv:1711.01575, 2017a.
Kuniaki Saito, Kohei Watanabe, Yoshitaka Ushiku, and Tatsuya Harada. Maximum classifier discrepancy for unsupervised domain adaptation. 2017b.
Kuniaki Saito, Yoshitaka Ushiku, Tatsuya Harada, and Kate Saenko. Adversarial dropout regularization. In ICLR, 2018.
Swami Sankaranarayanan, Yogesh Balaji, Carlos D Castillo, and Rama Chellappa. Generate to adapt: Aligning domains using generative adversarial networks. In CVPR, 2018.
Sainbayar Sukhbaatar and Rob Fergus. Learning from noisy labels with deep neural networks. arXiv preprint arXiv:1406.2080, 2014.
Baochen Sun and Kate Saenko. Deep coral: Correlation alignment for deep domain adaptation. In ECCV, 2016.
10

Under review as a conference paper at ICLR 2019
Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethinking the inception architecture for computer vision. In CVPR, 2016.
Kevin Tang, Vignesh Ramanathan, Li Fei-Fei, and Daphne Koller. Shifting weights: Adapting object detectors from image to video. In NIPS, 2012.
Antti Tarvainen and Harri Valpola. Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results. In NIPS, 2017.
Isaac Triguero, Salvador Garc´ia, and Francisco Herrera. Self-labeled techniques for semi-supervised learning: taxonomy, software and empirical study. Knowledge and Information Systems, 42(2): 245­284, 2015.
Yi-Hsuan Tsai, Wei-Chih Hung, Samuel Schulter, Kihyuk Sohn, Ming-Hsuan Yang, and Manmohan Chandraker. Learning to adapt structured output space for semantic segmentation. In CVPR, 2018.
Eric Tzeng, Judy Hoffman, Ning Zhang, Kate Saenko, and Trevor Darrell. Deep domain confusion: Maximizing for domain invariance. arXiv preprint arXiv:1412.3474, 2014.
Eric Tzeng, Judy Hoffman, Kate Saenko, and Trevor Darrell. Adversarial discriminative domain adaptation. In CVPR, 2017.
Zifeng Wu, Chunhua Shen, and Anton van den Hengel. Wider or deeper: Revisiting the resnet model for visual recognition. arXiv preprint arXiv:1611.10080, 2016.
Yiheng Zhang, Zhaofan Qiu, Ting Yao, Dong Liu, and Tao Mei. Fully convolutional adaptation networks for semantic segmentation. In CVPR, 2018.
Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A Efros. Unpaired image-to-image translation using cycle-consistent adversarial networks. In ICCV, 2017.
Yang Zou, Zhiding Yu, B. V. K. Vijaya Kumar, and Jinsong Wang. Unsupervised domain adaptation for semantic segmentation via class-balanced self-training. In ECCV, 2018.
Appendices
A ADAPTIVE t POLICES IN SELF-PACED SELF-TRAINING
Algorithm 1: Determining t by global thresholding Input : Neural network model W , all target samples xt, portion p Output: t, t 1 for xt  XT do 2 Cxt = lossxt 3 Cvec = [Cvec, Cxt ] 4 end 5 Csort = sort(Cvec, order = ascending) 6 l = length(Csort) × p # is the floor operator 7  = Csort(l) 8 t = ; t 9 return t; t
11

Under review as a conference paper at ICLR 2019

Algorithm 2: Determining t by class-balanced thresholding
Input : Neural network model W , all target samples xt, portion p Output: t, t 1 for xt  XT do 2 Cxt = lossxt 3 kxt = arg max{p(k|xt; W ), k = 1, ..., K} 4 Cv(kec=kxt ) = [Cv(kec=kxt ), Cxt ]
5 end
6 for k = 1 to K do 7 Cs(okr)t = sort(Cv(kec), order = ascending) 8 lk = length(Cs(okr)t) × p % is the floor operator 9 k = Cs(okr)t(lk) 10 end 11 for xt  XT do 12 t = kxt ; 13 end
14 return kc

B CONVERGENCE ANALYSIS

B.1 BASIC SELF-TRAINING

Convergence analysis In step a), the global minimum always can be achieved. In step b),

choosing a reasonable learning rate, the loss is monotonically non-increasing in every iteration

of the gradient descent. Step a) and b) are alternately used and the loss in 1 is monotonically

non-decreasing in every iteration. Moreover, the loss -

xs XS

K k=1

ys(k)

log(p(k|xs;

W ))

-

xt XT

K k=1

y^t(k)

log(p(k|xt;

W

))

has

a

lower

bound

zero.

The minimization 1 is able to con-

verge by alternating optimization. In practice, stochastic (mini-batch) gradient descent is usually

used to update the weights, instead of gradient descent.

B.2 SELF-PACED SELF-TRAINING

Similar to basic self-training, due to the achievable global minimum of label optimization and gradi-

ent descent in model retraining, the loss function in 5 will be monotonically non-increasing in every

iteration. Since -

xt XT

K k=1

y^t(k)

t

>

-

xt XT

K k=1

t,

it's

easy

to

see

the

loss

function

has a lower bound - xtXT Kk=1t, The alternating optimization is convergent.

12


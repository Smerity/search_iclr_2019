Under review as a conference paper at ICLR 2019
OBJECT-ORIENTED MODEL LEARNING THROUGH MULTI-LEVEL ABSTRACTION
Anonymous authors Paper under double-blind review
ABSTRACT
Object-based approaches for learning action-conditioned dynamics has demonstrated promise of strong generalization and interpretability. However, existing approaches suffer from structural limitations and optimization difficulties for common environments with multiple dynamic objects. In this paper, we present a novel self-supervised learning framework, called Multi-level Abstraction Objectoriented Predictor (MAOP), for learning object-based dynamics models from raw visual observations. MAOP employs three-level learning archicture that enables efficient dynamics learning for complex environments with a dynamic background. We also design a spatial-temporal relational reasoning mechanism to support instance-level dynamics learning and handle partial observability. Empirical results show that MAOP significantly outperforms previous methods in terms of sample efficiency and generalization over novel environments that have multiple controllable and uncontrollable dynamic objects and different static object layouts. In addition, MAOP learns semantically and visually interpretable disentangled representations.
1 INTRODUCTION
Model-based deep reinforcement learning (DRL) (Racanie`re et al., 2017; Chiappa et al., 2017; Finn & Levine, 2017) has recently attracted much attention for improving sample efficiency of DRL. One of the core problems for model-based reinforcement learning is to learn action-conditioned dynamics models by interacting with environments. Approaches have been proposed for such dynamics learning from raw visual perception, achieving remarkable performance in training environments (Oh et al., 2015; Watter et al., 2015; Chiappa et al., 2017).
To unlock sample efficiency of model-based DRL, learning action-conditioned dynamics models that generalize over unseen environments is critical yet challenging. (Finn et al., 2016) proposed an action-conditioned video prediction method that explicitly models pixel motion and thus is partially invariant to object appearances. Zhu & Zhang (2018) developed an object-oriented dynamics predictor, taking a further step towards generalization over unseen environments with different object layouts. However, due to structural limitations and optimization difficulties, these methods do not learn and generalize well for common environments with a dynamic background, which contain multiple moving objects in addition to controllable objects.
To address these limitations, this paper presents a novel self-supervised, object-oriented dynamics learning framework, called Multi-level Abstraction Object-oriented Predictor (MAOP). This framework simultaneously learns disentangled object representations and predicts object motions conditioned on their historical states, their interactions to other objects, and an agent's actions. To reduce the complexity of such concurrent learning and improve sample efficiency, MAOP employs a threelevel learning architecture from the most abstract level of motion detection, to dynamic instance segmentation, and to dynamics learning and prediction. The more abstract learning level solves an easier problem and has lower learning complexity, and its output provides a coarse-grained guidance for the less abstract learning level, improving its speed and quality of learning convergence. This multi-level architecture is inspired by humans' multi-level motion perception from cognitive science studies (Johansson, 1975; Lu & Sperling, 1995; Smith et al., 1998) and multi-level abstraction search in constraint optimization (Zhang & Shah, 2016). In addition, we design a novel spatial-temporal relational reasoning mechanism, which includes a CNN-based Relation Net to reason about spatial
1

Under review as a conference paper at ICLR 2019
relations between objects and an Inertia Net to learn temporal effects. This mechanism offers a disentangled way to handle physical reasoning in the setting of partial observation.
Empirical results show that MAOP significantly outperforms previous methods in terms of sample efficiency and generalization over novel environments that have multiple controllable and uncontrollable dynamic objects and different object layouts. It can learn from few examples and accurately predict the dynamics of objects as well as raw visual observations in previously unseen environments. In addition, MAOP learns disentangled representations and gains fruitful semantically and visually interpretable knowledge, including meaningful object masks, accurate object motions, disentangled reasoning process, and discovery of controllable agent.
2 RELATED WORK
Object-oriented reinforcement learning has received much research attention, which exploits efficient representations based on objects and their interactions. This learning paradigm is close to that of human cognition in the physical world and the learned object-level knowledge can be robustly generalized across environments. Early work on object-oriented RL requires explicit encodings of object representations and relations, such as relational MDPs (Guestrin et al., 2003), OO-MDPs (Diuk et al., 2008) and Object focused q-learning (Cobo et al., 2013). In this paper, we present an end-to-end, self-supervised neural network framework that automatically learns object representations and dynamics conditioned on actions and object relations from raw visual observations.
Action-conditioned dynamics learning aims to address one of the core problems for model-based DRL, that is, constructing an environment dynamics model. Several approaches have been proposed for learning how an environment changes in response to actions through unsupervised video prediction and achieve remarkable performance in training environments (Oh et al., 2015; Watter et al., 2015; Chiappa et al., 2017). Finn et al. (2016) develops a dynamics prediction method that explicitly models pixel motions and is partially invariant to object appearances, and its usage for model-based DRL is demonstrated with model predictive controller (Finn & Levine, 2017). Recently, Zhu & Zhang (2018) proposes an object-oriented dynamics learning paradigm that enables its learned model to generalize over unseen environments with different object layouts and be robust to changes of object appearances. However, this paradigm focuses environments with a single dynamic object. In this paper, we take a further step towards learning object-oriented dynamics model in more general environments with multiple controlled and uncontrollable dynamic objects. In addition, we design an instance-aware dynamics mechanism to support instance-level dynamics learning and handle partial observations.
Relation-based deep learning approaches make significant progress in a wide range of domains such as physical reasoning (Chang et al., 2016; Battaglia et al., 2016), computer vision (Watters et al., 2017; Wu et al., 2017), natural language processing Santoro et al. (2017), and reinforcement learning (Zambaldi et al., 2018; Zhu & Zhang, 2018). Relation-based nets introduce relational inductive biases into neural networks, which facilitate generalization over entities and relations and enables relational reasoning (Battaglia et al., 2018). (Zhu & Zhang, 2018) is similar to this paper, learning object-level dynamics conditioned on actions and object-to-object relations. In contrast to (Zhu & Zhang, 2018), this paper proposes a novel spatial-temporal relational reasoning mechanism, which includes an Inertia Net for learning temporal effects in addition to a CNN-based Relation Net for reasoning about spatial relations. This mechanism offers a disentangled way to handle physical reasoning in the setting of partial observability.
Instance Semantic Segmentation has been one of the fundamental problems in computer vision. Instance segmentation can be regarded as the combination of semantic segmentation and object localization. Many approaches have been proposed for instance segmentation, including DeepMask (Pinheiro et al., 2015), InstanceFCN (Dai et al., 2016), FCIS (Li et al., 2017), and Mask R-CNN He et al. (2017). Most models are supervised learning and require a large labeled training dataset. Liu et al. (2015) proposes a weakly-supervised approach to infer object instances in foreground by exploiting dynamic consistency in video. In this paper, we design a self-supervised, three-level instance segmentation approach for learning dynamic instance masks. At the most abstract level, the foreground detection module provides a coarse-grained guidance for producing region proposals at the instance segmentation level. The instance segmentation level then learns coarse instance segmentation. This coarse instance segmentation provides a guidance for learning accurate instance
2

Under review as a conference paper at ICLR 2019
masks at the dynamics learning level, whose instance segmentation not only considers object appearances and dynamics, but also motion prediction conditioned on object-to-object relations and actions.
3 MULTI-LEVEL ABSTRACTION OBJECT-ORIENTED PREDICTOR (MAOP)
In this section, we will present a novel self-supervised deep learning framework, aiming to learn object-oriented dynamics models that are able to generalize over unseen environments with different object layouts and multiple dynamic objects. Such an object-oriented dynamics learning approach requires simultaneously learning object representations and motions conditioned on their historical states, their interactions to other objects, and an agent's actions. This concurrent learning is very challenging for an end-to-end approach in complex environments with a dynamic background. Evidences from cognitive science studies (Johansson, 1975; Lu & Sperling, 1995; Smith et al., 1998) show that human beings are born with prior motion perception ability (in Cortical area MT) of perceiving moving and motionlessness, which enables learning more complex knowledge, such as object-level dynamics prediction. Inspired by these studies, we design a multi-level learning framework, called Multi-level Abstraction Object-oriented Predictor (MAOP), which incorporates motion perception levels to assist in dynamics learning.
Figure 1 illustrates three levels of the MAOP framework: dynamics learning, dynamic instance segmentation, and motion detection. The dynamics learning level is an end-to-end, self-supervised neural network, aiming to learn object representations and instance-level dynamics, and predict the next visual observation conditioned on an agent's action. To guide the learning of the object representations and instance localization at the dynamics learning level, the dynamic instance segmentation level provides coarse instant mask proposals in a self-supervised manner. It only utilizes the spatial-temporal information of locomotion property and appearance pattern to capture the region proposals of dynamic instances. To facilitate the learning of dynamic instance segmentation, MAOP employs the more coarse-grained motion detection level, which detects changes in image sequences and provides guidance on proposing regions potentially containing dynamic instance. As the learning proceeds, the knowledge distilled from the more coarse-grained level are gradually refined at the more fine-grained level by considering additional information. When the training is finished, the coarse-grained levels of dynamic instance segmentation and motion detection will be removed at the testing stage.
The multi-level learning design of MAOP shares some similar idea with the multi-level abstraction search approach (MASA) in constraint optimization (Zhang & Shah, 2016), which also employs multi-level course-gained abstract problems and greatly improves the search quality and speed in the original complex problem space. Two observations may explain why such multi-level abstraction approaches work (Zhang & Shah, 2016). First, the optimization in a coarse-grained abstract problem considers an easier objective and less information and thus is computationally cheap, but its output still provides an effective guidance for solving the original problem. Second, a coarsegrained abstract problem often has a smoother search space surface, avoiding earlier convergence to inferior local optima in the original search space.
In the rest of this section, we will describe in detail the design of each level of MAOP and their connections.
3.1 OBJECT-ORIENTED DYNAMICS LEARNING
The semantics of the dynamics learning level can be formulated as learning an object-based dynamics model with the attention proposals generated from the dynamic instance segmentation level. The learning architecture is shown at the top level of Figure 1. It is an end-to-end neural network and trained in a self-supervised manner. It learns disentangled representations (including objects, relations and effects) and follows an object-oriented paradigm. It takes multiple-frame video images and an agent's actions as input, learns the dynamics of controllable and uncontrollable dynamic object instances conditioned on the actions and object-to-object relations, and produce the predictions of raw visual observations. The dynamics learning network has four major components: A) an Object Detector that decomposes the input image into objects; B) an Instance Localization module responsible for localizing dynamic instances; C) a Dynamics Net for learning the motion of each dynamic
3

Under review as a conference paper at ICLR 2019

Dynamics Learning

Raw image

Object Masks Static

Dynamic Object Detector
Abstraction
Multi-frame Images

Background

Background Constructor

Action

Dynamics Net Object Motion

Objects + Relations

Instance Localization
Proposal
Selection Algorithm

Instance Bounding Box

Prediction

Instance Masks

Instance Segementation

Class A Class B Class C Class D

Previous frame Current frame

Concat

Region Proposal Sampling
Abstraction

Region Proposals
Proposal

Class A Class B Class C Class D
Dynamic Instance Segmentation Net

Motion Detection

A video set

Foreground Detection

Foreground

Figure 1: Multi-level abstraction framework from a top-down decomposition view.

instance conditioned on the effects from actions and object-to-object relations; and D) a Background Constructor that computes the background image from the learned static object masks.
Object Detector and Instance Localization Module. Object Detector learns to decompose the input image into object masks. An object mask describes the spatial distribution of an object, which forms the fundamental building block of our object-oriented framework. (Zhu & Zhang, 2018) presents a possible formulation of object masks mainly based on visual appearances by using CNN modules. However, this formulation suffers from limitations in scenes with multiple dynamic instances, because it is hard to distinguish two instances with a similar visual appearance but different motions and to divide them into two different masks. To address this issue, we introduce a new formulation of object masks, each of which either represents one dynamic instance or one class of static objects, and incorporate Object Detector with an Instance Localization Module to localize each dynamic instance to support instance-level dynamics learning. Instance localization is also a common technique in the area of region-based object detection (e.g. (Girshick et al., 2014), (Girshick, 2015), (Ren et al., 2015), (He et al., 2017)). The class-specific object masks in conjunction with instance localization build the bridge to connect visual perception (Object Detector) with dynamics learning (Dynamics Net), which allows learning objects from appearances, interactions, and dynamics.
Specifically, Object Detector receives image I(t)  RH×W ×3 at timestep t and then outputs object masks O(t)  [0, 1]H×W ×nO (including dynamic object masks D(t)  [0, 1]H×W ×nD and static object masks S(t)  [0, 1]H×W ×nS ) , where nD and nS denotes the maximum class number of dynamic and static objects respectively, and nO = nD +nS. Note that Object Detector uses the same CNN architecture with OODP. Then, Instance Localization module uses object masks produced by Object Detector to compute each single instance mask M:(,t:),i  [0, 1]HM ×WM (1  i  nM ) through region proposal sampling and instance mask selection (similar with Section 3.2), where nM denotes the maximum number of localized instances.
Dynamics Net. Dynamics Net is designed to be a disentangled knowledge learner, which learns instance-based motion effects of actions and object-to-object relations (using Relation Net) and
4

Under review as a conference paper at ICLR 2019

historical states (using Inertia Net), and then reasons about the motion of each dynamic instance based on these effects. Its architecture is illustrated in Figure 2. As shown in the left subfigure, instance-level dynamics learning is performed, which means the motion of each dynamic instance is individually computed. We take as an example the computation of the motion of the i-th instance M(:,t:),i to show the detailed structure of the Effect Net module.

SUM
... ...

Bounding Box

Class A

Class C

Class B

Object Masks
CoordConv X Map Y Map

Instances Object Masks
Effect Nets
Action

CNA

CNB

CNC

Object Motions

Tailor

Inertia Net

Concat with history

INC
Effect Net

Scalar product

Data stream

Relation Nets Effects
RNC, F
RNC, B RNC, A
Self-Effect

Figure 2: Architecture of Dynamics Net (left) and Effect Net (right). Different classes are distinguished by different letters (e.g., A, B, ... , F).

As shown in the right subfigure of Figure 2, we first use a sub-differentiable tailor module to enable
the inference of dynamics focusing on the relations with neighbour objects. This module crops a w-size "horizon" window from the concatenated masks of all objects O(t) centered on the expected location of M:(,t:),i, where w denotes the maximum effective range of relations. The details of the sub-differentiable cropping process can be found in (Zhu & Zhang, 2018). Then, the cropped object
masks are respectively concatenated with the constant x-coordinate and y-coordinate meshgrid map and fed into the corresponding Relation Nets (RN) according to their classes. We use C:(,t:),i,j to denote the cropped mask that crops the j-th class object O:(,t:),j centered on the expected location of the i-th dynamic instance (the class it belongs to is denoted as ci, 1  ci  nD). The effects of object class j on class ci, E(t)(ci, j)  R2×na (na denotes the number of actions) is calculated as,

E(t)(ci, j) = RNci,j concat C:(,t:),i,j , Xmap, Ymap .

Note that there are total nD × nO RNs for nD × nO pairs of object classes that share the same architecture but not their weights. To handle the partial observation problem, we add an Inertia Nets
(IN) to learn the self-effects of an object class through historical states, that is,

Es(etl)f(ci) = INci concat M:(,t:),i, M(:,t:+,i1), M(:,t:+,i2), . . . , M:(,t:+,iH) .

where H is the history length and there are total nD INs for nD dynamic object classes that share the same architecture but not their weights. To predict the motion vector mi(t)  R2 for the i-th dynamic instance, all these effects are summed up and then multiplied by the one-hot coding of
action a(t)  {0, 1}na , that is,

mi(t) =

E(t)(ci, j) + Es(etl)f(ci) · a(t).
j

Background Constructor. This module uses static object masks learned by Object Detector and composes the static background of its observations, which is then used with predicted motions of

5

Under review as a conference paper at ICLR 2019

dynamic instances to predict the next visual observation. As Object Detector can decompose its observation into objects in an unseen environment with a different object layout, Background Constructor is able to generate a corresponding static background and support the visual observation prediction in novel environments. Specifically, Background Constructor maintains an external background memory B  RH×W ×3 which is continuously updated (via moving average) by the static object mask learned by Object Detector. The updating formula is given by,

B(t)

=

 

B(t-1)

+

(1

-

)I(t)

i

S:(,t:),i ,

 0,

t > 0; t = 0,

where I(t) is the current frame and  is the decay rate.

Prediction and Training Loss. At the output end of our model, the prediction of the next frame
is produced by merging the learned object motions and the background B(t). We use a similar
STN-based merging process as (Zhu & Zhang, 2018). The pixels of a dynamic region indicated by
the i-th instance mask can be calculated as I(t)  M:(,t:),i, where  denotes masking an image with a mask. Then we use Spatial Transformer Network (STN) Jaderberg et al. (2015) to apply the learned
instance motion vector mi(t) on this dynamic region (denoted by STN(I(t)  M(:,t:),i, m(it))). First, we transform all the dynamic instances to obtain the pixels of dynamic instances in the next frame, that
is, ^ID(t+1) = i STN(I(t)  M:(,t:),i, m(it)). Second, we compute the rest pixels from background B(t), that is, ^I(St+1) = 1 - i STN(M:(,t:),i, m(it))  B(t). Thus, the prediction of the next frame I^(t+1), is calculated by,

I^(t+1) = I^D(t+1) + I^S(t+1) =

STN(I(t)  M:(,t:),i, m(it)) + 1 -

STN(M(:,t:),i, mi(t))  B(t).

ii

In this paper, we use l2 pixel loss to restrain image prediction error, Lprediction =

I^(t+1) -I(t+1)

2 2

.

To get earlier feedback before reconstructing images and facilitate the training process, we add a

highway loss here, Lhighway = i (u¯i, v¯i)(t) + m(it) - (u¯i, v¯i)(t+1) 22, where (u¯i, v¯i)(t) is the

excepted location of i-th instance mask M:(,t:),i. Finally, we add a proposal loss to utilize the dynamic

instance proposals provided by the abstracted problem to guide our optimization, which is given by,

Lproposal =

i(O(:,t:),i - P(:,t:),i) 22, where P denotes the region proposals of object masks. The total

loss is given by combining the previous losses with different weights,

Ltotal = Lhighway + 1Lprediction + 2Lproposal

3.2 DYNAMIC INSTANCE SEGMENTATION
The level of dynamic instance segmentation aims to generate region proposals of dynamic instances in a self-supervised manner to guide the learning of object masks and provide initial signals for instance localization at the dynamics learning level. This level captures dynamic instances only based on the spatial-temporal information of locomotion property and appearance pattern without using labeled data, and thus may generate noisy coarse region proposals of dynamic instances. The process of dynamic instance segmentation takes a sequence of image frames as input and is guided by the foreground region proposals generated by the Motion Detection level.
The dynamic instance segmentation module is composed of three parts. The first part generates region proposals which define the set of candidate regions for the following instance segmentation. The second part is Dynamic Instance Segmentation Network which employs object appearance and dynamics consistency to derive the instance segmentation. The third part is instance mask selection which is similar to non-maximum suppression (NMS) (Ren et al., 2015). This part tries to eliminate redundant and noisy instance masks. Although the entire process is analogous to the region-based object detector method (Liu et al., 2018), it focuses on generalizing detection results over different object layouts and exploits instance-level coherence without using labeled data.
Region proposal sampling. The advance of region proposal techniques (e.g. sliding window (Dalal & Triggs, 2005), selective search (Uijlings et al., 2013), region proposal network (RPN) (Ren et al., 2015)) is critical for the development of object detection methods (Liu et al., 2018). It is challenging

6

Under review as a conference paper at ICLR 2019

to quickly generate accurate region proposals without using labeled data. To address this challenge, we design a novel multi-scale learning-free sampling algorithm for region proposal. This algorithm generates multi-scale region proposals with a full coverage over the input mask. It effectively utilizes foreground mask proposals generated from the Motion Detection level to encourage its attentions on the dynamic regions. This algorithm is also used to localize every single instance in the dynamic object masks learned by Object Detector, which is described in Section 3.1. The detailed algorithm is described in Appendix A.

Dynamic Instance Segmentation Network (DISN). The dynamic instance segmentation network

receives the sampled instance candidate proposals and segments instances and background by in-

tegrating appearance and dynamics property. It consists of two parts: Foreground Detector and

Instance Detector. Foreground Detector learns to decompose the region proposal into foreground

mask F(t)  [0, 1]HR×WR and background mask B(t) = 1 - F(t), where HR and WR de-

notes the size of sampled proposal region. To guide learning of the Foreground Detector, we

use the foreground region proposals F(fotr)eground from the third abstraction by adding a l2 loss,

Lforeground =

F(t) - Ff(otr)eground

2 2

.

The

architecture

of

Foreground

Detector

is

similar

to

binary-class

Object Detector. The follow-up Instance Detector also uses the same architecture to decompose the

foreground mask F(t) into different semantic masks M(t)  [0, 1]HR×WR×nO , where nO is the max-

imum number of object classes. Since DISN leans semantic segmentations in the region proposals

of instance candidates, the learned semantic masks are exactly instance masks. We denote the i-th

instance by M:(,t:),i. CNNs offer the ability to identify object appearance, and we also introduce an instance-consistent loss to consider the dynamics property in the meanwhile, which is defined as,

Linstance = lconsistent(I(t-1), I(t), B(t-1), B(t)) +

lconsistent(I(t-1), I(t), M:(,t:-,k1), M(:,t:),k).

k

This loss function is based on two frames of images I(t-1), I(t), and the function lconsistent(·) (Equation 1) formally specifies instance temporal-spatial coherence.

Instance mask selection. Instance mask selection is designed to remove the redundant and noisy instance masks. Inspired by Non-maximum suppression (NMS) and selective search (SS) (Uijlings et al., 2013) in region-based object detection (e.g. (Girshick et al., 2014), (Girshick, 2015), (Ren et al., 2015), (He et al., 2017)), we propose a novel instance mask selection, which integrates the instance coherence quantification, overlapping discount and multi-scale detection approach to screen out high-consistency and non-overlapping instance masks from a large number of instance mask candidates. The details of the selection algorithm are discussed in Appendix B.

Instance-consistent loss and selection score function. We introduce a novel instance-consistent
loss that considers the spatial-temporal motion consistency of instances. This loss is used to train
DISN and a similar variant (see Appendix B) serves as a metric score for selecting instance masks.
We use STN to estimate the spatial transformation of an instance in two adjacent frames. We denote T = STN(T, v) as applying the motion vectors v and on the input tensor T and T is the transformed tensor. Conversely, the inverse function is denoted as v = STN-1(T, T ) where STN-1 function estimates the motion vector v from the two adjacent frames. The instance-consistent loss lconsistent(I, I , M, M ) is defined as,

lconsistent(I, I , M, M ) =

STN(I, STN-1(M, M )) - I

2 2

·

max(STN(M,

STN-1(M,

M

)),

M

),

(1)

where max() is element-wise max and we choose the translation form of STN for our experiment.

3.3 MOTION DETECTION
At this level, we employ foreground detection to detect changing regions from a sequence of image frames and provide coarse dynamic region proposals for assisting in dynamic instance segmentation. In our experiments, we use a simple unsupervised foreground detection approach proposed by (Lo & Velastin, 2001). Our MAOP framework is also compatible with many advanced unsupervised foreground detection methods (Lee, 2005; Zhou et al., 2013; Guo et al., 2014; Maddalena et al., 2008) that are more efficient or more robust to moving camera. These complex unsupervised foreground detection methods have the potential to improve the performance but are not the focus of this work.

7

Under review as a conference paper at ICLR 2019

Monster Kong

Flappy Bird

Training Unseen environments for testing Training

Unseen environments for testing

Figure 3: Examples of 1-to-3 generalization experiments.

4 EXPERIMENTS
We evaluate our model on two games, Monster Kong and Flappy Bird, from the Pygame Learning Environment (Tasfi, 2016), which allows us to test generalization ability over various scenes with different layouts. Here, the Monster Kong is the advanced version of that used by Zhu & Zhang (2018) which has a more general and complex setting. The monster wanders around and breathes out fires randomly, and the fires also move with some randomness. The agent explores with actions up, down, left, right, jump and noop. All these dynamic objects interact with the environments and objects according to their own physics engine. Moreover, gravity and jump model has a long-term dynamics effects, leading to a partial observation problem. To test whether our model can truly learn the underlying physical mechanism behind the visual observations and perform relational reasoning, we set the k-to-m generalization experiment, where we use k different environments for training and m different unseen environments for testing. Flappy Bird is a side-scroller game, where a bird flies between columns of green pipes with action jump and noop. Since the unseen environments will be similar with the training ones without limitation of samples in this game, we limit the samples for training. Two experimental settings are shown in Figure 3. We use random exploration on Monster Kong, and an expert guided random exploration on Flappy Bird because in this domain a totally random exploration will lead to an early death of the agent even at the very beginning. We compare MAOP with state-of-the-art action-conditioned dynamics learning baselines, AC Model (Oh et al., 2015), CDNA (Finn et al., 2016), and OODP (Zhu & Zhang, 2018).
4.1 GENERALIZATION AND SAMPLE EFFICIENCY
To make a sufficient comparison with the previous methods on the generalization ability and sample efficiency of object dynamics learning and image prediction, we conduct 1-5, 2-5 and 3-5 generalization experiments with a variety of evaluation indices on Monster Kong. We use n-error accuracy to measure the performance of object dynamics prediction, which is defined as the proportion that the difference between the predicted and ground-true agent locations is less than n pixel. We also add an extra pixel-based measurement (denoted by object RMSE), which compares the pixel difference near dynamic objects between the predicted and ground-truth images. To evaluate the image prediction, we adopt a typical image prediction loss RMSE. In Figure 4 and C7 (Appendix C), we plot the learning curve for better visualization of the comparison.
As shown in Table 1, MAOP significantly outperforms other methods in all experiment settings in terms of generalization ability and sample efficiency of both object dynamics learning and image prediction. It can achieve 0.84 0-error accuracy, even with a single training environment, which suggests MAOP is good at relational reasoning. Although AC Model achieves high accuracy in training environments, its performance in unseen scenes is much worse, which is probably because its pixel-level inference easily leads to overfitting. CDNA performs better than AC Model in those uncontrolled objects, but still cannot deal with complicated interactions in lack of knowledge on object-to-object relations. By the structural limitation of OODP, it has innate difficulty on frames with multiple dynamic objects. We also test our model on Flappy Bird, where we limit the training samples to 100 and 300 to form a sufficiently challenging generalization task. As shown in Table 2, our performance is similar with that on Monster Kong. Our generalization ability and sample efficiency significantly outperform other baselines. Surprisingly, only 100 samples are enough to reach almost perfect 1-error accuracy.
8

Under review as a conference paper at ICLR 2019

Models

1-5

Agent All

MAOP 0.67 0.80 0-error OODP 0.24 0.17 accuracy AC Model 0.04 0.59
CDNA 0.30 0.66

MAOP 0.90 0.91 1-error OODP 0.49 0.29 accuracy AC Model 0.07 0.63
CDNA 0.42 0.84

MAOP 0.95 0.94 2-error OODP 0.67 0.47 accuracy AC Model 0.10 0.64
CDNA 0.50 0.86

Object RMSE

MAOP OODP AC Model CDNA

31.99 65.51 62.02 53.89

Image RMSE

MAOP OODP AC Model CDNA

6.90 14.70 15.99 11.47

Training environments

1-5 2-5

Agent All Agent All

0.88 0.87 0.18 0.16 0.92 0.97 0.41 0.76

0.86 0.87 0.22 0.17 0.80 0.93 0.42 0.78

0.97 0.94 0.32 0.23 0.98 0.99 0.48 0.86

0.97 0.93 0.34 0.23 0.95 0.98 0.48 0.86

0.99 0.96 0.44 0.37 0.99 0.99 0.52 0.87

0.99 0.95 0.46 0.32 0.98 0.99 0.53 0.88

26.65 66.44 18.88 34.99

31.68 66.66 22.39 35.26

5.64 15.08 4.12 7.41

6.68 14.89 4.78 7.58

3-5
Agent All
0.80 0.83 0.26 0.20 0.77 0.92 0.44 0.74
0.96 0.93 0.35 0.25 0.94 0.98 0.51 0.87
0.98 0.94 0.49 0.39 0.97 0.98 0.54 0.88
30.33 64.73 21.30 35.94
6.46 14.42 4.69 7.68

1-5
Agent All
0.60 0.77 0.20 0.16 0.01 0.18 0.31 0.55
0.86 0.90 0.39 0.25 0.02 0.34 0.45 0.82
0.95 0.94 0.60 0.43 0.04 0.34 0.53 0.85
34.14 67.39 85.46 56.31
7.90 15.42 44.92 12.23

Unseen environments

1-5 2-5

Agent All Agent All

0.81 0.84 0.20 0.15 0.46 0.48 0.37 0.59

0.85 0.87 0.18 0.15 0.30 0.48 0.40 0.71

0.96 0.93 0.34 0.22 0.65 0.69 0.45 0.83

0.97 0.93 0.32 0.21 0.52 0.67 0.47 0.84

0.98 0.95 0.48 0.34 0.73 0.74 0.47 0.84

0.99 0.95 0.43 0.31 0.64 0.73 0.50 0.86

29.78 67.41 57.41 45.34

31.32 67.78 55.45 37.59

8.60 24.68 39.46 9.87

8.73 26.39 38.07 8.10

3-5
Agent All
0.80 0.85 0.26 0.18 0.45 0.66 0.41 0.70
0.95 0.93 0.34 0.22 0.66 0.77 0.48 0.86
0.98 0.95 0.46 0.36 0.77 0.81 0.51 0.87
30.80 64.95 43.48 37.80
6.55 14.52 38.12 8.16

Table 1: Prediction performance on Monster Kong. k-m means the k-to-m generalization problem.  indicates training with only 1000 samples. ALL represents all dynamic objects.

0-error accuracy
1-error accuracy
2-error accuracy

Models
MAOP OODP AC Model CDNA
MAOP OODP AC Model CDNA
MAOP OODP AC Model CDNA

Training environments

1-5

1-5

Agent All Agent All

0.84 0.90 0.87 0.93 0.01 0.29 0.01 0.32 0.39 0.64 0.48 0.75 0.13 0.78 0.41 0.84

0.99 1.00 0.97 0.97 0.05 0.52 0.04 0.56 0.48 0.80 0.57 0.87 0.26 0.82 0.57 0.89

1.00 1.00 0.99 0.99 0.14 0.66 0.12 0.67 0.53 0.85 0.63 0.90 0.37 0.84 0.66 0.92

Unseen environments
1-5 1-5
Agent All Agent All
0.83 0.89 0.83 0.92 0.01 0.18 0.02 0.15 0.03 0.18 0.04 0.23 0.10 0.77 0.16 0.79
0.99 0.99 0.98 0.97 0.06 0.39 0.07 0.39 0.07 0.37 0.14 0.45 0.22 0.81 0.36 0.84
1.00 1.00 0.99 0.98 0.16 0.59 0.16 0.56 0.12 0.53 0.24 0.64 0.36 0.84 0.49 0.87

Table 2: Performance of the object dynamics prediction on 1-5 generalization problem in Flappy Bird.  and  indicates training with only 100 and 300 samples.

Figure 4: Learning curves.

Figure 5: Our discovery of the controllable agent. 9

Under review as a conference paper at ICLR 2019

4.2 INTERPRETABLE REPRESENTATIONS AND KNOWLEDGE
MAOP takes a step towards interpretable deep learning and disentangled representation learning. Through interacting with environments, it learns fruitful visually and semantically interpretable knowledge in an unsupervised manner, which contributes to unlock the "black box" of neural networks and open the avenue for further researches on object-based planning, object-oriented modelbased RL, and hierarchical learning.
Visual interpretability. To demonstrate the visual interpretability of MAOP in unseen environments, we visualize the learned masks of dynamic and static objects. We highlight the attentions of the object masks by multiplying the raw images by the binarized masks. Note that MAOP does not require the actual number of objects but a maximum number and some learned object masks may be redundant. Thus, we only show the informative object masks. As shown in Figure 6, our model captures all the key objects in the environments including the controllable agents (the cowboy and the bird), the uncontrollable dynamic objects (the monster, fires and pipes), and the static objects that have effects on the motions of dynamic objects (ladders, walls and the free space). We also observe that model can learn disentangled object representations and distinguish the objects by both appearance and dynamic property.

Monster Kong

Flappy Bird

Figure 6: Visualization of the masked images in unseen environments. Left is the raw image.
Discovery of the controllable agent. With the learned knowledge in MAOP, we can easily uncover the action-controlled agent from all the dynamic objects, which is useful semantic information that can be used in heuristic algorithms. Specifically, the object that has the maximal variance of total effects over actions is the action-controlled agent. Denote the total effects as Ei = ( j E(ci, j)) + Eself(ci), Ei  R2×na , the label of the action-controlled agent is calculated as, arg maxi V ar(Ei). The histogram in Figure 5 plotting the ground-truth label distribution of our discovered actioncontrolled agents clearly demonstrates that our discovery of the controllable agent achieves perfect 100% accuracy.
Dynamical interpretability. To show the dynamical interpretability behind image prediction, we test our predicted motions by comparing RMSEs between the predicted and ground-truth motions in unseen environments (Table C3 in Appendix C). Intriguingly, most predicted motions are quite accurate, with the RMSEs less than 1 pixel. Such a visually indistinguishable error also verifies our dynamics learning.
5 CONCLUSIONS AND FUTURE WORK
This paper presents a self-supervised multi-level learning framework for learning action-conditioned object-based dynamics. This framework is example-efficient and generalizes object dynamics and prediction of raw visual observations to complex unseen environments with multiple dynamic objects. The learned dynamics model potentially enables an agent to directly plan or efficiently learn for unseen environments. Although a random policy or an expert's policy is used for exploration in our experiments, our framework can support smarter exploration strategies, e.g., curiosity-driven exploration. Our future work includes extending our model for deformation prediction (e.g., object appearing, disappearing and non-rigid deformation) and incorporating a camera motion prediction network module for applications such as FPS games and autonomous driving.
10

Under review as a conference paper at ICLR 2019
REFERENCES
Peter Battaglia, Razvan Pascanu, Matthew Lai, Danilo Jimenez Rezende, et al. Interaction networks for learning about objects, relations and physics. In Advances in Neural Information Processing Systems, pp. 4502­4510, 2016.
Peter W Battaglia, Jessica B Hamrick, Victor Bapst, Alvaro Sanchez-Gonzalez, Vinicius Zambaldi, Mateusz Malinowski, Andrea Tacchetti, David Raposo, Adam Santoro, Ryan Faulkner, et al. Relational inductive biases, deep learning, and graph networks. arXiv preprint arXiv:1806.01261, 2018.
Michael B Chang, Tomer Ullman, Antonio Torralba, and Joshua B Tenenbaum. A compositional object-based approach to learning physical dynamics. arXiv preprint arXiv:1612.00341, 2016.
Silvia Chiappa, Se´bastien Racaniere, Daan Wierstra, and Shakir Mohamed. Recurrent environment simulators. International Conference on Learning Representations, 2017.
Luis C Cobo, Charles L Isbell, and Andrea L Thomaz. Object focused q-learning for autonomous agents. In Proceedings of the 2013 international conference on Autonomous agents and multiagent systems, pp. 1061­1068. International Foundation for Autonomous Agents and Multiagent Systems, 2013.
Jifeng Dai, Kaiming He, Yi Li, Shaoqing Ren, and Jian Sun. Instance-sensitive fully convolutional networks. In European Conference on Computer Vision, pp. 534­549. Springer, 2016.
Navneet Dalal and Bill Triggs. Histograms of oriented gradients for human detection. In Computer Vision and Pattern Recognition, 2005. CVPR 2005. IEEE Computer Society Conference on, volume 1, pp. 886­893. IEEE, 2005.
Carlos Diuk, Andre Cohen, and Michael L Littman. An object-oriented representation for efficient reinforcement learning. In Proceedings of the 25th international conference on Machine learning, pp. 240­247. ACM, 2008.
Chelsea Finn and Sergey Levine. Deep visual foresight for planning robot motion. In Robotics and Automation (ICRA), 2017 IEEE International Conference on, pp. 2786­2793. IEEE, 2017.
Chelsea Finn, Ian Goodfellow, and Sergey Levine. Unsupervised learning for physical interaction through video prediction. In Advances in Neural Information Processing Systems, pp. 64­72, 2016.
Ross Girshick. Fast r-cnn. In Proceedings of the IEEE international conference on computer vision, pp. 1440­1448, 2015.
Ross Girshick, Jeff Donahue, Trevor Darrell, and Jitendra Malik. Rich feature hierarchies for accurate object detection and semantic segmentation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 580­587, 2014.
Carlos Guestrin, Daphne Koller, Chris Gearhart, and Neal Kanodia. Generalizing plans to new environments in relational mdps. In Proceedings of the 18th international joint conference on Artificial intelligence, pp. 1003­1010. Morgan Kaufmann Publishers Inc., 2003.
Xiaojie Guo, Xinggang Wang, Liang Yang, Xiaochun Cao, and Yi Ma. Robust foreground detection using smoothness and arbitrariness constraints. In European Conference on Computer Vision, pp. 535­550. Springer, 2014.
Kaiming He, Georgia Gkioxari, Piotr Dolla´r, and Ross Girshick. Mask r-cnn. In Computer Vision (ICCV), 2017 IEEE International Conference on, pp. 2980­2988. IEEE, 2017.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In International conference on machine learning, pp. 448­456, 2015.
Max Jaderberg, Karen Simonyan, Andrew Zisserman, et al. Spatial transformer networks. In Advances in Neural Information Processing Systems, pp. 2017­2025, 2015.
11

Under review as a conference paper at ICLR 2019

Gunnar Johansson. Visual motion perception. Scientific American, 232(6):76­89, 1975.

Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. CoRR, abs/1412.6980, 2014. URL http://arxiv.org/abs/1412.6980.

Dar-Shyang Lee. Effective gaussian mixture learning for video background subtraction. IEEE Transactions on Pattern Analysis &amp; Machine Intelligence, (5):827­832, 2005.

Yi Li, Haozhi Qi, Jifeng Dai, Xiangyang Ji, and Yichen Wei. Fully convolutional instance-aware semantic segmentation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 4438­4446, 2017.

Buyu Liu, Xuming He, and Stephen Gould. Multi-class semantic video segmentation with exemplarbased object reasoning. In IEEE Winter Conference on Applications of Computer Vision (WACV), pp. 1014­1021. IEEE, 2015.

Li Liu, Wanli Ouyang, Xiaogang Wang, Paul Fieguth, Jie Chen, Xinwang Liu, and Matti Pietika¨inen. Deep learning for generic object detection: A survey. arXiv preprint arXiv:1809.02165, 2018.

BPL Lo and SA Velastin. Automatic congestion detection system for underground platforms. In Intelligent Multimedia, Video and Speech Processing, 2001. Proceedings of 2001 International Symposium on, pp. 158­161. IEEE, 2001.

Zhong-Lin Lu and George Sperling. The functional architecture of human visual motion perception. Vision research, 35(19):2697­2722, 1995.

Lucia Maddalena, Alfredo Petrosino, et al. A self-organizing approach to background subtraction for visual surveillance applications. IEEE Transactions on Image Processing, 17(7):1168, 2008.

Junhyuk Oh, Xiaoxiao Guo, Honglak Lee, Richard L Lewis, and Satinder Singh. Action-conditional video prediction using deep networks in atari games. In Advances in Neural Information Processing Systems, pp. 2863­2871, 2015.

Pedro O Pinheiro, Ronan Collobert, and Piotr Dolla´r. Learning to segment object candidates. In Advances in Neural Information Processing Systems, pp. 1990­1998, 2015.

Se´bastien Racanie`re, The´ophane Weber, David Reichert, Lars Buesing, Arthur Guez, Danilo Jimenez Rezende, Adria` Puigdome`nech Badia, Oriol Vinyals, Nicolas Heess, Yujia Li, et al. Imagination-augmented agents for deep reinforcement learning. In Advances in Neural Information Processing Systems, pp. 5694­5705, 2017.

Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object detection with region proposal networks. In Advances in neural information processing systems, pp. 91­99, 2015.

Adam Santoro, David Raposo, David G Barrett, Mateusz Malinowski, Razvan Pascanu, Peter Battaglia, and Tim Lillicrap. A simple neural network module for relational reasoning. In Advances in neural information processing systems, pp. 4974­4983, 2017.

Andrew T Smith, Mark W Greenlee, Krish D Singh, Falk M Kraemer, and Ju¨rgen Hennig. The processing of first-and second-order motion in human visual cortex assessed by functional magnetic resonance imaging (fmri). Journal of Neuroscience, 18(10):3816­3830, 1998.

Norman Tasfi. Pygame learning environment. PyGame-Learning-Environment, 2016.

https://github.com/ntasfi/

Jasper RR Uijlings, Koen EA Van De Sande, Theo Gevers, and Arnold WM Smeulders. Selective search for object recognition. International journal of computer vision, 104(2):154­171, 2013.

Manuel Watter, Jost Springenberg, Joschka Boedecker, and Martin Riedmiller. Embed to control: A locally linear latent dynamics model for control from raw images. In Advances in neural information processing systems, pp. 2746­2754, 2015.

12

Under review as a conference paper at ICLR 2019
Nicholas Watters, Daniel Zoran, Theophane Weber, Peter Battaglia, Razvan Pascanu, and Andrea Tacchetti. Visual interaction networks. In Advances in Neural Information Processing Systems, pp. 4540­4548, 2017.
Jiajun Wu, Erika Lu, Pushmeet Kohli, Bill Freeman, and Josh Tenenbaum. Learning to see physics via visual de-animation. In Advances in Neural Information Processing Systems, pp. 152­163, 2017.
Vinicius Zambaldi, David Raposo, Adam Santoro, Victor Bapst, Yujia Li, Igor Babuschkin, Karl Tuyls, David Reichert, Timothy Lillicrap, Edward Lockhart, et al. Relational deep reinforcement learning. arXiv preprint arXiv:1806.01830, 2018.
Chongjie Zhang and Julie A Shah. Co-optimizating multi-agent placement with task assignment and scheduling. In IJCAI, pp. 3308­3314, 2016.
Xiaowei Zhou, Can Yang, and Weichuan Yu. Moving object detection by detecting contiguous outliers in the low-rank representation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 35(3):597­610, 2013.
Guangxiang Zhu and Chongjie Zhang. Object-oriented dynamics predictor. arXiv preprint arXiv:1806.07371, 2018.
13

Under review as a conference paper at ICLR 2019
APPENDIX A REGION PROPOSAL SAMPLING
To demonstrate the detailed pipeline in the region proposal sampling, we take as an example considering the foreground mask F(t) as our input mask. We use the multi-scale bounding boxes and multiple full coverage sampling methods to guarantee that each pixel of the foreground mask is covered in each scale coverage and then resize the region proposals into the same scale. We apply our sampling algorithm on the I(t) and get the byproduct region proposal from I(t-1) as well. Algorithm 1 Region proposal sampling. Require: foreground mask F(t)  [0, 1]H×W , foreground threshold , density threshold , the number of region proposal scales nS, the number of full coverage T . 1: Initialize proposal set P = . 2: for l = 1 . . . nS do 3: Select scale dx, dy depend on the level l. 4: for t = 1 . . . T do 5: Initialize foreground set S = {(i, j)|F(i,tj) > }. 6: while S =  do 7: Sample a pixel coordinate (x, y) from S. 8: Get a box B = {(i, j)| |i - x|  dx, |j - y|  dy}. 9: if (i,j)B Fi,j >  then 10: Insert B into the proposal set P  P  {B}. 11: end if 12: Update the remain foreground set S  S \ B. 13: end while 14: end for 15: end for 16: return P
14

Under review as a conference paper at ICLR 2019

APPENDIX B INTANCE MASK SELECTION

Algorithm 2 Intance mask selection.
Require: Overlapping threshold ; empty threshold ; the number of region proposal scales nS; the number of region proposals in one scale nC; the height and width sets of the multi-scale region proposals HR, WR where HR = {HRi |i = 1, . . . , nS} and WR = {WRi |i = 1, . . . , nS}; image and mask set of different scale MS = {(MS(ti-1), MS(ti))|MS(ti-1), M(Sti)  [0, 1]HRi ×WRi ×nC , i = 1, . . . , nS } and IS = {(IS(ti-1), IS(ti))|IS(ti-1), I(Sti)  [0, 1]HRi ×WRi ×3×nC , i = 1, . . . , nS }; selection score function g(·) of mask M  [0, 1]HRu ×WRu where u  {1, . . . , nS} which is mentioned in section B; a variant of NMS (Ren et al., 2015) fNMS (·) where fNMS (·) calculates the overlapping area by mask instead of bounding box.
Return: The mask set of single dynamic instance M = {Mi|Mi  [0, 1]HRx ×WRx ; i = 1, . . . , nI ; x  {1, . . . , nS}} where nI denotes the number of instances.

1: for i = 1, . . . , nS do 2: Let g (MS(ti)) = g(M(Sti-1), MS(ti), I(Sti-1), I(Sti)) and define
g Mx(t) M(yt) = g Mx(t-1) My(t-1) , M(xt) My(t) , Ix(t-1)

I(yt-1) , Ix(t)

Iy(t)

where denotes any operation.

3: Select the remaining proposal set Sx = fNMS (M(Sti), g (·), , ) where Sx  {1, . . . , nC } and

Mi

=

{M(t)
Si :,:,x

|x



Sx}.

4: end for

5: Initialize mask set M = .
6: for i = 1, . . . , nS do 7: for Mx  Mi do

8: if Mx M g (min(Mx, Mx )) > g ( Mx M min(Mx, Mx )) then

9: for Mz  M do

10:

Calculate the overlapping area koverlap =

HRi h

WRi w

min(Mx,

Mz )h,w .

11:

Calculate the whole area kwhole =

HRi h

M .WRi
w

zh,w

12: if koverlap > kwhole then

13: Remove Mz from the mask set M  M \ Mz.

14: end if

15: end for 16: Insert Mx to the mask set M  M  {Mx}. 17: end if

18: end for

19: end for

20: return M .

selection score function. We introduce an instance-consistent score function that measures the motion consistentcy quantity of instance appearance and shape. The score function is similar to the instance-consistent loss mentioned in the section 3.2, which denotes g(I, I , M, M ) where I, I , M, M are the images and masks in two adjacent frames, which is defined as,
g(I, I , M, M ) = STN(I, STN-1(M, M )) - I 2 · max(STN(M, STN-1(M, M )), M ) + STN(M, STN-1(M, M )) - M 2.
And the lower score indicates the better dynamic instance mask.

15

Under review as a conference paper at ICLR 2019

APPENDIX C TABLES AND FIGURES

Model

Monster Kong

1-5 1-5 2-5 3-5

MAOP 1.96 0.34 0.38 0.42

Flappy Bird 1-5 1-5 0.30 0.34

Table C3: Average motion prediction error in two experiment environments.  and  correspond to the same sample restriction experiments in Table 1 and 2

Figure C7: More learning curves. 16

Under review as a conference paper at ICLR 2019
APPENDIX D IMPLEMENTATION DETAILS
The architecture of the CNNs in Object Detector and Instance Detector is the same as that in OODP (Zhu & Zhang, 2018). Denote Conv(F, K, S) as the convolutional layer with the number of filters F , kernel size K and stride S. Let R(), S() and BN () denote the ReLU layer, sigmoid layer and batch normalization layer Ioffe & Szegedy (2015). The 5 convolutional layers in Object Detector can be indicated as R(BN (Conv(16, 5, 2))), R(BN (Conv(32, 3, 2))), R(BN (Conv(64, 3, 1))), R(BN (Conv(32, 1, 1))), and BN (Conv(1, 3, 1)), respectively. The 5 convolutional layers in Instance Detector can be indicated as R(BN (Conv(32, 5, 2))), R(BN (Conv(32, 3, 2))), R(BN (Conv(32, 3, 1))), R(BN (Conv(32, 1, 1))), and BN (Conv(1, 3, 1)), respectively. The architecture of Foreground Detector is similar to binary-class Object Detector and the 5 convolutional layers in Foreground Detector can be indicated as R(BN (Conv(32, 5, 2))), R(BN (Conv(32, 3, 2))), R(BN (Conv(32, 3, 1))), R(BN (Conv(32, 1, 1))), and S(BN (Conv(1, 3, 1))), respectively. The CNNs in Dynamics Net are connected in the order: R(BN (Conv(16, 3, 2))), R(BN (Conv(32, 3, 2))), R(BN (Conv(32, 3, 2))), and R(BN (Conv(32, 3, 2))). The last convolutional layer is reshaped and fully connected by the 64-dimensional hidden layer and the 2dimensional output layer successively. The CNNs in Inertia Net has the same architecture and hyperparameters as that in Dynamics Net. The hyperparameters and parameters for training MAOP in Monster Kong and Flappy Bird are listed as follows:
· The images in Monster Kong and Flappy Bird are down-sampled to size 160 × 160 × 3 and 160 × 80 × 3 , respectively.
· In MAOP, the weights for Lhighway, Lprediction, Lproposal, Linstance, and Lforeground are 1, 100, 1, 1, and 10, respectively. In addition, all the l2 losses are divided by HW to keep invariance to the image size.
· Batch size is 16 and the maximum number of training steps is set to be 1 × 105. · The size of the horizon window w is 33 in Monster Kong. · The size of the horizon window w is 41 in Flappy Bird. · The optimizer is Adam (Kingma & Ba, 2014) with learning rate 1 × 10-3. · The maximum number of static and dynamic masks is 4 and 12, respectively in Monster
Kong. · The maximum number of static and dynamic masks is 4 and 12, respectively in Flappy
Bird. · To augment the number of interactions of instances, we random sample two region propos-
als and merge them into a new region proposal which has double size.
17


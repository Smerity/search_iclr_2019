Under review as a conference paper at ICLR 2019
GENERATIVE PREDECESSOR MODELS FOR SAMPLE-
EFFICIENT IMITATION LEARNING
Anonymous authors Paper under double-blind review
ABSTRACT
We propose Generative Predecessor Models for Imitation Learning (GPRIL), a novel imitation learning algorithm that matches the state-action distribution to the distribution observed in expert demonstrations, using generative models to reason probabilistically about alternative histories of demonstrated states. We show that this approach allows an agent to learn robust policies using only a small number of expert demonstrations and self-supervised interactions with the environment. We derive this approach from first principles and compare it empirically to a stateof-the-art imitation learning method, showing that it outperforms or matches its performance on two simulated robot manipulation tasks and demonstrate significantly higher sample efficiency by applying the algorithm on a real robot.
1 INTRODUCTION
Training or programming agents to act intelligently in unstructured and sequential environments is a difficult and central challenge in the field of artificial intelligence. Imitation learning provides an avenue to tackle this challenge by allowing agents to learn from human teachers, which constitutes a natural way for experts to describe the desired behavior and provides an efficient learning signal for the agent. It is thus no surprise that imitation learning has enabled great successes on robotic (Chernova and Thomaz, 2014) as well as software domains (e.g. Aytar et al. (2018)). Yet, key challenges in the field are diverse and include questions such as how to learn from observations alone (e.g. Aytar et al. (2018)), learning the correspondence between the expert's demonstrations and the agent's observations (e.g. Sermanet et al. (2017)) as well as the question of how to integrate imitation learning with other approaches such as reinforcement learning (e.g. Vecerik et al. (2017)). However, at the core of the imitation learning problems lies the challenge of utilizing a given set of demonstrations to match the expert's behavior as closely as possible. In this paper, we approach this problem considering the setting where the set of expert demonstrations is given up-front and the dynamics of the environment can only be observed through interaction.
In principle, the imitation learning problem could be seen as a supervised learning problem, where the demonstrations are used to learn a mapping from observed states to actions. This solution approach is known as behavioral cloning. However, it has long been known that the sequential structure of the task admits more effective solutions. In particular, the assumptions made in supervised learning are restrictive and don't allow the agent to reason about the effect of its actions on it's future inputs. As a result, errors and deviations from demonstrated behavior tend to accumulate over time as small mistakes lead the agent to parts of the observation space that the expert has not explored (Ross and Bagnell, 2010). In this work, we propose a novel imitation learning algorithm, Generative Predecessor Models for Imitation Learning (GPRIL), based on a simple core insight: Augmenting the training set with state-action pairs that are likely to eventually lead the agent to states demonstrated by the expert is an effective way to train corrective behavior and to prevent accumulating errors.
Recent advances in generative modeling, such as Goodfellow et al. (2014); Kingma and Welling (2013); Van Den Oord et al. (2016b;a); Papamakarios et al. (2017); Dinh et al. (2016), have shown great promise at modeling complex distributions and can be used to reason probabilistically about such state-action pairs. Specifically, we propose to utilize Masked Autoregressive Flows (Papamakarios et al., 2017) to model long-term predecessor distributions, i.e. distributions over stateaction pairs which are conditioned on a state that the agent will see at some point in the future.
1

Under review as a conference paper at ICLR 2019
In this work, we train such a predecessor model by utilizing self-supervised interactions with the environment and propose an algorithm that iterates over the following steps:
1. Interact with the environment and observe state, action as well as a target state. To encode long-term corrective behavior, these states should be multiple steps apart.
2. Train a conditional generative model to produce samples like the observed state-action pair when conditioned on the observed target state.
3. Train the agent in a supervised way, augmenting the training set using data drawn from the model conditioned on the demonstrations. The additional training data shows the agent how to reach demonstrated states, enabling it to recover after deviating from expert behavior.
In the above, we laid out the sketch of an algorithm that intuitively learns to reason about the states it will observe in the future. In section 3 we derive this algorithm from first principles as a maximum likelihood approach to matching the state-action distribution of the agent to the expert's distribution. In section 4, we compare our approach to a state-of-the-art imitation learning method (Ho and Ermon, 2016) and show that it matches or outperforms this baseline on our domains while being significantly more sample efficient. Furthermore, we show that our approach can be used to learn effectively using demonstrated states alone which allows for a wider variety of methods that the expert can use to record demonstrations. Together these properties are sufficient to allow GPRIL to be applied in real-world settings, which we demonstrate in section 4.3. To our knowledge this is the first instance of dynamic, contact-rich and adaptive behavior being taught solely using the kinesthetic-teaching interface of a collaborative robot, without resorting to tele-operation, auxilliary reward signals, or manual task-decomposition.
2 BACKGROUND
2.1 MARKOV DECISION PROCESSES WITHOUT REWARDS
As is usual, we model the problem as a Markov decision process without reward. That is, given state and action sets S, A, the agent is observing states s  S and taking actions a  A. In this work, we use s and a to refer to states and actions observed during self-supervision and s  S and a  A to refer to target and demonstration states and actions. We furthermore use superscripts s(i), a(i) to refer to specific instances, e.g. specific demonstrated state-action pairs, and subscripts, e.g. st, at, to indicate temporal sequences. The observed transitions are guided by the Markovian dynamics of the environment and the probability of transitioning from state s to state s by taking action a is denoted as p(st+1 = s |st = s, at = a). The agent's behavior is defined by a stationary parametric policy (a|s) while the expert's behavior is modeled by a stationary distribution (a|s). We denote as dt (s) the probability of observing state s at time-step t when following policy . Under the usual ergodicity assumptions, each such policy induces a unique stationary distribution of observed states d(s) = limt dt (s) as well as a stationary joint state-action distribution (s, a) := (a|s)d(s). Furthermore, we use qt to refer to the dynamics of the time reversed Markov chain induced by a particular policy  at time-step t
qt(st = s, at = a|st+1 = s ) = dt+1(s )-1dt(s)(at|st)p(st+1 = s |st = s, at = a) (1) and define q(st = s, at = a|st+1 = s ) := limt qt(st = s, at = a|st+1 = s ). For the purposes of this work, we handle the episodic case with clear termination conditions by adding artificial transitions from terminal states to initial states. This creates a modified, ergodic MDP with identical state-distribution and allows us to assume arbitrarily large t such that qt = q. Finally, we extend this notation to multi-step transitions by writing q(st = s, at = a|st+j = s ).
2.2 IMITATION LEARNING
In this work, we are considering two settings of imitation learning. In the first setting, the agent is given a set of observed states s(1), s(2), · · · , s(N) and observed corresponding actions a(1), a(2), · · · , a(N) as expert demonstrations. The goal in this setting is to learn a policy  that matches the expert's behavior as closely as possible. In the second setting, the agent is given the states observed by the expert but is not aware of the actions the expert has taken. Recent years have
2

Under review as a conference paper at ICLR 2019

seen heightened interest in a related setting where to goal is to track expert state trajectories (Zhu et al., 2018; Peng et al., 2018; Pathak et al., 2018). These approaches do not learn general policies that can adapt to unseen situations. A straightforward approach, usually referred to as behavioral cloning (BC), trains a policy in the first setting by treating the task as a supervised learning problem (e.g. Pomerleau (1989)). However, as outlined in section 1, predictions made by (a|s) influence future observations thus violating a key assumption of supervised learning which states that inputs are drawn from an i.i.d. distribution. This has formally been analyzed by Ross et al. who introduce a family of algorithms (e.g. Ross and Bagnell (2010); Ross et al. (2011)) that provably avoid this issue. However, these approaches require the expert to continuously provide demonstrations and thus are not applicable when the set of demonstrations is assumed to be fixed. A popular avenue of research that considers this setting is inverse reinforcement learning (IRL). Inverse reinforcement learning (Ng and Russell, 2000) aims to learn a reward function for which  is optimal and thus captures the intent of the expert. The arguably most successful approach to IRL aims to match the state-action distribution to that of the demonstrations (Ziebart et al., 2008) with recent approaches extending these ideas to the model free case (Boularias et al., 2011; Finn et al., 2016; Fu et al., 2018).
However, inverse reinforcement learning is indirect and ill-defined as many reward functions induce the same behavior. Recently, methods have been proposed that aim to match state-action distributions directly and achieve state-of-the-art result without learning a reward function first. Generative Adversarial Imitation Learning (GAIL) (Ho and Ermon, 2016) uses an adversarial objective, training a discriminator to identify demonstrations and using TRPO to train a policy that fools the discriminator. While GAIL is able to achieve impressive results, the adversarial objective can make the learning procedure unstable and unpredictable. This is especially true when parts of the state-space are not under the agent's control, yielding a setting resembling a conditional GAN (Mirza and Osindero, 2014) which are prone to issues such mode collapse (Odena et al., 2017). We compare our approach with GAIL in section 4. State Aware Imitation Learning (SAIL) (Schroecker and Isbell, 2017) is an alternative method that aims to learn the gradient of the state-action distribution using a temporal-difference update rule. This approach is able to avoid instabilities prompted by the adversarial learning rule but is only applicable to policies with a small number of parameters where learning a representation of the gradient is feasible. In this work, we follow a gradient descent approach similar to SAIL but estimate the gradient without representing it explicitly by a neural network. These methods can also be used to learn from states alone by matching state distributions. While this does not necessarily induce a unique policy, including transition information such as velocities can allow the agent to learn solely from expert state trajectories nonetheless.

2.3 GENERATIVE MODELS

Recent years have seen great advances in deep generative models. A variety of approaches such

as Generative Adversarial Networks (Goodfellow et al., 2014), Variational Auto Encoders (Kingma

and Welling, 2013), autoregressive networks (e.g. Germain et al. (2015); Van Den Oord et al.

(2016b;a) and normalizing flows (Dinh et al., 2016) have been proposed which enable us to

learn complex distributions and efficiently generate samples. In this work, we use generative

models to model the distribution of long-term predecessor state-action pairs. While the approach

we propose is model agnostic, we choose to model this distribution using masked autoregressive

flows (Papamakarios et al., 2017) (MAF). MAFs are trained using a maximum likelihood ob-

jective which allows for a stable and straight-forward training procedure. Autoregressive mod-

els are capable of representing complex distributions p(x); x  Rn by factoring the distribution

p(x) = p1(x1)

N -1 i=1

pi+1(xi+1|x1,

.

.

.

,

xi)

and

learning

a

model

for

each

pi.

In this paper,

we

model each xi to be distributed by xi  N (·|µi, i) where each µi and i is a function of x1:i-1.

Masked autoencoders (Germain et al., 2015) provide a straight-forward approach to parameter shar-

ing and allow representing these functions using a single network. MAFs stack multiple autoregres-

sive models with different orderings and thus avoid the strong inductive bias imposed by the order

of variables. Using the reparameterization trick, the autoregressive model can be seen as a determin-

istic and invertible transformation of a random variable: xi = fi(zi) := µi + izi; zi  N (·|0, 1).

The change of variable formula then allows us to calculate the density of xi:

log p(xi) = log pN (fi-1(xi)) + log det(|J (fi-1(xi))|)

(2)

Where the autoregressive nature of f allows for tractable computation of the second term. MAFs

chain multiple such transformations to derive highly expressive explicit density models able to model

complex dynamics between target states and long-term predecessor state-action pairs.

3

Under review as a conference paper at ICLR 2019

Algorithm 1 Generative Predecessor Models for Imitation Learning (GPRIL)

1: function GPRIL(NB, N) 2: for i  0..#Iterations do

3: for k  0..NB do 4: for n  0..#BatchSize do 5: Sample st(n), at(n) from replay buffer 6: Sample j  Geom(1 - )

7: Sample st(+n)j from replay buffer

8:

Update s using gradient

NB n=0

s

log

Bs s (st(n)|st(+n)j )

9:

Update a using gradient

NB n=0

a

log

Baa (a(tn)|s(tn),

st(+n)j )

10: for k  0..N do
11: for n  0..#BatchSize do 12: Sample s(n), a(n) from expert demonstrations

13: Sample s(n)  Bs s (·|s(n)), a(n)  Baa (·|s(n), s(n))

14:

Update  using gradient

N n=0

 

log

 (a(n) |s(n) )

+

d

log

 (a(n) |s(n) )

3 GPRIL

In section 1, we provided an intuitive framework for using predecessor models to augment our training set and achieve robust imitation learning from few samples. In this section, we will derive this algorithm based on state-action distribution matching. To this end, we first derive the gradient of the logarithmic state distribution based on samples from a long-term predecessor distribution that we will define below. In section 3.2, we describe how to train a generative model of the predecessor distribution which will allow us to evaluate this gradient. Ascending on this gradient evaluated at demonstrated states leads the agent to stick to those states and thus provides a corrective measure (Schroecker and Isbell, 2017). Furthermore, reproducing the states of the expert can be sufficient to achieve the correct behavior if the state-space is chosen appropriately as we will show in section 4. We will show how to use this gradient to match state-action-distributions in section 3.3.

3.1 REASONING OVER CHANGES TO THE STATIONARY STATE DISTRIBUTION

Here, we will show that the samples drawn from a long-term predecessor distribution conditioned on s enable us to estimate the gradient of the logarithmic state distribution  log d (s) and, later, to match the agent's state-action-distribution to that of the expert. To achieve this goal, we can utilize
the fact that the stationary state distribution of a policy can be defined recursively in terms of the
state distribution at the previous time step, similar to Morimura et al. (2010):

d (s) = d (s)(a|s)p(st+1 = s|st = s, at = a)ds, a.

(3)

Taking the derivative shows that this notion extends to the gradient as well as its logarithm:

d (s) =  (s, a)p(st+1 = s|st = s, at = a) ( log d (s) +  log (a|s)) ds, a (4)

 log d (s) = q (st = s, at = a|st+1 = s) ( log d (s) +  log (a|s)) ds, a (5)

The recursive nature of this gradient then allows us to unroll the gradient indefinitely. However, this process is cumbersome and will be left for appendix A. We arrive at the following equality:

 log d (s) = lim
T 

T
q (st = s, at = a|st+j+1 = s) log (a|s)ds, a
j=0

(6)

The derivation of our approach now rests on two key insights: First, in ergodic Markov chains,
such as the ones considered in our setting, decisions that are made at time t affect the probability of seeing state s at time t + j more strongly if j is small. In the limit, as j  , the expectation of the gradient  log (at|st) vanishes and the decision at time t only adds variance to the gradient estimate. Introducing a discount factor  similar to common practice in reinforcement learning

4

Under review as a conference paper at ICLR 2019

(Sutton and Barto, 1998) places more emphasis on decisions that are closer in time and can thus greatly reduce variance. Second, by introducing a discount factor, the effective time-horizon is now finite. This allows us to replace the sum over all states and actions in each trajectory with a scaled expectation over state-action pairs. Formally, we can write this as follows and arrive at our main result:

 log d (s) 


jq (st = s, at = a|st+j+1 = s) log (a|s)ds, a
j=0

(7)

 Es,aB (·,·|s) [ log (a|s)]

where B corresponds to the long-term predecessor distribution modeling the distribution of states and actions that, under the current policy , will eventually lead to the given target state s:


B (s, a|s) := (1 - ) jq (st = s, at = a|st+j+1 = s)
j=0

(8)

3.2 LONG-TERM GENERATIVE PREDECESSOR MODELS

In the previous section, we derived the gradient of the logarithm of the stationary state distribution

as approximately proportional to the expected gradient of the log policy, evaluated at samples ob-
tained from the long-term predecessor distribution B . In this work, we propose to train a model B to represent B and use its samples to estimate  log d (s). However, rather than unrolling
a time-reversed Markov model in time, which is prone to accumulated errors, we propose to use a

generative model to directly generate jumpy predictions. We have furthermore found that impos-

ing a sensible order on autoregressive models achieves good results and thus propose to use two conditional MAFs (Papamakarios et al., 2017) Bss , Baa in a factored representation:

B (s, a|s) := Bs s (s|s)Baa (a|s, s).

(9)

To train this model, we collect training data using self-supervised roll-outs: We sample states, ac-
tions and target-states where the separation in time between the state and target-state is selected randomly based on the geometric distribution parameterized by  as a training set for B .

Training data for s0, a0, s1, a1, · · · ,

Bs s , Baa which we

are obtained by store in a replay

executing the current policy buffer. In practice, we store

to obtain data from

a sequence multiple it-

erations in this buffer in order to decrease the variance of the gradient. While our algorithm does

not explicitly account for off-policy samples, we found empirically that a short replay buffer does

not degrade final performance while significantly improving sample efficiency. To obtain a training

sample, we first pick s = st and a = at for a random t. We now select a future state s = st+j+1 from that sequence. For any particular st+j+1 we now have s, a  qt (st = ·, at = ·|st+j+1 = s)  q (st = ·, at = ·|st+j+1 = s). Note that in the episodic case, we can add transitions from terminal to initial states and pick t to be arbitrarily large such that the approximate equality

becomes exact (as outlined in section 2.1). In non-episodic domains, we find the approximation

error to be small for most t. Finally, we choose j at random according to a geometric distribution

j  Geom(1 - ) and have a training triple s, a, s that can be used to train Baa and Bss as it obeys


s, a  (1 - ) jqt (st = ·, at = ·|st+j+1 = s) = B (·, ·|s).
j=0

(10)

3.3 MATCHING STATE-ACTION DISTRIBUTIONS WITH GPRIL

State-action distribution matching has been a promising approach to sample-efficient and robust imitation learning (see section 2.2). While each policy induces a unique distribution of states and behavioral cloning would therefore be sufficient in the limit, it is sub-optimal the case of limited data. Matching the joint-distribution directly ensures that we minimize discrepancies between everything we observed from the expert and the behavior the agent exhibits. In this work, we propose a maximum likelihood based approach, ascending on the estimated gradient of the joint distribution:

 log  (s, a) =  log (a|s) +  log d (s).

(11)

5

Under review as a conference paper at ICLR 2019

1.0 Behavioral cloning
0.8 GAIL GPRIL
0.6

Success rate Success rate

0.4

0.2

(a)
1.0 GPRIL
0.8 GAIL

0.0 0
1.0 0.8

5 10 15 20 Number of demonstrations
(b)
GAIL GPRIL

25

0.6 0.6

0.4 0.4

0.2 0.2

0.0 101

102 103 104 105 Number of self-supervised episodes
(c)

0.0 0

5 10 15 20 Number of demonstrations
(d)

25

Figure 1: a) Depiction of the clip-insertion task. b) Median final success rate and interquartile range out of 100 roll-outs over 8 seeds. GPRIL achieves the highest success rate followed by GAIL. c) Median final success rate and IQR on clip insertion comparing sample efficiency. GPRIL is able to solve the task using several orders of magnitude fewer environment interactions. d) Comparison on clip insertion trained on states alone. Learning from states alone only slightly affects performance.

where  log (a|s) can be computed directly by taking the gradient of the policy using the demonstrated state-action pairs and  log d (s) can be evaluated up to a constant factor using samples drawn from B (·, ·|s) according to equation 7. We introduce scaling factors  and d to account for this constant factor as well as to allow for finer control, interpolating between matching states
only ( = 0) and behavioral cloning (d = 0). We arrive at the following estimate of the gradient:

 log  (s, a)   log (a|s) + dEs,aB (·,·|s) [ log (a|s)] .

(12)

This gives rise to the full-algorithm: We fill the replay buffer by asynchronously collecting experience using the current policy. Simultaneously, we repeatedly draw samples from the replay buffer to update the predecessor models and use expert samples in combination with an equal number of artificial samples to update the policy. This procedure is described fully in algorithm 1.

4 EXPERIMENTS
To evaluate our approach, we use a range of robotic insertion tasks similar to the domains introduced by Vecerik et al. (2017) but without access to a reward signal or, in some cases, expert actions. We choose these domains both for their practical use, and because they highlight challenges faced when applying imitation learning to the real world. Specifically, collecting experience using a robot arm is costly and demands efficient use of both demonstrations and autonomously gathered data. Furthermore, insertion tasks typically require complex searching behavior, particularly when the socket position is uncertain, and cannot be solved by open-loop tracking of a given demonstration trajectory. Insertion therefore poses a challenge for existing imitation learning and, especially, kinesthetic teaching approaches. We first compare against state-of-the-art imitation learning methods on a simulated clip insertion task, then explore the case of extremely sparse demonstrations on a simulated peg insertion task and finally, demonstrate real-world applicability on its physical counterpart.
4.1 CLIP INSERTION
In the first task, a simulated robot arm has to insert an elastic clip into a plug which requires the robot to first flex the clip in order to be able to insert it (see figure 1a). In real-world insertion tasks the

6

Under review as a conference paper at ICLR 2019

1.0 1.0

Success rate

0.8

Full Data

0.8

0.6

Sparse states Final state only

0.6

0.4 0.4 Full Data

0.2 0.2 Sparse states Final state only

0.0 0.0

0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8

Iterations (TRPO)

×104

Iterations (gradient descent)

×106

(a)
1.0
0.8
0.6
0.4 Full Data
0.2 Sparse states Final state only
0.0 0 5 10 15 20 25 Number of demonstrated trajectories
(d)

Timesteps until insertion

(b) 80 Full Data
Sparse states 60 Final state only
40
20 5 10 15 20 25 Number of demonstrated trajectories (e)

Best-seed success rate (smoothed)

1.00 0.75 0.50 0.25 0.00
0.0

(c)
Fixed pan-tilt Changing pan-tilt
0.5 1.0 1.5 2.0 Number of self-supervised episodes ×103
(f)

Success rate

Figure 2: a) Depiction of the peg insertion task b) Average success rate and 95% confidence interval of GAIL with 25 demonstrations across 10 runs (evaluated over 100 roll-outs). c) Average success rate of GPRIL across 5 seeds. Unlike GAIL, the performance of GPRIL doesn't drop off when provided with only final states. d) Average success rate and confidence interval of GPRIL. Final performance after 106 iterations increases steadily as the number of demonstrated trajectory increases but is unaffected by dropping steps from each demonstration. e) Median length and IRQ of trajectories that are successfully inserting the peg. Providing only final states is significantly faster. f) Best seed performance on both variations of peg insertion on the real robot.

pose of the robot, the socket, or the grasped object may be unknown. We capture this variability by mounting the socket on a pan-tilt unit which is randomized by ±0.8 and ±0.2 radians. To perform this behavior, the robot observes proprioceptive features, specifically joint position, velocity and torques as well as the position of the end-effector and the socket orientation as a unit quaternion. The task terminates when the robot leaves the work-space, reaches the goal, or after 50 seconds.
For comparative evaluation, we train a policy network to predict sufficient statistics of a multivariate normal distribution with independent dimensions over target velocities and train it using GPRIL, GAIL as well as behavioral cloning. We record expert demonstrations using tele-operation and normalize observations based on the recorded demonstrations. We then train GPRIL using a single asynchronous simulation and compare against the open source implementation of GAIL1 for which we use 16 parallel simulations. We select the best hyper parameters found on a grid around the hyperparameters used by Ho and Ermon (2016) but lower the batch size to 256 as it increases the learning speed and accounts for the significantly slower simulation of the task. We furthermore enable bootstrapping regardless of whether or not the episode terminated. As all discriminator rewards are positive, handling terminal transitions explicitly can induce a bias towards longer episodes. This is beneficial in the domains used by Ho and Ermon but harmful in domains such as ours where the task terminates on success. A detailed list of hyper-parameters can be found in appendix B.
We report final results after convergence and can see that both GAIL and GPRIL outperform behavioral cloning, indicating that generalizing over state-action trajectories requires fewer demonstrations than generalizing over actions alone. Furthermore, we observe a higher success rate using GPRIL and find that policies trained using GPRIL are more likely to retry insertion if the robot slides the clip past the insertion point. To compare sample efficiency of GPRIL to GAIL, we limit the rate at which the asynchronous actor is collecting data. While sample efficiency of GAIL could be increased by decreasing batch size or increasing various learning rates, we found that this can lead to unstable learning performance while reducing the amount of samples required by only a small amount. As can be seen in figure 1c, GPRIL requires several orders of magnitudes fewer environment interactions to learn this task. Finally, we evaluate the case where the expert's actions
1https://github.com/openai/baselines/tree/master/baselines/gail
7

Under review as a conference paper at ICLR 2019
are unknown. Since the state-space includes information about joint velocities as well as positions, we find that matching the state-distribution is sufficient to solve the task. GPRIL can achieve this by setting d = 1 and  = 0 . As can be seen in figure 1d, performance deteriorates only marginally with a similar difference in performance between both methods.
4.2 PEG INSERTION WITH PARTIAL DEMONSTRATIONS
The second task is a simulated version of the peg-insertion task depicted in figure 2a. In this task, the robot has to insert the peg into the hole which is again mounted on a pan-tilt that randomly assumes pan and tilt angles varying by 0.4 and 0.1 respectively. Hyperparameters are largely identical and we report minor differences in appendix B. Observation and action space are identical with the exception of the omission of torques from the observation space as they are not necessary to solve this task. We use this task to evaluate the performance of GAIL and GPRIL when learning from only a very limited set of demonstrated states. To this end, we compare three different scenarios in which the demonstrations are sparsified to varying degrees: In the first case, the agent has access to the full state-trajectories of the expert, in the second only every tenth state is available and in the third the agent sees only the final state of each of the 25 trajectories. Being able to learn from only partial demonstrations is a useful benchmark for the effectiveness of imitation learning methods but can also provide a convenient way of providing demonstrations and can free the agent to find more optimal trajectories between states (see for example Akgun et al. (2012); Schroecker et al. (2016)). As can be seen in figures 2d and 2e, GPRIL achieves similar final success rates in all three scenarios while being able to learn a significantly faster insertion policy when learning from final states alone. We find that in the first two scenarios, this holds for GAIL as well as can been in figure 2b while in the third case, GAIL becomes highly unstable and the resulting performance can vary wildly, leading to a low average success rate. We hypothesize that these instabilities are a result of the discriminator overfitting to the very small amount of negative samples in its training data.
4.3 PEG INSERTION ON A PHYSICAL SYSTEM
In previous sections we demonstrated sample-efficiency that indicates applicability of GPRIL to real-world physical systems. To test this, we evaluate our approach on two variations of the physical peg-insertion task depicted in figure 2a involving a physical pan-tilt unit which is fixed in one scenario and has pan and tilt angles varying by 0.1 and 0.02 radians in the second scenario. For each scenario we provide 20 demonstrations using kinesthetic teaching, which constitutes a natural way of recording demonstrations but provides state-trajectories only (Chernova and Thomaz, 2014). Hyper-parameters are altered from section 4.2 to trade off a small margin of accuracy for higher learning speeds and are reported in appendix B. Note, however, that tuning hyper-parameters precisely is very difficult on a physical system. As can be seen in figure 2f, GPRIL is able to learn a successful insertion policy that generalizes to unseen insertion angles using just a few hours of environment interactions2. We report best-seed performance as we observe a high amount of variability due to factors outside the agent's control, such as the pan-tilt unit not reporting accurate information after physical contact with the robot. However, we wish to point out that the increased difficulty due to less predictable control is also likely to introduce additional variance that could be reduced further with careful design of exploration noise and other hyper-parameters. We furthermore provide a video of the training procedure and final policy to highlight the efficiency of our method3.
5 CONCLUSION
We introduced GPRIL, a novel algorithm for imitation learning which uses generative models to model multi-step predecessor distributions and to perform state-action distribution matching. We show that the algorithm compares favorably with state-of-the-art imitation learning methods, achieving higher or equivalent performance while requiring several orders of magnitude fewer environment samples. Importantly, stability and sample-efficiency of GPRIL are sufficient to enable experiments on a real robot, which we demonstrated on a peg-insertion task with a variable-position socket.
2Total time to collect and train on 2000 roll-outs was 18.5 and 16.5 hours on the fixed and changing versions of the task respectively. However, GPRIL converged to good policies significantly sooner.
3https://youtu.be/Dm0OCNujEmE
8

Under review as a conference paper at ICLR 2019
REFERENCES
Akgun, B., M. Cakmak, J. W. Yoo, and A. L. Thomaz 2012. Trajectories and keyframes for kinesthetic teaching: a human-robot interaction perspective. In International Conference on Human-Robot Interaction, Pp. 391­398.
Aytar, Y., T. Pfaff, D. Budden, T. L. Paine, Z. Wang, and N. de Freitas 2018. Playing hard exploration games by watching YouTube. arXiv preprint arXiv:1805.11592.
Boularias, A., J. Kober, and J. Peters 2011. Relative entropy inverse reinforcement learning. In Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics, Pp. 182­189.
Chernova, S. and A. L. Thomaz 2014. Robot learning from human teachers. Synthesis Lectures on Artificial Intelligence and Machine Learning, 8(3):1­121.
Dinh, L., J. Sohl-Dickstein, and S. Bengio 2016. Density estimation using Real NVP. arXiv preprint arXiv:1605.08803.
Finn, C., S. Levine, and P. Abbeel 2016. Guided cost learning: Deep inverse optimal control via policy optimization. In International Conference on Machine Learning, Pp. 49­58.
Fu, J., K. Luo, and S. Levine 2018. Learning robust rewards with adverserial inverse reinforcement learning. In International Conference on Learning Representations.
Germain, M., K. Gregor, I. Murray, and H. Larochelle 2015. Made: Masked autoencoder for distribution estimation. In International Conference on Machine Learning, Pp. 881­889.
Goodfellow, I., J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio 2014. Generative adversarial nets. In Advances in Neural Information Processing Systems, Pp. 2672­2680.
Ho, J. and S. Ermon 2016. Generative adversarial imitation learning. In Advances in Neural Information Processing Systems, Pp. 4565­4573.
Kingma, D. P. and M. Welling 2013. Auto-encoding variational bayes. CoRR, abs/1312.6114.
Mirza, M. and S. Osindero 2014. Conditional generative adversarial nets. arXiv preprint arXiv:1411.1784.
Morimura, T., E. Uchibe, J. Yoshimoto, J. Peters, and K. Doya 2010. Derivatives of logarithmic stationary distributions for policy gradient reinforcement learning. Neural computation, 22(2):342­376.
Ng, A. and S. Russell 2000. Algorithms for inverse reinforcement learning. In International Conference on Machine Learning, Pp. 663­670.
Odena, A., C. Olah, and J. Shlens 2017. Conditional image synthesis with auxiliary classifier GANs. In International Conference on Machine Learning, Pp. 2642­2651.
Papamakarios, G., I. Murray, and T. Pavlakou 2017. Masked autoregressive flow for density estimation. In Advances in Neural Information Processing Systems, Pp. 2335­2344.
9

Under review as a conference paper at ICLR 2019
Pathak, D., P. Mahmoudieh, G. Luo, P. Agrawal, D. Chen, Y. Shentu, E. Shelhamer, J. Malik, A. A. Efros, and T. Darrell 2018. Zero-shot visual imitation. In International Conference on Learning Representations.
Peng, X. B., P. Abbeel, S. Levine, and M. van de Panne 2018. Deepmimic: Example-guided deep reinforcement learning of physics-based character skills. arXiv preprint arXiv:1804.02717.
Pomerleau, D. A. 1989. Alvinn: An autonomous land vehicle in a neural network. In Advances in Neural Information Processing Systems, Pp. 305­313.
Ross, S. and D. Bagnell 2010. Efficient reductions for imitation learning. In International Conference on Artificial Intelligence and Statistics, Pp. 661­668.
Ross, S., G. Gordon, and D. Bagnell 2011. A reduction of imitation learning and structured prediction to no-regret online learning. In International Conference on Artificial Intelligence and Statistics, Pp. 627­635.
Schroecker, Y., H. Ben Amor, and A. Thomaz 2016. Directing policy search with interactively taught via-points. In International Conference on Autonomous Agents & Multiagent Systems, Pp. 1052­1059.
Schroecker, Y. and C. L. Isbell 2017. State aware imitation learning. In Advances in Neural Information Processing Systems 30, Pp. 2915­2924.
Sermanet, P., C. Lynch, Y. Chebotar, J. Hsu, E. Jang, S. Schaal, and S. Levine 2017. Time-contrastive networks: self-supervised learning from video. arXiv preprint arXiv:1704.06888.
Sutton, R. S. and A. G. Barto 1998. Reinforcement learning: An introduction. MIT press.
Van Den Oord, A., S. Dieleman, H. Zen, K. Simonyan, O. Vinyals, A. Graves, N. Kalchbrenner, A. Senior, and K. Kavukcuoglu 2016a. Wavenet: A generative model for raw audio. arXiv preprint arXiv:1609.03499.
Van Den Oord, A., N. Kalchbrenner, and K. Kavukcuoglu 2016b. Pixel recurrent neural networks. arXiv preprint arXiv:1601.06759.
Vecerik, M., T. Hester, J. Scholz, F. Wang, O. Pietquin, B. Piot, N. Heess, T. Rotho¨rl, T. Lampe, and M. A. Riedmiller 2017. Leveraging demonstrations for deep reinforcement learning on robotics problems with sparse rewards. CoRR, abs/1707.08817.
Zhu, Y., Z. Wang, J. Merel, A. Rusu, T. Erez, S. Cabi, S. Tunyasuvunakool, J. Krama´r, R. Hadsell, N. de Freitas, et al. 2018. Reinforcement and imitation learning for diverse visuomotor skills. arXiv preprint arXiv:1802.09564.
Ziebart, B. D., A. L. Maas, J. A. Bagnell, and A. K. Dey 2008. Maximum entropy inverse reinforcement learning. In AAAI Conference on Artificial Intelligence, Pp. 1433­1438.
10

Under review as a conference paper at ICLR 2019

A DERIVATION OF EQUATION 6

Here, we derive equation 6 which unrolls the recursive definition of  log d (s) and rewrites it such that it can be replaced by an expectation over states and actions along trajectories leading to the state s. To do so, we first unroll said definition:

 log d (s) = q (st = s, at = a|st+1 = s) ( log d (s) +  log (a|s)) ds, a

= lim
T 


T -2

q (st+T -1, at+T -1|st+T = s)

q (st+j , at+j |st+j+1)

j=0


T
 log (at+j |st+j ) dst:t+T -1, at:t+T -1 +
j=0

q (st = s, at = a|st+T = s) log d (s)ds, a

(13)

Note that limT  q (st = s, at = a|st+T = s) =  (s, a) due to Markov chain mixing and, therefore, the second term of the above sum reduces to 0 as

d (s)(a|s) log d (s)ds, a = 0.

(14)

By pulling out the sum, we can now marginalize out most variables and shift indices to arrive at the desired conclusion:

T
 log d (s) = lim
T  j=0

q (st+j = s, at+j = a|st+T +1 = s) log (a|s)ds, a

T
= lim
T  j=0

q (st = s, at = a|st+T +1-j = s) log (a|s)ds, a

= lim
T 

T
q (st = s, at = a|st+j+1 = s) log (a|s)ds, a
j=0

(15)

11

Under review as a conference paper at ICLR 2019

B HYPERPARAMETERS

General parameters Total iterations Batch size  Replay memory size NB N Bs and Ba Stacked autoencoders Hidden layers Optimizer
Learning rate Burnin
L2-regularization min(i) Gradient clip, L2 norm Policy  Hidden layers Optimizer
Learning rate  bounds

2e6 256 0.9 50000 15000 5000
2 500, 500 Adam 2 · 105 50000 iterations 10-2 0.1 100
300, 200 Adam 104 (0.01, 0.1)

(a) GPRIL parameters for clip insertion

General parameters Total iterations #Processes Batch size Actor steps per iteration Discriminator steps per iteration  Actor Hidden layers KL step size  bounds Discriminator Hidden layers Optimizer
Learning rate Entropy regularization Optimizer Critic Hidden layers Optimizer
Learning rate

1e4 16 16 · 256 3 1 0.995
300, 200 0.01 (0.01, 0.1)
150, 100 Adam 104 1 Adam
300, 200 Adam 5 · 103

(b) GAIL parameters for clip insertion

General parameters

Total iterations 5e6

Batch size

256

Hidden layers

300, 200

 bounds

(0.01, 0.1)

Optimizer

Adam

Learning rate

104

L2-regularization 10-4

(c) BC parameters for clip insertion

General parameters Batch size  Replay memory size NB N Bs and Ba Stacked autoencoders Hidden layers Optimizer
Learning rate Burnin
L2-regularization min(i) Gradient clip (L2) Policy  Hidden layers Optimizer
Learning rate  bounds

Simulation
256 0.9 10000 10000 1000
2 500, 500 Adam 10-5 0 10-2 0.1 100
300, 200 Adam 10-4 (0.01, 0.1)

Real robot
0.7 50000 5000
3 · 10-5 10-3 0.01

(d) GPRIL parameters for peg insertion

General parameters #Processes Batch size Actor steps/iteration Discriminator steps/iteration  Actor Hidden layers KL step size  bounds Discriminator Hidden layers Optimizer
Learning rate Entropy regularization Optimizer Critic Hidden layers Optimizer
Learning rate

16 16 · 256 3 1 0.995
300, 200 0.01 (0.01, 0.1)
150, 100 Adam 10-4 1 Adam
300, 200 Adam 5 · 10-3

(e) GAIL parameters for simulated peg insertion

12


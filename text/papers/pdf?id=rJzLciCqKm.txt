Under review as a conference paper at ICLR 2019
LEARNING FROM POSITIVE AND UNLABELED DATA WITH A SELECTION BIAS
Anonymous authors Paper under double-blind review
ABSTRACT
We consider the problem of learning a binary classifier only from positive data and unlabeled data (PU learning). Recent methods of PU learning commonly assume that the labeled positive data are identically distributed as the unlabeled positive data. However, this assumption is unrealistic in many instances of PU learning because it fails to capture the existence of a selection bias in the labeling process. When the data has a selection bias, it is difficult to learn the Bayes optimal classifier by conventional methods of PU learning. In this paper, we propose a method to partially identify the classifier. The proposed algorithm learns a scoring function that preserves the order induced by the class posterior under mild assumptions, which can be used as a classifier by setting an appropriate threshold. Through experiments, we show that the method outperforms previous methods for PU learning on various real-world datasets.
1 INTRODUCTION
We consider a situation that there are only positive and unlabeled data, and train a binary classifier only from them (PU learning). This problem arises in various practical situations, such as information retrieval and outlier detection (Elkan & Noto, 2008; Ward et al., 2009; Scott & Blanchard, 2009; Blanchard et al., 2010; Li et al., 2009; Nguyen et al., 2011). One of the milestones of PU learning is Elkan & Noto (2008), who proposed a practically useful algorithm with theoretical analysis, and there is subsequent research called unbiased PU learning (du Plessis & Sugiyama, 2014; du Plessis et al., 2015) where an unbiased estimator of the classification risk is minimized.
We focus on the case-control scenario (a.k.a. the problem setting based on two samples of data; Ward et al., 2009; Elkan & Noto, 2008; Niu et al., 2016). In this scenario, positive data are obtained separately from unlabeled data, and unlabeled data are sampled from the whole population. As Elkan & Noto (2008) explained, we cannot identify a classifier without an assumption on how positive data are labeled. Therefore "selected completely at random" (SCR) is traditionally assumed, i.e., the positive labeled data are identically distributed as the positive unlabeled data (Elkan & Noto, 2008; du Plessis et al., 2015). The assumption of SCR, however, is unrealistic in many instances of PU learning, e.g., a patient's electronic health record (Bekker & Davis, 2018a) and a recommendation system (Marlin & Zemel, 2009; Schnabel et al., 2016). In these cases, there is a selection bias (Heckman, 1979; Manski, 2008; Angrist & Pischke, 2008); the distribution of the positive data may differ between the labeled data and the unlabeled data.
Several recent researches have also proposed alternative assumptions (He et al., 2018; Bekker & Davis, 2018a;b). However, in order to weaken SCR, they impose other additional assumptions. In this work, we consider a more natural assumption such that SCR becomes its special case. We assume that the order over {x} induced by the conditional probability of observation p(o = +1|x) is the same as the one induced by the conditional class probability p(y = +1|x). We call this property Invariance of Order (IO). In the real-world application, there are many situations with a selection bias which follows IO. Among them, we list the following two examples.
Example 1: (Anomaly Detection) The goal of anomaly detection is to find anomaly data in an unlabeled dataset based on another dataset that consists only of anomaly data. When the anomaly data is obtained, the more likely a
1

Under review as a conference paper at ICLR 2019

datum is an anomaly, the more likely it is noticed and gets labeled. Here, we consider transductive learning. It means, only from this two datasets, we classify the unlabeled dataset.
Example 2: (Face Recognition) The goal of this task is to identify a user from a set of face images based on some pictures of the user. Users may tend to provide pictures which are more easily identifiable as themselves. In this case, the positive data is the user's face and the unlabeled data consists of all users including the user. The user may provide pictures in which the face can be seen clearly, while in the unlabeled data there may be a lot of pictures in which the user's face is covered by the hair.
We name our problem setting PU learning with a Selection Bias (PUSB). In this paper, we propose a novel framework to deal with this problem setting. The experimental results show that our proposed method is appropriate for real-world applications compared to existing approaches for PU learning.

2 PROBLEM SETTING OF PU LEARNING WITH A SELECTION BIAS
We consider a binary classification problem to classify x  X  Rd into one of the two classes {-1, +1}. We assume that there exists a joint distribution p(x, y, o), where y  {-1, +1} is the class label of x, and o  {0, 1} is the observation status of y (observed if o = +1 and unobserved if o = 0). In other words, x is labeled if o = +1, and it is unlabeled otherwise.
In PU learning, there are two distinguished sampling schemes called one sample (OS) and two samples (TS) of data. In OS, a set of unlabeled data is sampled from the marginal density p(x). Then, if a data point x is positive, this positive data gets labeled with probability p(o = +1|x); if x is negative, it is never labeled. In TS, a set of positive data is drawn from the positive conditional density p(x|y = +1) and a set of unlabeled data is drawn from p(x). As Niu et al. (2016) stated, TS is slightly more general than OS setting, therefore we focus on TS.
Suppose that we have a positive dataset {xi}ni=1 and an unlabeled dataset {xi}ni=1
{xi}in=1  p(x|y = +1, o = +1), {xi}in=1  p(x).
We assume that negative data are never labeled.
In addition, we do not assume SCR. Therefore, p(x|y = +1) may differ from p(x|y = +1, o = +1). In the case they differ, we say that there is a selection bias.
The quantity  = p(y = +1) is called the class-prior. In our problem setting, we consider the case where the class-prior is known. For example, in anomaly detection, the percentage of anomaly in the whole batch of products can be reported based on past experiences. Although there are various methods for estimating the class-prior in the traditional framework of TS (du Plessis et al., 2016; Ramaswamy et al., 2016; Jain et al., 2016; Kato et al., 2018), we cannot estimate the class-prior in our problem setting. As we explain later, even if we do not know the class-prior, we only have to change the last step of our algorithm.
Our goal is to obtain a classifier h : X  {-1, 1} only from {xi}ni=1, {xi}ni=1, and , which is justifiable under some kind of evaluation measure.

2.1 IDENTIFICATION STRATEGY

As stated by Elkan & Noto (2008), in PU learning, we cannot estimate p(y = +1|x) only from positive data and unlabeled data without any assumption. In TS, a standard assumption is SCR, i.e. p(x|y = +1, o = +1) = p(x|y = +1, o = 0), so that p(y = +1|x) can be estimated from the data in principle. However, in many instances of PU learning, the SCR assumption is unreasonable. Therefore, we relax SCR and accommodate a selection bias. We can see how SCR makes the class posterior identifiable in the following equation:

p(y = +1|x) = p(x, y = +1) = p(x|y = +1)

=

p(x|y = +1, o = +1) .

p(x) p(x)

p(x)

SCR

2

Under review as a conference paper at ICLR 2019

Under the assumption of SCR, we can estimate each of p(x|y = +1) and p(x) from samples, in principle. However, without assuming SCR, p(x|y = +1) may differ from p(x|y = +1, o = +1), and p(y = +1|x) is not identifiable.
Therefore, instead of estimating p(y = +1|x), we consider extracting some useful information of p(y = +1|x) to learn a classifier. This kind of approach is known as "partial identification" (Manski, 2008) in statistics and economics. The idea is to extract some useful information of a function instead of attempting to identify the whole function. Firstly, we introduce an assumption that is weaker than SCR. Assumption 1 (Invariance of Order). For any xi, xj  X , if p(y = +1|xi) < p(y = +1|xj) holds, then
p(o = +1|xi) < p(o = +1|xj)
holds. p(o = +1|xi) < p(o = +1|xj) holds if and only if p(y = +1|xi) = p(y = +1|xj).
We try to partially identify p(y = +1|x) under this assumption. Our problem setting can be regarded as a generalization of the traditional TS because SCR is a special case of IO.

3 THE STRATEGY FOR PARTIAL IDENTIFICATION AND CLASSIFICATION

As we discussed in Section 2.1, we cannot estimate p(y = +1|x) when there is a selection bias. Our idea of partial identification is based on the following theorem. Let r(x) be

p(x|y = +1, o = +1)

r(x) =

.

p(x)

(1)

Theorem 1 (Order preserving property of the score function). Suppose that Assumption 1 holds. Then, for any xi, xj  X , if p(y = +1|xi) < p(y = +1|xj), then

r(xi) < r(xj)

holds. r(xi) < r(xj) holds if and only if p(y = +1|xi) < p(y = +1|xj).

A proof is provided in Appendix A. Even though we cannot estimate p(y = +1|·), Theorem 1 implies that we can still extract the total order in X induced by p(y = +1|·) if we can estimate r(·). Therefore, we propose to estimate r and use it as a score function that captures the total order induced by p(y = +1|·). After obtaining an estimator of r (denoted by r^), we set a threshold   R and use h(x) = sign(r(x) - ) as a classifier. There are various ways of determining a threshold. For
instance, we put labels from data with the highest density ratio under a constraint of the number
of data to which we can put labels (Hido et al., 2011). Here, we introduce one useful principle for choosing . We consider using a threshold  defined by the following equation,

 = 1[r(x)  ]p(x)dx.

(2)

The intuition behind the definition of  is that the proportion of the positive data in the test data points should not deviate so much from the true class-prior. This intuition becomes clearer in Section 4.3.

In Section 4, we discuss detailed methods for estimating r and setting  based on data. Our approach is summarized in the form of pseudo code in Algorithm 1. In the rest of this section, we theoretically justify  defined in (2).

Property of : Let us define four population quantities that depend on r and : true positives (TP), true negatives (TN), false positives (FP), and false negatives (FN) (Lipton et al., 2014).
Definition 1. TP, FP, TN and FN are values determined by the following equations.

TP =

p(y = +1|x)p(x)dx, F P =

p(y = -1|x)p(x)dx,

{x:r(x)}

{x:r(x)}

TN =

p(y = -1|x)p(x)dx, F N =

p(y = +1|x)p(x)dx.

{x:r(x)<}

{x:r(x)<}

Then, the precision and the recall of a classifier are expressed as precision =

TP T P +F P

recall =

TP T P +F N

, respectively.

and

3

Under review as a conference paper at ICLR 2019

For , we have the following result.
Theorem 2. If we use  = , then precision = recall holds.
A proof is shown in Appendix B. A threshold which makes the precision and the recall the same is known as precision­recall breakeven point (BEP) (Sammut & Webb, 2010). BEP is originally used to evaluate a generic classification model with a score function and a threshold. In addition, we can interpret BEP as a point which balances a prediction result. As explained by Powers (2015), a classifier using BEP as a threshold put the same weight to the false positives and false negatives. Knowing BEP is also useful for deciding on a threshold which put unblanced weight on the precision and the recall because we can tell if we are weighting precision more or recall more. If the threshold is higher than BEP, we can tell that we are using a classifier that puts more weight on the precision, and vice versa. By setting the threshold relative to BEP, we can choose which of precision and recall is priortized.

4 ALGORITHM

Here,

we

propose

two

directions

for

estimating

r(x)

=

p(x|y=+1,o=+1) p(x)

under the

IO assumption,

namely minimizing a pseudo classification risk and direct density ratio estimation. We also discuss

how to set  based on the data. A pseudo-code of our algorithm is shown in Algorithm 2.

4.1 ESTIMATION OF r BY MINIMIZING THE PSEUDO CLASSIFICATION RISK

Firstly, we introduce minimization of the pseudo classification risk. The idea is to minimize the classification risk used in traditional PU learning (du Plessis et al., 2014; 2015) as if there is no selection bias. Under a selection bias, we cannot construct unbiased risk function, but the minimizer can be substituted for the density ratio in (1).

Conventional PU risk formulation: Let : R × {±1}  R+ be a loss function, where R+ is the set of non-negative real values, and F be a set of measurable functions from X to [ , 1 - ], where is
a small positive value. This is introduced to make the following optimization problem well-defined.

du Plessis et al. (2015) showed that the classification risk of f  F in the traditional PU problem setting with SCR can be expressed as

RPU(f ) = Ep[ (f (X), +1)] - Ep[ (f (X), -1)] + Eu[ (f (X), -1)],

(3)

where Ep and Eu are the expectations over p(x|y = +1) and p(x), respectively. When there is no selection bias, we can simply replace the expectations with the corresponding sample averages to
obtain an unbiased estimator of the classification risk.

The pseudo classification risk: In our problem setting, we only have samples from p(x|y = +1, o = +1) and not from p(x|y = +1). Therefore, we cannot use our sample to obtain an empirical version of RPU. However, we still consider the pseudo classification risk of f  F:
RPbiUas(f, ) = Ebpias[ (f (X), +1)] - Ebpias[ (f (X), -1)] + Eu[ (f (X), -1)],
where Ebpias is the expectation over p(x|y = +1, o = +1). We call this functional the pseudo classification risk because it is not the true classification risk. An unbiased estimator for the pseudo classification risk can be obtained by replacing the expectations with the corresponding sample averages even if there is a selection bias. For the loss function, we use the logarithmic loss: (f (x), +1)) = - log(f (x)) and (f (x), -1) = - log(1 - f (x)). In this case, the pseudo classification risk of f  F becomes
RPbiUas(f ) = -Ebpias[log(f (X))] + Ebpias[log(1 - f (X))] - Eu[log(1 - f (X))]. (4)
Justification for minimizing the pseudo classification risk: For the pseudo classification risk with the logarithmic loss function, the following theorem justifies its use. Let us denote a minimizer of (4) by f , that is,
f   arg min RPbiUas(f ).
f F
For the minimizer of (4), we derive the following theorem.

4

Under review as a conference paper at ICLR 2019

Theorem 3. f  is given by





f (x)

=

 p(x|y=+1,o=+1)
p(x)

1 -

(x / D1), (x  D1  D2), (x / D2),

where D1 = {x|p(x|y = 1, o = +1)  p(x)} and D2 = {x|p(x|y = 1, o = +1)  (1 - )p(x)}.

A proof is provided in Appendix C. Theorem 3 implies that the minimization of the empirical version of the pseudo classification risk allows us to estimate r.

Empirical Estimation: When we have training samples, we can naively replace the expectations by the corresponding sample averages. For a hypotheses set F~, let us define the following risk
minimization problem,

f^ = arg min -E^pbias[log(f (X))] + E^pbias[log(1 - f (X))] - E^u[log(1 - f (X))] , (5)
f F~
where E^bpias denotes the empirical mean using the positive dataset with a selection bias and E^u denotes the empirical mean using the unlabeled dataset. du Plessis et al. (2015) showed that, under SCR, the empirical version of the risk becomes unbiased toward the classification risk.

However, Kiryo et al. (2017) pointed out that unbiased PU learning does not work with deep learning. Minimizing an empirical risk of (3) with deep learning easily causes over-fitting because the risk is not bounded from below by 0. In order to implement PU learning with deep learning, Kiryo et al. (2017) proposed the following non-negative risk,

f^ = arg min -E^bpias[log(f (X))] + max 0, E^bpias[log(1 - f (X))] - E^u[log(1 - f (X))] .
f F~
(6)

After

obtaining

f^,

we

set

r^ =

1 

f^.

4.2 ESTIMATION OF r BY DIRECT DENSITY RATIO ESTIMATION

For

another

approach,

we

consider

estimating

the

density

ratio

r(x)

=

p(x|y=+1,o=+1) p(x)

directly.

In

order to estimate the ratio, there are various methods. For example, we can estimate the probability

density functions of the numerator and the denominator. However, as Vapnik's principle, we should

avoid solving more difficult intermediate problems. Sugiyama et al. (2012) summarized various

methods estimating the density ratio directly. Among existing methods of density ratio estimation, we

employ Least-squares importance fitting (LSIF), which uses the squared loss for density-ratio function

fitting. The reason for this choice is that there is an algorithm called unconstrained Least-Squares

Importance Fitting (uLSIF), which is based on LSIF and allows us to obtain the closed-form solution

that can be computed just by solving a system of linear equations. Thus, uLSIF is numerically stable

when it is regularized properly. Moreover, the leave-one-out cross-validation score for uLSIF can

also be computed analytically, which significantly improves the computational efficiency in model

selection.

Here, we introduce the formulation of LSIF. Let S be a class of non-negative measurable functions s : X  R+. We consider minimizing the following squared error between s and r:

RDR(s) = Eu[(s(X) - r(X))2] = Eu[(r(X))2] - 2Ebpias[s(X)] + Eu[(s(X))2].

(7)

The first term of the last equation does not affect the result of minimization and we can ignore the

term, i.e., the density ratio is estimated through the following minimization problem:

s = arg min RDR(s) = arg min

sS

sS

1 2

Eu

[(s(X

))2]

-

Epbias[s(X

)]

.

Empirical Estimation: As mentioned above, to minimize the empirical version of (7), we introduce uLSIF (See Kanamori et al. (2009)). We obtain r^ by

r^ = arg min
sS

1 2

E^ u

[(s(X

))2]

-

E^ bpias

[s(X

)]

.

(8)

5

Under review as a conference paper at ICLR 2019

Algorithm 1 Conceptual Algorithm in Population
Input: p(x|y = +1), p(x) and the class-pror . Using p(x|y = +1) and p(x), calculate r(x) by minimization of either (4) or (7). Using r(x), calculate  in (2). Using the density ratio r(x) and the threshold , obtain a classifier h(x) = sign(r(x) - ).
Algorithm 2 PUSB
Input: A positive dataset {xi}ni=1, an unlabeled dataset {xi}ni=1, a test dataset {xite}ni=te1 and the class-pror . Using the positive dataset and the unlabeled dataset, estimate r(x) by any of (5), (6) or (8) and obtain r^(x). Using r^(x), estimate  by (9) and obtain ^. Using an estimator r^(x) and ^, obtain a classifier h(x) = sign(r^(x) - ^).

4.3 EMPIRICALLY ESTIMATING 

We consider replacing the threshold defined by (2) with a value which can be calculated only from samples. In the case that we have access to the test inputs

{xite}ni=te1  p(x),

we choose a ^ that satisfies the following equation,

nte
nte = 1[r^(xtie) > ^].
i=1

(9)

Here, we used the knowledge of , the class-prior. This choice of ^ amounts to classifying top- test data as positive after ranking the inputs by r^.

In the case we do not have access to the test inputs, we use held-out training data instead of test data to estimate the threshold analogically to (9).

5 EXPERIMENTS
In this section, we report experimental results which were conducted using synthetic data and real-world datasets1). We used six classification datasets, MNIST, shuttle, pageblocks, usps, connect-4 and spambase, from UCI repository2), CIFAR-103) and a document dataset obtained from SwissProt (Boeckmann et al., 2003)4). MNIST and CIFAR-10 have 10 and 10 classes originally, and we constructed the positive and negative datasets from them as follows: MNIST was preprocessed in such a way that 0, 2, 4, 6, 8 constitute the positive class, while 1, 3, 5, 7, 9 constitute the negative class; for CIFAR-10, the positive dataset is formed by `airplane', `automobile', `ship' and `truck', and the negative dataset is formed by `bird', `cat', `deer', `dog', `frog' and `horse'. Except for the document dataset, we show the details of datasets in Table 1 and made positive data with a selection bias based on estimators of p(y = +1|x). For six datasets of the UCI repository and the CIFAR-10, we made positive datasets with a selection bias artifically, but, for the document dataset, we have an unlabeled dataset and a positive dataset, which is gathered for classifying the labels in the unlabeled dataset.
We call unbiased PU learning "PU", unbiased PU learning with a threshold estimated by (9) "PUSB", uLSIF with a threshold estimated by (9) "DRSB", nonnegative PU learning "nnPU" and nonnegative PU learning with a threshold estimated by (9) "nnPUSB".
1)All codes of experiments can be downloaded from github 2)The UCI data were downloaded from https://archive.ics.uci.edu/ml/index.php and https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/. 3)See https://www.cs.toronto.edu/~kriz/cifar.html. 4)The data can be downloaded from http://www.cs.ucsd.edu/users/elkan/posonly.

6

Under review as a conference paper at ICLR 2019

Table 1: Dataset statistics (Pos.frac.: Positive fraction, Dim: Dimension).

Dataset waveform mushroom spambase
MNIST usps connect-4 CIFAR-10

# of samples 5,000 8,124 4,601 70,000 9,298 67,557 60,000

Pos. frac. 0.492 0.517 0.394 0.511 0.524 0.658 0.400

Dim. 21 112 57 784 256 126
3,072

Let us denote a real-valued function as g(x) and use it for approximating f and s. In (5), (6) and (7), we assume a generic real-valued function for g(x), but here we use linear-in-parameter model in (5) and (7), and deep neural network in (6). When we used linear-in-parameter model, we assumed the following model,

g(x) =  (x),

(10)

where (x) = [1, 1(x), ..., m(x)] is a set of basis functions. For basis functions, we used the Gaussian functions centered around sample points  (x) = exp - x - c 2/(22) , where

{c1, ..., cm} = {x1, ..., xn, x1, ..., xn } and m = n + n . A regularization parameter is given by



and

insert

a

regularization

term

 2



 to (5) and 

m i=1

i

to

(7).

The

model

for

deep

neural

network were explained in the following sections for each dataset. We mainly used the same structure

proposed in Kiryo et al. (2017) in order to compare the performances.

For approximating s(x), we used g(x) directly.

For approximating f , we used the sigmoid function:

1 f (x) = 1 + exp(-g(x)) .

In this case, the loss is the same as the logistic loss and unbiased PU learning becomes convex.
In order to made a selection biases for the datasets, we estimated p(y = +1|x) firstly. For the estimation, we used all samples with the true positive and negative labels and logistic regression with the same model as that we used for PU learning.

5.1 TEST WITH SYNTHETIC DATA
This experiment shows a classifier given by PUSB. We used samples from a mixture distribution of the following two class-conditional distributions:
p(x|y = +1) = Nx(1, 22) and p(x|y = -1) = Nx(-1, 22),
where N (µ, 2) denotes the univariate normal distribution with mean µ and variance 2. A positive dataset with a selection bias is sampled from
p(o = +1|x = +1, y = +1)  (p(y = +1|x))10.

We generated 1, 000 positive samples and 10, 000 unlabeled samples. We made two datasets with
the different class-priors  = 0.3,  = 0.7. We plotted three classifiers estimated by PU, PUSB and p(y = +1|x). The results are shown in Figure 1. The classifiers obtained by our algorithm nearer to the Bayesian optimal classifier for the classification error rate, sign(p(y = +1|x) - 1/2).

5.2 BENCHMARK TEST In this subsection, we investigated the experimental performance in detail.

7

Under review as a conference paper at ICLR 2019

Figure 1: Two Gaussians: The horizontal axis is value of x and the vertical axis is probability density.
The vertical line represents the classifier. The distribution of positive data, negative data, unlabeled data and p(x = +1|o = +1, y = +1) is plotted.

Linear-in-parameter model: We used the mushrooms, shuttle, pageblocks, usps, connect-4 and spambase datasets.
Firstly, we estimated p(y = +1|x) using the logistic regression with the same linear model. Then, from positive data, we sampled positive dataset with a selection bias as
p(o = +1|x, y = +1)  (p(y = +1|x))20.
Then, we trained a classifier by minimizing (4) and (7) with the model (10).
For each binary labeled dataset, we made 12 different pairs of positive and unlabeled data with 4 different class-priors, {0.2, 0.4, 0.6, 0.8}, and 3 different numbers of unlabeled data, {800, 1, 600, 3, 200}. The number of positive data was fixed at 400. We used 1, 000 test data sampled from the same distribution as the unlabeled data. We ran the experiments 100 times and calculated the mean and standard deviation for the test dataset with PU, PUSB, and DRSB. The results are shown in Table 2. The classifiers obtained by our algorithm always show preferable performance to existing methods.

Table 2: The error rate of classification in test data (%) are shown for the different class-priors and the different number of samples. For all experiments, linear-in-parameter model was used. Best and equivalent methods (under 5% t-test) are bold.

PU

PUSB

DRSB

Dataset 

800

1,600

3,200

800

1,600

3,200

800

1,600

3,200

mushrooms 0.2 5.1 (.076) 5.0 (.055) 5.5 (.083) 4.1 (.007) 3.8 (.007) 4.0 (.007) 10.7 (.010) 9.9 (.011) 9.9 (.012)

0.4 7.0 (.012) 6.6 (.010) 6.8 (.025) 6.9 (.012) 6.7 (.010) 6.8 (.010) 15.8 (.014) 15.0 (.016) 15.2 (.012)

0.6 10.8 (.020) 11.3 (.015) 11.3 (.016) 8.2 (.013) 8.5 (.010) 8.3 (.011) 18.5 (.014) 18.8 (.012) 18.5 (.013)

0.8 21.1 (.022) 21.6 (.021) 21.7 (.016) 8.1 (.015) 8.0 (.012) 7.6 (.014) 14.2 (.013) 14.3 (.014) 14.4 (.011)

shuttle 0.2 23.8 (.006) 23.8 (.007) 23.8 (.007) 5.2 (.008) 5.0 (.007) 5.0 (.007) 5.1 (.008) 5.1 (.008) 5.1 (.007)

0.4 15.5 (.023) 15.5 (.019) 15.0 (.016) 7.8 (.011) 7.7 (.011) 7.6 (.012) 6.7 (.025) 7.6 (.029) 7.2 (.027)

0.6 26.1 (.018) 26.4 (.017) 26.4 (.016) 11.1 (.015) 10.8 (.012) 11.0 (.015) 7.8 (.028) 7.4 (.028) 7.1 (.028)

0.8 14.6 (.006) 14.6 (.005) 14.8 (.007) 12.3 (.014) 12.4 (.014) 12.3 (.012) 7.9 (.021) 7.6 (.023) 7.4 (.022)

pageblocks 0.2 39.5 (.009) 39.6 (.010) 39.6 (.008) 41.6 (.021) 42.5 (.022) 43.7 (.019) 23.1 (.016) 23.1 (.015) 22.2 (.012)

0.4 56.5 (.011) 56.7 (.011) 56.6 (.008) 33.7 (.020) 33.7 (.019) 33.5 (.024) 23.6 (.012) 23.7 (.014) 23.6 (.014)

0.6 22.4 (.023) 23.4 (.033) 28.1 (.028) 20.9 (.016) 21.1 (.016) 21.5 (.020) 18.7 (.011) 18.6 (.012) 18.1 (.017)

0.8 19.8 (.003) 20.0 (.001) 20.0 (.001) 13.9 (.022) 13.6 (.023) 16.7 (.043) 15.4 (.011) 14.8 (.014) 14.3 (.019)

usps 0.2 9.0 (.011) 8.5 (.009) 8.2 (.010) 8.0 (.009) 7.7 (.007) 7.4 (.009) 18.2 (.016) 19.6 (.013) 19.7 (.014)

0.4 10.5 (.012) 10.3 (.013) 10.0 (.010) 10.5 (.013) 10.2 (.012) 10.0 (.010) 30.5 (.029) 30.2 (.022) 29.9 (.023)

0.6 12.5 (.015) 12.3 (.015) 12.1 (.013) 11.2 (.016) 10.9 (.015) 10.6 (.013) 32.9 (.026) 33.2 (.031) 33.1 (.030)

0.8 19.8 (.020) 19.8 (.016) 19.5 (.017) 10.3 (.016) 9.8 (.014) 9.6 (.014) 25.9 (.028) 25.3 (.027) 25.8 (.030)

connect-4 0.2 22.3 (0.017) 21.9 (.014) 21.6 (.013) 21.8 (.013) 21.5 (.012) 21.2 (.011) 26.7 (.012) 26.6 (.012) 26.4 (.011)

0.4 31.2 (.015) 31.0 (.015) 30.7 (.016) 31.2 (.015) 31.0 (.015) 30.7 (.016) 39.4 (.016) 39.6 (.018) 39.2 (.016)

0.6 32.0 (.017) 32.1 (.013) 31.9 (.015 31.7 (.016) 31.7 (.013) 31.4 (.016) 40.9 (.015) 41.1 (.017) 41.0 (.017)

0.8 33.6 (.018) 33.5 (.016) 33.4 (.017) 24.2 (.013) 23.9 (.013) 23.8 (.013) 29.8 (.011) 29.6 (.013) 29.5 (.012)

spambase 0.2 20.1 (0.002) 20.1 (.002) 20.1 (.003) 13.6 (.013) 13.8 (.014) 14.0 (.014) 18.1 (.026) 18.3 (.025) 17.9 (.023)

0.4 36.2 (.024) 35.9 (.025) 30.7 (.024) 19.0 (.020) 18.7 (.021) 18.9 (.018) 27.4 (.042) 27.8 (.043) 27.7 (.039)

0.6 40.0 (.001) 39.9 (.001) 31.9 (.001 20.8 (.019) 20.7 (.018) 20.0 (.017) 31.2 (.037) 30.3 (.035) 31.2 (.035)

0.8 20.0 (.000) 20.0 (.000) 20.0 (.000) 18.0 (.015) 17.7 (.013) 17.3 (.013) 24.5 (.058) 23.9 (.017) 24.2 (.017)

Neural network model: We used the MNIST and CIFAR-10 datasets. The model of g(x) for the MNIST dataset was a 3-layer multilayer perceptron (MLP) with ReLU (Nair & Hinton, 2010) (more specifically, 784 - 100 - 1). The model for the CIFAR-10 dataset was an all convolutional

8

Under review as a conference paper at ICLR 2019
Figure 2: Experimental results of training deep neural networks. Left: MNIST; Center: CIFAR-10; Right: RealData: All measures are calculated for test data sampled from the marginal distribution p(x). The horizontal line is epoch of training the network, the vertical line of the upper low is error rate and the vertical line of the lower low is value of the precision and the recall.
net (Springenberg et al., 2015): (32 × 32 × 3)-[C(3 × 3, 96)] × 2-C(3 × 3, 96, 2)-[C(3 × 3, 192)] × 2-C(3 × 3, 192, 2)-C(3 × 3, 192)-C(1 × 1, 192)-C(1 × 1, 10)-1000-1000-1, where the input is a 32 × 32 RGB image, C(3 × 3, 96) means 96 channels of 3 × 3 convolutions followed by ReLU, [·] × 2 means there are two such layers, C(3 × 3, 96, 2) means a similar layer but with stride 2, etc.; it is one of the best architectures for CIFAR-10. Batch normalization (Ioffe & Szegedy, 2015) was applied before hidden layers. Firstly, we estimated p(y = +1|x) using the logistic regression with the same network structure and the positive and unlabeled datasets in the unlabeled dataset. Next, from the positive dataset, we resampled positive dataset with an observation, which follows
p(o = +1|x, y = +1)  (p(y = +1|x))10. Then, we trained a classifier by minimizing (6) with the model defined above. We used 10, 000 test data sampled from the same distribution as the unlabeled data. We ran the experiments 100 times and calculated the mean of the error rate, standard deviation of the error rate, the mean of recall and the mean of precision for each epoch in training with nnPU and nnPUSB. The results are shown on the left side and center of Figure 2. In the upper row, we show the mean and standard deviation of the error rate. In the lower row, we show the mean of recall and the mean of precision. As shown in Figure 2, the mean of the error rate of our algorithm was lower and the variance was also lower than the existing method. As a matter of fact should be noted, for our results, FP = FN held as we discussed in Section 4.3.
5.3 REAL-WORLD DATA TEST We demonstrate the effectiveness of our algorithm in a real-world application, we used a document dataset recorded from the SwissProt database and released by Elkan & Noto (2008). The dataset consists of three datasets, a positive dataset P, another positive dataset Q, and a negative dataset N. We call this real dataset RealData. The positive dataset P contains 2, 453 examples obtained from a specialized database named TCDB (Saier Jr et al., 2015). Meanwhile, there are 4, 906 records in the unlabeled dataset U, which are randomly selected from SwissProt excluding those in the TCDB. In other words, those 2 datasets P and U are disjoint. Hence, this belongs to TS. Furthermore, Das et al. (2007) also manually labeled the unlabeled dataset. They identified 348 positive members from the whole dataset N and called the new positive examples as Q and the leftover was N. We used Bag-of-Words to represent all examples of documents as vectors. We represented each document as a
9

Under review as a conference paper at ICLR 2019
78894-dimensional real-valued vector. This real-world dataset is often used to evaluate algorithms of PU learning such as Elkan & Noto (2008); He et al. (2018).
The model of g(x) for this dataset was a 5-layer multilayer perceptron (MLP) with ReLU (more specifically, 78, 894-300-300-300-300-1). We trained a classifier using positive and unlabeled data. Then, after finding a threshold estimated by (9), we evaluated the same evaluation measures as the previous experiments of a neural network by classifying U, i.e., we regarded the unlabeled data as test data. The results are shown on the right side of Figure 2. The result obtained from our algorithm outperformed the existing method. In addition, we made positive data with a selection bias by ourselves for Benchmark Test, but FP=FN also held in the real dataset with a selection bias. It means that we cannot ignore a selection bias in real-world applications and our algorithm could be more useful in practice.
6 CONCLUSION
In this paper, we proposed a novel framework for PU learning with a selection bias in positive data. We put the IO assumption and showed the density ratio of labeled positive data and unlabeled data has the same order as the conditional class distribution for inputs. Based on this result, we proposed an idea of partial identification in which we first estimate the density ratio and then use it as a classifier by setting a threshold. We conducted experiments to confirm the effectiveness of our approach. As we showed in the experiments, our method outperforms previous PU methods on real-world data.
REFERENCES
Joshua D. Angrist and Jörn-Steffen Pischke. Mostly Harmless Econometrics: An Empiricist's Companion. Princeton University Press, December 2008.
Jessa Bekker and Jesse Davis. Learning from positive and unlabeled data under the selected at random assumption. arXiv, 2018a.
Jessa Bekker and Jesse Davis. Beyond the selected completely at random assumption for learning from positive and unlabeled data. arXiv, 2018b.
Gilles Blanchard, Gyemin Lee, and Clayton Scott. Semi-supervised novelty detection. Journal of Machine Learning Research, 11(Nov):2973­3009, 2010.
Brigitte Boeckmann, Amos Bairoch, Rolf Apweiler, Marie-Claude Blatter, Anne Estreicher, Elisabeth Gasteiger, Maria J Martin, Karine Michoud, Claire O'donovan, Isabelle Phan, et al. The SWISSPROT protein knowledgebase and its supplement trembl in 2003. Nucleic acids research, 31(1): 365­370, 2003.
Sanmay Das, Milton H Saier, and Charles Elkan. Finding transport proteins in a general protein database. In European Conference on Principles of Data Mining and Knowledge Discovery, pp. 54­66. Springer, 2007.
M. C. du Plessis, Gang. Niu, and Masashi Sugiyama. Analysis of learning from positive and unlabeled data. In NIPS, pp. 703­711, 2014.
Marthinus Christoffel du Plessis and Masashi Sugiyama. Class prior estimation from positive and unlabeled data. IEICE Transactions on Information and Systems, E97-D(5):1358­1362, 2014.
Marthinus Christoffel du Plessis, Gang. Niu, and Masashi Sugiyama. Convex formulation for learning from positive and unlabeled data. In ICML, pp. 1386­1394, 2015.
Marthinus Christoffel du Plessis, Gang Niu, and Masashi Sugiyama. Class-prior estimation for learning from positive and unlabeled data. In ACML, pp. 221­236, 2016.
Charles Elkan and Keith Noto. Learning classifiers from only positive and unlabeled data. In ICDM, pp. 213­220, 2008.
Izrail Moiseevitch Gelfand, Richard A Silverman, et al. Calculus of variations. Courier Corporation, 2000.
10

Under review as a conference paper at ICLR 2019
Fengxiang He, Tongliang Liu, Geoffrey I Webb, and Dacheng Tao. Instance-dependent PU learning by bayesian optimal relabeling. arXiv, 2018.
James Heckman. Sample selection bias as a specification error. Econometrica, 47(1):153­61, 1979.
Shohei Hido, Yuta Tsuboi, Hisashi Kashima, Masashi Sugiyama, and Takafumi Kanamori. Statistical outlier detection using direct density ratio estimation. Knowledge and Information Systems, 26(2): 309­336, Feb 2011.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In ICML, pp. 448­456, 2015.
Shantanu Jain, Martha White, Michael W Trosset, and Predrag Radivojac. Nonparametric semisupervised learning of class proportions. In NIPS, 2016.
Takafumi. Kanamori, Shohei Hido, and Masashi Sugiyama. A least-squares approach to direct importance estimation. Journal of Machine Learning Research, 10(Jul.):1391­1445, 2009.
Masahiro Kato, Liyuan Xu, Gang Niu, and Masashi Sugiyama. Alternate estimation of a classifier and the class-prior from positive and unlabeled data. arXiv, 2018.
Ryuichi Kiryo, Gang Niu, Marthinus Christoffel du Plessis, and Masashi Sugiyama. Positiveunlabeled learning with non-negative risk estimator. In NIPS, pp. 1675­1685, 2017.
Xiao-Li Li, Philip S Yu, Bing Liu, and See-Kiong Ng. Positive unlabeled learning for data stream classification. In ICDM, pp. 259­270, 2009.
Zachary C. Lipton, Charles Elkan, and Balakrishnan Naryanaswamy. Optimal thresholding of classifiers to maximize F1 measure. In Machine Learning and Knowledge Discovery in Databases, pp. 225­239, 2014.
Charles Manski. Partial Identification in Econometrics. Palgrave-Macmillan, 2 edition, 2008.
Benjamin M. Marlin and Richard S. Zemel. Collaborative prediction and ranking with non-random missing data. In ACM Conference on Recommender Systems, RecSys '09, pp. 5­12, 2009.
Vinod Nair and Geoffrey E. Hinton. Rectified linear units improve restricted boltzmann machines. In ICML, 2010.
Minh Nhut Nguyen, Xiaoli-Li Li, and See-Kiong Ng. Positive unlabeled leaning for time series classification. In IJCAI, pp. 1421­1426, 2011.
Gang Niu, Marthinus Christoffel du Plessis, Tomoya. Sakai, Y. Ma, and Masashi Sugiyama. Theoretical comparisons of positive-unlabeled learning against positive-negative learning. In NIP, pp. 1199­1207, 2016.
David M. W. Powers. Visualization of tradeoff in evaluation: from precision-recall & PN to LIFT, ROC & BIRD. arXiv, 2015.
Harish Ramaswamy, Clayton Scott, and Ambuj Tewari. Mixture proportion estimation via kernel embeddings of distributions. In ICML, pp. 2052­2060, 2016.
Milton H Saier Jr, Vamsee S Reddy, Brian V Tsu, Muhammad Saad Ahmed, Chun Li, and Gabriel Moreno-Hagelsieb. The transporter classification database (TCDB): recent advances. Nucleic acids research, 44(D1):D372­D379, 2015.
Claude Sammut and Geoffrey I. Webb (eds.). Breakeven Point, pp. 137­138. Springer US, Boston, MA, 2010. ISBN 978-0-387-30164-8. doi: 10.1007/978-0-387-30164-8_88.
Tobias Schnabel, Adith Swaminathan, Ashudeep Singh, Navin Chandak, and Thorsten Joachims. Recommendations as treatments: Debiasing learning and evaluation. In ICML, volume 48, pp. 1670­1679, New York, New York, USA, 20­22 Jun 2016.
Clayton Scott and Gilles Blanchard. Novelty detection: Unlabeled data definitely help. In AISTATS, pp. 464­471, 2009.
11

Under review as a conference paper at ICLR 2019

J.T. Springenberg, A. Dosovitskiy, T. Brox, and M. Riedmiller. Striving for simplicity: The all convolutional net. In ICLR (workshop track), 2015.
Masashi Sugiyama, Taiji Suzuki, and Takafumi Kanamori. Density Ratio Estimation in Machine Learning. Cambridge University Press, New York, NY, USA, 1st edition, 2012.
Gill Ward, Trevor Hastie, Simon Barry, Jane Elith, and John R Leathwick. Presence-only data and the em algorithm. Biometrics, 65(2):554­563, 2009.

A PROOF OF THEOREM 1

Proof. We assumed that no negative data can be labeled. As a result, we have p(o = +1|x, y = -1) = 0 for arbitrary x  X . Therefore,

p(o = +1|x) = p(y = +1|x)p(o = +1|x, y = +1) + p(y = -1|x)p(o = +1|x, y = -1)

= p(y = +1|x)p(o = +1|x, y = +1).

(11)

By Bayes' theorem, we can expand the density ratio in (1) as follows:

p(x|o = +1, y = +1) r(x) =
p(x)

p(y = +1|x, o = +1)p(x|o = +1) 1 = p(y = +1|o = +1) p(x)

= p(y = +1|x, o = +1) 1 p(x|o = +1) p(y = +1|o = +1) p(x)

p(y = +1|x)p(o = +1|x, y = +1) 1 p(o = +1)p(x|o = +1)

=

p(o = +1|x)

p(y = +1|o = +1)p(o = +1)

p(x)

Because

p(y=+1|x)p(o=+1|x,y=+1) p(o=+1|x)

=

1 from (11), this is

equivalent to

r(x) = Cp(o = +1|x)

where C

=

1 p(y=+1,o=+1)

.

Hence, if Assumption 1 holds, the following relationship holds for

xi, xi  X :

r(xi) = Cp(o = +1|xi)  Cp(o = +1|xj) = r(xj).

Therefore, if p(y = +1|xi) < p(y = +1|xj), r(xi) < r(xj) holds. Similarly, r(xi) = r(xj) holds if and only if p(y = +1|xi) = p(y = +1|xj) holds.

B PROOF OF THEOREM 2

Proof. Regarding the probability as a classifier, let us decide a threshold for p(y = +1|x) as follows:

 = 1[p(y = +1|x)  ]p(x)dx.

(12)

This is equivalent to

p(y = +1|x)p(x)dx = 1[p(y = +1|x)  ]p(x)dx.

The left hand side is equal to

p(y = +1|x)p(x)dx +

p(y = +1|x)p(x)dx.

{x|p(y=+1|x)<}

{x|p(y=+1|x)}

The right hand side is equal to

p(y = +1|x)p(x)dx +

p(y = -1|x)p(x)dx.

{x|p(y=+1|x)}

{x|p(y=+1|x)}

12

Under review as a conference paper at ICLR 2019

Hence, (12) is equivalent to the following equation,

p(y = +1|x)p(x)dx =

p(y = -1|x)p(x)dx.

{x|p(y=+1|x)<}

{x|p(y=+1|x)}

From the definition of true positive (TP), false positive (FP), true negative (TN) and false negative

(FN), the left hand side of the

above equation is equal to

FP T P +F P +T N +F N

and the

right hand side

of

the

above

equation

is

equal

to

FN T P +F P +T N +F N

.

Therefore,

this

means

that

the

threshold

classifies

data as the number of negative data predicted as positive is equal to the number of positive data

predicted as negative, i.e., F P = F N .

Then, we state the following lemma about the relationship between  and .

Lemma 1. For all x  X , (r(x) - )(p(y = +1|x) - )  0 holds almost surely.

Proof. By (2) and (12), the following equation also hold,

1[r(x) > ]p(x)dx = 1[p(y = +1|x) > ]p(x)dx.

Hence, we can derive (1[r(x) > ] - 1[p(y = +1|x) > ])p(x)dx = 0.

This is equivalent to

(1[(r(x) - )(p(y = +1|x) - ) < 0]p(x)dx = 0.

From this equation, (r(x) - )(p(y = +1|x) - )  0 holds almost surely.

Therefore, if p(y = +1|x) < , then r(x) < ; if p(y = +1|x)  , then r(x)   almost surely.

Because precision =

TP T P +F P

and recall =

TP T P +F N

, F P = F N means that precision =

recall.Therefore, using a threshold defined by (2), we can obtain a classifier whose precision and the

recall are the same.

C PROOF OF THEOREM 3

Proof. Let us consider minimizing RPbUias for f  F , which is a function f : X  [ , 1 - ]. Let V be a class of measurable functions M : Rd  R with range (0, +). We use (x)  V and (x)  V as Lagrangian variables. Then, we introduce the Lagrange functional

L(f ; , ) = - p(x|y = +1)(log f (x) - log(1 - f (x)))

- log(1 - f (x))p(x) + (x)(-1 + + f (x)) + (x)( - f (x)) dx.

Then we consider the maximization of L with respect to  and , i.e.,

inf sup L(f ; , ).
f F V,V

If we assume f < , then supV,W ((x)(-1 + + f (x)) - (x)f (x)) = +. If we assume  f  1 - , then supV,W ((x)(-1 + + f (x)) - (x)f (x)) = 0. If we assume f  1 - ,
then supV,W ((x)(-1 + + f (x)) - (x)f (x)) = +. Now we obtain

inf L(f ; , ) =
f

RPbUias(f ) +

if  f  1 - otherwise.

As a direct consequence, we conclude that

f,

inf
f 1-

RPbUias(f

)

=

inf
f

sup
,

L(f

;

,

).

13

Under review as a conference paper at ICLR 2019

Because of the convexity of RPbUias(f ) for f , the strong duality holds. When the strong duality holds, the following equality holds.

inf sup L(f ; , ) = sup inf L(f ; , ).

f ,

, f

Hence, we discuss the solution of the dual problem, sup, inff L(f ; , ).

Given  and , we apply the Euler-Lagrange equation (Theorem 1 of Gelfand et al. (2000)) for calculating inff L(f ; , ). To use the result of Gelfand et al. (2000) directly, we introduce . As a result, the minimizer f (x) satisfies the following equation,

p(x|y = +1)

11 f (x) + 1 - f (x)

-

1

p(x) - f (x)

-

(x)

+

(x)

=

0.

Therefore,
((x) - (x))(f (x))2 + (p(x) + (x) - (x))f (x) - p(x|y = +1) = 0. (13)
Let us denote f (x), (x) and (x) as the optimal functions. The system of optimality conditions for (f , , ) is often called Karush-Kuhn-Tucker (KKT) system. As a result, we can derive the following KKT condition:
((x) - (x))(f (x))2 + (p(x) + (x) - (x))f (x) - p(x|y = +1) = 0, f (x)  1 - , f (x)  , (x))  0, (x)  0, (x)(f (x) - 1 - ) = 0, (x)f (x) = 0.

From this condition, the following results:

1.

If

(x)

=

(x)

=

0.

f (x)

=

p(x|y=+1) p(x)

.

This

solution satisfies the constraint only

when x is in the domain D, where D = {x|p(x|y = 1)  (1 - )p(x)}.

2. If (x) > 0 and (x) = 0, f (x) = 1 - . It is because inff L(f ; , )  + as (x)  + if f (x) < 1 - . (x) > 0 holds only when x is in the domain R\D2, where R\D2 = {x|p(x|y = 1) < (1 - )p(x)}, because
- (x)(1 - )2 + (p(x) + (x))(1 - ) - p(x|y = +1) = 0

 -(x)(1 - )2 + (x)(1 - ) + p(x)(1 - ) - p(x|y = +1) = 0



(x) =

p(x)(1 - ) - p(x|y = +1)

(1 - )2 + (1 - )

.

3. If (x) = 0 and (x) > 0, f (x) = . It is because inff L(f ; , )  + as (x)  + if f (x) > . This solution satisfies (13), only when x is in the domain R\D1, where R\D1 = {x|p(x|y = 1) > p(x)}, because

(x)( )2 + (p(x) - (x)) - p(x|y = +1) = 0



(x) = p(x)

- p(x|y = +1) 2- .

Otherwise, there is no feasible solution. As a result, the solution for the optimization problem f (x) = arg minf(x)(0,1- ] RPbUias(f ) is





f (x)

=

 p(x|y=+1,o=+1)
p(x)

1 -

(x / D1), (x  D1  D2), (x / D2),

where D1 = {x|p(x|y = 1, o = +1)  p(x)} and D2 = {x|p(x|y = 1, o = +1)  (1 - )p(x)}.

14


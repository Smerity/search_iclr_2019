Under review as a conference paper at ICLR 2019
SUPERVISED COMMUNITY DETECTION WITH LINE GRAPH NEURAL NETWORKS
Anonymous authors Paper under double-blind review
ABSTRACT
We study data-driven methods for community detection on graphs, an inverse problem that is typically solved in terms of the spectrum of certain operators or via posterior inference under certain probabilistic graphical models. Focusing on random graph families such as the stochastic block model, recent research has unified both approaches and identified both statistical and computational signal-tonoise detection thresholds. This graph inference task can be recast as a node-wise graph classification problem, and, as such, computational detection thresholds can be translated in terms of learning within appropriate models. We present a novel family of Graph Neural Networks (GNNs) and show that they can reach those detection thresholds in a purely data-driven manner without access to the underlying generative models, and even improve upon current computational thresholds in hard regimes. For that purpose, we propose to augment GNNs with the non-backtracking operator, defined on the line graph of edge adjacencies. We also perform the first analysis of optimization landscape on using GNNs to solve community detection problems, demonstrating that under certain simplifications and assumptions, the loss value at the local minima is close to the loss value at the global minimum/minima. Finally, the resulting model is also tested on real datasets, performing significantly better than previous models.
1 INTRODUCTION
Graph inference problems encompass a large class of tasks and domains, from posterior inference in probabilistic graphical models to community detection and ranking in generic networks, image segmentation, or graph inverse problems. They are motivated both by practical applications, such as PageRank, but also by fundamental complexity questions, which ask for the intrinsic algorithmic hardness of solving a certain class of graph inference tasks.
These problems can be formulated in either unsupervised, semi-supervised or purely supervised learning settings. In the latter, one assumes a dataset of graphs with labels on its nodes and/or edges, and attempts to perform node/edge classification by optimizing a loss over a certain parametric class, e.g. neural networks. Graph Neural Networks ((Gori et al., 2005), (Bronstein et al., 2017) and references therein) are natural extensions of Convolutional Neural Networks to graph-structured data, and have emerged as a powerful class of algorithms to perform complex graph inference leveraging labeled data. In essence, these neural networks learn cascaded linear combinations of intrinsic graph operators interleaved with node-wise (or edge-wise) activation functions. Since they learn from intrinsic graph operators, they can be applied to varying input graphs, and they offer the same parameter sharing advantages as their CNN counterparts.
In this work, we focus on community detection problems, a wide class of node classification tasks that attempt to discover a clustered, segmented structure within a graph. The algorithmic approaches to this problem include a rich class of spectral methods, which take advantage of the spectrum of certain operators defined on the graph, as well as approximate message-passing methods such as belief propagation (BP), which performs approximate posterior inference under predefined graphical models. Focusing on the supervised setting, we study the ability of GNNs to approximate, generalize or even improve upon these class of algorithms. Our motivation is two-fold. On the one hand, this problem exhibits algorithmic hardness on some settings, opening up the possibility to discover more
1

Under review as a conference paper at ICLR 2019

efficient algorithms than the current ones. On the other hand, many practical scenarios fall beyond pre-specified probabilistic models, requiring data-driven solutions.
We propose key modifications to the GNN architecture allowing it to exploit edge adjacency information through the non-backtracking operator of the graph. This operator is defined over the edges of the graph and allows a directed flow of information even when the original graph is undirected. We refer to the resulting model as a Line Graph Neural Network (LGNN). Focusing on important random graph families exhibiting community structure, such as the stochastic block model and the geometric block model, we demonstrate improvements in the performance by LGNN, even in regimes within the so-called computational-to-statistical gap. A perhaps surprising aspect is that these gains can be obtained even with linear GNNs, which become parametric versions of power iteration algorithms.
This motivates our second main contribution: the analysis of the optimization landscape of such linear GNN models when trained with planted solutions of a given graph distribution. We show that under reparametrization, these landscapes have an interesting property, namely the presence of an energy gap controlling the energy difference between local and global minima. With certain assumptions on the spectral concentration of certain random matrices, this energy gap shrinks as the size of the input graphs increases, which would mean that the optimization landscape is benign on large enough graphs.
Summary of Main Contributions:
· We propose an extension of GNNs that operate on the line graph using the non-backtracking operator, which yields significant improvements on hard community detection regimes.
· We show that on the stochastic block model we reach detection thresholds in a purely data-driven fashion and improve upon belief-propogation in hard SBM detection regimes, as well as in the geometric block model.
· We perform the first analysis of the learning landscape of GNN models, showing that under cetain simplifications and assumptions, they exhibit a form of "energy gap", where local mimima are confined in low-energy configurations.
· We show how our model can be applied to real-world datasets, leading to state-of-the-art community detection results.

2 PROBLEM SETUP

We are interested in a specific class of node-classification tasks in which given an input graph

G = (V, E), a signal y : V  {1, C} encoding a partition of V into C groups is to be predicted

at each node. We assume that a training set {(Gt, yt)}tT is given, which we use to learn a model

y^ = (G, ) trained by minimising

1

L() = T

((Gt, ), yt) .

tT

Since y encodes a partition of C groups, the specific label of each node is only important up to

a global permutation of {1, C}. Section 4.3 describes how to construct losses (a, b) with such a

property. A permutation of the observed nodes translates into the same permutation applied to the

labels, which justifies models  that are equivariant to permutations. Also, we are interested in

inferring properties of community detection algorithms that do not depend on the specific size of the

graphs1. We therefore require that the model  accepts graphs of variable size for the same set of

parameters, similarly as in sequential RNN or spatial CNN models. In our study of random graph

models (SBM and GBM), we construct a training set of planted solutions. Labels yi are generated

by sampling a balanced partition uniformly at random, and then we produce the input graphs Gi by sampling Gi|yi according to each random graph model.

3 RELATED WORK
GNN was first proposed in Gori et al. (2005); Scarselli et al. (2009). Bruna et al. (2013) generalized convolutional neural networks on general undirected graphs by using the graph Laplacian's eigenbasis.
1In this work, however, we assume that C is fixed.

2

Under review as a conference paper at ICLR 2019
This was the first time the Laplacian operator was used in a neural network architecture to perform classification on graph inputs. Defferrard et al. (2016) considers a symmetric Laplacian generator to define a multiscale GNN architecture, demonstrated on classification tasks. Similarly, Kipf & Welling (2016) uses a similar generator as effective embedding mechanisms for graph signals and applies it to semi-supervised tasks. This is the closest application of GNNs to our current contribution. However, we highlight that semi-supervised learning requires bootstrapping the estimation with a subset of labeled nodes, and is mainly interested in generalization within a single, fixed graph. In comparison, our setup considers community detection across a distribution of input graphs and assumes no initial labeling on a given test-set input graph. There have been several extensions of GNNs (Li et al., 2015; Sukhbaatar et al., 2016; Duvenaud et al., 2015; Niepert et al., 2016) by modifying their non-linear activation functions, their parameter sharing strategies, and their choice of graph operators. In particular, Gilmer et al. (2017) interpreted the GNN architecture as learning an approximate message-passing algorithm, which extends the learning of hidden representations to graph edges in addition to graph nodes. Recently, Velickovic et al. (2017) relates adjacency learning with attention mechanisms, and Vaswani et al. (2017) proposes a similar architecture in the context of machine translation. Another recent and related piece of work is Kondor et al. (2018), which proposes a generalization of GNN that captures high-order node interactions through covariant tensor algebra. Our approach to extend the expressive power of GNN using the line graph may be seen as an alternative to capture such high-order interactions. Our energy landscape analysis is related to the recent paper (Shamir, 2018), which establishes an energy bound on the local minima arising in the optimization of ResNets. In our case, we exploit the properties of the community detection problem to produce an energy bound that depends on the concentration certain random matrices, which one may hope for as the size of the input graphs increases. Finally, Zhang (2016)'s work on data regularization for clustering and rank estimation is also motivated by the success of using Bethe-Hessian-like perturbations to improve spectral methods on sparse networks. It finds good perturbations via matrix perturbations, and also has success on the stochastic block model. Yang & Leskovec (2012a) curates benchmark datasets for community detection and quantifies the quality of these datasets, while Yang & Leskovec (2012b) develops new algorithms for community detection by fitting data to newly designed generative models, which exhibit similar statistical structure learned from their analysis of the aforementioned datasets.
4 LINE GRAPH NEURAL NETWORKS
Figure 1. Overview of the architecture of our LGNN. Given an input graph G, we construct its line graph L(G) using the non-backtracking operator (see Figure 2) and we propagate the degree signal through multiple layers of graph diffusion in G and L(G); see equations (1) and (2). The output node features are used to predict node-wise labels, and the whole network is trained end-to-end using standard backpropagation using a label permutation invariant loss (see Section 4.3). The trained model can then be used to infer communities on input graphs of arbitrary size and connectivity.
3

Under review as a conference paper at ICLR 2019

This section introduces our GNN architectures based on the power graph adjacency (Section 4.1) and its extension to line graphs using the non-backtracking operator (Section 4.2), as well as the design of losses invariant to global label permutations (Section 4.3).

4.1 POWER GRAPH NEURAL NETWORKS

The Graph Neural Network (GNN), introduced in (Scarselli et al., 2009) and later simplified in (GGS;
Duvenaud et al., 2015; com) is a flexible neural network architecture that is based on local operators
on a graph G = (V, E). We start by briefly reviewing the generic GNN architecture, and next describe our modifications to make it suitable to our interests. Given some input signal x  R|V |×b on the vertices of G, we consider graph intrinsic linear operators that act locally on this signal: The degree operator is the linear map D : F  DF where (Dx)i := deg(i) · xi , D(x) = diag(A1)x . The adjacency operator A is the linear map given by the adjacency matrix Ai,j = 1 iff (i, j)  E. In this way, J-th powers of A encode J-hop neighborhoods of each node, and allow us to combine and
aggregate local information at different scales. We consider in this work the power graph adjacency Aj = min(1, A2j ), which encodes 2j-hop neighborhoods into a binary graph.

We consider a multiscale GNN layer that receives as input a signal x(k)  R|V |×bk and produces x(k+1)  R|V |×bk+1 as

J -1
x(k+1)i,l =  x(ik)1(k,l) + (Dx(k))i2(k,l) + (A2j x(k))i3(k+)j,l , l = 1, . . . bk+1/2, i  V, (1)
j=0

J -1

x(k+1)i,l = xi(k)1(k,l) + (Dx(k))i2(k,l) +

(A2j x(k))i3(k+)j,l, l = bk+1/2 + 1, . . . bk+1, i  V,

j=0

where  = {1(k), . . . , J(k+)3}, s(k)  Rbk×bk+1 are trainable parameters and (·) is a point-wise nonlinearity, chosen in this work to be (z) = max(0, z). We thus consider a layer with linear "residual connections" (He et al., 2016), both to ease with the optimization when using large number of layers and to give the model the ability to perform power iterations. Since the spectral radius of the learned linear operators in (1) can grow as the optimization progresses, the cascade of GNN layers can become unstable to training. In order to mitigate this effect, we consider spatial batch normalization (Ioffe & Szegedy, 2015) at each layer.

As explained in Section B.1, the Krylov subspace generated by the graph Laplacian (Defferrard

et al., 2016) is not sufficient in this case to operate well in the sparse regime, as opposed to the

generators {I, D, A}. The expressive power of each layer is increased by adding multiscale versions

of A, although this benefit comes at the cost of computational efficiency, especially in the sparse

regime. The network depth is chosen to be of the order of the graph diameter, so that all nodes obtain

information from the entire graph. In sparse graphs with small diameter, this architecture offers

excellent scalability and computational complexity. Indeed, in many social networks diameters are

constant (due to hubs), or  log(|V |), as in the stochastic block model in the constant average degree

regime (Riordan & Wormald, 2010). This results in a model with computational complexity of the

order of  |V | log(|V ), making it amenable to large-scale graphs. In our setup, batch normalization

not only prevents gradient blowup, but also performs the orthogonalisation relative to the constant

vector, which is associated with the smallest eigenvector of the graph operator whose spectrum

contains community information. This reinforces the analogy between cascading layers of (1) and the

power iterations to obtain the Fiedler vector of such operator. Indeed, if one wants to extract the Fiedler

vector of a matrix M , whose smallest eigenvector is known to be v, one can do so by performing

power iterations on M~ =

M I - M as y(n+1) = M~ x(n) , x(n+1) =

y(n+1)-vT vy(n+1) y(n+1)-vT vy(n+1)

. If v is a

constant vector, then the normalization above is precisely performed within the batch normalization

step.

4.2 LGNN: POWER GNN ON LINE GRAPHS WITH NON-BACKTRACKING OPERATOR
For graphs that have few cycles, posterior inference can be remarkably approximated by loopy belief propagation (Yedidia et al., 2003). As described in Section B.2, the message-passing rules are defined over the edge adjacency graph; see equation 57. Although its second-order approximation

4

Under review as a conference paper at ICLR 2019

around the critical point can be efficiently approximated with a power method over the original graph, a data-driven version of BP requires accounting for the non-backtracking structure of the message-passing. In this section we describe how to upgrade the GNN model so that it can exploit non-backtracking operators.

The line graph L(G) = (VL, EL) is the graph representing the edge adjacency structure of G. If G = (V, E) is an
undirected graph, then the vertices VL of L(G) are the ordered edges in E, that is VL = {(i  j); (i, j)  E}  {(j  i); (i, j)  E}, so |VL| = 2|E|. The nonbacktracking operator B  R2|E|×2|E| encodes the edge
adjacency structure as follows. Two nodes in L(G) are
connected if

B(ij),(i j ) =

1 if j = i and j = i , 0 otherwise.

This operator thus enables the propagation of directed

information through the graph. The message-passing rules

of BP can be expressed as a diffusion in the line graph

L(G) using this non-backtracking operator, with specific

choices of activation function that turn product of beliefs

into sums.

Figure 2. Construction of the line graph L(G) using the non-Backtracking Operator.

A natural extension of the GNN architecture presented in The nodes of L(G) correspond to oriented

Section 4.1 is thus to consider a second GNN defined on edges of G.

L(G), generated by the corresponding non-backtracking

operator B and degree DB = diag(B1) operators. This

effectively defines edge features that are diffused and updated according to the edge adjacency of

G. Edge and node features are combined at each layer using the edge indicator matrices Pm, Pd 

{0, 1}|V |×2|E|, defined as Pmi,(ij) = 1, Pmj,(ij) = 1, Pdi,(ij) = 1, Pdj,(ij) = -1 and 0 otherwise. Dropping the skip linear connections for ease of exposition, the resulting model becomes

J -1

x(k+1)i,l =  xi(k)1(k,l) + (Dx(k))i2(k,l) +

(A2j x(k))i3(k+)j,l + {Pm, Pd}y(k)3(k+)J,l , i  V

j=0

(2)

y(k+1)i ,l

J -1

=  yi(k)1(k,l) + (DL(G)y(k))i 2(k,l) +

(AL2j(G)y(k))i 3(k+)j,l + [{Pm, Pd} x(k+1)]i 3(k+)J,l , i  VL.

j=0

with additional parameters {1(k), . . . , J(k+)3}, s(k)  Rbk×bk+1 . The resulting architecture is named as a Line Graph Neural Network (LGNN).
It can be verified that the resulting model (G) := x(K) satisfies the permutation equivariance property required for the task: (G) = (G), where  is the permutation matrix associated with . Several authors have proposed combining node and edge feature learning (Gori et al., 2005; Gilmer et al., 2017; Velickovic et al., 2017), although we are not aware of works that considered the edge adjacency structure provided by the line graph and the non-backtracking operator. For graph families with constant average degree d, the line graph has size 2|E| = d|V | of the same order, making this model feasible from the computational point of view. The line graph construction can be iterated with L(L(G)), L(. . . L(L(G)) . . . ) to yield a graph hierarchy, which would capture high-order interactions between the elements of G. Such hierarchical construction relates to other recent efforts to generalize GNNs (Kondor et al., 2018). In our experiments, we use the input signals x(0) = deg(G) and y(0) = deg(L(G)) in the line graph version.

Relationship between LGNN and edge feature learning approaches: The GNN on the line graph using the non-backtracking operator can be interpreted as learning directed edge features from an undirected graph. Indeed, if each node i contains two distinct sets of features xs(i) and xr(i), the non-backtracking operator constructs edge features from node features while preserving orientation: For an edge e = (i, j), our model constructs oriented edge features fij = g(xs(i), xr(j)) and

5

Under review as a conference paper at ICLR 2019

fji = g(xr(i), xs(j)) (where g is trainable and not necessarily commutative on its arguments) that are subsequently propagated through the graph. Constructing such local oriented structure is shown to significantly improve performance in the next section. (Battaglia et al., 2016) introduced edge features over directed and typed graphs, but does not discuss the undirected case. (Kearnes et al., 2016; Gilmer et al., 2017) learn edge features on undirected graphs using fe = g(x(i), x(j)) for an edge e = (i, j), where g is now commutative on its arguments. Finally, (Velickovic et al., 2017) learns directed edge features on undirected graphs using stochastic matrices as adjacencies (which are either row or column-normalized).

4.3 A LOSS FUNCTION INVARIANT UNER LABEL PERMUTATION

Let C = {c1, . . . , cC} denote the possible community labelings that each node can take. Consider first the case where communities do not overlap: C equals the number existing communities. We define
the network output at each node using standard softmax, computing the conditional probability that node i belongs to community c: oi,c = p(yi = c |, G). Let y  CV be the ground truth community structure. Since community belonging is defined up to global label changes in communities, we
define the loss associated with a given graph instance as

()

=

inf
SC

-
iV

log oi,(yi)

,

(3)

where SC denotes the permutation group of C elements. In our experiments we considered examples with small number of communities C  {2, 5}, but general scenarios, where C is suspected to be
much larger, might make the evaluation of (3) over the permutation group of C elements impractical. A possible solution is to randomly partition for each sample C/C~ labels into C~ groups, then marginalize the model outputs oi,c, c  C into o¯i,c¯ = cc¯ oi,c and use () = infSC~ - iV log o¯i,(y¯i) , which only involves a permutation group of size C~!. Finally, if we are in a setup where nodes can belong to multiple communities, we simply redefine C to include subsets of communities instead of
just singletons, and modify the permutation group SC accordingly.

5 ENERGY LANDSCAPE OF LINEAR GNN OPTIMIZATION

As described in the numerical experiments, we found that the GNN models without non-linear activations already provide substantial gains relative to baseline (non-trained) algorithms, by finding suitable generalizations of power iterations. This section studies the optimization landscape resulting from this linear assumption. Despite defining a non-convex objective, we prove that the landscape is `benign' under certain further simplifications, in the sense that the local minima are confined on sublevel sets of low energy.

For simplicity, we consider only the binary c = 2 case where we replace the node-wise binary

cross-entropy by the squared cosine distance2, and we assume a single feature map (dk = 1 for all

k), and focus on the power GNN described in Section 4.1 (although our analysis carries equally to

describe the line graph version; see remarks below). We also make the simplifying assumption to

replace the layer-wise spatial batch normalization by a simpler projection onto the unit 2 ball (thus

we do not remove the mean). Without loss of generality, assume that the input graph G has size n,

and denote by F = {A1, . . . , AQ} the family of graph operators appearing in (1). Each layer thus

applies an arbitrary polynomial

Q q=1

q(k)Aq

to

the

incoming

node

feature

vector

x(k).

Given

an

input node vector w  Rn, the network output can thus be written as



Y^ =

e

K
, with e = 

e

q(k)Aq w .

k=1 qQ

(4)

We highlight that this multilinear GNN setup is fundamentally different from the multilinear fullyconnected neural networks whose landscape is well understood (Kawaguchi, 2016). First, the output is normalized in the sphere, which has important effects in the geometry. Next, the network parametrization is intrinsic (the operators Oj depend on the input), which introduces fluctuations in the

2to account for the invariance up to global flip of label

6

Under review as a conference paper at ICLR 2019

landscape that we analyze. In general, the operators in F are not commutative, but by considering the

generalised Krylov subspace generated by powers of F , F K = {O1 = A1K , O2 = A1AK2 -1, O3 =

A1A2A1K-2, . . . OQK = AQK }, one can reparametrise (4) as e =

QK j=1

j Oj w

with





RM ,

with

M

=

QK .

Given the target

y



Rn,

the loss

incurred

by each pair (G, y) becomes

|

e,y e

|2
2

,

and

therefore the population loss, when expressed in terms of , equals

 Ln() = EXn,Yn 

Yn , with Xn

(5)

 (O1w)  Yn = znzn  RM×M , (zn)j = Oj w, y and Xn = UnUn  RM×M , Un =  . . .  .
(OM w)
The landscape is thus specified by a pair of random matrices Yn, Xn  RM×M . The following theorem establishes that under appropriate assumptions, the concentration of certain random matrices around their mean controls the energy gaps between local and global maxima of L.

We define a "mean-field" loss function L~n()

= E T Yn
Xn,Yn T EXn

=

,T EYn
T EXn

and

consider

Ln

as a perturbation of L~n. Assuming that EXn 0, we write the Cholesky decomposition of

EXn as EXn = RnRnT , and define An = Rn-1Yn(Rn-1)T , A¯n = EAn = Rn-1EYn(Rn-1)T , Bn = Rn-1Xn(Rn-1)T , and Bn = Bn - In. Given a symmetric matrix K  RM×M , we let

1(K), 2(K), ..., M (K) denote the eigenvalues of K in nondecreasing order.

Theorem 5.1. For a given n, let n = (1(A¯n) - 2(A¯n))-1, µn = E[|1(An)|6], n = E[|1(Bn)|-6], n = E[ Bn 6], and assume that all four quantities are finite. Then if l  SM-1 is a local minimum of Ln, and g  SM-1 is a global minimum of Ln, we have Ln(l)  (1 - n,µn,n,n ) · Ln(g), where n,µn,n,n = O(n) for given n, µn, n as n  0 and its formula is given in the appendix.

Corollary 5.2. If (n)nN , (µn)nN , (n)nN are all bounded sequences, and limn n = 0, then  > 0, n such that n > n , |Ln(l) - Ln(g)|  · Ln(g).

The main strategy of the proof is to consider the actual loss function Ln as a perturbation of L~n, which has a landscape that is easier to analyze and does not have poor local maxima, since it is equivalent to a quadratic form defined over the sphere SM-1. For a given graph inverse problem, this theorem thus requires estimating spectral fluctuations of the pair Xn, Yn, which in turn involve the spectrum of C algebras generated by the non-commutative family F. That said, one should expect concentration to happen in general, since the dimension M is fixed as n grows. Another interesting question is to understand how the asymptotics of our landscape analysis relate to the hardness of estimation as a function of the Signal-to-Noise ratio. Finally, another open question is to what extent our result could be extended to the non-linear residual GNN case, perhaps leveraging ideas from (Shamir, 2018).

6 EXPERIMENTS

We present experiments on synthetic community detection (Sections 6.1, 6.2 and Appendix C) as

well as real-world detection (Section 6.3). In the synthetic experiments, our performance measure is

the overlap between predicted (y^) and true labels (y), which quantifies how much better than random

guessing a predicted labelling is. The overlap is given by

1 n

u

y(u),y^(u)

-

1 C

/(1

-

1 C

)

where



is the Kronecker delta function, and the labels are defined up to global permutation. The GNNs were

all trained with 30 layers, 2 feature maps and J = 2. We used Adamax (Kingma & Ba, 2014) with

learning rate 0.004 across all experiments. 3

6.1 BINARY STOCHASTIC BLOCK MODEL
The stochastic block model is a random graph model with planted community structure. In its simplest form, one assigns |V | = n nodes to C classes at random with y : V  {1, C} and draws an
3Code will be publicly released at ???.???.???

7

Under review as a conference paper at ICLR 2019

GNN

LGNN

LGNN linear GAT (Velickovic et al., 2017)

BP

0.17 ± 0.012 0.207 ± 0.015 0.165 ± 0.015

0.164 ± 0.047

0.1435 ± 0.02

Table 1: Performance of different models on 5-community dissociative SBM graphs with n = 400, C = 5, p = 0, q = 18/n, corresponding to average degree d = 14.5.

edge connecting any two vertices u, v independently at random with probability p if y(v) = y(u), and with probability q otherwise. The sparse binary case C = 2 when p, q 1/n is well understood and provides an initial platform to compare the GNN against provably optimal recovery algorithms; see Appendix B. We consider two learning scenarios. In the first scenario, we train parameters  conditional on p and q, by producing 6000 samples G  SBM (n = 1000, pi, pi, C = 2) for different pairs (pi, qi) and estimating the resulting (pi, qi). In the second scenario, reported in Appendix D, we train a single set of parameters  from a sample of 6000 samples containing a mixture of SBM with different parameters p, q and average degree. This setup is important as it shows our GNN is not just approximating known algorithms such as BP, since the parameters are not constant in this dataset. Figure 3 reports the performance of our models on the binary SBM model for different SNR regimes and compares it with the belief-propagation baseline from (Decelle et al., 2011), as well as the baseline spectral method using the normalized Laplacian. We observe that our models reach the statistical detection threshold, given in this case by the BP algorithm. Notice that even the linear GNN matches the performance, in accordance to the spectral approximations of BP given by the Bethe Hessian (see supplementary), and significantly outperforms performing 30 power iterations on that operator. We notice the line-graph version of our GNN slightly outperforms the baseline GNN, and that even the linear model that only considers residual connections reaches the statistical threshold. We also notice that our models outperform the Graph Attention Network (GAT) 4 in this task, which we also set to have 30 layers and 2 feature maps (Velickovic et al., 2017). A possible reason is that our graph operators include the degree matrix, which is important in sparse graphs to prevent hub nodes from dominating the diffusion. We ran experiments in the disassociative case (q > p), as well as with C = 3 communities, and obtained similar results, not reported here.

6.2 COMPUTATIONAL-TO-STATISTICAL THRESHOLDS IN THE SBM

The previous section showed that for small num-

ber of communities (k < 4), the GNN-based 1

model is able to reach the information theoretic 0.9 (IT) threshold. In such regimes, it is known 0.8 (Abbe, 2017; Massoulié, 2014; Coja-Oghlan 0.7

Lap BP GNN LGNN

et al., 2016) that BP provably reaches such IT 0.6
threshold. The situation is different for k > 4, 0.5 where it is conjectured that a computational- 0.4

LGNN-linear power GAT

to-statistical gap emerges between the theoret- 0.3 ical performance of MLE estimators and any 0.2 polynomial-time estimation procedure (Decelle 0.1
0
et al., 2011). In this context, one can use the 0.5 1 1.5 2 2.5 3

GNN model to search the space of BP general-

izations, and attempt to improve the detection performance of BP for signal-to-noise ratios

Figure 3. SBM detection. C = 2 associative, X-axis corresponds to SNR, Y-axis to overlap; see text.

falling within the computational-to-statistical

gap. Table 1 presents results for the 5-community disassociative case, with p = 0 and q = 18/n, and

n = 400. This amounts to solving a graph coloring problem in a sparse regime, which falls above the

IT threshold but below the regime where BP is able to detect (Decelle et al., 2011), asymptotically

as n  . We see that the GNN models significantly outperform BP in this regime, and that the

line GNN version provides the best overlap performance, opening up the possibility to reduce the

computation-information gap. That said, our model may be picking finite-size effects, which may

vanish as n  ; the asymptotic study of these gains is left for future work.

4We based our implementation from https://github.com/Diego999/pyGAT. We modified the code so that the number of layers in the network is flexible, and also added spatial batch normalization at the end of each layer. Our experiments showed that including spatial batch normalization improves the performance.

8

Under review as a conference paper at ICLR 2019

6.3 REAL DATASETS FROM SNAP

We now train the GNNs on real datasets with community labels provided by SNAP. These datasets have ground truth community labels ranging from social networks to hierarchical co-purchasing networks. We obtain the training set as follows. For each SNAP dataset, we start by focusing only on the 5000 top quality communities provided by the dataset. We then identify edges (i, j) that cross at least two different communities. For each of such edges, we consider the two largest communities C1, C2 such that i / C2 and j / C1, i  C1, j  C2, and extract the subgraph determined by C1  C2, which is connected since all the communities are connected. Finally, we divide the train and test sets by enforcing test examples to contain disjoint communities from those in the training set. In this experiment, due to computational limitations, we restrict our attention to the three smallest graphs in the SNAP collection (Youtube, DBLP and Amazon), and we restrict the largest community size to 800 nodes, which is a conservative bound, since the average community size on these graphs is below 30. We compare our GNN's performance with the Community-Affiliation Graph Model (AGM) and with a variant of the LGNN that considers symmetric edge features instead of the non-backtracking operator, which fits into the framework of MPNNs (Gilmer et al., 2017) using ReLU activations. The AGM is a generative model defined in Yang & Leskovec (2012b) that allows overlapping communities where overlapping area have higher density. This was a statistical property observed in many real datasets with ground truth communities, but not present in generative models before AGM and was shown to outperform algorithms before that. Table 2 compares the performance, measured with a 3-class {1, 2, 1 + 2} classification accuracy up to global permutation 1  2. It illustrates the benefits of data-driven models that strike the right balance between expressive power to adapt to model misspecifications and structural assumptions of the task at hand.

Table 2: Snap Dataset Comparison between GNN and AGM. We report node classification accuracy. We compare against our implementation of MPNNs based on symmetric edge adjacencies (see text).

Dataset Amazon DBLP Youtube

(train/test) 268 / 52 2831 / 510 48402 / 7794

Avg |V | 60 26 61

Avg |E| 346 164 274

GNN 0.78 ± 0.13 0.85 ± 0.03 0.86 ± 0.02

LGNN 0.96 ± 0.1 0.87 ± 0.04 0.89 ± 0.02

MPNN 0.93 ± 0.2 0.86 ± 0.04 0.87 ± 0.02

AGMFit 0.81 ± 0.08 0.64 ± 0.01 0.57 ± 0.01

7 CONCLUSION
In this work we have studied data-driven approaches to community detection with graph neural networks. Our results confirm that, even when the signal-to-noise ratio is at the lowest detectable regime, it is possible to backpropagate detection errors through a graph neural network that can `learn' to extract the spectrum of appropriate operators. This is made possible by considering a family of graph operators that work effectively in sparsely connected graphs, in particular by considering a hierarchical extension that uses the non-backtracking operator in the line graph. We also provide a theoretical analysis of the optimization landscapes in the linearized regime, which shows an interesting transition from rugged to simple as the size of the graphs increase under appropriate concentration conditions.
One word of caution is that our empirical results are inherently non-asymptotic. Whereas models trained for given graph sizes can be used for inference on arbitrarily sized graphs (owing to the parameter sharing of GNNs), further work is needed in order to understand the generalisation properties as |V | increases. Nevertheless, we believe our work opens up interesting questions, namely better understanding how our results on the energy landscape depend upon specific signal-to-noise ratios, or whether the network parameters can be interpreted mathematically. This could be useful in the study of computational-to-statistical gaps, where our model can be used to inquire about the form of computationally tractable approximations. Other directions of future research include the extension to the case where the number of communities is unknown and variable, and potentially increasing with |V |, as well as applications to ranking and edge-cut problems.
REFERENCES

9

Under review as a conference paper at ICLR 2019
Emmanuel Abbe. Community detection and stochastic block models: recent developments. arXiv preprint arXiv:1703.10146, 2017.
Emmanuel Abbe, Afonso S. Bandeira, and Georgina Hall. Exact recovery in the stochastic block model. arXiv:1405.3267v4, 2014.
Peter Battaglia, Razvan Pascanu, Matthew Lai, Danilo Jimenez Rezende, et al. Interaction networks for learning about objects, relations and physics. In Advances in Neural Information Processing Systems, pp. 4502­4510, 2016.
Michael M Bronstein, Joan Bruna, Yann LeCun, Arthur Szlam, and Pierre Vandergheynst. Geometric deep learning: going beyond euclidean data. IEEE Signal Processing Magazine, 2017.
Joan Bruna, Wojciech Zaremba, Arthur Szlam, and Yann LeCun. Spectral networks and locally connected networks on graphs. arXiv:1312.6203., 2013.
Amin Coja-Oghlan, Florent Krzakala, Will Perkins, and Lenka Zdeborova. Information-theoretic thresholds from the cavity method. arXiv preprint arXiv:1611.00814, 2016.
Aurelien Decelle, Florent Krzakala, Cristopher Moore, and Lenka Zdeborová. Asymptotic analysis of the stochastic block model for modular networks and its algorithmic applications. Physical Review E, 84(6):066106, 2011.
Michaël Defferrard, Xavier Bresson, and Pierre Vandergheynst. Convolutional neural networks on graphs with fast localized spectral filtering. In Advances in Neural Information Processing Systems, pp. 3837­3845, 2016.
David Duvenaud, Dougal Maclaurin, Jorge Aguilera-Iparraguirre, Rafael Gómez-Bombarelli, Timothy Hirzel, Alán Aspuru-Guzik, and Ryan P Adams. Convolutional networks on graphs for learning molecular fingerprints. In Neural Information Processing Systems, 2015.
Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. Neural message passing for quantum chemistry. arXiv preprint arXiv:1704.01212, 2017.
M. Gori, G. Monfardini, and F. Scarselli. A new model for learning in graph domains. In Proc. IJCNN, 2005.
Karol Gregor and Yann LeCun. Learning fast approximations of sparse coding. ICML, 2010.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 770­778, 2016.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. arXiv preprint arXiv:1502.03167, 2015.
Kenji Kawaguchi. Deep learning without poor local minima. In Advances in Neural Information Processing Systems, pp. 586­594, 2016.
Steven Kearnes, Kevin McCloskey, Marc Berndl, Vijay Pande, and Patrick Riley. Molecular graph convolutions: moving beyond fingerprints. Journal of computer-aided molecular design, 30(8): 595­608, 2016.
Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.
Thomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. arXiv preprint arXiv:1609.02907, 2016.
Risi Kondor, Hy Truong Son, Horace Pan, Brandon Anderson, and Shubhendu Trivedi. Covariant compositional networks for learning graphs. arXiv preprint arXiv:1801.02144, 2018.
10

Under review as a conference paper at ICLR 2019
Florent Krzakala, Cristopher Moore, Elchanan Mossel, Joe Neeman, Allan Sly, Lenka Zdeborová, and Pan Zhang. Spectral redemption in clustering sparse networks. Proceedings of the National Academy of Sciences, 110(52):20935­20940, 2013.
Yujia Li, Daniel Tarlow, Marc Brockschmidt, and Richard Zemel. Gated graph sequence neural networks. arXiv preprint arXiv:1511.05493, 2015.
Laurent Massoulié. Community detection thresholds and the weak ramanujan property. In Proceedings of the forty-sixth annual ACM symposium on Theory of computing, pp. 694­703. ACM, 2014.
Elchanan Mossel, Joe Neeman, and Allan Sly. A proof of the block model threshold conjecture. arXiv:1311.4115, 2014.
Mark EJ Newman. Modularity and community structure in networks. Proceedings of the national academy of sciences, 103(23):8577­8582, 2006.
Mathias Niepert, Mohamed Ahmed, and Konstantin Kutzkov. Learning convolutional neural networks for graphs. In International conference on machine learning, pp. 2014­2023, 2016.
Oliver Riordan and Nicholas Wormald. The diameter of sparse random graphs. Combinatorics, Probability and Computing, 19(5-6):835­926, 2010.
Alaa Saade, Florent Krzakala, and Lenka Zdeborová. Spectral clustering of graphs with the bethe hessian. In Advances in Neural Information Processing Systems, pp. 406­414, 2014.
Abishek Sankararaman and François Baccelli. Community detection on euclidean random graphs. In Proceedings of the Twenty-Ninth Annual ACM-SIAM Symposium on Discrete Algorithms, pp. 2181­2200. SIAM, 2018.
Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Hagenbuchner, and Gabriele Monfardini. The graph neural network model. IEEE Trans. Neural Networks, 20(1):61­80, 2009.
Ohad Shamir. Are resnets provably better than linear predictors? arXiv preprint arXiv:1804.06739, 2018.
Dan Spielman. Spectral graph theory, am 561, cs 662, 2015.
Sainbayar Sukhbaatar, Rob Fergus, et al. Learning multiagent communication with backpropagation. In Advances in Neural Information Processing Systems, pp. 2244­2252, 2016.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Information Processing Systems, pp. 5998­6008, 2017.
Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua Bengio. Graph attention networks. arXiv preprint arXiv:1710.10903, 2017.
Jaewon Yang and Jure Leskovec. Defining and evaluating network communities based on ground-truth. ICDM., 7(2):43­55, 2012a.
Jaewon Yang and Jure Leskovec. Community-affiliation graph model for overlapping network community detection. Proceeding ICDM '12 Proceedings of the 2012 IEEE 12th International Conference on Data Mining, 390(.):1170­1175, 2012b.
Jonathan S Yedidia, William T Freeman, and Yair Weiss. Understanding belief propagation and its generalizations. Exploring artificial intelligence in the new millennium, 8:236­239, 2003.
Pan Zhang. Robust spectral detection of global structures in the data by learning a regularization. In Arxiv preprint, pp. 541­549, 2016.
11

Under review as a conference paper at ICLR 2019

A PROOF OF THEOREM 5.1

We

recall

the

notations

used

in

Theorem

5.1:

L~n()

=

E T Yn
Xn,Yn T EXn

=

T EYn T EXn

is

the

mean-field

RlGoin-svs1eYfnunan(csRytin-mon1m). TeWt,rieAc¯wnmra=itteriExthAKenCh=oRlReMsn-k×1yMEd,Yewnc(oeRmlen-pt 1o)s1Ti(t,iKoBn)n,of=2E(KXRn-)n,1a.X.s.,nE(XMRn(n-K1=))TRd, enanRnodnTte,tahBnedneidg=eefinBnveanlAu-ensIo=nf.

K in nondecreasing order.

First, we have

|Ln(l) - Ln(g)|  |Ln(l) - L~n(l)| + |L~n(l) - L~n(g)| + |L~n(g) - Ln(g)| (6)

Let us denote by ~g a global minimum of the mean-field loss L~n. Taking a step further, we can extend this bound to the following one (the difference is in the second term on the right hand side):

Lemma A.1.

|Ln(l) - Ln(g)|  |Ln(l) - L~n(l)| + |L~n(l) - L~n(~g)| + |L~n(g) - Ln(g)| (7)

Proof of Lemma A.1. We consider two separate cases: The first case is when L~n(l)  L~n(g). Then L~n(l) - L~n(~g)  L~n(l) - L~n(g)  0, and so |Ln(l) - Ln(g)|  |Ln(l) - L~n(l)| + |L~n(l) - L~n(~g)| + |L~n(g) - Ln(g)|.
The other case is when L~n(l) < L~n(g). Note that Ln(l)  Ln(g). Then |Ln(l) - Ln(g)|  |Ln(l) - L~n(l)| + |L~n(g) - Ln(g)|  |Ln(l) - L~n(l)| + |L~n(l) - L~n(~g)| + |L~n(g) - Ln (g )|.

Hence, to bound the "energy gap" |Ln(l) - Ln(g)|, if suffices to bound the three terms on the right hand side of Lemma A.1 separately. First, we consider the second term, |L~n(l) - L~n(~g)|.
Let l = RnT l, g = RnT g and ~g = RnT ~g. Define Sn() = Ln(Rn-T ) and S~n() = L~n(Rn-T ), for any   RM . Thus, we apply a change-of-variable and try to bound |S~n(l) - S~n (~g )|.
Since l is a local maximum of Ln, 1(2Ln(l))  0. Since 2Sn(l) = Rn-12Ln(l)Rn-T , where Rn is invertible, we know that 1(2Sn(l))  0, thanks to the following lemma: Lemma A.2. If R, Q  RM×M , R is invertible, Q is symmetric and q > 0 is an eigenvalue of Q, then 1(RQRT )   · M (RRT )

Proof of Lemma A.2. Say Qw = w for some vector w  RM . Let v = R-T w. Then

vT (RQRT )v = wT Qw =  w 2. Note that w 2 = vT RRT v  v 2M (RRT ). Hence

1(RQRT )



vT (RQRT )v v2



w

w2 2/M (RRT )



 · M (RRT )

Since 2Sn(l) = 2S~n(l) + (2Sn(l) - 2S~n(l)), there is 0  1(2Sn(l))  1(2S~n(l)) - 2Sn(l) - 2S~n(l) . Hence,

1(2S~n(l))  2Sn(l) - 2S~n(l)

(8)

Next, we relate the left hand side of the inequality above to cos(l, ~g), thereby obtaining an upper bound on [1 - cos2(l, ~g)], which will then be used to bound |S~n(l) - S~n(~g)|.
Lemma A.3.   Rd,

1(2S~n()) 

2 

2

{[1

-

cos2(,

~g )]

·

[1(A¯n)

-

2(A¯n)]

-

2



· S~n() }

12

Under review as a conference paper at ICLR 2019

Proof of Lemma A.3.

2S~n() =2E

(T )An - (T An)I (T )2

+

4(T An)T - 4(T )AnT (T )3

=2E

(T )An - (T An)I (T )2

+

4[(T )An - (T An)I]T (T )3

(9)

=2

(T )A¯n - (T A¯n)I (T )2

+

4[(T )A¯n - (T A¯n)I]T (T )3

Thus, if we define Q1 = (T )[(T )A¯n - (T A¯n)I], Q2 = 4[(T )A¯n - (T A¯n)I]T , we

have

2S~n() =

2  6 (Q1 - Q2)

(10)

To bound 1(2S~n()), we bound 1(Q1) and Q2 as follows:

Since A¯n is symmetric, let ^1, . . . ^M be the orthonormal eigenvectors of A¯n corresponding to

nonincreasing eigenvalues l1, . . . lM . Note that the global minimum satisfies ~g = ±^1. Write

=

M i=1

i^i,

and

let

¯i

=



i
M i=1

i2

.

Then

|

cos(,

~g )|

=

|

cos(,

^1)|

=

|¯1|.

Then,

MM
1(Q1) =(T ) l1 i2 - lii2
i=1 i=1

M
(T ) ( i2) - 12 (l1 - l2)
i=1
=(T )2[(1 - ¯12)(l1 - l2)]

(11)

To bound Q2 :

MM

M

[(T )A¯n - (T A¯n)I] =

lk i2 - lii2

k=1 i=1

i=1

Note that given vectors v, w  RM ,

v · wT = |vT w|

k ^k

(12)

Therefore,

Q2 =4

M
k ^k
k=1

T

MM

M

[lk( i2) - ( lii2)]k^k

k=1 i=1

i=1

=4 (T )2 T S~() 2

2(T )2  S~()

(13)

Thus, 1(Q1 - Q2) 1(Q1) - Q2 (T )2([(1 - ¯12)(l1 - l2)] - 2 
This yields the desired lemma.

S() )

(14)

Combining inequality 8 and Lemma A.3, we get

1 - cos2(l, ~g)  2 l

·

S~n(l)

+

l 2 2

2Sn(l) - 2S~n(l)

1(A¯n) - 2(A¯n)

(15)

Thus, to bound the angle between l and ~g, we can aim to bound S~n(l) and 2Sn(l) - 2S~n(l) as functions of the quantities µn, n and n.

13

Under review as a conference paper at ICLR 2019

Lemma A.4.

l · S~n(l)  2µnnn(1 + 3n + n)

(16)

Proof of Lemma A.4.

Sn()

=

2E



An T Bn



-

2E

(T An)Bn (T Bn)2

S~n()

=

2E

An T 

-

2E

(T An) (T )2

(17) (18)

Combining equations 17 and 18, we get

Sn() - S~n() = E

2(T  - T Bn)An (T Bn)(T )

-

2(T An)[(T )2Bn - (T Bn)2] (T Bn)2(T )2

(19)

Since Sn(l) = 0, we have

S~n(l)

=

E

2(lT l - lT Bnl)Anl (lT Bnl)(lT l)

-

2(lT Anl)[(lT l)2Bnl - (lT Bnl)2l] (lT Bnl)2(lT l)2



2 l

E

|1(An)| Bn |M (Bn)|

+

3

|1

(An)| Bn 2M (Bn)

+

|1(An)| Bn 2M (Bn)

2

(20)

Then, by the generalized Hölder's inequality,

S~n(l)

2 l

1

E|1(An)|3E

Bn

3E

|M

1 (Bn)|3

3

1

+3

E|1(An)|3E

Bn

3E

|M

1 (Bn

)|6

3

1

+

E|1(An)|3E

Bn

6E

|M

1 (Bn

)|6

3

.

Hence, written in terms of the quantities µn, n and n, we have

l · S~n(l) 2(µnnn + 3µnn2n + µnn2 n2) =2µnnn(1 + 3n + n)

(21) (22)

Lemma A.5.

With n = (E

Bn

6

)

1 6

,

E|1(Bn)|6



64

+

63n6

Proof of Lemma A.5.

E|1(Bn)|6 =E Bn 6

=E I + Bn 6

E( I + Bn )6

=E(1 + Bn )6

Note that

gma

E(1 + X)6 = EX6 + 6EX5 + 15EX4 + 20EX3 + 15EX2 + 6EX + 1

and for k  {1, 2, 3, 4, 5}, if X is a nonnegative random variable,

EXk =1X>1EXk + 1X1EXk

1 + 1X1EX6

1 + EX6

Therefore, E|1(Bn)|6  64 + 63E Bn 6.

(23) (24)
(25)

14

Under review as a conference paper at ICLR 2019

From

now

on,

for

simplicity,

we

introduce

n

=

(64

+

63n6

)

1 6

,

as

a

function

of

n.

Lemma A.6.   RM ,

l 2 · 2Sn() - 2S~n() µnnn(10 + 14n + 2nn + 16n2 + 16nn

+ 8nn2 + 8nn + 8nn)

(26)

Proof of Lemma A.6.

2Sn() - 2S~n() =2E[H1] - 2E[H2] + 8E[H2] - 8E[H4]

(27)

where

H1

=

(T )An - (T Bn)An (T Bn)(T )

(28)

H2

=

(T An)[(T )2Bn - (T Bn)2]I) (T B)2(T )2

(29)

H3

=

(T An)[(T )3BnT BnT - (T Bn)3T ] (T Bn)3(T )3

(30)

H4

=

(T )2AnT Bn - (T Bn)2AT (T Bn)2(T )2

(31)

Thus, 2Sn() - 2S~n()  2E H1 + 2E H2 + 8E H3 + 8E H4 , and we try to bound

each term on the right hand side separately.

For the first term, there is

H1



1 2

Bn |1(An)| |M (Bn)|

(32)

Applying generalized Hölder's inequality, we obtain

1

 2 · E H1 

1 E |M (Bn)|3

3

(E|1

(An

)|3

)

1 3

(E

Bn

3

)

1 3

µnnn .

(33)

For the second term, there is

H2

=

(T An)[(T )2Bn - 2(T )(T Bn)I (T Bn)2(T )2

-

(T Bn)2I]

(34)

Hence,

H2



11  2 M2 (Bn) |1(An)|(3 Bn

+

Bn 2)

Applying generalized Hölder's inequality, we obtain

 2 · E H2

1



3 E |M (Bn)|6

3

(E|1

(An

)|3

)

1 3

(E

Bn

3

)

1 3

1

+

3 E |M (Bn)|6

3

(E|1

(An

)|3

)

1 3

(E

Bn

6

)

1 3

µnnn(3n + nn)

(35) (36)

For H3, note that

(T )3BnT BnT - (T Bn)3T =(T )3(Bn - I)T Bn + (T )3T (Bn - I)

+ [(T )3 - (T Bn)3]T

=(T )3BnT Bn + (T )3T Bn

+ [(T Bn)2(-T Bn)T + (T Bn)(-T Bn)T

+ (-T Bn)T ]

(37)

15

Under review as a conference paper at ICLR 2019

Hence,

H3 =(T An)

(T )3BnT Bn + (T )3T Bn + (-T Bn)T (T Bn)3(T )3

+

(-T Bn)T (T Bn)2(T )

+

(-T Bn)T (T Bn)(T )2

(38)

Thus,

H3



|1(An)| 2

1 |3M (Bn)| (

Bn

|1(Bn)| + 2

Bn

1 ) + M2 (Bn)

Bn

1

+ |M (Bn)|

Bn

(39)

Applying generalized Hölder's inequality, we obtain

1

 2 · E H3 

1 E |M (Bn)|6

2

(E|1

(An

)|6

)

1 6

(E

Bn

6

)

1 6

(E|1

(Bn

)|6

)

1 6

1

+2

1 E |M (Bn)|6

2

(E|1

(An

)|3

)

1 3

(E

Bn

6

)

1 6

1

+

1 E |M (Bn)|6

3

(E|1

(An

)|3

)

1 3

(E

Bn

3

)

1 3

1

+

1 E |M (Bn)|3

3

(E|1

(An

)|3

)

1 3

(E

Bn

3

)

1 3

µnnn(nn2 + 2n2 + n + 1)

(40)

For the last term,

H4

=

[-2(T )(T Bn)I

-

(T Bn)2I]AnT Bn (T Bn)2(T )2

+

(T Bn)2AnT Bn

(41)

Thus,

H4



1 2

1 M2 (Bn) (2 Bn

+

Bn

2)|1(An)||1(Bn)|

+

M2

1 (Bn

)

|21

(Bn)||1

(An

)|

Bn

(42)

Applying generalized Hölder's inequality, we obtain

1

 2 · E H4 2

1 E |M (Bn)|6

3

(E|1

(An

)|3

)

1 3

(E

Bn

6

)

1 6

(E|1

(Bn

)|6

)

1 6

1

+

1 E |M (Bn)|6

3

(E|1

(An

)|6

)

1 6

(E

Bn

6

)

1 3

(E|1

(Bn

)|6

)

1 6

1

+

1 E |M (Bn)|6

3

(E|1

(An

)|6

)

1 6

(E

Bn

6

)

1 6

(E|1

(Bn

)|6

)

1 3

µnnn(2nn + nnn + n2n)

(43)

Therefore, summing up the bounds above, we obtain l 2 · 2Sn() - 2S~n() µnnn(10 + 14n + 2nn + 16n2 + 16nn + 8nn2 + 8nn + 8nn)

(44)

Hence, combining inequality 15, Lemma A.4 and Lemma A.6, we get

1

-

cos2(l, ~g)

n[4µnnn(1

+

3nnµn)

+

1 2 µnnn(10

+

14n

+

2nn

+

16n2

+ 16nn + 8nn2 + 8nn + 8nn)]

=µnnnn(9 + 19n + 5nn + 8n2 + 8nn + 4nn2 + 4nn + 4nnn) (45)

16

Under review as a conference paper at ICLR 2019

For simplicity, we define C(n, n) = 9+19n +5nn +8n2 +8nn +4nn2 +4nn +4nnn.

Thus,

1 - cos2(l, ~g)  µnnnnC(n, n)

(46)

Following the notations in the proof of Lemma A.3, we write l = and | cos(, ^i)| = |¯i|. Thus,

L~n(l) =S~n(l)

=

M i=1

i2li

M i=1

i2

=

M
¯i2li
i=1

M i=1

i^i.

Note

that

~g

=

±^1,

(47)

Since Yn is positive semidefinite, EYn is also positive semidefinite, and hence A¯n = RnT EYn(Rn-1)T is positive semidefinite as well. This means that li  0, i  {1, ..., M }. Since L~n(~g) = S~n(~g) = S~n(^1) = l1, there is

|L~n(~g) - L~n(l)|  (1 - ¯12)l1  (1 - cos2(l, ~g))1(A¯n)

(48)

Next, we bound the first and the third term on the right hand side of the inequality in Lemma A.1.

Lemma A.7. ,

1

|Ln() - L~n()|  (E

Bn

3

)

1 3

·

(E|1

(An

)|3

)

1 3

·

E|

M

1 (Bn)

|3

3

(49)

Proof of Lemma A.7. Let  = TnT .

|Ln() - L~n()| =Sn() - S~n()

=

(T Bn)(T An) E (T Bn)(T )

E

Bn |1(An)| |M (Bn)|

Thus, we get the desired lemma by the generalized Hölder's inequality.

(50)

Combining inequality 46, inequality 48 and Lemma A.7, we get

|Ln(l) - Ln(g)| 2(E

Bn

3

)

1 3

·

(E|1(An)|3)

1 3

·

13 E M (Bn)

2µnnn + µnnnnC(n, n) · 1(A¯n)

1 3
+ (1 - cos2(l, ~g))1(A¯n)
(51)

Meanwhile,

|Ln(g) - L~n(~g)|  max{|Ln(g) - L~n(g)|, |Ln(~g) - L~n(~g)|}

(E

Bn

3

)

1 3

·

(E|1

(An

)|3

)

1 3

·

(E|

M

1 (Bn

)

|3

)

1 3

µnnn

Hence,

Ln(g) L~n(~g) - µnnn 1(A¯n) - µnnn n-1 - µnnn

(52) (53)

17

Under review as a conference paper at ICLR 2019

, or

1(A¯n)  Ln(g) + µnnn

(54)

Therefore,

|Ln(l) - Ln(g)| 2µnnn + (1 - cos2(l, ~g))[Ln(g) + µnnn] µnnn[2 + nµnnnC(n, n)] + nµnnnC(n, n)Ln(g)

Ln (g )

µnnn[2 + nµnnnC(n, n-1 - µnnn

n)]

+

n µn n n C (n ,

n)

=

2nµnnn[2 + C(n 1 - nµnnn

,

n)]

Ln(g

)

(55)

Hence, we have proved the theorem, with

= .n,µn,n,n

2n µn n n [2+C (n ,n )] 1-n µn n n

B BACKGROUND

B.1 GRAPH MIN-CUTS AND SPECTRAL CLUSTERING

We consider graphs G = (V, E), modeling a system of N = |V | elements presumed to exhibit some form of community structure. The adjacency matrix A associated with G is the N × N binary matrix such that Ai,j = 1 whenever (i, j)  E. We assume for simplicity undirected graphs, yielding symmetric adjacency matrices. The community structure is encoded in a discrete label vector s : V  {1, K} that assigns a community label to each node, and the goal is to estimate s from
observing the adjacency matrix.

In the setting of binary, associative communities, where s(i) = ±1, two nodes i, j with s(i) = s(j) are more likely to be connected (Ai,j = 1) than two nodes i, j with s(i) = s(j). Thus a quantity of the form
(1 - s(i)s(j))Ai,j
i,j
measures the cost associated with cutting the graph between communities encoded by s that we wish to minimize under appropriate constraints (Newman, 2006). Note that i,j Ai,j = sT Ds, with D = diag(A1) (called the degree matrix), so the cut cost can be expressed as a positive semidefinite quadratic form
min sT (D - A)s = sT s
s(i)=±1
that we wish to minimize. This shows a fundamental connection between the community structure and the spectrum of certain linear operators of the graph, which provides a powerful and stable relaxation of the discrete combinatorial optimization problem of estimating the community labels for each node. In the case of the graph Laplacian  = D - A, its eigenvector associated with the smallest eigenvalue is trivial, but its Fiedler vector (the eigenvector associated with the second smallest eigenvalue) reveals important community information of the graph (Newman, 2006) under appropriate conditions, and is associated with the graph conductance (Spielman, 2015) under certain normalization schemes.

For a given linear operator L(A) extracted from the graph (that we assume symmetric), we are thus
interested in extracting eigenvectors at the edge of its spectrum. A particularly simple algorithmic framework is given by the power iteration method. Indeed, the Fiedler vector of L(A) can be obtained by first extracting the leading eigenvector v of A~ = L(A) - L(A), and then iterating

y(n) = A~w(n-1) , w(n) =

y(n) - y(n), v v y(n) - y(n), v v

.

Unrolling power iterations and recasting the resulting model as a trainable neural network is akin to the LISTA (Gregor & LeCun, 2010) sparse coding model, which unrolled iterative proximal splitting algorithms.

Despite the appeal of graph Laplacian spectral approaches, it is well known (Krzakala et al., 2013) that these methods fail in sparsely connected graphs. Indeed, in such scenarios, the eigenvectors of

18

Under review as a conference paper at ICLR 2019

graph Laplacians concentrate on nodes with dominant degree, losing their ability to correlate with community structure. In order to overcome this important limitation, authors have resorted to ideas inspired from statistical physics, as explained next.

B.2 PROBABILISTIC GRAPHICAL MODELS AND BELIEF-PROPAGATION

Graphs with labels on nodes and edges can be cast as a graphical model where the aim of clustering is to optimize label agreement. This can be seen as a posterior inference task. If we simply assume the graphical model is a Markov Random Field (MRF) with trivial compatibility functions for cliques greater than 2, the probability of a label configuration  is given by

1

P() = Z

i(i)

ij (i, j ).

iV ijE

(56)

Generally, computing marginals of multivariate discrete distributions is exponentially hard. For instance, in the case of P(i) we are summing over |X|n-1 terms (where X is the state space of discrete variables). But if the graph is a tree, we can factorize the MRF more efficiently to compute the marginals in linear time via a dynamic programming method called the sum-product algorithm, also known as belief propagation (BP). An iteration of BP is given by

1 bij (i) = Zij i(i) ki\j kX ik(i, k)bki(k).

(57)

The beliefs (bij(i)) are interpreted as the marginal distributions of i. Fixed points of BP can be used to recover marginals of the MRF above. In the case of the tree, the correspondence is exact: Pi(i) = bi(i). Some sparse graphs, like the Stochastic Blockmodel with constant degree (MNS) are locally similar to trees for such an approximation to be successful. BP approximates the MLE solutions but convergence is not guaranteed in graphs that are not trees. Furthermore, in order to apply BP, we need a generative model and the correct parameters of the model. If unknown, the parameters can be derived using expectation maximization, further adding complexity and instability to the method since iterations may learn parameters for which BP does not converge.

B.3 NON-BACKTRACKING OPERATOR AND BETHE HESSIAN
The BP equations have a trivial fixed-point where every node takes equal probability in each group. Linearizing the BP equation around this point is equivalent to spectral clustering using the nonbacktracking matrix (NB), a matrix defined on the edges of the graph that indicates whether two edges are adjacent and do not coincide. Spectral clustering using NB gives significant improvements over spectral clustering with versions of the Laplacians (L) and the adjacency matrix (A); High degree fluctuations drown out the signal of the informative eigenvalues in the case of A and L, whereas NB's eigenvalues are confined to a disk in the complex plane, so its eigenvalues corresponding to community structure lay outside the disk and are easily distinguishable.
NB matrices are still not optimal in that they are matrices on the edge set, and are not symmetric (so cannot enjoy tools of numerical linear algebra for symmetric matrices). Recently (Saade et al., 2014) showed that a spectral method can do as well as BP in this regime, using the Bethe Hessian operator given by BH(r) := (r2 - 1)I - rA + D (where r is a scalar value). This is due to a one-to-one correspondence between the fixed points of BP and the stationary points of the Bethe free energy (corresponding Gibbs energy of the Bethe approximation) (Saade et al., 2014). The Bethe Hessian is a scaling of the Hessian of the Bethe free energy at an extrema corresponding to the trivial fixed point of BP. Negative eigenvalues of BH(r) correspond to phase transitions in the Ising model where new clusters become identifiable. This all gives theoretical motivation for why [I, D, A] defined in Section 3 are a good family of generators to do spectral clustering on. In the case of the SBM, they generate the Bethe Hessian which can achieve community detection down to the information theoretic threshold. The GNN is capable of expressing spectral approximations of complicated functions of [I, D, A], and performing nonlinear power method iterations in order to infer global structure (for instance community structure). Furthermore, unlike belief propagation, the method does not require a generative model, oftentimes requires a lot statistical analysis to motivate and is exposed to model misspecifications when deployed on real data. Instead, our framework finds structure in a data driven way, learning it from the available training data.

19

Under review as a conference paper at ICLR 2019

B.4 STOCHASTIC BLOCK MODEL

We briefly review the main properties needed in our analysis, and refer the interested reader to (Abbe, 2017) for an excellent recent review. The Stochastic Blockmodel (SBM) is a random graph model denoted by SBM (n, p, q, C). Implicitly there is an F : V  {1, C} associated with each SBM graph, which assigns community labels to each vertex. One obtains a graph from this generative model by starting with n vertices and connecting any two vertices u, v independently at random with probability p if F (v) = F (u), and with probability q if F (v) = F (u). We say the SBM is balanced if the communities are the same size. Let F¯n : V  {1, C} be our predicted community labels for SBM (n, p, q, C), Fn's give exact recovery on a sequence {SBM (n, p, q)}n if P(Fn = F¯n) n 1, and give detection  > 0 : P(|Fn - F¯n|  1/k + ) n 1 (i.e F¯n's do better than random guessing).

It is harder to tell communities apart if p is close to q (if p = q we just get an Erdos Renyi random

graph, which has no communities). In the two community case, It was shown that exact recovery

is

possible

on

SBM (n, p

=

a

log n

n

,

q

=

b

log n

n

)

if

and

only

if

a+b 2



1+

ab (Mossel et al., 2014;

Abbe et al., 2014). For exact recovery to be possible, p, q must grow at least O(log n) or else the

sequence of graphs will not to be connected, and thus vertex labels will be underdetermined. There

is no information-computation gap in this regime, so there are polynomial time algorithms when

recovery is possible (Abbe, 2017; Mossel et al., 2014)). In the much sparser regime of constant

degree

SBM

(n, p

=

a n

,

q

=

b n

),

detection

is

the

best

we

hope

for.

The

constant

degree

regime

is

also

of most interest to us for real world applications, as most large datasets have bounded degree and are

extremely sparse. It is also a very challenging regime; spectral approaches using the Laplacian in

its various (un)normalized forms and the adjacency matrix, as well as SDP methods cannot detect

communities in this regime (Abbe, 2017) due to large fluctuations in the degree distribution that

prevent eigenvectors form concentrating on the clusters. (Decelle et al., 2011) first proposed the BP

algorithm on the SBM, which was proven to yield Bayesian optimal values in (Coja-Oghlan et al.,

2016).

In the constant degree regime with balanced k communities, the Kesten-Stigum threshold is given by SN R := (a - b)2/(k(a + (k + 1)b)) (Abbe, 2017). It has been shown for k = 2 that SN R = 1 is
both the information theoretic and efficient computational threshold where belief propagation (BP) via a polynomial time algorithm. For k > 4 a gap emerges between the information theoretic threshold and computational one. It's conjectured that no polynomial time algorithm exist for SN R < 1, while a BP algorithm works for SN R > 1 (Abbe, 2017; Decelle et al., 2011). The existence of the gap was shown by (Abbe, 2017) by proving a non-polynomial algorithm can do detection for some SN R < 1.

C GEOMETRIC BLOCK MODEL

The success of Belief-Propagation on

Table 3: Overlap performance (in percentage) of the GNN model the SBM relies on its locally hyperon the Geometric Block Model compared with several Spectral bolic properties, which make it tree-

Approaches

like with high probability. This be-

havior is completely different if one

Model

S=1

S=2

S=4

considers random graphs with locally

Norm. Laplacian Bethe Hessian GNN G
GNN {G, L(G)}

1 ± 0.5 18 ± 1 20 ± 0.4 22 ± 0.4

1 ± 0.6 38 ± 1 39 ± 0.5 50 ± 0.5

1±1 38 ± 2 39 ± 0.5 76 ± 0.5

Euclidean geometry. The Geometric Block Model (Sankararaman & Baccelli, 2018) is a random graph generated as follows. One starts by sam-

pling n iid points x1, . . . , xn from a Gaussian mixture model given by means µ1, . . . µk  Rd at distance S apart and identity covariances.

The labels correspond to which Gaussian each sample belongs to. We draw an edge between two nodes i, j if xi - xj  T / n. Due to the triangle inequality, the model contains a large number of

short cycles, impacting the performance of loopy belief propagation. This motivates other estimation

algorithms based on motif-counting (Sankararaman & Baccelli, 2018) that require knowledge of the

model likelihood function. Table 3 shows the performance of the GNN models on the binary GBM model, obtained with d = 2, n = 500, T = 5 2 and varying S, and its comparison with several

spectral methods, including the Bethe Hessian, which approximates BP around its stationary solution.

We verify that the line GNN model, thanks to its added flexibility and the multiscale nature of its

generators, is able to significantly outperform both spectral methods as well as the baseline GNN.

20

Under review as a conference paper at ICLR 2019
Figure 4. GNN mixture (Graph Neural Network trained on amixture of SBMwith average degree 3), GNN full mixture (GNN trained over different SNR regimes), BH( d¯) and BH(- d¯). left: k = 2. We verify that BH(r) models cannot perform detection at both ends of the spectrum simultaneously.
D FURTHER EXPERIMENTS ON THE SBM
We report here our experiments on the SBM mixture, generated with G  SBM (n = 1000, p = kd¯- q, q  Unif(0, d¯- d¯), C = 2) ,
where the average degree d¯is either fixed constant or also randomized with d¯  Unif(1, t). Figure 4 shows the overlap obtained by our model compared with several baselines. Our GNN model is either competitive with BH or beats BH, which achieves the state of the art along with BP (Saade et al., 2014), despite not having any access to the underlying generative model (especially in cases where GNN was trained on a mixture of SBM and thus must be able to generalize the r parameter in BH). They all beat by a wide margin spectral clustering methods using the symmetric Laplacian (Ls) and power method (pm) applied to BH I - BH using the same number of layers as our model. Thus GNN's ability to predict labels goes beyond approximating spectral decomposition via learning the optimal r for BH(r). The model architecture allows it to learn a higher dimensional function of the optimal perturbation of the multiscale adjacency basis, as well as nonlinear power iterations, that amplify the informative signals in the spectrum; In a data driven way it can generalize the problem without needing to study a generative model.
21


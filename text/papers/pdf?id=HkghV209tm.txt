Under review as a conference paper at ICLR 2019

OPTIMISTIC ACCELERATION FOR OPTIMIZATION
Anonymous authors Paper under double-blind review

ABSTRACT
We consider new variants of optimization algorithms. Our algorithms are based on the observation that mini-batch of stochastic gradients in consecutive iterations do not change drastically and consequently may be predictable. Inspired by the similar setting in online learning literature called Optimistic Online learning, we propose two new algorithms, OPTIMISTIC-AMSGRAD and OPTIMISTIC-ADAM that exploit the predictability of gradients. OPTIMISTIC-AMSGRAD and OPTIMISTIC-ADAM combine the idea of momentum method, adaptive gradient method, and algorithms in Optimistic Online learning, which leads to speed up in training deep neural nets in practice.

1 INTRODUCTION

Nowadays deep learning has been shown to be very effective in several tasks, from robotics (e.g. Levine et al. (2017)), computer vision (e.g. He et al. (2016); Goodfellow et al. (2014)), reinforcement learning (e.g. Mnih et al. (2013), to natural language processing (e.g. Graves et al. (2013)). Typically, the model parameters of a state-of-the-art deep neural net is very high-dimensional and the required training data is also in huge size. Therefore, fast algorithms are necessary for training a deep neural net. To achieve this, there are number of algorithms proposed in recent years, such as AMSGRAD (Reddi et al. (2018)), ADAM (Kingma & Ba (2015)), RMSPROP (Tieleman & Hinton (2012)), ADADELTA (Zeiler (2012)), and NADAM (Dozat (2016)), etc.

All the prevalent algorithms for training deep nets mentioned above combines two ideas: the idea of adaptivity in ADAGRAD (Duchi et al. (2011); McMahan & Streeter (2010)) and the idea of momentum as NESTEROV'S METHOD (Nesterov (2004)) or the HEAVY BALL method (Polyak (1964)). ADAGRAD is an online learning algorithm that works well compared to the standard online gradient descent when the gradient is sparse. The update of ADAGRAD has a notable feature: the learning rate is different for different dimensions, depending on the magnitude of gradient in each dimension, which exploits the geometry of data and performs informative update. On the other hand, NESTEROV'SMETHOD or the Momentum Method (Polyak (1964)) is an accelerated optimization algorithm whose update not only depends on the current iterate and current gradient but also depends on the past gradients (i.e. momentum). State-of-the-art algorithms like AMSGRAD (Reddi et al. (2018)) and ADAM (Kingma & Ba (2015)) leverages these two ideas to get fast training for neural nets.

In this paper, we propose an algorithm that goes further than the hybrid of the adaptivity and momentum approach. Our algorithm is inspired by Optimistic Online learning (Chiang et al. (2012); Rakhlin & Sridharan (2013); Syrgkanis et al. (2015); Abernethy et al. (2018)). Optimistic Online learning considers that a good guess of the loss function in the current round of online learning is available and plays an action by exploiting the good guess. By exploiting the

guess, those algorithms in Optimistic Online learning have regret in the form of O(

T t=1

gt - mt

), where gt is

the gradient of loss function in round t and mt is the "guess"of gt before seeing the loss function in round t (i.e. before getting gt). This kind of regret can be much smaller than O( T ) when one has a good guess mt of gt. We combine the

Optimistic Online learning idea with the adaptivity and the momentum ideas to design new algorithms in training deep

neural nets, which leads to OPTIMISTIC-AMSGRAD and OPTIMISTIC-ADAM. We also provide theoretical analysis

of OPTIMISTIC-AMSGRAD. The proposed OPTIMISTIC- algorithms not only adapt to the informative dimensions

and exhibit momentums but also take advantage of a good guess of the next gradient to facilitate acceleration. We

evaluate our algorithms with (Kingma & Ba (2015)), (Reddi et al. (2018)) and (Daskalakis et al. (2018)). Experiments

show that our OPTIMISTIC-algorithms are faster than the baselines.

2 PRELIMINARIES
Both AMSGRAD (Reddi et al. (2018)) and ADAM (Kingma & Ba (2015)) are actually Online learning algorithms. Their theoretical analysis are the regret analysis in online learning. Since one can convert an online learning algorithm

1

Under review as a conference paper at ICLR 2019

to an offline optimization algorithm by online-to-batch conversion (Cesa-Bianchi et al. (2004)), one can design an offline optimization algorithm by designing and analyzing its counterpart in online learning. Therefore, we would like to give a brief review of Online learning and Optimistic-Online learning.

2.1 BRIEF REVIEW OF ONLINE LEARNING

In the typical setting of online learning, there is a learner playing an action and then receiving a loss function in
each round t. Specifically, the learner plays an action wt  K in round t, where wt is chosen in a compact and convex set K  Rn, known as the decision space. Then, the learner sees the loss function t(·) and suffers loss t(wt) for the choice. No distributional assumption is made on the loss functions sequence { 1(·), 2(·), . . . , T (·)} in
online learning. Namely, the loss functions can be adversarial. The goal of an online learner is minimizing its regret,

which is

TT

RegretT :=

t(wt

)

-

min
wK

t(w).

t=1 t=1

(1)

We can also define average regret as RegretT

:=

RegretT T

,

which

is

regret

divided

by

number

of

rounds

T

.

In Online

learning literature, No-regret algorithms means online learning algorithms satisfying RegretT  0 as T  .

2.2 BRIEF REVIEW OF OPTIMISTIC ONLINE LEARNING

In recent years, there is a branch of works in the paradigm of Optimistic Online learning (e.g. Chiang et al. (2012);
Rakhlin & Sridharan (2013); Syrgkanis et al. (2015); Abernethy et al. (2018)). The idea of Optimistic Online learning is as follows. Suppose that, in each round t, the learner has a good guess mt(·) of the loss function t(·) before playing an action wt. (Recall that the learner receives the loss function after the learner commits an action!) Then, the learner should exploit the guess mt(·) to choose an action wt, as mt(·) is close to the true loss function t(·). 1

For example, (Syrgkanis et al. (2015)) proposed an optimistic-variant of the so called FOLLOW-THE-REGULARIZEDLEADER (FTRL). The update rule of FTRL (see e.g. Hazan (2016)) is

1

wt

=

arg min
wK

w, Lt-1

+ R(w), 

(2)

where R(·) is a 1-strong convex function with respect to a norm ( · ) on the constraint set K, Lt-1 :=

t-1 s=1

gs

is

the cumulative sum of gradient vectors of the loss functions (i.e. gs :=  s(ws) ) up to but not including t, and  is a

parameter. The OPTIMISTIC-FTRL of (Syrgkanis et al. (2015)) has update

1

wt

=

arg min
wK

w, Lt-1

+

mt

+ R(w), 

(3)

where mt is the learner's guess of the gradient vector gt :=  t(wt). Under the assumption that loss functions are

convex, the regret of OPTIMISTIC-FTRL satisfies RegretT  O(

T t=1

gt - mt

), which can be much smaller

than T of FTRL if one has a good guess. That is, if mt is close to gt, then OPTIMISTIC-FTRL will have much

smaller regret than FTRL. The concern is that how to get good mt. Apparently, if mt is far from gt, then the regret

of OPTIMISTIC-FTRL would be a constant factor worse than that of FTRL without optimistic update. In the later

section, we'll respond to the concern and provide a way to get mt. Here, we just want to emphasize the importance of leveraging a good guess mt for updating wt to get a fast convergence rate (or equivalently, small regret). We also note that the works of Optimistic Online learning Chiang et al. (2012); Rakhlin & Sridharan (2013); Syrgkanis et al.

(2015)) has been shown to accelerate the convergence of some zero-sum games.

2.3 ADAM AND AMSGRAD
ADAM (Kingma & Ba (2015)) is a very popular algorithm for training deep nets. It combines the idea of ADAGRAD (Duchi et al. (2011)), which has individual learning rate for different dimensions. The learning rate of ADAGRAD in iteration t for a dimension j is proportional to the inverse of ts=1gs[j]2, where gs[j] is the jth element of the gradient vector gs in time s. This adaptive learning rate may help for accelerating the convergence when the gradient vector is sparse (Duchi et al. (2011)). However, when applying ADAGRAD to train deep nets, it is observed that the
1Imagine that if the learner would had been known t(·) before committing its action, then it would exploit the knowledge to determine its action and minimizes the regret.

2

Under review as a conference paper at ICLR 2019

Algorithm 1 AMSGRAD (Reddi et al. (2018))

1: Required: parameter 1, 2, and t.

2: Init: w1.

3: for t = 1 to T do

4: Get mini-batch stochastic gradient vector gt  Rd at wt.

5: t = 1t-1 + (1 - 1)gt.

6: vt = 2vt-1 + (1 - 2)gt2.

7: v^t = max(v^t-1, vt).

8:

wt+1

=

wt

-

t

t v^t

.

(element-wise division)

9: end for

learning rate might decay too fast (Kingma & Ba (2015)). Therefore, (Kingma & Ba (2015)) proposes using a moving average of gradients (element-wise) divided by the root of the second moment of the moving average to update model parameter w (i.e. line 5,6 and line 8 of Algorithm 1). Yet, ADAM (Kingma & Ba (2015)) does not converge to some specific convex functions. AMSGRAD (Reddi et al. (2018)) fixes the issue. The algorithm of AMSGRAD is shown in Algorithm 1. The difference between ADAM and AMSGRAD lies on line 7 of Algorithm 1. ADAM does not have the update of line 7. (Reddi et al. (2018)) adds the step to guarantee a non-increasing learning rate, t . which helps for
v^t
the convergence (i.e. average regret RegretT  0.) For the parameters of AMSGRAD, it is suggested that 1 = 0.9, 2 = 0.99 and t = / t for a number .

3 OPTIMISTIC-AMSGRAD

In this section, we propose a new algorithm for training deep nets: OPTIMISTIC-AMSGRAD, shown in Algorithm 2.

OPTIMISTIC-AMSGRAD has an optimistic update, which is line 9 of Algorithm 2. It exploits the guess mt+1 of

gt+1 to get wt+1, since the vector ht+1 uses mt+1. Notice that the gradient vector is computed at wt instead of

wt-

1 2

and

the

moving

average

of

gradients

is

used

to

update

wt+

1 2

.

Also,

wt+

1 2

is

updated

from

wt-

1 2

instead

of

wt.

Therefore, while OPTIMISTIC-AMSGRAD looks like doing an additional optimistic update compared to AMSGRAD,

the

difference

of

update

is

subtle.

We

also

want

to

emphasize

that

although

the

learning

rate

on

line

9

contains

4 1-1

factor. We suspect that it is due to the artifact of our theoretical analysis. In our experiments, the learning rate on line

9

does

not

have

the

factor

of

4 1-1

.

That

is,

in

practice,

we

implement

line

9

as

wt+1

=

wt+

1 2

-

t+1

ht+1 v^t

.

We

leave

closing the gap between theory and practice as the future work.

We see that OPTIMISTIC-AMSGRAD inherits three properties

· Adaptive learning rate of each dimension as ADAGRAD (Duchi et al. (2011)). (line 8)
· Exponentially moving average of the past gradients as NESTROV'S METHOD (Nesterov (2004)) and the HEAVY-BALL method (Polyak (1964)). (line 5)
· Optimistic update that exploits a good guess of the next gradient vector as optimistic online learning algorithms (Chiang et al. (2012); Rakhlin & Sridharan (2013); Syrgkanis et al. (2015); Abernethy et al. (2018)). (line 9)

The first property helps acceleration when the gradient has sparse structure. The second one is the well-recognized idea of momentum which can achieve acceleration. The last one, perhaps less known outside the Online learning community, can actually achieve acceleration when the prediction of the next gradient is good. We are going to elaborate this property in the later section where we give the theoretical analysis of OPTIMISTIC-AMSGRAD.

To obtain mt, we use the extrapolation algorithm of (Scieur et al. (2016)). Extrapolation studies estimating the limit of sequence using the last few iterates (Brezinski & Zaglia (2013)). Some classical works include Anderson acceleration
(Walker & Ni. (2011)), minimal polynomial extrapolation (Cabay & Jackson (1976)), reduced rank extrapolation (Eddy (1979)). These method typically assumes that the sequence {xt}  Rd has a linear relation

xt = A(xt-1 - x) + x,

(4)

for an unknown matrix A  Rd×d (not necessarily symmetric). The goal is to use the last few iterates {xt} to estimate the fixed point x. (Scieur et al. (2016)) adapt the classical extrapolation methods to the iterates/updates of an optimization algorithm and propose an algorithm that produces a solution that is better than the last iterate of the

3

Under review as a conference paper at ICLR 2019

Algorithm 2 OPTIMISTIC-AMSGRAD

1: Required: parameter 1, 2, and t.

2: Init: w1.

3: for t = 1 to T do

4: Get mini-batch stochastic gradient vector gt  Rd at wt.

5: t = 1t-1 + (1 - 1)gt.

6: vt = 2vt-1 + (1 - 2)gt2.

7: v^t = max(v^t-1, vt).

8:

wt+

1 2

=

wt-

1 2

-

t

t v^t

.

(element-wise division)

9:

wt+1

=

wt+

1 2

-

t+1

4 1-1

,ht+1
v^t

where

ht+1

:=

1t-1

+ (1 - 1)mt+1

and

mt+1

is

the

guess

of

gt+1.

(In

practice,

use

wt+1

=

wt+

1 2

-

t+1

ht+1 v^t

.)

10: end for

Algorithm 3 REGULARIZED APPROXIMATE MINIMAL POLYNOMIAL EXTRAPOLATION (RMPE) (Scieur et al. (2016))

1: 2:

Input: some Compute

sequence matrix U

{xs  Rd}ss==r0, = [x1 - x0, . . .

parameter  , xr - xr-1]

> 

0 Rd×r

3: Obtain z by solving (U U + I)z = 1.

4: Get c = z/(z 1)

5: Output: ri=-01cixi, the approximation of the fixed point x.

underlying optimization algorithm in practice. The algorithm of (Scieur et al. (2016)) (shown in Algorithm 3) allows

the iterates {xt} to be nonlinear

xt - x = A(xt-1 - x) + et,

(5)

where et is a second order term (namely, satisfying the distance between the output and x is provided

et 2 = O( in (Scieur et

xt-1 - x al. (2016)).

22).

Some

theoretical

guarantees

regarding

In OPTIMISTIC-AMSGRAD, we use Algorithm 3 to get mt. Specifically, mt is obtained by

· Call Algorithm 3 with input consists of some past r + 1 gradients, {gt, gt-1, gt-2, . . . , gt-r} to obtain mt, where r is a parameter.
· Set mt := ri=-01cigt-r+i from the output of Algorithm 3.

For this extrapolation method to work well in predicting gt+1, the gradient vectors at a specific time span is assumed to be captured by (5). If the gradient does not change significantly, this will be a mild condition.
OPTIMISTIC-ADAM By removing line 7 in Algorithm 2, the step of making monotone weighted second moment, we obtain an algorithm which we call it OPTIMISTIC-ADAM, as the resulting algorithm can be viewed as an OPTIMISTICvariant of ADAM.

3.1 THEORETICAL ANALYSIS OF OPTIMISTIC-AMSGRAD

We provide the regret analysis here. We denote the Mahalanobis norm · H = ·, H· for some PSD matrix H. For the PSD matrix diag{v^t}, where diag{v^t} represents the diagonal matrix such that its ith diagonal element
is v^t[i] in Algorithm 2, we define the the corresponding Mahalanobis norm · t := ·, diag{v^t}1/2· , where we use the notation t to represent the matrix diag{v^t}-1/2. We can also define the the corresponding dual norm
· t := ·, diag{v^t}-1/2· .

We assume that the model parameter w is in d-dimensional space. That is, w  Rd. Also, the analysis of OPTIMISTIC-

AMSGRAD is for unconstrained optimization. Thus, we assume that the constraint K of the benchmark in the regret

definition, minwK

T t=1

t(w), is a finite norm ball that contains the optimal solutions to the underlying offline

unconstrained optimization problem.

4

Under review as a conference paper at ICLR 2019

Now we can conduct our analysis. First of all, we can decompose the regret as follows.

T TT

T

RegretT :=

t(wt) - min
wK

t(w)  wt - w,  t(wt) :=

wt - w, gt

t=1 t=1 t=1

t=1

TT

=

wt-

1 2

- w, gt

+

wt

-

wt-

1 2

,

gt

t=1 t=1

(6)

TT

T

=

wt-

1 2

- w, gt

+

wt

-

wt-

1 2

,

gt

-

ht

+

wt

-

wt-

1 2

,

ht

.

t=1 t=1

t=1

where the first inequality is by assuming that the loss function t(·) is convex and that we use the notation gt := t(wt) which we adopt throughout the following proof for brevity.

Given the decomposition, let us analyze the first term

T t=1

wt-

1 2

- w, gt

in (6).

Lemma 1. Denote D = maxt

wt-

1 2

-w

. We have that

T t t=1 (1-1)

t

2 t

+

D2

T t=1

.d 1v^t[i]1/2
i=1 2t(1-1)

T t=1

wt-

1 2

-w

,

gt



2T

1 (1-1

)

D2

d i=1

v^T

[i]2

+

Astute readers may realize that the bound in Lemma 1 is actually the bound of AMSGRAD. Indeed, since in online

learning setting the loss vectors gt come adversarially, it does matter how gt is generated. Therefore, the regret of

T t=1

wt-

1 2

- w, gt

can be bounded in the same way as AMSGRAD. In Appendix B, we provide the detail proof

of Lemma 1.

Now we switch to bound the other sums

T t=1

wt

-

wt-

1 2

,

gt

- ht

+

T t=1

wt

-

wt-

1 2

,

ht

in (6). The proof is

available in Appendix C.

Lemma 2.

T t=1

wt

-

wt-

1 2

,

gt

-

ht

w .2

t-

1 2

1-1 8

t-1

+

T t=1

wt

-

wt-

1 2

,

ht



T t t=1 2

gt - ht

-2

(

1-1 8

t-1 )

3 2t

wt -

Combining (6) and Lemma 1 and 2 leads to

RegretT



2T

1 (1 -

1)

D2

d
v^T [i]2
i=1

+

T t=1

t (1 - 1)

t

2 t

+

D2

T t=1

d i=1

1v^t[i]1/2 2t(1 - 1)

+

T t=1

t 2

gt - ht

32

- 2(

1-1 8

t-1 )

t

wt

-

wt-

1 2

.2

1-1 8

t-1

(7)

Now we can Theorem 1.

conclude the Denote  :=

following theorem. The proof is 1/ 2 < 1 and D = maxt

in Appendix

wt-

1 2

- w

D. .

Then,

RegretT



2T

1 (1 -

1)

D2

d Td
v^T [i]2 + D2
i=1 t=1 i=1

1v^t[i]1/2 2t(1 - 1)

T
+ 2t(1 - 1) gt - mt
t=1

2 t

+

T

t 2

t=1

gt - ht

.2

(

1-1 8

t-1 )

(8)

One should compare the bound with that of AMSGRAD (Reddi et al. (2018)), which is

RegretT



2T

1 (1 -

1)

D2

d i=1

v^T

[i]2

+

D2

T t=1

d i=1

1v^t[i]1/2 2t(1 - 1)

 +  1 + logT

d

(1 - 1)2(1 - ) 1 - 2 i=1

g1:T [i] 2,

(9)

 where t = / t in their setting. We see that the term


T t=1

2t(1

-

1)

gt - mt

2 t

of

OPTIMISTIC-AMSGRAD

can

be

much

smaller

than

the

term

 1+logT (1-1)2(1-) 1-2

d i=1

g1:T [i] 2 of AMSGRAD if gt and mt are close. Yet,

5

Under review as a conference paper at ICLR 2019

OPTIMISTIC-AMSGRAD also has an additional term 

T t t=1 2

than O( T ) so that we are good here. To see this, let us rewrite

gt - ht
T t t=1 2

.(

1-1 8

t-1

)

We claimed that it is smaller 

gt - ht

(

1-1 8

t-1

)

and

set

t

=

/

t as

(Reddi et al. (2018)); it is

T
O(
t=1

d i=1

t

(gt[i]2 - ht[i]2) ) v^t-1[i]

=

T
O(
t=1

d i=1

 t

(gt[i]2 - ht[i]2) ). v^t-1[i]

Assume that if each (gt[i]2-ht[i]2) in the inner sum is bounded by a constant c, we will get
v^t-1 [i]

(10)

T t=1

O( 1 )
t

=

 O( T ).

Yet, the denominator v^t-1 is non-decreasing so that we can actually have a smaller bound. To summarize, when mt is close to gt, OPTIMISTIC-AMSGRAD can have a smaller regret (thus better convergence rate) than ADAM (Kingma & Ba (2015)) and AMSGRAD (Reddi et al. (2018)).

3.2 COMPARISON OF ADAM-DISZ IN DASKALAKIS ET AL. (2018)

Algorithm 4 ADAM-DISZ Daskalakis et al. (2018)

1: Required: parameter 1, 2, and t.

2: Init: w1  K.

3: for t = 1 to T do

4: Get mini-batch stochastic gradient vector gt  Rd at wt.

5: t = 1t-1 + (1 - 1)gt.

6: vt = 2vt-1 + (1 - 2)gt2.

7:

wt+1

=

k [wt

-

2t

t vt

+ t vt-t-11 ].

8: end for

We are aware of Algorithm 1 in Daskalakis et al. (2018), which was also motivated by Optimistic Online learning 2.

For comparison, we replicate ADAM-DISZ in Algorithm 4. We are going to describe the differences of the algorithms

and the differences of the contributions between our work and their work. First of all, by comparing OPTIMISTIC-

AMSGRAD (Algorithm 2) or OPTIMISTIC-ADAM (Algorithm 2 without line 7) with ADAM-DISZ (Algorithm 4),

we see that the updates are indeed different. The update cannot be written into the same form as our OPTIMISTIC-

AMSGRAD (and vise versa). OPTIMISTIC-AMSGRAD (Algorithm 2) actually uses two interleaving sequences of

updates

{wt}Tt=1,

{wt-

1 2

}tT=1

.

Furthermore,

by

rewriting

the

update

as

wt+1

=

k [wt

-

t

t vt

-

t(

t vt

-

vt-t-11 )],

we

see

that

it

is

unclear

that

the

term

t vt

-

vt-t-11

would

be

a

good

guess

of

the

next

gradient

at

t

+ 1.

On

the

other

hand, our algorithm uses the extrapolation method to get a good estimate of the next gradient. Also, Daskalakis et al.

(2018) does not have any theoretical analysis of ADAM-DISZ. The theoretical analysis in Daskalakis et al. (2018) is

for other algorithms. Second, the contributions are very different. Daskalakis et al. (2018) focus on training GANs

Goodfellow et al. (2014). GANs is a two-player zero-sum game. There have been some related works in Optimistic

Online learning like Chiang et al. (2012); Rakhlin & Sridharan (2013); Syrgkanis et al. (2015)) showing that if both

players use some kinds of OPTIMISTIC-update, then acceleration to the convergence of the minimax value of the game

is possible. Daskalakis et al. (2018) was inspired by these related works and showed that OPTIMISTIC-MIRROR-

DESCENT can avoid the cycle behavior in a bilinear zero-sum game, which accelerates the convergence. Our work is

about solving minx f (x) (e.g. empirical risk) quickly. The goals are different.

We also show that ADAM-DISZ suffers the non-convergence issue as ADAM. The proof is available in Appendix E.

Theorem 2. There exists a convex online learning problem such that ADAM-DISZ has nonzero average regret (i.e.

Regret T

=

o(T ))

One might wonder if the non-convergence issue can be avoided if one let the weighted second moment of ADAMDISZ be monotone by adding the step v^t = max(v^t-1, vt) as AMSGRAD, which we call it AMSGRAD-DISZ. Unfortunately, we are unable to prove if the step guarantees convergence or not. Yet, even if we assume that the step
for monotone weighted second moment is adopted and that the regret is like O( t gt - mt 2), as one would expect for an OPTIMISTIC-ONLINE LEARNING algorithm, the update implies that the algorithm achieves acceleration when mt := gt-1 - gt-2 is close to gt. The condition may not hold.

2 We notice that the term OPTIMISTIC-ADAM was already used in Daskalakis et al. (2018) to describe their Algorithm 1 in the paper. To avoid confusion, let us still use OPTIMISTIC-ADAM to refer to Algorithm 2 without line 7 in this paper, while we use ADAM-DISZ to refer to their algorithm, where DISZ are the initials of the authors' last names of the paper Daskalakis et al. (2018).

6

Under review as a conference paper at ICLR 2019

4 EXPERIMENTS

To demonstrate the effectiveness of our proposed method, we test its performance with various neural network architectures, including fully-connected neural networks, convolutional neural networks (CNN's) and recurrent neural networks (RNN's). The results illustrate that OPTIMISTIC-AMSGRAD is able to speed up the convergence of stateof-art AMSGRAD algorithm, making the learning process more efficient.
For AMSGRAD algorithm, we set the parameter 1 and 2, respectively, to be 0.9 and 0.999, as recommended in Reddi et al. (2018). We tune the learning rate  over a fine grid and report the results under best-tuned parameter setting. For OPTIMISTIC-AMSGRAD and AMSGRAD-DISZ, we use same 1, 2 and learning rate as those for AMSGRAD to make a fair comparison of the enhancement brought by the optimistic step. We use the same weight initialization for all algorithms. The remaining tuning parameter of OPTIMISTIC-AMSGRAD is r, the number of previous gradients that we use to predict the next move. We conduct OPTIMISTIC-AMSGRAD with different values of r and observe similar performance. Hence, we report r = 15 for all experiments for tidiness of plots. To follow previous works of Reddi et al. (2018) and Kingma & Ba (2015), we compare different methods on MNIST, CIFAR10 and IMDB datasets in our experiment. For MNIST, we use a noisy version named as MNIST-back-rand in Larochelle et al. (2007) to increase the training difficulty.

4.1 MNIST-BACK-RAND + MULTI-LAYER NEURAL NETWORKS

Our experiments start with fully connected neural network for multi-classification problems. MNIST-back-rand dataset consists of 12000 training samples and 50000 test samples, where random background is inserted in original MNIST hand written digit images. The input dimension is 784 (28 × 28) and the number of classes is 10. We investigate a multi-layer neural networks with input layer followed by a hidden layer with 200 cells, which is then connected to a layer with 100 neurons before the output layer. All hidden layer cells are rectifier linear units (ReLu's). We use mini-batch size 128 to calculate stochastic gradient in each iteration. Model performance is evaluated by multi-class cross entropy loss. The training loss with respect to number of iterations is reported in figure 1.

Training Loss Training Loss

0.8 AMSGrad AMSGrad-DISZ
0.6 OPTIMISTIC AMSGrad
0.4
0.2 1 500 1000 1500 2000 # Iterations

1.2 1.1
1 0.9 0.8 0.7 0.6 0.5
0

AMSGrad AMSGrad-DISZ OPTIMISTIC AMSGrad

1234 # Epochs

5

Figure 1: Fully connected neural networks on MNIST-Back-Rand dataset. Left panel: training loss vs. number of iterations. Right panel: training loss only for 5 epochs. One epoch means all training data points are used once.

On this dataset, we empirically observe obvious improvement of OPTIMISTIC-AMSGRAD in terms of both convergence speed and training loss. On the other hand, AMSGRAD-DISZ performs similarly to AMSGRAD in general.

4.2 CIFAR10 + CONVOLUTIONAL NEURAL NETWORKS (CNN)
Convolutional Neural Networks (CNN) have been widely studied and playing an important role in various deep learning applications such as computer vision and natural language processing. We test the effectiveness of OPTIMISTICAMSGRAD in deep CNN's with dropout. We use the CIFAR10 dataset, which includes 60,000 images (50,000 for training and 10,000 for testing) of size 32 × 32 in 10 different classes. ALL-CNN architecture proposed in Springenberg et al. (2015) is implemented with two blocks of 3 × 3 convolutional filter, 3 × 3 convolutional layer with stride 2 and dropout layer with keep probability 0.5. Another block of 3 × 3, 1 × 1 convolutional layer and a 6 × 6 global averaging pooling is added before the output layer. We apply another dropout with keep probability 0.8 on the input layer. The cost function is multi-class cross entropy. The batch size is 128. The images are all whitened. The training loss is provided in figure 2. The result shows that OPTIMISTIC-AMSGRAD accelerates the learning process significantly and gives lowest training cost after 10000 iterations. For this dataset, the performance of AMSGRAD-DISZ is worse than original AMSGRAD.

7

Under review as a conference paper at ICLR 2019

Training Loss Training Loss

2 2.5

AMSGrad

AMSGrad

1.6

AMSGrad-DISZ OPTIMISTIC AMSGrad

2

AMSGrad-DISZ OPTIMISTIC AMSGrad

1.2 1.5

0.8 1

0.4 1 2000 4000 6000 8000 10000 # Iterations

0.5 012345 # Epochs

Figure 2: CIFAR10 c96-c96-c96-c192-c192-c192-c192-c192-c10 ConvNet with dropout. Left pane: training loss vs. number of iterations. Right panel: training loss in the first 5 epochs.

4.3 IMDB + LONG-SHORT TERM MEMORY (LSTM)

As another important application of deep learning, natural language processing tasks often benefit from considering sequence dependency in the models. Recurrent Neural Networks (RNN's) achieves this goal by adding hidden state units that act as "memory". Long-Short Term Memory (LSTM) is the most popular structure in building RNN's. We use IMDB movie review dataset from Maas et al. (2011) to test the performance of OPTIMISTIC-AMSGRAD in RNN's under the circumstance of high data sparsity. IMDB is a binary classification dataset with 25000 training and test samples respectively. Our model includes a word embedding layer with 5000 input entries representing most frequent words in the dataset and each word is embedded into a 32 dimensional space. The output of embedding layer is passed to 100 LSTM units, which is then connected to 128 fully connected ReLu's before reaching the output layer. Binary cross-entropy loss is used and the batch size is 100. We provide the results in figure 3.

0.8 0.8

AMSGrad

AMSGrad

0.6

AMSGrad-DISZ OPTIMISTIC AMSGrad

0.6

AMSGrad-DISZ OPTIMISTIC AMSGrad

0.4 0.4

Training Loss Training Loss

0.2 0.2

0

1

1000

2000

3000

# Iterations

0 012345 # Epochs

Figure 3: Embedding-LSTM100-f100 RNN: on the left is the training loss against number of iterations. On the right is the plot for first 5 epochs.

We observe a considerable improvement in convergence speed. In the first epoch, the result is more exciting. At epoch 0.5, OPTIMISTIC-AMSGRAD already achieves the training loss that vanilla AMSGRAD can produce with more than 1 epoch. The sample efficiency is significantly improved. On this dataset, AMSGRAD-DISZ performs less effectively and may be trapped in local minimum. We remark that in each iteration, only a small portion of gradients in embedding layer is non-zero. Thus, this experiment demonstrates that OPTIMISTIC-AMSGRAD could also perform well with sparse gradient. (See additional experiments in Appendix A.)
5 EXTENSIONS AND FUTURE WORK

In this paper, we propose OPTIMISTIC-AMSGRAD, which combines optimistic learning and AMSGRAD to strengthen the learning process of optimization problems, in particular, deep neural networks. The idea of adding optimistic step can be easily extended to other optimization algorithms, e.g ADAM and ADAGRAD. We provide OPTIMISTIC-ADAGRAD algorithm and theoretical results in Appendix F. A potential directions based on this work is to improve the method for predicting next gradient. We expect that optimistic acceleration strategy could be widely used in various optimization problems.

8

Under review as a conference paper at ICLR 2019
REFERENCES
Jacob Abernethy, Kevin A. Lai, Kfir Y. Levy, and Jun-Kun Wang. Faster rates for convex-concave games. Computational Learning Theory (COLT), 2018.
C. Brezinski and M. R. Zaglia. Extrapolation methods: theory and practice. Elsevier, 2013.
S. Cabay and L. Jackson. A polynomial extrapolation method for finding limits and antilimits of vector sequences. SIAM Journal on Numerical Analysis, 1976.
Nicolo´' Cesa-Bianchi, Alex Conconi, and Claudio Gentile. On the generalization ability of on-line learning algorithms. Information Theory, IEEE Transactions, 2004.
Chao-Kai Chiang, Tianbao Yang, Chia-Jung Lee, Mehrdad Mahdavi, Chi-Jen Lu, Rong Jin, and Shenghuo Zhu. Online optimization with gradual variations. Computational Learning Theory (COLT), 2012.
Constantinos Daskalakis, Andrew Ilyas, Vasilis Syrgkanis, and Haoyang Zeng. Training gans with optimism. International Conference on Learning Representations (ICLR), 2018.
Timothy Dozat. Incorporating nesterov momentum into adam. International Conference on Learning Representations (ICLR) Workshop Track, 2016.
John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and stochastic optimization. Journal of Machine Learning Research (JMLR), 2011.
R. Eddy. Extrapolating to the limit of a vector sequence. Information linkage between applied mathematics and industry, Elsevier, 1979.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. Annual Conference on Neural Information Processing Systems (NIPS), 2014.
Alex Graves, Abdel rahman Mohamed, and Geoffrey Hinton. Speech recognition with deep recurrent neural networks. International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2013.
Elad Hazan. Introduction to online convex optimization. Foundations and Trends in Optimization, 2016.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. Conference on Computer Vision and Pattern Recognition (CVPR), 2016.
Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. International Conference on Learning Representations (ICLR), 2015.
Hugo Larochelle, Dumitru Erhan, Aaron Courville, James Bergstra, and Yoshua Bengio. An empirical evaluation of deep architectures on problems with many factors of variation. International Conference on Machine Learning (ICML), 2007.
Sergey Levine, Chelsea Finn, Trevor Darrell, and Pieter Abbeel. End-to-end training of deep visuomotor policies. Annual Conference on Neural Information Processing Systems (NIPS), 2017.
Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts. Learning word vectors for sentiment analysis. Annual Meeting of the Association for Computational Linguistics (ACL), 2011.
H. Brendan McMahan and Matthew J. Streeter. Adaptive bound optimization for online convex optimization. Computational Learning Theory (COLT), 2010.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin Riedmiller. Playing atari with deep reinforcement learning. Annual Conference on Neural Information Processing Systems (NIPS) Deep Learning Workshop, 2013.
Yurii Nesterov. Introductory lectures on convex optimization: A basic course. Springer, 2004.
B. T. Polyak. Some methods of speeding up the convergence of iteration methods. Mathematics and Mathematical Physics, 1964.
9

Under review as a conference paper at ICLR 2019

Alexander Rakhlin and Karthik Sridharan. Optimization, learning, and games with predictable sequences. Annual Conference on Neural Information Processing Systems (NIPS), 2013.
Sashank J. Reddi, Satyen Kale, and Sanjiv Kumar. On the convergence of adam and beyond. International Conference on Learning Representations (ICLR), 2018.
Damien Scieur, Alexandre d'Aspremont, and Francis Bach. Regularized nonlinear acceleration. Annual Conference on Neural Information Processing Systems (NIPS), 2016.
Jost Springenberg, Alexey Dosovitskiy, Thomas Brox, and Martin Riedmiller. Striving for simplicity: The all convolutional net. International Conference on Learning Representations (ICLR), 2015.
Vasilis Syrgkanis, Alekh Agarwal, Haipeng Luo, and Robert E. Schapire. Fast convergence of regularized learning in games. Annual Conference on Neural Information Processing Systems (NIPS), 2015.
T. Tieleman and G. Hinton. Rmsprop: Divide the gradient by a running average of its recent magnitude. COURSERA: Neural Networks for Machine Learning, 2012.
H. F. Walker and P. Ni. Anderson acceleration for fixed-point iterations. SIAM Journal on Numerical Analysis, 2011.
Matthew D. Zeiler. Adadelta: An adaptive learning rate method. arXiv:1212.5701, 2012.

A MORE EXPERIMENTAL RESULTS
A.1 CHOOSING PARAMETER r
In OPTIMISTIC-AMSGRAD, parameter r, the number of previous gradients used, may affect the performance dramatically. If we choose r too small (e.g r < 10), the optimistic updates will start at very early stage, but the information we collect from the past is limited. This may make our training process off-track at first several iterations. On the other hand, if r is chosen to be too large (e.g r >= 30), although we may get a better prediction of next gradient, the optimistic step will start late so OPTIMISTIC-AMSGRAD may miss great chances to improve the learning performance at early stages. Additionally, we need more room to store past gradients, hence the operational time will increase.

Training Loss Training Loss

0.8 2.5

AMSGrad

AMSGrad

0.6

AMSGrad-DISZ OPTIMISTIC AMSGrad

2

AMSGrad-DISZ OPTIMISTIC AMSGrad

1.5 CIFAR10-CNN (r = 10, 15, 20)

0.4 1

MNIST-Rand (r = 5 to 20) 0.2
1 500 1000 1500
# Iterations

2000

0.5 1 2000 4000 6000 8000 10000 # Iterations

Figure 4: The training performance on two datasets for different r values.

The empirical impact of different r value is reported in figure 4. We suggest 10  r  20 as an ideal range, and r = 15 tend to perform well in most training tasks. Actually, we may make the algorithm more flexible by "early start". For example, when we set r = 20, we can instead start adding optimistic step at iteration 10, and gradually increase the number of past gradients we use to predict next move from 10 to 20 in next 10 iterations. After iteration 21, we fix the moving window size for optimistic prediction as 20. This may bring enhancement to OPTIMISTIC-AMSGRAD because it can seek opportunities to accelerate learning in first several iterations, which is critical for indicating a better direction towards the minimum loss.

10

Under review as a conference paper at ICLR 2019

A.2 OPTIMISTIC ADAM
We also conduct experiments on OPTIMISTIC-ADAM and ADAM-DISZ. The experiment setting is similar to OPTIMISTIC-AMSGRAD, where we fix 1, 2 and compare the performance using the best learning rate with respect to ADAM. We provide a brief summary of the results in figure 5, with r = 15. The improvement brought by OPTIMISTIC-ADAM is obvious, indicating that adding optimistic step could also enhance the performance of ADAM optimizer.

Training Loss Training Loss

0.8 2

ADAM

ADAM

0.6

ADAM-DISZ OPTIMISTIC ADAM

1.6

ADAM-DISZ OPTIMISTIC ADAM

1.2

0.4 0.8

0.2 0.4

1 500 1000 1500 2000

1 2000 4000 6000 8000 10000

# Iterations

# Iterations

Figure 5: Training loss of OPTIMISTIC-ADAM. Left panel: MNIST-Back-Rand. Right panel: CIFAR10 (CNN).

B PROOF OF LEMMA 1

Lemma 1

Denote D

=

maxt

wt-

1 2

- w .

T t t=1 (1-1)

t

2 t

+

D2

T t=1

.d 1v^t[i]1/2
i=1 2t(1-1)

T t=1

wt-

1 2

- w, gt



2T

1 (1-1

)

D2

d i=1

v^T

[i]2

+

Proof. The proof of this lemma basically follows that of (Reddi et al. (2018)). We have that

wt+

1 2

- w

2 t

=

wt-

1 2

-

tdiag(v^t)-1/2t

-

w

2 t

=

wt-

1 2

- w

2 t

+

t2

t

2 t

-

2t

t

,

wt-

1 2

- w

=

wt-

1 2

- w

2 t

+

t2

t

2 t

-

2t

1t-1

+

(1

-

1)gt,

wt-

1 2

- w

.

By rearranging the terms above and summing it from t = 1, . . . , T ,

(11)

T

wt-

1 2

- w, gt

t=1

T1

=

t=1

2t(1

-

{ 1)

wt-

1 2

- w

2 t

-

wt+

1 2

- w

2 t

}

+

2(1

t -

1)

t

2 t

+

1

1 - 1

t-1

,

wt-

1 2

- w

T


1{

t=1 2t(1 - 1)

wt-

1 2

-

w

2 t

-

wt+

1 2

- w

2 t

}

+

2(1

t -

1)

t

2 t

+ 1t 2(1 - 1)

t-1

2 t

+

1 2t(1 -

1)

wt-

1 2

-

w

2 t

T


1{

t=1 2t(1 - 1)

wt-

1 2

-

w

2 t

-

wt+

1 2

- w

2 t

}

+

(1

t - 1)

t

2 t

+ 1 2t(1 - 1)

wt-

1 2

- w

2 t

.

(12)

where the first inequality is due to Young's inequality and the second one is due to the constraint 0 < 1  1 so that

t 2(1-1 )

+

1 t 2(1-1 )



t (1-1

)

.

11

Under review as a conference paper at ICLR 2019

Now we can bound

T t=1

wt-

1 2

- w, gt

in (6) as

T

wt-

1 2

- w, gt

t=1

T


1{

t=1 2t(1 - 1)

wt-

1 2

- w

2 t

-

wt+

1 2

-

w

2 t

}

T
+

t

t=1 (1 - 1)

t

2 t

+

T t=1

1 2t(1 -

1)

wt-

1 2

- w

2 t

1 21(1 - 1)

w 1 - w 2

2 1

+

2(1

1 -

1)

T
{
t=2

wt+

1 2

- w

t

2
t -

wt+

1 2

-

w

2
}t-1

t-1

+

T t=1

(1

t - 1)

t

2 t

+

T t=1

1 2t(1 -

1)

wt-

1 2

- w

2 t

=

1 21(1 - 1)

d i=1

v^1[i]2(w 1 [i] - w[i])2 2

+

1 2(1 - 1)

T t=2

d

(wt-

1 2

[i]

-

w[i])2

i=1

v^t1/2[i] - v^t1-/21[i] t t-1

T
+

t

t=1 (1 - 1)

t

2 t

+

T t=1

1 2t(1 -

1)

wt-

1 2

- w

2 t

,

where the first inequality is due to (12).

Continue the analysis, we have

T

wt-

1 2

- w, gt

t=1



1 21(1 - 1)

d i=1

v^1[i]2(w 1 [i] - w[i])2 2

+

1 2(1 - 1)

T t=2

d

(wt-

1 2

[i]

-

w[i])2

i=1

v^t1/2[i] - v^t1-/21[i] t t-1

T
+

t

t=1 (1 - 1)

t

2 t

+

T t=1

1 2t(1 -

1)

wt-

1 2

- w

2 t

,



1 21(1 -

1) D2

d i=1

v^1[i]2

+

2(1

1 -

1)

D2

T t=2

d i=1

v^t1/2[i] - v^t1-/21[i] t t-1

T
+

t

t=1 (1 - 1)

t

2 t

T
+

1

t=1 2t(1 - 1)

wt-

1 2

- w

2 t

=

2T

1 (1 -

1

)

D2

d i=1

v^T [i]2

+

T t=1

(1

t - 1)

t

2 t

+

T t=1

1 2t(1 -

1)

wt-

1 2

- w

2 t



2T

1 (1 -

1

)

D2

d i=1

v^T [i]2

+

T t=1

(1

t - 1)

t

2 t

+

D2

T t=1

d i=1

1v^t[i]1/2 2t(1 - 1

)

,

where the last equality is by telescoping sum.

(13) (14)

12

Under review as a conference paper at ICLR 2019

C PROOF OF LEMMA 2

Lemma 2

T t=1

wt

-

wt-

1 2

,

gt

-

ht

w .2

t-

1 2

1-1 8

t-1

+

T t=1

wt

-

wt-

1 2

,

ht



T t t=1 2

gt - ht

-2

(

1-1 8

t-1 )

3 2t

wt -

Proof.

From

the

update,

wt

=

wt-

1 2

-

t

4 1-1

vhtt-1

,

wt

-

wt-

1 2

,

ht

=

-t

ht,

1

4 - 1

ht diag{vt}-1/2

=-2 t

t

1

4 -

1

ht diag{vt}1/2

,

1

- 8

1

diag{vt}1/2t

1

4 -

1

ht diag{vt}1/2

2 =-
t

wt

-

wt-

1 2

.2

1-1 8

t-1

Therefore, we have that

(15)

T

wt

-

wt-

1 2

,

gt

-

ht

t=1

T

+

wt

-

wt-

1 2

,

ht

t=1

T

=

wt

-

wt-

1 2

,

gt

-

ht

t=1

-2 t

wt

-

wt-

1 2

2

1-1 8

t-1

(a) T

t=1

wt

-

wt-

1 2

2

g - h -1-1 8

t-1

t

t

(

1-1 8

t-1 )

t

wt

-

wt-

1 2

2

1-1 8

t-1

(b) T


1

t=1 2t

wt

-

wt-

1 2

2

+1-1 28

t-1

t

gt - ht

22

- (

1-1 8

t-1 )

t

wt

-

wt-

1 2

2

1-1 8

t-1

= T t 2
t=1

gt - ht

32

- 2(

1-1 8

t-1 )

t

wt

-

wt-

1 2

,2

1-1 8

t-1

where (a) is by Ho¨lder's inequality and (b) is by Young's inequality.

(16)

D PROOF OF THEOREM 1

 Theorem 1 Denote  := 1/ 2 < 1 and D = maxt

wt-

1 2

- w

. Then,

RegretT



2T

1 (1 -

1)

D2

d Td
v^T [i]2 + D2
i=1 t=1 i=1

1v^t[i]1/2 2t(1 - 1)

T
+ 2t(1 - 1) gt - mt
t=1

2 t

+

T

t 2

t=1

gt - ht

.2

(

1-1 8

t-1 )

Proof. Recall that from (7), we show that

RegretT



2T

1 (1 -

1)

D2

d
v^T [i]2
i=1

+

T t=1

t (1 - 1)

t

2 t

+

D2

T t=1

d i=1

1v^t[i]1/2 2t(1 - 1)

+

T t=1

t 2

gt - ht

32

- 2(

1-1 8

t-1 )

t

wt

-

wt-

1 2

.2

1-1 8

t-1

To proceed, let us analyze

T t t=1 (1-1)

t

-2 3
t 2t

wt

-

wt-

1 2

2

1-1 8

t-1

above.

Notice

that

t (1 - 1)

t

2 t

=

t (1 - 1)

d i=1

t[i]2 , v^t[i]

13

(17) (18) (19)

Under review as a conference paper at ICLR 2019

and

3 2t

wt

-

wt-

1 2

2

1-1 8

t-1

=

3 2t

t

1

4 - 1

 ht v^t-1

,

1

- 1 8

diag(v^t-1)1/2t

1

4 - 1

 ht v^t-1

=

3t 1 - 1

d i=1

ht[i]2 , v^t-1[i]

where

we

use

the

update

rule,

wt

=

wt-

1 2

-

t

4 1-1

 ht .
v^t-1

Therefore,

we

have

that

(20)

T t t=1 (1 - 1)

t

2 t

-

3 2t

wt

-

wt-

1 2

2

1-1 2

t-1

T


t

d t[i]2 - T 3t d

t=1 (1 - 1) i=1 v^t[i] t=1 (1 - 1) i=1

ht[i]2 v^t-1[i]

T


t

d t[i]2 - T

t

d ht[i]2 ,

t=1 (1 - 1) i=1 v^t[i] t=1 (1 - 1) i=1 v^t[i]

=

T t=1

(1

t - 1)

d i=1

(t[i]2

- ht[i]2) v^t[i]

T


2t

d (t[i] - ht[i])2

t=1 (1 - 1) i=1

v^t[i]

T
=

2t

t=1 (1 - 1)

t - ht

2 t

,

T

=

2t(1 - 1)

gt - mt

2 t

,

t=1

(21)

where the second inequality is due to that the sequence {v^t[i]} is non-decreasing and the last equality is because t - ht = 1t-1 + (1 - 1)(gt) - 1t-1 + (1 - 1)(mt) = (1 - 1)(gt - mt).

To summarize,

RegretT



2T

1 (1 -

1)

D2

d i=1

v^T

[i]2

+

D2

T t=1

d i=1

1v^t[i]1/2 2t(1 - 1)

T
+ 2t(1 - 1) gt - mt
t=1

T

2 t

+

t 2

t=1

gt - ht

.2

(

1-1 8

t-1 )

(22)

14

Under review as a conference paper at ICLR 2019

E PROOF OF THEOREM 2

Theorem 2 There exists a convex online learning problem such that ADAM-DISZ has nonzero average regret (i.e.

Regret T

=

o(T ))

We basically follow the same setting as Theorem 1 of Reddi et al. (2018). In each round, the loss function t(w) is linear and the learner's decision space is K = [-1, 1]

ft(w) =

Cw , for t mod 3 = 1 -w , otherwise

,

where C  4. For this loss function sequences, the point w = -1 achieves the minimum regret, i.e. - 1 =

arg minwK

T t=1

ft(w)

when

T



.

Consider the execution of ADAM-DISZ with 1

=

0, 2

=

1 1+2C

2

,

t

=

 t

with 

<

1 3

 1

-

2

.

Notice that

12/ 2 < 1 in this case, which satisfies the conditions of Kingma & Ba (2015).

The goal is to show that for all t, w3t+1

=

1, and wt

>

0.

Let us denote w^t+1

:=

wt

-

2t

t vt

+ t

t-1 vt-1

and

wt+1 := k[w^t+1]. Assume the initial point is 1. As pointed it out by Reddi et al. (2018), the assumption of the

initial point is without loss of generality. If the initial point is not 1, then one can translate the coordinate system to a

new coordinate system so that the initial point is 1 and then choose the loss function sequences for the new coordinate

system. Therefore, the base case x1 = 1 is true.

Now assume that for some t > 0, x3t+1 = 1 and xs > 0 for all s  3t + 1. Observe that.

ft(w) =

C , for t mod 3 = 1 -1 , otherwise

,

According to the update of ADAM-DISZ

w^3t+2 = w3t+1 -

2C - (3t + 1)(2v3t + (1 - 2)C2)

 (3t + 1)(2v3t-1 + (1 - 2))

2C 

=1-

-

(3t + 1)(2v3t + (1 - 2)C2)

(3t + 1)(2v3t-1 + (1 - 2))

 1 - 2 -



(3t + 1)(1 - 2)

(3t + 1)(1 - 2)

 1 - 3 > 0. (3t + 1)(1 - 2)

So, w^3t+2 = w3t+2.

(23)

2 C

w^3t+3 = w3t+2 +

+ (3t + 2)(2v3t+1 + (1 - 2))

> 0. (3t + 2)(2v3t + (1 - 2)C2)

15

(24)

Under review as a conference paper at ICLR 2019

For w^3t+4, let us first consider the case that w^3t+3 < 1

w^3t+4 = min(w^3t+3, 1) +

2 - (3t + 3)(2v3t+2 + (1 - 2))

 (3t + 3)(2v3t+1 + (1 - 2)

2 C

= w3t+2 +

+ (3t + 2)(2v3t+1 + (1 - 2))

(3t + 2)(2v3t + (1 - 2)C2)

+ 2 -



(3t + 3)(2v3t+2 + (1 - 2))

(3t + 3)(2v3t+1 + (1 - 2))

 1 - 2C -



(3t + 1)(2v3t + (1 - 2)C2)

(3t + 1)(2v3t-1 + (1 - 2))

 C

++

+

(3t + 2)(2v3t+1 + (1 - 2))

(3t + 2)(2v3t + (1 - 2)C2)

=

1

-

(1

-

3t

+

1 )

C

3t + 2 (3t + 1)(2v3t + (1 - 2)C2)



-+

(3t + 1)(2v3t-1 + (1 - 2))

(3t + 1)(2v3t+1 + (1 - 2))

2 (3t + 3)(2v3t+2 + (1 - 2))

- C + 2

(3t + 1)(2v3t + (1 - 2)C2)

(3t + 3)(2v3t+2 + (1 - 2))



1

-

(1

-

3t

+

1 )



3t + 2 (3t + 1)(1 - 2))

- 2 +  +

(3t

+

1)(1 

-

2

))

(3t + 1)(2C2 + (1 - 2))

(=a) 1 - (1 - 3t + 1 )



3t + 2 (3t + 1)(1 - 2))

2 (3t + 3)(2C2 + (1 - 2))

(25)

(2 - 1 )

-

3/2
+

2

(3t

+

1)(1 

-

2

))

(3t + 3)(2C2 + (1 - 2))



1

-

(1

-

3t

+

1 )



3t + 2 (3t + 1)(1 - 2))

(2 - 1 )

-

3/2
+

 3

(3t + 1)(1 - 2))

(3t + 1)(2C2 + (1 - 2))



(=a) 1 - (1 - 3t + 1 )

3t + 2





1

-

(1

-

3t

+

1 )

3t + 2

- (3t + 1)(1 - 2))

(2 - 1 ) 3/2 +
(3t + 1)(1 - 2))



+ 0.23

(3t + 1)(1 - 2))

(3t + 1)(1 - 2)

 2
(3t + 1)(1 - 2))

(b)
 1 - 0.12



+ 0.23



1

(3t + 1)(1 - 2))

(3t + 1)(1 - 2)

where (a) uses

2 C 2 +(1-2 ) 3/2

=

 1 - 2 for the choice of 2

=



1/(1

+

2C 2 ),

(b)


is

because

1

-

3t+1 3t+2

is a

decreasing function of t and has its largest value at t = 1 for any t > 0, which is 1 - ( 4 )  0.12. To summarize, we
5

have that w3t+4 = 1.

Now if it is the case that w^3t+3  1

w^3t+4 = min(w^3t+3, 1) +

2 - (3t + 3)(2v3t+2 + (1 - 2))

 (3t + 3)(2v3t+1 + (1 - 2))

= 1 + 2 -  > 1

(3t + 3)(2v3t+2 + (1 - 2))

(3t + 3)(2v3t+1 + (1 - 2))

(26)

16

Under review as a conference paper at ICLR 2019

where the last inequality is because

To see this,

2 2v3t+1 + (1 - 2)  2v3t+2 + (1 - 2)

(27)

2v3t+2 + (1 - 2) = 2(2v3t+1 + (1 - 2)) + (1 - 2) = (22)v3t+1 + 1 - 22 So, the above inequality is equivalent to

(28)

2 2v3t+1 + (1 - 2)  (22)v3t+1 + 1 - 22,

(29)

which is true. This means that w3t+4 = 1. Therefore, we have completed the induction.
To summarize, we have 3t+1(w3t+1) + 3t+2(w3t+2) + 3t+3(w3t+3) - 3t+1(-1) - 3t+2(w-1) - 3t+3(-1)  2C - 4. That is, for every 3 steps, the algorithms suffers regret at least 2C - 4, this means that the regret over T rounds would be (2C - 4)T /3, which is not sublinear to T . Now we have completed the proof. One may follow the analysis of Theorem 2 of Reddi et al. (2018) to generalize the result so that ADAM-DISZ does not converge for any 1, 2  [0, 1) such that 1 < 2.

F OPTIMISTIC-ADAGRAD
The optimistic update step can also be extended to other algorithms as well. For example, based on ADAGRAD, we propose OPTIMISTIC-ADAGRAD by including the optimistic update step.

Algorithm 5 OPTIMISTIC-ADAGRAD (UNCONSTRAINED)

1: Required: parameter .

2: Init: w1. 3: for t = 1 to T do

4: Get current gradient gt at wt.

5:

wt+

1 2

=

wt-

1 2

- diag(Gt)-1/2gt



t s=1

gt[1]2

where diag(Gt) := 

0

0

6:

wt+1

=

wt+

1 2

- diag(Gt)-1/2mt+1,

where mt+1 is the guess of gt+1.

7: end for

0

t s=1

gt

[2]2

0

0
0 ·

F.1 ANALYSIS:

We provide the regret analysis here. Let us recall the notations and assumptions first. We denote the Mahalanobis norm

· H = ·, H· for some PSD matrix H. We let t(x) := x, diag{v^t}1/2x for the PSD matrix diag{v^t}, where diag{v^t} represents the diagonal matrix such that its ith diagonal element is v^t[i] in Algorithm 2. Consequently, t(·)

is 1-strongly convex with respect to the norm · t := ·, diag{v^t}1/2· . Namely, t(·) satisfies t(u)  t(v) +

t(v), u - v

+

1 2

u-v

2 t

for

any

point

u, v.

A

consequence

of

1-strongly

convexity

of

t(·)

is

that

Bt (u, v)



1 2

u-v

2 t

,

where

the

Bregman

divergence

Bt

(u,

v)

is

defined

as

Bt

(u,

v)

:=

t(u)-t(v)-

t(v), u-v

and t

is called the distance generating function. We can also define the associate dual norm as t(x) := x, diag{v^t}-1/2x .

We assume that the model parameter w is in d-dimensional space. That is, w  Rd. It suffices to analyze Algorithm 6,

which holds for any convex set K. The algorithm reduces to Algorithm 5 when K = Rd.

17

Under review as a conference paper at ICLR 2019

Algorithm 6 OPTIMISTIC-ADAGRAD

1: Required: parameter t.

2: Init: w1.

3: for t = 1 to T do

4: Get current gradient gt at wt.

5:

wt+

1 2

= arg minwK t

w, gt

+

Bt

(w,

wt-

1 2

).

6:

wt+1 = arg minwK t+1 w, mt+1

+

Bt

(w,

wt+

1 2

)

where mt+1 is the guess of gt+1.

7: end for

By regret decomposition, we have that

T TT

RegretT :=

t(wt) - min
wK

t(w)  wt - w,  t(wt)

t=1 t=1 t=1

T

=

wt

-

wt+

1 2

,



t(wt) - mt

+

wt

-

wt+

1 2

,

mt

+

wt+

1 2

- w, 

t(wt)

.

t=1

(30)

Now we are going to exploit a useful inequality: for any update of the form w^ = arg minwK w,  + B(w, v), we

have that

w^ - u,   B(u, v) - B(u, w^) - B(w^, v),

(31)

for any u  K. Using the above inequality, we have

1

wt

-

wt+

1 2

,

mt



t

(Bt-1

(wt+

1 2

,

wt-

1 2

)

-

Bt-1

(wt+

1 2

,

wt)

-

Bt-1

(wt,

wt-

1 2

)),

and

wt+

1 2

- w, 

t(wt)



1 t

(Bt

(w,

wt-

1 2

)

-

Bt

(w,

wt+

1 2

)

-

Bt

(wt+

1 2

,

wt-

1 2

)).

So, by (30) (32) and (33), we obtain

(32) (33)

T

RegretT 

wt

-

wt+

1 2

,



t(wt) - mt

+

wt

-

wt+

1 2

,

mt

+

wt+

1 2

- w, 

t(wt)

t=1

T



wt

-

wt+

1 2

t-1

f (wt) - mt

t-1

t=1

1

+ t

Bt-1

(wt+

1 2

,

wt-

1 2

)

-

Bt-1

(wt+

1 2

,

wt)

-

Bt-1

(wt,

wt-

1 2

)

+

Bt

(w,

wt-

1 2

)

-

Bt

(w,

wt+

1 2

)

-

Bt

(wt+

1 2

,

wt-

1 2

)

T1 
t=1 2t

wt

-

wt+

1 2

2 t-1

+

t 2

f (wt) - mt

2 t-1

1 +
t

Bt-1

(wt+

1 2

,

wt-

1 2

)

-

1 2

wt+

1 2

-

wt

2 t-1

-

Bt-1

(wt

,

wt-

1 2

)

+

Bt

(w,

wt-

1 2

)

-

Bt

(w,

wt+

1 2

)

-

Bt

(wt+

1 2

,

wt-

1 2

)



T

t 2

f (wt) - mt t-1

t=1

1 +
t

Bt

(w,

wt-

1 2

)

-

Bt

(w,

wt+

1 2

)

+

Bt-1

(wt+

1 2

,

wt-

1 2

)

-

Bt

(wt+

1 2

,

wt-

1 2

)

,

where the third inequality is because

(34)

wt

-

wt+

1 2

t-1

f (wt) - mt

1

t-1

=

inf
>0

2

wt

-

wt+

1 2

2 t-1

+

 2

f (wt) - mt

,2
t-1

(35)

18

Under review as a conference paper at ICLR 2019

and that t-1(·) is 1-strongly convex with respect to · .t-1 To proceed, notice that, by definition of Bregman divergence, we also have

Bt+1 (w,

wt+

1 2

)

-

Bt (w,

wt+

1 2

)

=

w

-

wt+

1 2

,

diag(st+1

-

st)(w

-

wt+

1 2

)



max(w[i]
i

-

wt+

1 2

[i])2

st+1 - st

1.

(36)

and

Bt-1

(wt+

1 2

,

wt-

1 2

)

-

Bt

(wt+

1 2

,

wt-

1 2

)

=

wt+

1 2

-

wt-

1 2

,

diag(st-1

-

st

)(wt+

1 2

-

wt-

1 2

)

 0.

(37)

where the vector st  Rd is defined as follows: its ith entry st[i] is

st[i] :=

t
gs[i]2.
s=1

Now we have

RegretT 

T

t 2

f (wt) - mt

2 t-1

t=1

1 +
t

Bt

(w,

wt-

1 2

)

-

Bt

(w,

wt+

1 2

)

+

Bt-1

(wt+

1 2

,

wt-

1 2

)

-

Bt

(wt+

1 2

,

wt-

1 2

)



1 1

B1

(w,

w1/2)

+

T t=1

t 2

f (wt) - mt

2 t-1

+

T -1 t=1

1 t

max(w[i]
i

-

wt+

1 2

[i])2

st+1 - st

1



1 1

B1

(w,

w1/2)

+

1 min

max
tT

w

-

wt-

1 2

d 2 
i=1

T
g1:T [i] 2 +

t 2

t=1

f (wt) - mt

.2
t-1

(38) (39)

Therefore, we conclude the following theorem.

Theorem 3. For any learning rate t := , OPTIMISTIC-ADAGRAD (Algorithm 6) has regret

RegretT



1 

B1

(w,

w1/2)

+

1 

max
tT

w

-

wt-

1 2

2 

d

T
g1:T [i] 2 +

 2

f (wt) - mt

2 t-1

i=1 t=1

(40)

One need to compare it with the bound of ADAGRAD Duchi et al. (2011), which has regret 3

d

RegretT

=

O(max
tT

w

-

wt-

1 2

2 

g1:T [i] 2).

i=1

(41)

We see that when mt predicts f (wt) very well, the last term is dominated by the second one. One can set  of OPTIMISTIC-ADAGRAD to be a large step size so that OPTIMISTIC-ADAGRAD has a smaller regret than ADAGRAD.

3The bound here is the result of optimizing over  of ADAGRAD. 19


Under review as a conference paper at ICLR 2019

GUARANTEED RECOVERY OF ONE-HIDDEN-LAYER NEURAL NETWORKS VIA CROSS ENTROPY
Anonymous authors Paper under double-blind review

ABSTRACT
We study model recovery for data classification, where the training labels are generated from a one-hidden-layer fully-connected neural network with sigmoid activations, and the goal is to recover the weight vectors of the neural network. We prove that under Gaussian inputs, the empirical risk function using cross entropy exhibits strong convexity and smoothness uniformly in a local neighborhood of the ground truth, as soon as the sample complexity is sufficiently large. This implies that if initialized in this neighborhood, which can be achieved via the tensor method, gradient descent converges linearly to a critical point that is provably close to the ground truth without requiring a fresh set of samples at each iteration. To the best of our knowledge, this is the first global convergence guarantee established for the empirical risk minimization using cross entropy via gradient descent for learning one-hidden-layer neural networks, at the near-optimal sample and computational complexity with respect to the network input dimension.

1 INTRODUCTION

Neural networks have attracted a significant amount of research interest in recent years due to the success of deep neural networks (LeCun et al., 2015) in practical domains such as computer vision and artificial intelligence (Russakovsky et al., 2015; He et al., 2016; Silver et al., 2016). However, the theoretical underpinnings behind such success remains mysterious to a large extent. Efforts have been taken to understand which classes of functions can be represented by deep neural networks (Cybenko, 1989; Hornik et al., 1989; Barron, 1993; Telgarsky, 2016), when (stochastic) gradient descent is effective for optimizing a non-convex loss function (Dauphin et al., 2014), and why these networks generalize well (Zhang et al., 2016; Bartlett et al., 2017; Brutzkus et al., 2017).

One important line of research that has attracted extensive attention is a model-recovery setup, i.e.,

given that the training samples (xi, yi)  (x, y) are generated i.i.d. from a distribution D based on a

neural network model with the ground truth parameter W , the goal is to recover the underlying model

parameter W , which is important for the network to generalize well (Mondelli & Montanari, 2018).

Previous studies along this topic can be mainly divided into two types of data generations. First, a

regression

problem,

for

example,

assumes

that

each

sample

y

is

generated

as

y

=

1 K

K k=1

(wk

x),

where wk  Rd is the weight vector of the kth neuron, 1  k  K, and the input x  Rd is Gaussian.

This type of regression problem has been studied in various settings. In particular, (Soltanolkotabi,

2017) studied the single-neuron model under ReLU activation, (Zhong et al., 2017b) studied the one-

hidden-layer multi-neuron network model, and (Li & Yuan, 2017) studied a two-layer feedforward

networks with ReLU activations and identity mapping. Second, for a classification problem, suppose

each

label

y



{0, 1}

is

drawn

under

the

conditional

distribution

P(y

=

1|x)

=

1 K

K k=1

(wk

x),

where wk  Rd is the weight vector of the kth neuron, 1  k  K, and the input x  Rd is Gaussian.

Such a problem has been studied in (Mei et al., 2016) in the case with a single neuron.

For both the regression and the classification settings, in order to recover the neural network parameters, all previous studies considered (stochastic) gradient descent over the squared loss, i.e.,

1 qu (W ; x, y) = 2

y- 1 K

K



wi x

i=1

2
,

(1)

1

Under review as a conference paper at ICLR 2019

which yields gradient and Hessian in relatively simple forms to assist the landscape characterization of the function as well as model recovery analysis.

However, for the classification problem, the cross entropy objective used in practice takes the following form

(W ; x, y) = -y · log

1K K  wi x

i=1

- (1 - y) · log

1- 1 K

K



wi x

i=1

. (2)

The geometry as well as the model recovery problem based on the entropy loss function have not yet been understood. It is expected that such a loss function is very challenging to analyze, not just because it is nonconvex with multiple neurons, but also because the gradient and Hessian take much more complicated forms compared with the squared loss. The main focus of this paper is to develop technical analysis for guaranteed model recovery under the challenging cross entropy loss function in eq. (2) for the classification problem in the multi-neuron case.

Furthermore, previous studies provided two types of statistical guarantees for such model recovery problems using the squared loss. More specifically, (Zhong et al., 2017b) showed that in the local neighborhood of the ground truth, the Hessian of the empirical loss function is positive definite for each given point under independent high probability event. Hence, their guarantee for gradient descent to converge to the ground truth requires a fresh set of samples at every iteration, thus the total sample complexity will depend on the number of iterations. On the other hand, studies such as (Mei et al., 2016; Soltanolkotabi, 2017) establish certain types of uniform geometry such as strong convexity so that resampling per iteration is not needed for gradient descent to have guaranteed linear convergence as long as it enters such a local neighborhood. However, such a stronger statistical guarantee without per-iteration resampling have only been shown for the squared loss function. In this paper, we aim at developing such a strong statistical guarantee for the loss function in eq. (2), which is much more challenging but more practical than the squared loss for the classification problem.

1.1 OUR CONTRIBUTIONS

This study provides the first performance guarantee for the recovery of one-hidden-layer neural networks using the cross entropy loss function, to the best of our knowledge. More specifically, our contributions are summarized as follows.

· For multi-neuron classification problem with sigmoid activations, we show that, if the input is

Gaussian,

the

empirical

risk

function

fn(W )

=

1 n

n i=1

(W ; xi) based on the cross entropy

loss in eq. (2) is uniformly strongly convex in a local neighborhood of the ground truth W of size

O(1/K3/2) as soon as the sample size is O(dK5 log2 d), where d is the input dimension and K is

the number of neurons.

· We further show that, if initialized in this neighborhood, gradient descent converges linearly to a critical point Wn (which we show to exist), with a sample complexity of O(dK5 log2 d), which is near-optimal up to a polynomial factor in K and log d. Due to the nature of quantized labels here,

the recover of W is only up to certain statistical accuracy, and Wn converges to W at a rate
of O( dK9/2 log n/n) in the Frobenius norm. Furthermore, such a convergence guarantee does
not require a fresh set of samples at each iteration due to the uniform strong convexity in the local neighborhood. To obtain -accuracy, it requires a computational complexity of O(ndK2 log(1/ )).

· We adopt the tensor method proposed in (Zhong et al., 2017b), and show it provably provides an initialization in the neighborhood of the ground truth. In particular, our proof replaces the homogeneous assumption on activation functions in (Zhong et al., 2017b) by a mild condition on the curvature of activation functions around W , which holds for a larger class of activation functions including sigmoid and tanh.

In order to analyze the challenging cross-entropy loss function, our proof develops various new machineries in order to exploit the statistical information of the geometric curvatures, including the gradient and Hessian of the empirical risk, and to develop covering arguments to guarantee uniform concentrations. Our technique also yields similar performance guarantees for the classification problem using the squared loss in eq. (1), which we omit due to space limitations, as it is easier to analyze than cross entropy.

2

Under review as a conference paper at ICLR 2019
1.2 RELATED WORK
Due to page limitations we focus on the most relevant literature on theoretical and algorithmic aspects of learning shallow neural networks via nonconvex optimization.
The parameter recovery viewpoint is relevant to the success of non-convex learning in signal processing problems such as matrix completion, phase retrieval, blind deconvolution, dictionary learning and tensor decomposition (Sun & Luo, 2016; Cande`s et al., 2015; Ge & Ma, 2017; Ge et al., 2016; Sun et al., 2015; Bhojanapalli et al., 2016; Ma et al., 2017), to name a few. The statistical model for data generation effectively removes worst-case instances and allows us to focus on average-case performance, which often possess much benign geometric properties that enable global convergence of simple local search algorithms.
The studies of one-hidden-layer network model can be further categorized into two classes, landscape analysis and model recovery. In the landscape analysis, it is known that if the network size is large enough compared to the data input, then there are no spurious local minima in the optimization landscape, and all local minima are global (Soltanolkotabi et al., 2017; Boob & Lan, 2017; Safran & Shamir, 2016; Nguyen & Hein, 2017). For the case with multiple neurons (2  K  d) in the under-parameterized setting, the work of Tian (Tian, 2017) studied the landscape of the population squared loss surface with ReLU activations. In particular, there exist spurious bad local minima in the optimization landscape (Ge et al., 2017; Safran & Shamir, 2017) even at the population level. Zhong et. al. (Zhong et al., 2017b) provided several important characterizations for the local Hessian for the regression problem for a variety of activation functions for the squared loss.
In the model recovery problem, the number of neurons is smaller than the dimension of inputs. In the case with a single neuron (K = 1), under Gaussian input, (Soltanolkotabi, 2017) showed that gradient descent converges linearly when the activation function is ReLU, i.e. (z) = max{z, 0}, with a zero initialization, as long as the sample complexity is O(d) for the regression problem. On the other end, (Mei et al., 2016) showed that when (·) has bounded first, second and third derivatives, there is no other critical points than the unique global minimum (within a constrained region of interest), and (projected) gradient descent converges linearly with an arbitrary initialization, as long as the sample complexity is O(d log2 d) with sub-Gaussian inputs for the classification problem using the squared loss. Moreover, (Zhong et al., 2017b) shows that the ground truth From a technical perspective, our study differs from all the aforementioned work in that the cross entropy loss function we analyze has a very different form. Furthermore, we study the model recovery classification problem under the multi-neuron case, which has not been studied before.
Finally, we note that several papers study one-hidden-layer or two-layer neural networks with different structures under Gaussian input. For example, (Brutzkus & Globerson, 2017; Du et al., 2017a;b; Zhong et al., 2017a) studied the non-overlapping convolutional neural network, (Li & Yuan, 2017) studied a two-layer feedforward networks with ReLU activations and identity mapping, and (Feizi et al., 2017) introduced the Porcupine Neural Network. These results are not directly comparable to ours since both the networks and the loss functions are different.
1.3 PAPER ORGANIZATION AND NOTATIONS
The rest of the paper is organized as follows. Section 2 describes the problem formulation. Section 3 presents the main results on local geometry and local linear convergence of gradient descent. Section 4 discusses the initialization method. Numerical examples are demonstrated in Section 5, and finally, conclusions are drawn in Section 6.
Throughout this paper, we use boldface letters to denote vectors and matrices, e.g. w and W . The transpose of W is denoted by W , and W , W F denote the spectral norm and the Frobenius norm. For a positive semidefinite (PSD) matrix A, we write A 0. The identity matrix is denoted by I. The gradient and the Hessian of a function f (W ) is denoted by f (W ) and 2f (W ), respectively. Let i (W ) denote the i-th singular value of W . Denote · 1 as the sub-exponential norm of a random variable. We use c, C, C1, . . . to denote constants whose values may vary from line to line. For nonnegative functions f (x) and g(x), f (x) = O (g(x)) means there exist positive constants c and a such that f (x)  cg(x) for all x  a; f (x) =  (g(x)) means there exist positive constants c and a such that f (x)  cg(x) for all x  a.
3

Under review as a conference paper at ICLR 2019

2 PROBLEM FORMULATION

We first describe the generative model for training data, and then describe the gradient descent algorithm for learning the network weights.

2.1 MODEL

Suppose N (0, I).

we are given n training samples Assume the activation function is

{s(igxmi,oyiid),}iin.=e.1(z()x=, y1)/t(h1at+aree-dzr)afwornail.li.zd..,Cwohnedrietioxned

on x  Rd, we consider the classification setting, where y is mapped to a discrete label using the

one-hidden layer neural network model as follows:

P(y

=

1|x)

=

1 K

K

(wk

x).

k=1

(3)

and P(y = 0|x) = 1 - P(y = 1|x), where K is the number of neurons.

Our goal is to estimate W

= [w1, · · · , wK ], via minimizing the following empirical risk function:

1n

fn(W ) = n

(W ; xi) ,

i=1

(4)

where (W ; x) := (W ; x, y) is the cross entropy loss, i.e., the negative log-likelihood function, i.e.,

(W ; x) = - y · log

1K K  wi x

i=1

+ (1 - y) · log

1- 1 K

K



wi x

i=1

.

Let w = vec(W ) = w1 , · · · , wK  RdK be the vectorized form of W . With slight abuse of notation, we denote the gradient and Hessian of (W ; x) with respect to the vector w.

2.2 GRADIENT DESCENT
To estimate W , since (4) is a highly nonconvex function, vanilla gradient descent with an arbitrary initialization may get stuck at local minima. Therefore, we implement the gradient descent algorithm with a well-designed initialization scheme that is described in detail in Section 4. The update rule is given as
Wt+1 = Wt - fn (Wt) , where  is the step size. The algorithm is summarized in Algorithm 1.

Algorithm 1 Gradient Descent

Input: Training data {(xi, yi)}in=1, step Initialization: W0  INITIALIZATION

size , iteration ({(xi, yi)}in=1)

T

Gradient Descent: for t = 0, 1, · · · , T

Wt+1 = Wt - fn (Wt) .

Output: WT

We note that throughout the execution of the algorithm, the same set of training samples is used which is the standard implementation of gradient descent. This is in sharp contrast to existing work such as Zhong et al. (2017b) that employs the impractical scheme of resampling, where a fresh set of training samples is used at every iteration of gradient descent.

3 MAIN RESULTS
Before stating our main results, we first introduce an important quantity regarding (z) that captures the geometric properties of the loss function, distilled in (Zhong et al., 2017b).

4

Under review as a conference paper at ICLR 2019

Function value of 

4 ×10-3
3.5
3
2.5
2
1.5
1
0.5
0 0.5 0.6 0.7 0.8 0.9 1 1.1 1.2 1.3 1.4 1.5

Figure 1:  () for sigmoid activation. Definition 1. Let q() = EzN (0,1)[ ( · z)zq], q  {0, 1, 2}, and q() = EzN (0,1)[ 2( · z)zq], q  {0, 2}. Define () as
() = min{0() - 02() - 12(), 2() - 12() - 22()}.
Note that the definition here is different from that in (Zhong et al., 2017b, Property 3.2) but consistent with (Zhong et al., 2017b, Lemma D.4) which removes the third term in (Zhong et al., 2017b, Property 3.2). For the activation function considered in this paper, the first two terms suffice. We depict () as a function of  in a certain range for the sigmoid activation in Fig. 1. It is easy to observe that () > 0 for all  > 0.

3.1 LOCAL STRONG CONVEXITY We first characterize the local strong convexity of fn(W ) in a neighborhood of the ground truth W . Let B (W , r) denote a Euclidean ball centered at W  Rd×K with a radius r, i.e.
B (W , r) = W  Rd×K : W - W F  r .

Let i := i (W ) denote the i-th singular value of W . Let the condition number be  = 1/K ,

and  =

K i=1

(i/K

).

The

following

theorem

guarantees

the

Hessian

of

the

empirical

risk

function

fn(W ) in the local neighborhood of W is positive definite with high probability.

Theorem 1. For the classification model (3) with sigmoid activation function, assume W F  1, then there exists some constant C, such that if

n  C · dK5 log2 d ·

2 2 ,

 (K )

then with probability at least 1 - d-10, for all W  B(W , r),



1 K2

·

 (K ) 2

·I

2fn (W )

C ·I

hold, where r := min

C
3
K2

·

(K 2 

)

,

0.7

.

We note that all column permutations of W are equivalent global minima of the loss function, and Theorem 1 applies to all such permutation matrices of W . The proof of Theorem 1 is outlined in
Appendix A. Theorem 1 guarantees that the Hessian of the empirical cross-entropy loss function fn(W ) is positive definite (PD) in a neighborhood of the ground truth W , as long as (K) > 0 (i.e. W is full-column rank), when the sample size n is sufficiently large for the sigmoid activation. The bounds in Theorem 1 depend on the dimension parameters of the network (n and K), as well as the activation function and the ground truth ((K), ). As a special case, suppose W is composed of orthonormal columns with (K) = O(1),  = 1,  = 1. Then,Theorem 1 guarantees (1/K2)I 2fn (W ) C within the neighborhood B(W , (1/K K)), as soon as the sample complexity n = (dK5 log2 d). The sample complexity is order-wise near-optimal in d up to polynomial factors of K and log d, since the number of unknown parameters is dK.

5

Under review as a conference paper at ICLR 2019

3.2 PERFORMANCE GUARANTEES OF GRADIENT DESCENT

For the classification problem, due to the nature of quantized labels, W is no longer a critical point

of fn(W ). By the strong convexity of the empirical risk function fn(W ) in the local neighborhood of W , there can exist at most one critical point in B(W , r), which is the unique local minimizer in

B (W , r) if it exists. The following theorem shows that there indeed exists such a critical point Wn,

which is provably close to the ground truth W , and gradient descent converges linearly to Wn.

Theorem 2. For the classification model (3) with sigmoid activation function, and assume W F 

1, there exist some constants C, C1 > 0 such that if the sample size n  C · dK5 log2 d ·

2  (K )

2
,

then with probability at least 1 - d-10, there exists a unique critical point Wn in B(W , r) with

r := min

c K 3/2

·

(K 2 

)

,

0.7

, which satisfies

K9/42 d log n

Wn - W

 C1
F

 (K )

. n

(5)

Moreover, if the initial point W0  B (W , r), then gradient descent converges linearly to Wn, i.e.

where Hmin = 

Wt - Wn  (1 - Hmin)t W0 - Wn
FF

1 K2

·

(K ) 2 

, as long as the step size  = 

1 K2

·

(K ) 2 

.

(6)

Similarly to Theorem 1, Theorem 2 also holds for all column permutations of W . The proof can be found in Appendix B. Theorem 2 guarantees that there exists a critical point Wn in B(W , r) which converges to W at the rate of O(K9/4 d log n/n), and therefore W can be recovered
consistently as n goes to infinity. Moreover, gradient descent converges linearly to Wn at a linear rate, as long as it is initialized in the basin of attraction. To achieve -accuracy, i.e. Wt - Wn  , it
F
requires a computational complexity of O ndK2 log (1/ ) , which is linear in n, d and log(1/ ).

4 INITIALIZATION

Our initialization adopts the tensor method proposed in (Zhong et al., 2017b). In this section, we first briefly describe this method, and then present the performance guarantee of the initialization with remarks on the differences from that in (Zhong et al., 2017b).
4.1 PRELIMINARY AND ALGORITHM

This subsection briefly introduces the tensor method proposed in (Zhong et al., 2017b), to which a

reader can refer for more details. We first define a product  as follows. If v  Rd is a vector and I

is the identity matrix, then vI = dj=1[v  ej  ej + ej  v  ej + ej  ej  v]. If M is a

symmetric rank-r matrix factorized as M =

r i=1

si

vi

vi

and I is the identity matrix, then

r d6

M I = si

Al,i,j ,

i=1 j=1 l=1

where A1,i,j = vi  vi  ej  ej , A2,i,j = vi  ej  vi  ej , A3,i,j = ej  vi  vi  ej , A4,i,j = vi  ej  ej  vi, A5,i,j = ej  vi  ej  vi and A6,i,j = ej  ej  vi  vi.

Definition 2. Define M1, M2, M3, M4 and m1,i, m2,i, m3,i, m4,i as follows: M1 = E[y · x], M2 = E[y · (x  x - I)], M3 = E[y · (x3 - xI)], M4 = E[y · (x4 - (x  x)I + II)],
m1,i = 1( wi ), m2,i = 2( wi ) - 0( wi ), m3,i = 3( wi ) - 31( wi ), m4,i = 4( wi ) + 30( wi ) - 62( wi ), where j() = EzN (0,1)[( · z)zj], j = 0, 1, 2, 3, 4.

6

Under review as a conference paper at ICLR 2019

Definition 3. Let   Rd denote a randomly picked vector. We define P2 and P3 as follows: P2 = Mj2 (I, I, , · · · , ),1 where j2 = min{j  2|Mj = 0}, and P3 = Mj3 (I, I, I, , · · · , ), where j3 = min{j  3|Mj = 0}.

We further denote w = w/ w . The initialization algorithm based on the tensor method is

summarized in Algorithm 2, which includes two major steps. Step 1 first estimates the di-

rection of each column of W by decomposing P2 to approximate the subspace spanned by

{w1, w2, · · · tensor R3 =

, wK} P3 (V ,

(denoted V,V) 

by V ), RK×K

then reduces the third-order tensor P3 ×K , and applys non-orthogonal tensor

to a lower-dimension decomposition on R3

to output the estimate siV wi , where si  {1, -1} is a random sign. Step 2 approximates the

magnitude of wi and the sign si by solving a linear system of equations.

Algorithm 2 Initialization via Tensor Method Input: Partition n pairs of data {(xi, yi)}in=1 into three parts D1, D2, D3. Output:
1: Estimate P2 of P2 from data set D1. 2: V  POWERMETHOD(P2, K). 3: Estimate R3 of P3(V , V , V ) from data set D2. 4: {ui}i[K]  KCL(R3). 5: {wi(0)}i[K]  RECMAG(V , {ui}i[K], D3).

4.2 PERFORMANCE GUARANTEE OF INITIALIZATION

For the classification problem, we make the following technical assumptions, similarly in (Zhong et al., 2017b, Assumption 5.3) for the regression problem.

Assumption 1. The activation function (z) satisfies the following conditions:1. If Mj = 0, then

K j-2

mj,i wi 

wi wi

i=1

= 0 j,

K
mj,i wi  j-3 (V wi )vec((V
i=1
2. At least one of M3 and M4 is non-zero.

wi )(V

wi ) )

=0

for j  3

Furthermore, we do not require the homogeneous assumption ((i.e., (az) = apz for an integer
p)) required in (Zhong et al., 2017b), which can be restrictive. Instead, we assume the following
condition on the curvature of the activation function around the ground truth, which holds for a larger
class of activation functions such as sigmoid and tanh.
Assumption 2. Let l1 be the index of the first nonzero Mi where i = 1, . . . , 4. For the activation function  (·), there exists a positive constant  such that ml1,i(·) is strictly monotone over the interval ( wi - , wi + ), and the derivative of ml1,i(·) is lower bounded by some constant for all i.

We next present the performance guarantee for the initialization algorithm in the following theorem.

Theorem 3. For the classification model (3), under Assumptions 1 and 2, if the sample size n  dpoly (K, , t, log d, 1/ ), then the output W0  Rd×K of Algorithm 2 satisfies

W0 - W F  poly (K, ) W F,

(7)

with probability at least 1 - d-(t).

The proof of Theorem 3 consists of (a) showing the estimation of the direction of W is sufficiently accurate and (b) showing the approximation of the norm of W is accurate enough. Our proof of part (a) is the same as that in (Zhong et al., 2017b), but our argument in part (b) is different, where we relax the homogeneous assumption on activation functions. More details can be found in the supplementary materials in Appendix C.
1See (15) in the supplemental materials for definition.

7

Under review as a conference paper at ICLR 2019

Success rate NMSE NMSE

1 0.8 0.6 0.4 0.2
0 0

(a) 101

10 0

10 -1

20 40 n/(d log 2 d)

d=15 d=20 d=25
60

10 -2 10 -3
0

(b)

d=20 d=35 d=50

10 -1

100

200

300

n/d log(n)

400

10 -2

(c)
Cross Entropy Loss Square Loss

50

100

150

n/d log(n)

200

Figure 2: Fix K = 3. (a) Success rate of converging to the same local minima with respect to the sample complexity for various d; (b) Average estimation error of gradient descent in a local neighborhood of the ground truth with respect to the sample complexity for various d; (c) Average estimation error of gradient descent using different objective functions in a local neighborhood of the ground truth with respect to the sample complexity when d = 20.

5 NUMERICAL EXPERIMENTS

In this section, we first implement gradient descent to verify that the empirical risk function is strongly convex in the local region around W . If we initialize multiple times in such a local region, it is

expected that gradient descent converges to the same critical point Wn, with the same set of training samples. Given a set of training samples, we randomly initialize multiple times, and then calculate the variance of the output of gradient descent. Denote the output of the th run as wn( ) = vec(Wn( ))

and the mean of the runs as w¯. The error is calculated as SDn =

1 L

L =1

wn( ) - w¯ 2, where

L = 20 is the total number of random initializations. Adopted in (Mei et al., 2016), it quantifies the

standard deviation of the estimator Wn under different initializations with the same set of training samples. We say an experiment is successful, if SDn  10-2.

Figure 2 (a) shows the successful rate of gradient descent by averaging over 50 sets of training samples for each pair of n and d, where K = 3 and d = 15, 20, 25 respectively. The maximum iterations for gradient descent is set as itermax = 3500. It can be seen that as long as the sample complexity is large enough, gradient descent converges to the same local minima with high probability.

We next show that the statistical accuracy of the local minimizer for gradient descent if it is ini-

tialized close enough to the ground truth. Suppose we initialize around the ground truth such

that W0 - W F  0.1 · W F. We calculate the average estimation error as

L =1

Wn( ) -

W

2 F

/(L

W

F2 ) over L = 100 Monte Carlo simulations with random initializations. Fig. 2 (b)

shows the average estimation error with respect to the sample complexity when K = 3 and

d = 20, 35, 50 respectively. It can be seen that the estimation error decreases gracefully as we

increase the sample size and matches with the theoretical prediction of error rates reasonably well.

We further compare the performance of gradient descent algorithm applied to both the cross entropy loss and the squared loss, respectively. As shown in Fig 2 (c), when K = 3, d = 20, cross entropy loss with gradient descent achieves a much lower error than the squared loss. Clearly, the cross entropy loss is favored in the classification problem over the squared loss.

6 CONCLUSIONS

In this paper, we have studied the model recovery of a one-hidden-layer neural network using the cross entropy loss in a multi-neuron classification problem. In particular, we have characterized the sample complexity to guarantee local strong convexity in a neighborhood (whose size we have characterized as well) of the ground truth when the training data are generated from a classification model. This guarantees that with high probability, gradient descent converges linearly to the ground truth if initialized properly. In the future, it will be interesting to extend the analysis in this paper to more general class of activation functions, particularly ReLU-like activations; and more general network structures, such as convolutional neural networks (Du et al., 2017b; Zhong et al., 2017a).

8

Under review as a conference paper at ICLR 2019
REFERENCES
Andrew R Barron. Universal approximation bounds for superpositions of a sigmoidal function. IEEE Transactions on Information theory, 39(3):930­945, 1993.
Peter L Bartlett, Dylan J Foster, and Matus J Telgarsky. Spectrally-normalized margin bounds for neural networks. In Advances in Neural Information Processing Systems, pp. 6241­6250, 2017.
Srinadh Bhojanapalli, Behnam Neyshabur, and Nati Srebro. Global optimality of local search for low rank matrix recovery. In Advances in Neural Information Processing Systems, pp. 3873­3881, 2016.
Digvijay Boob and Guanghui Lan. Theoretical properties of the global optimizer of two layer neural network. arXiv preprint arXiv:1710.11241, 2017.
Alon Brutzkus and Amir Globerson. Globally optimal gradient descent for a convnet with gaussian inputs. arXiv preprint arXiv:1702.07966, 2017.
Alon Brutzkus, Amir Globerson, Eran Malach, and Shai Shalev-Shwartz. Sgd learns overparameterized networks that provably generalize on linearly separable data. arXiv preprint arXiv:1710.10174, 2017.
E. J. Cande`s, X. Li, and M. Soltanolkotabi. Phase retrieval via Wirtinger flow: Theory and algorithms. IEEE Transactions on Information Theory, 61(4):1985­2007, April 2015.
George Cybenko. Approximation by superpositions of a sigmoidal function. Mathematics of control, signals and systems, 2(4):303­314, 1989.
Yann N Dauphin, Razvan Pascanu, Caglar Gulcehre, Kyunghyun Cho, Surya Ganguli, and Yoshua Bengio. Identifying and attacking the saddle point problem in high-dimensional non-convex optimization. In Advances in neural information processing systems, pp. 2933­2941, 2014.
Simon S Du, Jason D Lee, and Yuandong Tian. When is a convolutional filter easy to learn? arXiv preprint arXiv:1709.06129, 2017a.
Simon S Du, Jason D Lee, Yuandong Tian, Barnabas Poczos, and Aarti Singh. Gradient descent learns one-hidden-layer cnn: Don't be afraid of spurious local minima. arXiv preprint arXiv:1712.00779, 2017b.
Soheil Feizi, Hamid Javadi, Jesse Zhang, and David Tse. Porcupine neural networks:(almost) all local optima are global. arXiv preprint arXiv:1710.02196, 2017.
Rong Ge and Tengyu Ma. On the optimization landscape of tensor decompositions. arXiv preprint arXiv:1706.05598, 2017.
Rong Ge, Jason D Lee, and Tengyu Ma. Matrix completion has no spurious local minimum. In Advances in Neural Information Processing Systems, pp. 2973­2981, 2016.
Rong Ge, Jason D Lee, and Tengyu Ma. Learning one-hidden-layer neural networks with landscape design. arXiv preprint arXiv:1711.00501, 2017.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770­778, 2016.
Kurt Hornik, Maxwell Stinchcombe, and Halbert White. Multilayer feedforward networks are universal approximators. Neural networks, 2(5):359­366, 1989.
Serge Lang. Real and functional analysis. Springer-Verlag, New York,, 10:11­13, 1993.
Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. Nature, 521(7553):436­444, 2015.
Yuanzhi Li and Yang Yuan. Convergence analysis of two-layer neural networks with relu activation. In Advances in Neural Information Processing Systems, pp. 597­607, 2017.
9

Under review as a conference paper at ICLR 2019
Cong Ma, Kaizheng Wang, Yuejie Chi, and Yuxin Chen. Implicit regularization in nonconvex statistical estimation: Gradient descent converges linearly for phase retrieval, matrix completion and blind deconvolution. arXiv preprint arXiv:1711.10467, 2017.
Song Mei, Yu Bai, and Andrea Montanari. The landscape of empirical risk for non-convex losses. arXiv preprint arXiv:1607.06534, 2016.
Marco Mondelli and Andrea Montanari. On the connection between learning two-layers neural networks and tensor decomposition. arXiv preprint arXiv:1802.07301, 2018.
Quynh Nguyen and Matthias Hein. The loss surface of deep and wide neural networks. In International Conference on Machine Learning, pp. 2603­2612, 2017.
Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual recognition challenge. International Journal of Computer Vision, 115(3):211­252, 2015.
Itay Safran and Ohad Shamir. On the quality of the initial basin in overspecified neural networks. In International Conference on Machine Learning, pp. 774­782, 2016.
Itay Safran and Ohad Shamir. Spurious local minima are common in two-layer relu neural networks. arXiv preprint arXiv:1712.08968, 2017.
David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering the game of go with deep neural networks and tree search. nature, 529(7587):484­489, 2016.
Mahdi Soltanolkotabi. Learning relus via gradient descent. arXiv preprint arXiv:1705.04591, 2017.
Mahdi Soltanolkotabi, Adel Javanmard, and Jason D Lee. Theoretical insights into the optimization landscape of over-parameterized shallow neural networks. arXiv preprint arXiv:1707.04926, 2017.
J. Sun, Q. Qu, and J. Wright. Complete dictionary recovery using nonconvex optimization. International Conference on Machine Learning, pp. 2351­2360, 2015.
Ruoyu Sun and Zhi-Quan Luo. Guaranteed matrix completion via non-convex factorization. IEEE Transactions on Information Theory, 62(11):6535­6579, 2016.
Matus Telgarsky. benefits of depth in neural networks. In Conference on Learning Theory, pp. 1517­1539, 2016.
Yuandong Tian. An analytical formula of population gradient for two-layered relu network and its applications in convergence and critical point analysis. arXiv preprint arXiv:1703.00560, 2017.
R. Vershynin. Introduction to the non-asymptotic analysis of random matrices. Compressed Sensing, Theory and Applications, pp. 210 ­ 268, 2012.
Roman Vershynin. Introduction to the non-asymptotic analysis of random matrices. arXiv preprint arXiv:1011.3027, 2010.
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep learning requires rethinking generalization. arXiv preprint arXiv:1611.03530, 2016.
Kai Zhong, Zhao Song, and Inderjit S Dhillon. Learning non-overlapping convolutional neural networks with multiple kernels. arXiv preprint arXiv:1711.03440, 2017a.
Kai Zhong, Zhao Song, Prateek Jain, Peter L. Bartlett, and Inderjit S. Dhillon. Recovery guarantees for one-hidden-layer neural networks. In Proceedings of the 34th International Conference on Machine Learning, volume 70, pp. 4140­4149, 2017b.
10

Under review as a conference paper at ICLR 2019

A PROOF OF THEOREM 1

To begin, denote the population loss function as f (W ) = E [fn(W )] = E [ (W ; x)] ,
where the expectation is taken with respect to the distribution of the training sample (x; y). The proof of Theorem 1 follows the following steps:

(8)

1. We first show that the Hessian 2f (W ) of the population loss function is smooth with respect to 2f (W ) (Lemma 1);
2. We then show that 2f (W ) satisfies local strong convexity and smoothness in a neighborhood of W , B(W , r) with appropriately chosen radius by leveraging similar properties of 2f (W ) (Lemma 2);
3. Next, we show that the Hessian of the empirical loss function 2fn(W ) is close to its popular counterpart 2f (W ) uniformly in B(W , r) with high probability (Lemma 3).
4. Finally, putting all the arguments together, we establish 2fn(W ) satisfies local strong convexity and smoothness in B(W , r).

We will first show that the Hessian of the population risk is smooth enough around W following lemma.

Lemma 1. For sigmoid activations, assume W F  1, we have

2f (W ) - 2f (W )

C



K1 2

·

W -W

F,

holds for some large enough constant C, when W - W F  0.7.

in the (9)

The proof is given in Appendix D.2. Lemma 1 together with the fact that 2f (W ) be lower and upper bounded, will allow us to bound 2f (W ) in a neighborhood around ground truth, given below.
Lemma 2 (Local Strong Convexity and Smoothness of Population Loss). For sigmoid activations, there exists some constant C, such that

4 K2

·

 (K ) 2

·

I

2f (W )

C · I,

holds for all W  B(W , r) with r := min

C
3
K2

·

(K 2 

)

,

0.7

.

The proof is given in Appendix D.3. The next step is to show the Hessian of the empirical loss function is close to the Hessian of the population loss function in a uniform sense, which can be summarized as following.
Lemma 3. For sigmoid activations, there exists constant C such that as long as n  C · dK log dK, with probability at least 1 - d-10, the following holds

sup 2fn (W ) - 2f (W )  C
W B(W ,r)

dK log n ,
n

where r := min

C
3
K2

·

(K 2 

)

,

0.7

.

The proof can be found in Appendix D.4.

The final step is to combine Lemma 3 and Lemma 1 to obtain Theorem 1 as follows,

Proof of Theorem 1. By Lemma 3 and Lemma 2, we have with probability at least 1 - d-10,

2fn(W ) 2f (W ) - 2fn (W ) - 2f (W ) · I



1 K2

·

 (K ) 2

·I -

C·

dK log n n

· I.

11

Under review as a conference paper at ICLR 2019

As long as the sample size n is set such that

C·

dK log n n



1 K2

·



(K 2

)

,

i.e. n  C · dK5 log2 d ·

2  (K )

2
, we have

2fn(W )



1 K2

·

 (K ) 2

· I.

holds for all W  B (W , r). Similarly, we have

holds for all W  B (W , r).

2fn(W ) C · I

B PROOF OF THEOREM 2

We have established that fn (W ) is strongly convex in B(W , r) in Theorem 1, thus there exists at most one critical point in B(W , r). The proof of Theorem 2 follows the steps below:

1. We first show that the gradient fn (W ) concentrates around f (W ) in B(W , r) (Lemma 4), and then invoke (Mei et al., 2016, Theorem 2) to guarantee there indeed exists a critical point Wn in B(W , r);
2. We next show Wn is close to W and gradient descent converges linearly to Wn with a properly chosen step size.

The following lemma establishes that fn (W ) uniformly concentrates around f (W ).

Lemma 4. For sigmoid activation function, assume W F  1, there exists constant C such that

as long as n  CdK log(dK), with probability at least 1 - d-10, the following holds



sup fn (W ) - f (W )  C
W B(W ,r)

d K log n ,
n

where r := min

C
3
K2

·

(K 2 

)

,

0.7

.

Notice that for the population risk function, f (W ), W is the unique critical point in B(W , r) due to local strong convexity. With Lemma 3 and Lemma 4, we can invoke (Mei et al., 2016, Theorem 2), which guarantees the following.
Corollary 1. There exists one and only one critical point Wn  B (W , r) that satisfies
fn Wn = 0.

We first show that Wn is close to W . By the intermediate value theorem, W  B (W , r) such that

fn Wn = fn (W ) + fn (W ) , vec Wn - W

1

+ vec 2

Wn - W

 fn (W ) ,

2fn (W ) vec Wn - W

(10)

where the last inequality follows from the optimality of Wn. By Theorem 1, we have

1 vec
2

Wn - W

2fn (W ) vec Wn - W



1 ·  (K ) K2 2

Wn - W

2
.
F
(11)

12

Under review as a conference paper at ICLR 2019

On the other hand, by the Cauchy-Schwarz inequality, we have

fn (W ) , vec Wn - W

 fn (W ) 2 Wn - W F

dK1/2 log n   n Wn - W F,

where the last line follows from Lemma 4. Plugging (11) and (12) into (10), we have

K

9 4

2



d log n

Wn - W F    (K )

. n

(12) (13)

Now we have established there indeed exists a critical point in B(W , r). We can establish local linear convergence of gradient descent as below. Let Wt be the estimate at the t-th iteration. According to the update rule, we have

22
Wt+1 - Wn = Wt - fn (Wt) - Wn
FF

=

Wt - Wn

2 F

+

2

fn (Wt)

2 F

-

2

fn (Wt) , vec

Wt - Wn

.

(14)

Moreover, by the fundamental theorem of calculus (Lang, 1993), fn (Wt) can be written as

fn (Wt) = fn (Wt) - fn Wn
1
= 2fn Wn +  Wt - Wn
0

d vec Wt - Wn ,

where W () = Wn +  Wt - Wn for   [0, 1]. By Theorem 1, we have

Hmin · I 2fn (W ()) Hmax · I,

where Hmin = 

1 K2

·

(K ) 2 

and Hmax = C. Therefore, we have

Hence,

2

fn (Wt)

2 F



Hm2 ax

Wt - Wn

.

F

Wt+1 - Wn

2 F





1 - 2Hmin + 2Hm2 ax

Wt - Wn

2 F

1

-

1 2 Hmin

2

Wt - Wn

2 F

as long as we set 

<

Hmin Hm2 ax

:=



1 K2

·

(K ) 2 

. In summary, gradient descent converges linearly to

the local minimizer Wn.

C PROOF OF THEOREM 3

The proof contains two parts. Part (a) proves that the estimation of the direction of W is sufficiently accurate, which follows the arguments similar to those in (Zhong et al., 2017b) and is only briefly summarized below. Part (b) is different, where we do not require the homogeneous condition for the activation function, and instead, our proof is based on a mild condition in Assumption 2. We detail our proof in part (b).

We first define a tensor operation as follows. For a tensor T  Rn1×n2×n3 and three matrices A  Rn1×d1 , B  Rn2×d2 , C  Rn3×d3 , the (i, j, k)-th entry of the tensor T (A, B, C) is given
by
n1 n2 n3

Ti ,j ,k Ai ,iBj ,j Ck ,k.

(15)

ijk

13

Under review as a conference paper at ICLR 2019

(a) In order to estimate the direction of each wi for i = 1, . . . , K, (Zhong et al., 2017b) shows that for the regression problem, if the sample size n  dpoly (K, , t, log d), then

wi - siV ui  poly (K, )

(16)

holds with high probability. Such a result also holds for the classification problem with only slight

difference in the proof as we describe as follows. The main idea of the proof is to bound the estimation

error of P2 and R3 via Bernstein inequality. For the regression problem, Bernstein inequality was applied to terms associated with each neuron individually, and the bounds were then put together via

triangle inequality in (Zhong et al., 2017b), whereas for the classification problem here, we apply

Bernstein inequality to terms associated with all neurons all together. Another difference is that the

label yi of the classification model is bounded by nature, whereas the output yi in the regression model needs to be upper bounded via homogeneously bounded conditions of the activation function.

A reader can refer to (Zhong et al., 2017b) for the details of the proof for this part.

(b) In order to estimate wi for i = 1, . . . , K, we provide a different proof from (Zhong et al., 2017b), which does not require the homogeneous condition on the activation function, but assumes a
more relaxed condition in Assumption 2.

We define a quantity Q1 as follows: Q1 = Ml1 (I, , · · · , ),

(17)

(l1 -1)
where l1 is the first non-zero index such that Ml1 = 0. For example, if l1 = 3, then Q1 takes the following form

1 Q1 = M3 (I, , ) = K

K

m3,i( wi

)



wi

2 wi ,

i=1

where w = w/ w and by definition

m3,i( wi ) = E  ( wi · z) z3 - 3E [ ( wi · z) z] .

(18) (19)

Clearly, Q1 has information of wi , which can be estimated by solving the following optimization problem:

1K

 = argminRK K

isiwi - Q1 ,

i=1

(20)

where each entry of the solution takes the form

i = si3m3,i( wi ) T siwi 2 .

(21)

In the initialization, we substitute Q1 (estimated from training data) for Q1, V ui (estimated in part

(a)) for siwi into (20), and obtain an estimate  of  . We then substitute  for  and V ui for siwi into (21) to obtain an estimate ai of wi via the following equation

i = si3m3,i(ai) T V ui 2 .

(22)

Furthermore, since ml1,i(x) has fixed sign for x > 0 and for l1  1, si can be estimated correctly

from the sign of i for i = 1, . . . , K.

For notational simplicity, let 1,i

:=

i si3(T siwi

)2

and 1,i

:=

i si3(T V

ui )2

,

and

then

(21)

and

(22)

become

1,i = m3,i(ai), 1,i = m3,i( wi ).

(23)

By Assumption 2 and (21), there exists a constant  > 0 such that the inverse function g(·) of m3,1(·) has upper-bounded derivative in the interval (1,i -  , 1,i +  ), i.e., |g (x)| <  for a constant .
By employing the result in (Zhong et al., 2017b), if the sample size n  dpoly (K, , t, log d), then

Q1 and Q1, V ui and siwi

can

be

arbitrarily

close

so

that

|1,i

-

1,i|

<

min{

,

r }.
K

Thus, by (23) and mean value theorem, we obtain

|ai - wi | = |g ()||1,i - 1,i|

(24)

where  is between 1,i and 1,i, and hence |g ()| < . Therefore, |ai -

wi

|  r , which is
K

the desired result.

14

Under review as a conference paper at ICLR 2019

D PROOF OF TECHNICAL LEMMAS

D.1 PRELIMINARIES

We introduce some useful definitions and results that will be used in the proofs. The first one is the definition of norms of random variable, i.e.

Definition 4 (Sub-gaussian and Sub-exponential norm). The sub-gaussian norm of a random variable X, denotes as X 2 , is defined as

X

2

=

sup

p-

1 2

(E

[|X

|p

])

1 p

,

p1

(25)

and the sub-exponential norm of X, denoted as X 1 , is defined as

X

1

=

sup

p-1

(E

[|X

|p

])

1 p

.

p1

(26)

The definition is summarized from (Vershynin, 2012, Def 5.7,Def 5.13), and if X 2 is upper bounded, then X is a sub-gaussian random variable and it satisfies

P (|X| > t)  exp

1 - ct2/

X

2 2

for all t  0.

(27)

Next we provide the calculations of the gradient and Hessian of E [ (W ; x)]. Let's denote p (W ) =

1 K

K i=1



wi x

, and then



E

 (W ; x) wj

=

E

-

1 K



1 K

K i=1



wi

x

-

1 K

K i=1



wi x

1 K

K i=1



wi x

1

-

1 K

K i=1



wi x

=E

-1 K

wj x

·

p (W p (W )

) - p (W ) (1 - p (W ))

·

x

 ·  wj x  x ,
(28)

E

2 (W ; x) wj wl

=E

j,l (p (W ) (1

(W ) - p (W )))2

·

xx

(29)

where if j = l,

1 j,l (W ) = K2 

wj x 

wl x ·

p (W )2 + p (W ) - 2p (W ) p (W )

,

and if j = l,

1 j,j (W ) = K2 

wj x 2 ·

p (W )2 + p (W ) - 2p (W ) p (W )

1

- K

wj x (p (W ) - p (W )) (p (W ) (1 - p (W ))) .

D.2 PROOF OF LEMMA 1
Proof. Let  = 2f (W ) - 2f (W ). For each (j, l)  [K] × [K], let j,l  Rd×d denote the (j, l)-th block of . Let a = [a1 , · · · , aK ]  RdK . Since by definition,

2f (W ) - 2f (W ) = max a (2f (W ) - 2f (W ))a
a =1

KK

= max
a =1

aj j,lal.

j=1 l=1

(30)

15

Under review as a conference paper at ICLR 2019

Next we will evaluate j,l. From (29) we can write the hessian block more concisely as

2f (W ) wj wl

=E

gj,l (W ) · xx

,

(31)

where gj,l (W )

=

j,l(W ) (p(W )(1-p(W )))2



R, and then by the mean value theorem, we can write gj,l (W )

as

K gj,l W

gj,l (W ) = gj,l (W ) +
k=1

wk , wk - wk

(32)

where W =  · W + (1 - ) W for some   (0, 1). Thus we can calculate j,l as

j,l

=

2f (W ) wj wl

-

2f (W ) wj wl

= E gj,l (W ) · xx - E gj,l (W ) · xx


K
= E 
k=1


gj,l W wk , wk - wk  · xx  ,

(33)

and plug it back to (30) we can obtain

2f (W ) - 2f (W )

KK

= max
a =1

aj j,lal

j=1 l=1



KK

K

= max

E 

a =1

j=1 l=1

k=1


gj,l W wk , wk - wk  · aj x

 al x 

KK

= max

E

a =1

j=1 l=1

K
Tj,l,k x, wk - wk
k=1

· aj x al x

KK
 max
a =1 j=1 l=1

K
E Tj2,l,k ·
k=1

K
E ( x, wk - wk )2 aj x 2 al x 2
k=1

KK
 max
a =1 j=1 l=1

K
E Tj2,l,k ·
k=1

K

wk - wk

2 2

·

aj

2 2

·

al 22,

k=1

(34)

for

the

third

equality

we

have

used

the

fact

that

gj,l(W )
wk

can

be

written

as

Tj,l,k

· x,

where

Tj,l,k



R,

since the variable of gj,l W is in the form of wi x. and for the last two inequalities, we have used

Cauchy-Schwarz inequality. Our next goal is to upper bound E Tj2,l,k . Further since

gj,l (W ) wk

=

1

·



(wj

x)

(wl

x)·(p(W )2+p(W )-2p(W
(p(W )(1-p(W )))2

K2 wk

)p(W )) ,

which

aligns

with

x

and

the

scalar

coefficient

is

upper

bounded

by

1 K2

·

(p(W

C
)(1-p(W

)))3

,

since

 (·),  (·),  (·) are all upper bounded, thus we leave only the denominator. And then



E

Tj2,l,k

C



K4

·

E

 

p

W

1 1-p W



6

 



C K4

·e

W

2
F,

(35)

holds for some constant C, where the second inequality follows from Lemma 5.

16

Under review as a conference paper at ICLR 2019

Lemma 5. Let x  N (0, I), t = max { w1 2, · · · wK 2} and z  Z such that z  1 , for the

sigmoid

activation

function

 (x)

=

1 1+e-x

,

the

following


E 
1 K

K i=1



wi x

1

1

-

1 K

K i=1



wi x

z     C · et2 ,

(36)

holds for a large enough constant C which depends on the constant z.

Plugging (35) into (34), we can obtain

2f (W ) - 2f (W

)



C

K

3 2

e

W

·2
F

W -W



C

K

1 2

e

W

·2
F

W -W

KK

F · max

aj 2 al 2

a =1

j=1 l=1

F,

(37)

Further since e W

2 F



C

· (1 +

W -W

F) when W - W

F  0.7, where we have used

the assumption that W F  1 thus we can conclude that if W - W F  0.7, then

2f (W ) - 2f (W

)

C



K

1 2

W -W

F

(38)

holds for some constant C.

D.3 PROOF OF LEMMA 2

Proof. We will first present upper and lower bounds of the Hessian of the population risk at ground truth, i.e. 2f (W ), and then apply Lemma 1 to obtain a uniform bound in the neighborhood of
W . As a reminder,



2f (W wj2

)

=

1 E K2

·





2f (W )

1

wjwl = E  K2 · 

1 K
1 K

 wj x 2

K i=1



wi

x

1

-

1 K

K i=1



wi

x

 wj x  wl x

K i=1



wi

x

1

-

1 K

K i=1



wi

x

  xx 
  xx  ,

(39) (40)

and let a = [a1 , · · · , aK ]  RdK , we can write

2f (W ) =

min a 2f (W ) a · I
a 2=1



1

min
a 2=1

K

2

E

 

1 K

K i=1



K i=1



wi

x

2
wi x ai x

1

-

1 K

K i=1



wi

x



4

min
a 2=1

K

2

E



K

i=1

wi x

ai x

2 

4 K2

·

 (K ) 2

·

I,


 

(41)

the second inequality holds due to the fact that

1 K

K i=1



wi

x

1

-

1 K

K i=1



wi

x



1 4

,

and

the

last

inequality

follows

from

(Zhong

et

al.,

2017b,

Lemmas

D.4

and

D.6).

17

Under review as a conference paper at ICLR 2019

Further more, we can uppder bound 2f (W ) as 2f (W )

max a 2f (W ) a · I
a 2=1



1

=

max
a 2=1

K2

E

 

1 K2

K i=1



wi

x

K i=1

K j=1



wi

x

2
ai x
1 -  wj x

 ·I 



1

max
a 2=1

K2

E



1 K2

K i=1



wi

x2

·

K i=1

K j=1



wi

x

K i=1

ai x 2

1 -  wj x

 ·I

C 14

K i=1



wi

x

max
a 2=1

K

2

E



1 K2

K i=1



wi

x

·

K i=1

ai x 2

1 -  wi x

 ·I

1

max
a 2=1

K

2

E

= C · I,

CK2 K 4
i=1

ai x 2

·I

by Cauchy-Schwarz inequality (42)

where for the third and fourth inequality we have used the fact that  wi x

1 -  wi x



1 4

and

KK
 wi
i=1 j=1

x

1 -  wj x

K
  wi x
i=1

1 -  wi x

K
=  wi x .
i=1

Thus together with the lower bound (41) we can conclude that

4 K2

·

 (K ) 2

·

I

2f (W )

C · I,

From Lemma 1, we have

2f (W ) - 2f (W )

C

K1 2

W -W

F,

therefore, when W - W F  0.7 and

C

K1 2

·

W -W

F



4 K2

·



(K 2

)

,

(43) (44)

i.e., when W - W

F  min

C
3
K2

·

(K 2 

)

,

0.7

for some constant C, we have

min 2f (W )  min 2f (W ) - 2f (W ) - 2f (W )

4 K2

·

 (K ) 2

-

C K1
2

W -W

F

4 K2

·



(K 2

)

.

Moreover, within the same neighborhood, by the triangle inequality we have

2f (W )  2f (W ) - 2f (W ) + 2f (W ) C.

(45) (46)
(47)

D.4 PROOF OF LEMMA 3
Proof. We adapt the analysis in (Mei et al., 2016) to our setting. Let N be the -covering number of the Euclidean ball B (W , r). It is known that log N  dK log (3r/ ) (Vershynin, 2010). Let W = {W1, · · · , WN } be the -cover set with N elements. For any W  B (W , r), let j (W ) = argminj[N ] W - Wj(W ) F  for all W  B (W , r).
18

Under review as a conference paper at ICLR 2019

For any W  B (W , r), we have

2fn (W ) - 2f (W )

1 n + +

n

2 (W ; xi) - 2 Wj(W ); xi

i=1

1

n
2

n

i=1

Wj(W ); xi - E 2

Wj(w); x

E 2 Wj(W ); x - E 2 (W ; x) .

Hence, we have

P sup 2fn (W ) - 2f (W )  t  P (At) + P (Bt) + P (Ct) ,
W B(W ,r)

where the events At, Bt and Ct are defined as

At = Bt =

1 sup W B(W ,r) n

n
2 (W ; xi) - 2
i=1

Wj(W ); xi

sup
W W

1 n

n

2

(W ; xi) - E 2

(W ; x)

i=1

t 3

t 3
,

,

Ct =

sup E 2 Wj(W ); x - E 2 (W ; x)

W B(W ,r)

t . 3

(48)
(49) (50) (51)

In the sequel, we will bound the terms P (At), P (Bt), and P (Ct), separately.

1. Upper bound P (Bt). Before continuing, let us state a simple technical lemma that is useful for our proof, whose proof can be found in (Mei et al., 2016).

Lemma 6. Let M  Rd×d be a symmetric d × d matrix and V be an -cover of unit-Euclidean-

norm ball B (0, 1), then

M  1 sup | v, M v |. 1 - 2 vV

(52)

Let V 1 be a 4

1 4

-cover of the ball B(0, 1)

= {W

 Rd×K

:

W F = 1}, where log |V 1 |  4

dK log 12. From Lemma 6, we know that

1 n

n

2

(W ; xi) - E 2

(W ; x)

i=1

 2 sup
vV 1
4

Taking the union bound over W and V 1 yields 4

v,

1 n

n

2

(W ; xi) - E

2

(W ; x)

i=1

(53)



P (Bt)  P  sup
W W ,vV 1
4

1n n
i=1

v,

2

(W ; xi) - E

2

(W ; x)

v



t 6

 edK(log 3r +log 12) sup P
W W ,vV 1
4

1n n

v, 2 (W ; xi) - E 2 (W ; x)

i=1

v t 6
(54)

v .

.

Let Gi = v, 2 (W ; xi) - E 2 (W ; x) v where E[Gi] = 0. Let a = a1 , · · · , aK  RdK . Then we can show that Gi 1 is upper bounded, which we summariz as follows.
Lemma 7. There exists some constant C such that
Gi 1  C :  2.

19

Under review as a conference paper at ICLR 2019

Applying the Bernstein inequality for sub-exponential random variables (Mei et al., 2016, Theorem 9) to (54), we have for fixed W  W , v  V 1 ,
4

P

1n n

v, 2 (W ; xi) - E 2 (W ; x)

v

t 6

 2 exp -c · n · min

t2 t 4, 2

i=1

,

(55)

for some universal constant c. As a result,

P (Bt)  2 exp -c · n · min

t2 t 4, 2

3r + dK log + dK log 12 .

(56)

Thus as long as



 t > C · max

4

dK

log

36r

+

log

4 

2 ,

dK

log

36r

+

log

4 



n

n

(57)

for

some

large

enough

constant

C,

we

have

P (Bt)



 2

.

2. Upper bound P (At) and P (Ct). These two events will be bounded in a similar way. Let J satisfy

2 (W , x) - 2 (W , x)

E sup
W =W B(W ,r)

W -W F

J .

(58)

Let us look at the deterministic event Ct first. Since

sup E 2 Wj(W ); x - E 2 (W ; x)
W B(W ,r)

 sup

E 2 Wj(W ); x - E 2 (W ; x)

W B(W ,r)

W - Wj(W ) F

J · .

· sup W - Wj(W ) F
W B(W ,r)
(59)

Therefore, Ct holds as long as

t  3J · .

(60)

We can bound the event At as below.

P (At) = P

1 sup W B(W ,r) n

n
2 (W ; xi) - 2
i=1

Wj(W ); xi



3 tE

sup
W B(W ,r)

1n n

2 (W ; xi) - 2

i=1

Wj(W ); xi

t 3

(61)



3 tE

sup
W B(W ,r)

2 (W ; xi) - 2

Wj(W ); xi



3 tE

 3J t

sup
W B(W ,r)

2 (W ; xi) - 2 Wj(W ); xi W - Wj(W ) F

· sup

W - Wj(W ) F

W B(W ,r)

(62)

where (61) follows from the Markov inequality. Thus, taking

t 6 J 

(63)

ensures that P (At)



 2

.

It now boils down to control the quantity J

, which we have the following

lemma, whose proof is in Appendix E.3.

20

Under review as a conference paper at ICLR 2019

Lemma 8. There exists some constant C such that

2 (W , x) - 2 (W , x)

E sup
W =W B(W ,r)

W -W F

 C·d K J .

(64)

3. Final step. Let

=

6J

 2 ·ndK

,



=

d-10

plugging

into

(57)

we

need



 t >  2 · max

1

,C ·

 ndK



dK

log(36rnd11 K )

+

log

4 

,

dK

log(36rnd11 K )

+

log

4 

 .

n n

Since the middle term can be expressed as

(65)

dK

log(36rnd11K) + 10 log d



dK

log n

+

dK

log 36r

+

11dK log dK

+

10 log d ,

(66)

n

nn

nn

when n  C · dK log dK for some large enough constant C, the first term, dK log n dominants and is on the order of dK log dK. Moreover, it decreases as n increases when n  3. Thus we can
set

t  2

dK

log(36rnd11 K )

+

log

4 

n

(67)

which holds as t  C ·  2

dK log n

n

for

some

constant

C

.

By setting t := C 2

dK log n n

for sufficiently large C, as long as n  C

· dK log dK,

P sup 2fn (W ) - 2f (W )  C 2
W B(W ,r)

dK log n n

 d-10.

(68)

D.5 PROOF OF LEMMA 4

Proof. We nedd the following Lemma for the proof. Lemma 9

Lemma 9. Assume x  N (0, I). Let u be a fixed unit norm vector u =

with u 2 = 1, the following

 u  (W ; x) 2  K,

hold.

u1 , · · · , uK

 RdK

By a similar argument (details omitted) as the proof of Lemma 3, and applies Lemma 9, we can get the following concentration inequality:

sup fn (W ) - f (W ) 2  C ·
W B(W ,r)

 d K log n
, n

(69)

holds with probability at least 1 - d-10, as long as the sample size n  C · dK log(dK).

E PROOF OF AUXILIARY LEMMAS

E.1 PROOF OF LEMMA 5

Proof. We can rewrite the left-hand side as



1 KK

E  K2

 wi x

i=1 j=1

1 -  wj x

-z   ,

(70)

21

Under review as a conference paper at ICLR 2019

which is upper bounded by E

1 K2

K i=1

K j=1



wi x

1 -  wj x

-z , since f (x) = x-z

is convex for x > 0 and z  1. And apply Cauchy-Schwarz inequality we can have

E  wi x 1 -  wj x -z  E  wi x -2z · E 1 -  wj x -2z . (71)

Further

since

1 (x)

=

1

+ e-x,

1 1-(x)

=

1 + ex

and

g

=

wi

x



N

0, i2 =

wi

2 2

, then we can

exactly calculate the two terms in the above equation, i.e.,

E  (g)-2z = E and in the same way,

2z
1 + e-g 2z = E
l=0

2z e-lg l

2z
=
l=0

2z

e

i2 l2 2

,

l

(72)

2z
E (1 -  (g))-2z = E (1 + eg)2z =
l=0

2z

e

i2 l2 2

,

l

(73)

since g is a Gaussian random which is a symmetric random variable. Plugging this back into (71) we can conclude that for t = max ( w1 2, · · · , wK 2) and p  1,


E 
1 K

K i=1



wi x

1

1

-

1 K

K i=1



wi x

p    C · et2 ,

(74)

holds.

E.2 PROOF OF LEMMA 7

Proof. The sub-exponential norm of Gi can be bounded as

Gi 1  u, 2 (W ; z) u 1 + 2f (W ; z) ,

where

2f (W ; z)

is

upper

bounded

by

C K

according

to

lemma

2,

and

denote

the

(j, l)-th

block

of 2 (W ; z) as j,l · xx , we can write

Note that · for j = l

u, 2 (W ; z) u

KK

1 

j,l · uj xx ul 1

l=1 j=1

KK
 sup
l=1 j=1 t1

1
t-1 E j,l · uj xx ul t t .

1  wj x  wl x · p (W )2 + y - 2y · p (W )

j,l = K2

(p (W ) (1 - p (W )))2

,

further since then

p (W )2 + y - 2y · p (W ) 

p (W )2 (1 - p (W ))2

p (W ) p (W )

> 

1 2 1 2

,

 1  (wj x) (wl x)

|j,l|



 K2
1



(1-p(W ))2
(wj x) (wl

x)



K2 p(W )2

p (W )

>

1 2

,

p (W )



1 2

(75)
(76) (77) (78)

22

Under review as a conference paper at ICLR 2019

moreover,

p (W )2 =

1K K  wi x
i=1

2



1 K2 

wj x



wl x

(1 - p (W ))2 =

1- 1 K

K



wi x

i=1

2



1 K2

1-

wj x

and recall that  (x) (1 -  (x)) =  (x), together we can obtain

1 -  wl x

|j,l| 

 wj x  wl x  1 1 -  wj x · 1 -  wl x

1

p (W ) p (W )

> 

1 2 1 2

.

(79)

· for j = l:

|j,j | 

1 K2

wj x 

wl x · p (W )2 + y - 2y · p (W ) (p (W ) (1 - p (W )))2

+

1  wj x (y - p (W )) K p (W ) (1 - p (W ))

,

(80)

the first term is upper bounded by a constant, and for the second term



wj x 1{y=1} - p (W ) p (W ) (1 - p (W ))



 


 (wj x)
(1-p(W ))
 (wj x)
p(W )

 

K K

y=0 ,
y=1

(81)

where we have used the fact that the second derivative is  (x) =  (x) (1 -  (x)) (1 - 2 (x)), the absolute value of which can be upper bounded by  (x) or 1 -  (x). Thus we can show that

|j,j |  C.

(82)

Finally, we conclude that

holds for all j, l. And

|j,l|  C,

u, 2 (W ; z) u

KK

1  C ·

sup

l=1 j=1 t1

1
t-1E | uj xx ul| t t

KK

C·

sup

l=1 j=1 t1

t-1

E uj x 2t · E ul x 2t

KK

C·

uj 2 ul 2 · sup

t-1

((2t

-

1)!!)

1 t

l=1 j=1

p1

KK

C

uj 2 ul 2

l=1 j=1

KK
C

uj

2 2

+

ul

2 2

2

l=1 j=1

 C :  2

Thus we can conclude that

Gi 1  C :  2

(83)
(84)
1 t
(85)
(86)
(87)
(88) (89)

23

Under review as a conference paper at ICLR 2019

E.3 PROOF OF LEMMA 8

Proof. As noted before, we can write the (j, l)-th block of 2 (W ; z) as gj,l (W ) xx , where

gj,l

(W )

=

j,l (p (W ) (1

(W ) - p (W )))2 ,

(90)

then we can obtain the following bound,

KK

2 (W ; z) - 2 (W ; z) 

|gj,l (W ) - gj,l (W ) | · xx

j=1 l=1

.

(91)

Using the same method as shown in the proof of Lemma 1, we can upper bound |gj,l (W )-gj,l (W ) | as

|gj,l (W )

- gj,l (W

)|



1 K2

p

W

1 1-p W

 6 · x 2 · K · W - W F (92)

where W = W + (1 - ) W for   (0, 1). And thus, when W - W F  0.7 we have



2 (W ) - 2 (W )

E sup
W =W

W -W F



C K3
2

· K2 · E  

p

W

1 1-p W

 C·d K

 Thus we only need to set J  C · d K for some large enough C.



6 · x 2 · xx

 

(93)

E.4 PROOF OF LEMMA 9

Proof. By definition, we have

K
 (W ) , u =
k=1

 (W ) wk , uk



1K

= K


1

k=1 K

y

-

1 K

K i=1



wi x

K i=1



wi x

1

-

1 K

·  wk x

K i=1



wi x

  uk x ,

and then we can upper bound the sub-gaussian norm as

 (W ) , u



1



 

K

2 

1

 

K

K k=1
K k=1

 (wk x)·uk x

(1-

1 K

(K
i=1



wi

x))

uk

x


2

K k=1

uk x 2


1 K

(wk x)·uk

(K
i=1



wi

x
x)

uk

x


2

K k=1

uk x 2

Thus we can have

K

 (W ) , u 2 

uk 2 K,

k=1
 and conclude that the directional gradient is K-sub-Gaussian.

y=0 .
y=1
(94)

24


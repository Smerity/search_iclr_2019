Under review as a conference paper at ICLR 2019
GEOMETRIC AUGMENTATION FOR ROBUST NEURAL NETWORK CLASSIFIERS
Anonymous authors Paper under double-blind review
ABSTRACT
We introduce a novel geometric perspective and unsupervised model augmentation framework for transforming traditional deep (convolutional) neural networks into adversarially robust classifiers. Class-conditional probability densities based on Bayesian nonparametric mixtures of factor analyzers (BNP-MFA) over the input space are used to design soft decision labels for feature to label isometry. Classconditional distributions over features are also learned using BNP-MFA to develop plug-in maximum a posterior (MAP) classifiers to replace the traditional multinomial logistic softmax classification layers. This novel unsupervised augmented framework, which we call geometrically robust networks (GRN), is applied to CIFAR-10, CIFAR-100, and to Radio-ML (a time series dataset for radio modulation recognition). We demonstrate the robustness of GRN models to adversarial attacks from fast gradient sign method, Carlini-Wagner, and projected gradient descent.
1 INTRODUCTION
DeepConvNets are already prevalent in speech, vision, self-driving cars, biometrics, and robotics. However, they possess discontinuities that are easy targets for attacks as evidenced in dozens of papers (see (Goodfellow et al., 2015; Nguyen et al., 2015; Papernot et al.) and references therein). Adversarial images can be made to be robust to translation, scale, and rotation (Athalye et al., 2017). Adversarial attacks have also been applied to deep reinforcement learning (Huang et al., 2017; Kos & Song, 2017) and speech recognition (Carlini & Wagner, 2018a). In this work we will also consider attacks on automatic modulation recognition using deep convolutional networks (O'Shea et al., 2016). Previous work in creating adversarially robust deep neural network classifiers includes robust optimization with saddle point formulations (Madry et al., 2018), adversarial training (see e.g., (Kurakin et al., 2017)), ensemble adversarial training (Tramer et al., 2018), defensive distillation (Papernot et al., 2016), and use of detector-reformer networks (Meng & Chen, 2017). Defensive distillation has been found to be an insufficient defense (Carlini & Wagner, 2016; 2017) and MagNet of (Meng & Chen, 2017) was also shown to be defeatable in (Carlini & Wagner, 2018b). A summary of the attacks and defenses from the NIPS 2017 competition on adversarial attack and defense can be found in (Kurakin et al., 2018).
In this paper we propose a statistical geometric model augmentation approach to designing robust neural networks. We argue that signal representations involving projections onto lower-dimensional subspaces lower mean square error distortion. We implement a statistical union of subspaces learned using a mixture of factor analyzers to create the auxiliary signal space structural information that neural networks can use to improve robustness. We use the geometry of the input space to create unsupervised soft probabilistic decision labels to replace traditional hard one-hot encoded label vectors. We also use the geometry of the feature space (after soft-decision supervised training) to create accurate class-conditional probability density estimates for MAP classifiers (to replace neural network classification layers). We call this unsupervised geometric augmentation framework geometrically robust networks (GRN). The main contributions of this paper are:
1. Geometric analysis of problems with current neural networks. 2. A novel soft decision label coding framework using unsupervised statistical-geometric union
of subspace learning.
1

Under review as a conference paper at ICLR 2019

3. Maximum a posteriori classification framework based on class-conditional feature vector density estimation.
The rest of this paper is organized as follows. In Section 2 we analyze neural networks from a geometric vantage point and recommend solution pathways for overcoming adversarial brittleness. In Section 3 we describe the full details of the proposed geometrically robust network design framework. We give experimental results on two datasets and three attacks in Section 4 and conclude in Section 5.

2 ANALYSIS OF NEURAL NETWORKS FROM GEOMETRIC VIEWPOINT

A deep (convolutional) neural network is a nested nonlinear function approximator that we can write

as

g(x) = hCc (hCc -1 (· · · h1c ( hFf (hFf -1 (· · · h1f (x))))))

(1)

classification

feature extraction

where in our notation c denotes parameters (weights and biases) associated with the classification stages c = 1, 2, ..., C, f denotes parameters associated with the feature extraction stages f =

1, 2, ..., F , and the parameters are nested unions as l  l-1 culminating in . In principle an ideal function g which could be resilient to bounded adversarial noise is guaranteed to exist by the

universality theorem for neural networks (Cybenko, 1992), so this drives an investigation into what

is making current architectures brittle.

2.1 NEED FOR SOFT DECISION LABELS

The cross entropy loss objective function typically used to train function approximator g  RK (K classes) in (1) on n samples {xi}ni=1 with label vectors y(xi) has the form:

argmin


-

1 n

i{1,...,n}

k{1,...,K}

yk (xi )

log

gk,(xi)

(2)

For hard decision labels, y(xi) is an indicator vector (i.e. one-hot encoding), and (2) collapses to argmax n-1 i{1,...,n} log gt(xi),(xi) where t(xi) is the element in indicator vector y(xi) that is equal to one for the ith sample xi. Following the analysis in (Papernot et al., 2016), this means the stochastic gradient descent training algorithm minimizing (2) will inherently constrain the weights c in the final classification layer(s) of (1) to try to output zeros for elements not corresponding to the correct class. This artificially contrains the network to be overconfident and introduces brittleness.

There is also a geometric argument against one-hot encoding. The mapping from feature space to label space is a surjective map since the mapping to hard decisions reduces the set size of the codomain to be equal to the number of classes, and the number of unique features will generally be larger than the number of unique classes. This prevents the formation of an injective map from feature space to estimated label space. If we can enlarge the set of labels to be infinite (i.e. soft decisions), then we allow room for an injective map to be learned from feature to estimated label space. The resulting bijection (albeit a nonlinear bijection) then opens the door for distance preserving maps (isometries) which can guarantee that small distances between points in feature space remain small distances in the label space. This kind of isometry is exactly what we need to be able to have adversarially robust networks.

2.2 NEED FOR GEOMETRIC MODELS

Letting · 2 denote L2-norm, P(·) a projection matrix, P(·) the orthogonal complement, x a natural

input, and xadv the adversarially perturbed input, we know that x - xadv 2  Px - Pxadv 2

since

x

2 2

=

Px

2 2

+

Px

2 2

by Pythagorean theorem.

If the data x

is well approximated by

an information-preserving projection into another subspace, then we can reduce distortion of the

adversarial input in the projected latent space. If a network can be made to exploit knowledge of latent

spaces with distortion-reducing representations of the data, then the overall classification performance

would be less sensitive to adversarial perturbations. A density estimate built upon this geometrical

structure would then implicitly capture projected data representations and ultimately minimize the

2

Under review as a conference paper at ICLR 2019

(a) Conceptual illustration of how large deviations in the ambient observation space can translate into small deviations in a lower dimensional latent space that captures most of the signal information.

(b) Illustration of a union of subspaces depicted by 2-d overlapping planes.

Figure 1: Geometric modeling of signals as points that lie close to one or more linear subspaces. The signal data (either input observations to a neural network or feature vectors learned from the network) are modeled as small error displacements from a low dimensional linear subspace. These union of subspaces can be learned statistically using a mixture of factor analyzers. Since the number of subspaces and dimensionality of each subspace are unknown a priori, we must use a Bayesian nonparametric model.

label space deviation. The vast majority of current deep neural network models make no use of geometrical-statistical models of the data and are solely supervised learning on labeled inputs. We must use unsupervised learning to learn the latent manifold or union of subspaces topology to assist the supervised learning piece. The latent structure of the data can be captured in both the input space and feature space as we will do in this study.

Here we briefly introduce the union of subspaces (UoS) model for modeling inputs and features. To illustrate this, we take a vectorized signal segment x as shown in Figure 1(b) as a D-dimensional point living close to a union of T linear subspaces

x  Tt=1(St + t)  RD

(3)

where

St = {Atw + µt : w  Rdt } .

(4)

The matrix At = [a1,t, a2,t, ..., adt,t] is the matrix of basis vectors centered at µt for subspace index t, w is the coordinate of x at t, and t  N (0, 2I) is the modeling error. The subspace coordinates
wi and the closest subspace index t(i) are the latent variables for observation xi. In Figure 1(b),
we show the signal vector x, subspace offset vector µt, local basis vectors ajt, and modeling error t. The locus of all potential signal vectors of interest is assumed to lie on or near one of the local
subspaces. Since the observation is assumed to lie close to one of the T subspaces we can therefore write the ith observation as

xi = At(i)wi + µt(i) + t(i)

(5)

3 GEOMETRICALLY ROBUST NETWORKS

In this section we provide the complete framework for inserting our statistical-geometric viewpoint from Section 2 into a robust design approach that we will call geometrically robust networks (GRN). We propose to use unsupervised learning on the input space for label encoding and unsupervised learning on the feature space for density estimation in a MAP classifier. In this study we target the classification layers in g of (1) as the key layers for improvement assuming the supervised feature learning has adequately reached the information bottleneck limit Shwartz-Ziv & Tishby (2017). We will improve upon the classification layers in two fundamental ways:
1. Use soft decision labels to train the neural network.

3

Under review as a conference paper at ICLR 2019

(a) Graphical model of BNP-MFA (Chen et al., 2010).

(b) Hierarchical roll out of conjugate exponential BNP-MFA model.

Figure 2: The Bayesian nonparametric mixture of factor analyzers (BNP-MFA) from (Chen et al., 2010) which is our building block for estimating the union of subspaces. The tunable hyperparameters are the Dirichlet process (DP) concentration parameter which influences the number of mixtures/subspaces and the Beta process (BP) parameters which influence the dimensionality of each subspace.

2. Replace the classification layers hCc , hCc -1 , ..., h1c which generally implement a softmax multinomial logistic regression with a Bayesian maximum a posteriori (MAP) classifier using plug-in class-conditional density estimates.
In Subsection 3.1 we summarize the Bayesian nonparametric mixture of factor analyzers (BNP-MFA) model for union of subspace learning. In Subsection 3.2 we describe label encoding and MAP classifition steps which directly follow from learning the BNP-MFA.

3.1 BAYESIAN NONPARAMETRIC MIXTURE OF FACTOR ANALYZERS

To estimate the geometric model described above in Section 2.2 we use the Bayesian nonparametric formulation of the mixture of factor analyzers (BNP-MFA introduced in (Chen et al., 2010)) which has several advantages for estimating our required statistical union of subspaces:

1. Accuracy: Mixture of factor analyzer models empirically show higher test log-likelihoods (model evidence) than Boltzmann machine based models (Tang et al., 2012).
2. Speed: Since the BNP-MFA is a conjugate-exponential model it can be learned in online variational Bayes form with stochastic variational inference (Hoffman et al., 2013) giving it orders of magnitude speed improvements compared to Markov chain Monte Carlo methods.
3. Scales with data: Since the model is Bayesian nonparametric, there is no model overfitting and no need for regularization.
4. Hyperparameter Insensitive Only two hyperparameters need to be set are they are very insensitive to overall performance.

Under the BNP-MFA framework we infer the number of subspaces and subspace rank from the data

using Bayesian nonparametrics. A Dirichlet process mixture model is used to model the clusters,

while a Beta process is used to estimate the local subspace in each cluster. The conjugate-exponential

directed graphical model shown in Figure 2(a) and hierarchical roll out in Figure 2(b) is taken

from (Chen et al., 2010). Here, the {xi}in=1 are vector-valued observations in RN with component

weights given by the vectors {w^i}in=1 in RK , {A~ t, t, t, zt, t, µt, t, vt}Tt=1 are various global

parameters for each of the T mixture components,   R is a global parameter for the Dirichlet

process,

µ

=

1 n

n i=1

xi

is

the

(fixed)

sample

mean

of

the

data

and

a-h

and

0

are

fixed

constants.

4

Under review as a conference paper at ICLR 2019

For each t, the global variables have dimensions A~ t  RN×K , t  RK×K , t  RK , zt  RK , t  RK , µt  RN , t  R and vt  RT . More details on this model and the motivation for its construction can be found in (Chen et al., 2010) After the BNP-MFA model finishes training, we have the all the parameters (centroids, subspace spanning vectors, and cluster weights) that we need to estimate the class conditional probability density function (6) which we will use for both MAP classification and soft-decision label encoding as we show in section 3.

3.2 SOFT DECISION LABELS AND MAP CLASSIFIERS FROM GEOMETRIC MODEL

The idea of soft decision labels was used in defensive distillation (Papernot et al., 2016). Papernot et. al. used the first pass through their target neural network g(x) with annealed softmax to learn to the soft decision labels y = g(x). They then used those learned labels y in the second pass through
the same network but with different softmax thermal parameters to create the distilled network. As pointed out in (Papernot et al., 2016), the distilled network gd(·) will converge toward the originally network g(·) under a cross entropy loss given enough training data. Thus, the distilled network can
still possess some of the brittle nature of the original network trained with hard decision labels. This
vulnerability was revealed to be the case in (Carlini & Wagner, 2016; 2017).

We deviate from Papernot's defensive distillation approach here by using class conditional density estimate p^(x|k) on the class-partitioned input data with K total classes to create our labels. Here, we use the fact that the BNP-MFA is a demarginalization of a Gaussian mixture model (GMM) to form density estimates. The term demarginalization from Robert & Casella (2005) is taken here to mean the formation of a latent variable probability density which is the integrand under a marginalization integral. We learn the BNP-MFA model with parameters  from the original class-partitioned signals/images as input and then estimate the class-conditional pdf over the input space as

T
p^(x|k) = kt
t=1

N x; A~kt(ktdiag(zki))w^k + µkt, k-t1IN

T

= ktN (x; kt, kt)

t=1

kt = µkt + A~ ktktdiag(zki)kt

kt = A~ ktktdiag(zki)ktdiag(zki)ktA~ kTt + k-t1IN

N (w^k; kt, kt) dw^k (6)

Then, we assign our label vector as the posterior

yki = ki p^(xi|k)p(k) i = 1, 2, ..., n and k = 1, 2, ..., K

(7)

where p(k) to scale the

is the class prior. The term ki correct class label higher than

is a combined correction incorrect classes for the

factor and normalization factor cases where yi[k]  yi[k] and

where k is the correct class index. The ki term also enforces that k yki = 1, i. Soft decision

label encoding based on class conditional likelihood has the advantage that it is independent of any

deep architecture.

Once we learn the labels {yi}ni=1 from (7), we apply those labels to learn the neural network g from g(xi) = yi, i = 1, ..., n using traditional backprogation with SGD training on a cross entropy loss function. After the model converges, we extract features zi = hf (xi), i = 1, ..., n and learn a new BNP-MFA with model parameters  over the feature space. To learn the feature space
class-conditional pdfs we simply swap out x with z and the  with  in (6) to obtain the approximate
class-conditional likelihood functions over the feature vectors. The approximate posterior pdf is then simply p^(k|z)  p^(z|k)p(k). For the datasets we benchmark over the prior p(k) = 1/K is uniform, and the MAP classifier reduces to maximum likelihood (ML) classification. However, in the
real world class priors are almost never uniform and MAP classification gives a significant boost not
only over multinomial logistic regression but ML classification as well. We summarize the training
and testing stage procedures of GRN in Algorithm 1.

In Figure 3 we show plots of the negative squared Mahalanobis distance for each cluster of the class conditional input space pdf in (6) for a single image confuser sample from a Carlini-Wagner attack compared to the original unperturbed image. We see that the adversarial attack has almost no

5

Under review as a conference paper at ICLR 2019

Algorithm 1 Geometrically Robust Networks (GRN) Augmentation Framework

1: procedure TRAINING PHASE SUMMARY

2: Given {xi, ki}in=1 learn p^(x|k) using BNP-MFA 3: Label encode: yki = ki p^(x|k)p(k) i = 1, ..., n 4: Learn base model {g(xi) = yi}in=1 using SGD backprop on cross entropy loss to select
feature extraction layer hf (xi) in (1) 5: Extract features: {zi = hf (xi)}ni=1 6: Given {zi, ki}in=1 learn p^(z|k) using BNP-MFA
7: end procedure

8: procedure TESTING PHASE SUMMARY

9:

Given

test

inputs

{xi

}ntest
i=1

,

nonlinear

function

hf

(·),

class

prior

p(k),

and

pdf

estimate

p^(·|k), estimate class label as k^i = argmaxk p^(hf (xi)|k)p(k)

10: end procedure

Figure 3: Matrix of ten plots (one for each of the ten CIFAR-10 classes) showing the negative squared Mahalanobis distance for each cluster of the class conditional input space pdf in (6) for a single image confuser sample from a Carlini-Wagner attack compared to the original unperturbed image. We see that the adversarial attack has almost no influence on the pdf components.
influence on the pdf in (6) and, therefore, practically no variation on the corresponding label in (7). This demonstrates the concept depicted in Figure 1(a) of how the projected data points have very little deviation in the latent space.
4 RESULTS
For our base neural network from which we build the GRN in step 4 of Algorithm 1 for CIFAR-10 and CIFAR-100 we use Springenberger's "All convolutional network" (Springenberg et al., 2015) which uses only convolutional layers for entire stack. Our black box network from which we craft adversarial samples for CIFAR-10 and CIFAR-100 is: [3x3 conv 32 LeakyReLU(0.2), 3x3 conv 32 LeakyReLU(0.2), 2x2 MaxPool Dropout(0.2), 3x3 conv 64 LeakyReLU(0.2), 3x3 conv 64 LeakyReLU(0.2), 2x2 MaxPool Dropout(0.3), 3x3 conv 128 LeakyReLU(0.2), 3x3 conv 128 LeakyReLU(0.2), 2x2 MaxPool Dropout(0.4), Dense 512 ReLU Dropout(0.5), Dense 10 Softmax]. The Radio-ML dataset https://radioml.com/datasets is a relatively new time series dataset for benchmarking radio modulation recognition tasks. It has 11 modulation schemes (3
6

Under review as a conference paper at ICLR 2019

Table 1: Probability of Correct Classification (all attacks black box)

Dataset CIFAR-10
CIFAR-100

Model
Geometrically Robust Network All-Conv-Net
Geometrically Robust Network All-Conv-Net

No Attack
0.84 0.9
0.51 0.52

FGSM
0.81 0.17
0.49 0.37

Carlini-Wagner
0.72 0.12
TBD TBD

PGD
0.75 0.13
TBD TBD

analog, 8 digital) undergoing sample rate offset, center frequency offset, frequency flat fading, and AWGN. We measure probability of correct classification as a function of signal-to-noise ratio (SNR) for that dataset. For the Radio-ML dataset our base neural network from which we build the GRN in step 4 of Algorithm 1 is given at the top of Figure 4(b). The black box attack network for Radio-ML is a version of LeNet-5 CNN used in (O'Shea et al., 2016) and is shown at the bottom of Figure 4(b).

For both datasets the data was scaled to lie between zero and one with respect to the adversarial parameter settings. Using cleverhans (Nicolas Papernot, 2017), we craft adversarial samples from fast gradient sign method (FGSM), Carlini Wagner (CW) (Carlini & Wagner, 2017), and projected gradient descent (PGD) (Madry et al., 2018) on the CIFAR-10 and CIFAR-100 dataset. With FGSM we use eps=.005. With CW method we use initial tradeoff-constant = 10, batch size = 200, 10 binary search steps, and 100 max iterations. With PGD we use eps = 1, number of iterations = 7, and a step size for each attack iteration = 2. These attack parameter settings were more than enough to confuse the base classifier while keeping the mean structural similarity index (Wang et al., 2004) relatively constant between natural and adversarial images. For the Radio-ML dataset we only experiment with FGSM (eps=.03). For the CIFAR-100 dataset we used the following data augmentation parameters: (1) 10 degree random rotations, (2) zoom range form .8 to 1.2, (3) width shift range percentatage = 0.2, (4) height shift range percentage = 0.2, and (5) random horizontal flips.

As shown in Table 1 the proposed GRN model for CIFAR-10 performed remarkably well in the face

of all three attacks suffering only about 10-20 percent accuracy compared to base network with no

attack (natural test samples only). (At the time of this submission we are running the CW and PGD

attacks on CIFAR-100 and plan to include results on next iteration of paper.) The CIFAR-100 results

under no attack did not match the reported results in (Springenberg et al., 2015) likely because we

did not use as extensive data augmentation. In Figure 4(a) we plot the accuracy versus SNR for the

four cases using the Radio-ML dataset: 1) base model no attack, 2) base model FGSM attack, 3)

GRN model no attack, and 4) GRN model FGSM attack. Again, we see that the GRN is relatively

unaffected by the adversarial attack. We also experimented with hyperparameter settings for the

Dirichlet observed

process concentration that up to one order of

parameter magnitude

 and the in change

ratio of in  and

tabhethBeeretawparsocdeesmsopnasrtarambeletecrshaabn.geWine

the ultimate classification performance.

5 CONCLUSION AND FUTURE WORK
We have demonstrated that geometrical statistically augmented neural network models can achieve state-of-the-art robustness on CIFAR-10 under three different adversarial attack methods. We hope that this work will be the start of further investigation into the idea of using geometrically centered unsupervised learning methods to assist in making deep learning models robust, not only to adversarial noise but to all types of noise. There is more work that could be done to understand the best way to engineer soft decision labels given auxiliary data models. We need to also understand if the training algorithms themselves can be directly manipulated to incorporate outside structural data models.
A main selling point of Bayesian nonparametrics has been that the complexity of the model can grow as more data is observed. However, the current training algorithm for the BNP-MFA model is Gibbs sampling, which fails to scale to massive data sets. Stochastic variational inference (Hoffman et al., 2013) has been introduced as one such way to perform variational inference for massive or streaming data sets. We are currently working to cast the BNP-MFA into a stochastic variational framework so that the GRN model can be extended to very large (or even streaming) datasets.
7

Under review as a conference paper at ICLR 2019

(a) Probability of correction classification vs SNR for the base model and proposed GRN model.

(b) (Top) The base defense network for Radio-ML from which we build our GRN in Algorithm 1. (Bottom) The black box network used to craft the FGSM attack for Radio-ML (modified LeNet-5 from (O'Shea et al., 2016).)

Figure 4: Network specification and performance results for proposed geometrically robust networks applied to the Radio-ML dataset (modulation recognition over 11 modulation formats).

REFERENCES
Anish Athalye, Logan Engstrom, Andrew Ilyas, and Kevin Kwok. Synthesizing robust adversarial examples. Technical Report arXiv:1707.07397, 2017. URL https://arxiv.org/abs/ 1707.07397.
Nicholas Carlini and David Wagner. Defensive distillation is not robust to adversarial examples. Technical Report arXiv:1607.04311, 2016. URL https://arxiv.org/abs/1707.07397.
Nicholas Carlini and David Wagner. Towards evaluating the robustness of neural networks. In IEEE Symposium on Security and Privacy (SP), 2017.
Nicholas Carlini and David Wagner. Audio adversarial examples: Targeted attacks on speech-totext. Technical Report arXiv:1801.01944, 2018a. URL https://arxiv.org/abs/1801. 01944.
Nicholas Carlini and David Wagner. Magnet and efficient defenses against adversarial attacks are not robust to adversarial examples. Technical Report arXiv:1711.08478, 2018b. URL https: //arxiv.org/abs/1711.08478.
Minhua Chen, Jorge Silva, John William Paisley, Chunping Wang, David B. Dunson, and Lawrence Carin. Compressive sensing on manifolds using a nonparametric mixture of factor analyzers: Algorithm and performance bounds. IEEE Transactions on Signal Processing, 58(12):6140­6155, 2010.
G. Cybenko. Approximation by superpositions of a sigmoidal function. Mathematics of Control, Signals, and Systems, 5(4), 1992.
Ian J. Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples. In ICLR, 2015.
Matt Hoffman, David Blei, Chong Wang, and John Paisley. Stochastic variational inference. Journal of Machine Learning Research, 2013.
8

Under review as a conference paper at ICLR 2019
Sandy Huang, Nicolas Papernot, Ian Goodfellow, Yan Duan, and Pieter Abbeel. Adversarial attacks on neural network policies. In ICLR, 2017.
Jernej Kos and Dawn Song. Delving into adversarial attacks on deep policies. In ICLR, 2017.
Alexey Kurakin, Ian J. Goodfellow, and Samy Bengio. Adversarial machine learning at scale. In International Conference on Learning Representations, 2017.
Alexey Kurakin, Ian Goodfellow, Samy Bengio, Yinpeng Dong, Fangzhou Liao, Ming Liang, Tianyu Pang, Jun Zhu, Xiaolin Hu, Cihang Xie, Jianyu Wang, Zhishuai Zhang, Zhou Ren, Alan Yuille, Sangxia Huang, Yao Zhao, Yuzhe Zhao, Zhonglin Han, Junjiajia Long, Yerkebulan Berdibekov, Takuya Akiba, Seiya Tokui, and Motoki Abe. Adversarial attacks and defences competition. Technical Report arXiv:1804.00097v1, March 2018.
Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep learning models resistant to adversarial attacks. In International Conference on Learning Representations, 2018.
Dongyu Meng and Hao Chen. Magnet: A two-pronged defense against adversarial examples. In Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security, CCS '17, pp. 135­147, 2017.
Anh Nguyen, Jason Yosinski, and Jeff Clune. Deep neural networks are easily fooled: High confidence predictions for unrecognizable images. In CVPR, 2015.
Ian Goodfellow Reuben Feinman Fartash Faghri Alexander Matyasko Karen Hambardzumyan Yi-Lin Juang Alexey Kurakin Ryan Sheatsley Abhibhav Garg Yen-Chen Lin Nicolas Papernot, Nicholas Carlini. cleverhans v2.0.0: an adversarial machine learning library. arXiv preprint arXiv:1610.00768, 2017.
Timothy J. O'Shea, Johnathan Corgan, and T. Charles Clancy. Convolutional radio modulation recognition networks. In 17th International Conference on Applications of Neural Networks, 2016.
Nicolas Papernot, Patrick McDaniel, Ian Goodfellow, Somesh Jha, Z. Berkay Celik, and Ananthram Swami. Practical black-box attacks against machine learning.
Nicolas Papernot, Patrick McDaniel, Xi Wu, Somesh Jha, and Ananthram Swami. Distillation as a defense to adversarial perturbations against deep neural networks. In IEEE Symposium on Security and Privacy, 2016.
Christian P. Robert and George Casella. Monte Carlo Statistical Methods (Springer Texts in Statistics). Springer-Verlag, Berlin, Heidelberg, 2005. ISBN 0387212396.
Ravid Shwartz-Ziv and Naftali Tishby. Opening the black box of deep neural networks via information. Technical Report arXiv:1703.00810, 2017.
Jost Tobias Springenberg, Alexey Dosovitskiy, Thomas Brox, and Martin Riedmiller. Striving for simplicity: The all convolutional net. In ICLR, 2015.
Yichuan Tang, Ruslan Salakhutdinov, and Geoffrey E. Hinton. Deep mixtures of factor analysers. In ICML, 2012.
Florian Tramer, Alexey Kurakin, Nicolas Papernot, Ian Goodfellow, Dan Boneh, and Patrick McDaniel. Ensemble adversarial training: Attacks and defenses. In International Conference on Learning Representations, 2018.
Zhou Wang, Alan C. Bovik, Hamid R. Sheikh, and Eero P. Simoncelli. Image quality assessment: from error visibility to structural similarity. IEEE Transactions on Image Processing, 13(4): 600­612, 2004.
9


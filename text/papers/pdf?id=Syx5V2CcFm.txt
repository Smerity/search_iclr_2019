Under review as a conference paper at ICLR 2019
UNIVERSAL STAGEWISE LEARNING FOR NONCONVEX PROBLEMS WITH CONVERGENCE ON AVERAGED SOLUTIONS
Anonymous authors Paper under double-blind review
ABSTRACT
Although stochastic gradient descent (SGD) method and its variants (e.g., stochastic momentum methods, ADAGRAD) are algorithms of choice for solving nonconvex problems (especially deep learning), big gaps still remain between the theory and the practice with many questions unresolved. For example, there is still a lack of theories of convergence for SGD and its variants that use stagewise step size and return an averaged solution in practice. In addition, theoretical insights of why adaptive step size of ADAGRAD could improve non-adaptive step size of SGD is still missing for non-convex optimization. This paper aims to address these questions and fill the gap between theory and practice. We propose a universal stagewise optimization framework for a broad family of non-smooth nonconvex problems with the following key features: (i) at each stage any suitable stochastic convex optimization algorithms (e.g., SGD or ADAGRAD) that return an averaged solution can be employed for minimizing a regularized convex problem; (ii) the step size is decreased in a stagewise manner; (iii) an averaged solution is returned as the final solution. Our theoretical results of stagewise ADAGRAD exhibit its adaptive convergence, therefore shed insights on its faster convergence than stagewise SGD for problems with slowly growing cumulative stochastic gradients. To the best of our knowledge, this is the first work for addressing the unresolved issues of existing theories mentioned earlier. Besides theoretical contributions, our empirical studies show that our stagewise SGD and ADAGRAD improve the generalization performance of existing variants/implementations of SGD and ADAGRAD.
1 INTRODUCTION
Non-convex optimization has recently received increasing attention due to its popularity in emerging machine learning tasks, particularly for learning deep neural networks. One of the keys to the success of deep learning for big data problems is the employment of simple stochastic algorithms such as SGD or ADAGRAD (Krizhevsky et al., 2012; Dean et al., 2012). Analysis of these stochastic algorithms for non-convex optimization is an important and interesting research topic, which already attracts much attention from the community of theoreticians (Ghadimi & Lan, 2013; 2016; Yang et al., 2016; Davis & Drusvyatskiy, 2018b; Ward et al., 2018; Li & Orabona, 2018). However, one issue that has been largely ignored in existing theoretical analysis is that the employed algorithms in practice usually differ from their plain versions that are well understood in theory. Below, we will discuss several important heuristics used in practice for training deep neural networks and the gap between the practice and the theory to motivate this work.
First, a trick for setting the step size in training deep neural networks is to change it in a stagewise manner from a large value to a small value, i.e., a constant step size is used in a stage for a number of iterations and is decreased for the next stage (Krizhevsky et al., 2012; Ren et al., 2018). Although this trick has been adopted by most open-sourced libraries, e.g., Caffe (Jia et al., 2014), TensorFlow (Abadi et al., 2015), Pytorch (Paszke et al., 2017), it still lacks theoretical analysis to date for non-convex optimization. A related question that stands out is how and when to decrease the step size. On standard benchmark datasets for academic use such as CIFAR-10, CIFAR-100 (Krizhevsky et al.), people could follow the setting reported in previous studies to get a good result, which however might not work well for new datasets. Hence, a better solution to setting the stagewise step size
1

Under review as a conference paper at ICLR 2019
with insights from theory would be much preferred. However, in the existing literature of theory for non-convex optimization (Ghadimi & Lan, 2013; Davis & Drusvyatskiy, 2018b), only strategies based on an iteratively decreasing step size or a small constant step size have been well analyzed. For example, the existing theory usually suggests an iteratively decreasing step size proportional to 1/ t at the t-th iteration or a small constant step size, e.g., proportional to 2 with 1 for finding an -stationary solution whose gradient's magnitude (in expectation) is small than .
Second, the averaging heuristic is usually used in practice, i.e., an averaged solution is returned for prediction (Bottou, 2010), which could yield improved stability and generalization (Hardt et al., 2016). However, existing theory for many stochastic non-convex optimization algorithms only provides guarantee on a uniformly sampled solution or a non-uniformly sampled solution with decreasing probabilities for latest solutions (Ghadimi & Lan, 2013; Yang et al., 2016; Davis & Drusvyatskiy, 2018b). In particular, if an iteratively decreasing step size proportional to 1/ t at the t-th iteration is employed, the convergence guarantee was provided for a random solution that is non-uniformly selected from all iterates with a sampling probability proportional to 1/ t for the t-th iterate. This means that the latest solution always has the smallest probability to be selected as the final solution, which contradicts to the common wisdom. If a small constant step size is used, then usually a uniformly sampled solution is provided with convergence guarantee. However, both options are rarely used in practice and cannot justify the heuristic that returns the last solution.
A third common approach in practice is to use adaptive coordinate-wise step size of ADAGRAD (Dean et al., 2012). Although adaptive step size has been well analyzed for convex problems (e.g., why and when it could yield faster convergence than SGD) (Duchi et al., 2011; Chen et al., 2018b), it still remains an mystery for non-convex optimization with missing insights from theory. Several recent studies have attempted to analyze ADAGRAD for non-convex problems (Ward et al., 2018; Li & Orabona, 2018; Chen et al., 2018a; Zou & Shen, 2018). Nonetheless, none of them are able to exhibit the adaptive convergence of ADAGRAD to data as in the convex case and its potential advantage over SGD for non-convex problems.
To overcome the shortcomings of existing theories for stochastic non-convex optimization, this paper analyzes new algorithms that employ some or all of these commonly used heuristics in a systematic framework, aiming to fill the gap between theory and practice. The main results and contributions are summarized below: · We propose a universal stagewise optimization framework for solving a family of non-convex
problems, i.e., weakly convex problems, which is broader than smooth non-convex problems and includes some non-smooth non-convex problems. At each stage, any suitable stochastic convex optimization algorithms (e.g., SGD, ADAGRAD) with a constant step size parameter can be employed for optimizing a regularized convex problem with a number of iterations, which usually return an averaged solution. The step size parameter is decreased in a stagewise manner following a polynomial decaying scheme.
· We analyze several variants of the proposed framework by employing different basic algorithms, including SGD, ADAGRAD, stochastic heavy-ball (SHB) method, and stochastic Nesterov's accelerated gradient (SNAG) method. We prove the convergence of their stagewise versions for an averaged solution that is randomly selected from all stagewise averaged solutions.
· To justify a heuristic approach that returns the last averaged solution in stagewise learning, we present and analyze a non-uniform sampling strategy over stagewise averaged solutions with sampling probabilities increasing as the stage number.
· Regarding the convergence results, for stagewise SGD, SHB, SNAG, we establish the same order of iteration complexity for finding a nearly stationary point as the existing theories of their nonstagewise variants. For stagewise ADAGRAD, we establish an adaptive convergence for finding a nearly stationary point, which is provably better than (stagewise) SGD, SHB, and SNAG when the cumulative growth of stochastic gradient is slow.
· Besides theoretical contributions, we also empirically verify the effectiveness of the proposed stagwise algorithms. In particular, our empirical studies show that (i) the stagewise ADAGRAD dramatically improves the generalization performance of existing variants of ADAGRAD, (ii) stagewise SGD, SHB, SNAG also outperform their plain variants with an iteratively decreasing step size; (iii) the proposed stagewise algorithms achieve similar if not better generalization performance than their heuristic variants implemented in existing libraries on standard benchmark datasets.
2

Under review as a conference paper at ICLR 2019

2 RELATED WORK
SGD for unconstrained smooth non-convex problems was first analyzed by Ghadimi & Lan (2013), who established an O(1/ 4) iteration complexity for finding an -stationary point x in expectation satisfying E[ f (x) ]  , where f (·) denotes the objective function. As mentioned earlier, the returned solution is either a uniformly sampled solution or a non-uniformly sampled one with sampling probabilities proportional to the decreasing step size. Similar results were established for the stochastic momentum variants of SGD (i.e., SHB, SNAG) by Yang et al. (2016); Ghadimi & Lan (2016). Recently, SGD was also analyzed for (constrained) weakly convex problems, whose objective function is non-convex and not necessarily smooth, by Davis & Drusvyatskiy (2018b). However, none of these studies provide results for algorithms that return an averaged solution, and these analyzed algorithms differ significantly from that used in practice for achiving the state-of-the-art results (Krizhevsky et al., 2012; Ren et al., 2018; Loshchilov & Hutter, 2017).
Although adaptive variants of SGD, e.g., ADAGRAD (Duchi et al., 2011), ADAM (Kingma & Ba, 2015; Reddi et al., 2018), were widely used for training deep neural networks, there are few studies on theoretical analysis of these algorithms for non-convex problems. Several recent studies attempted to analyze ADAGRAD for non-convex problems (Ward et al., 2018; Li & Orabona, 2018; Chen et al., 2018a; Zou & Shen, 2018). Although these studies have established an iteration complexity of O(1/ 4) for different variants of ADAGRAD for finding an -stationary solution of a stochastic non-convex optimization problem, none of them can exhibit the potential adaptive advantage of ADAGRAD over SGD as in the convex case. Besides that, these studies also suffer from the following shortcomings: (i) they all assume smoothness of the objective function, while we consider non-smooth and non-convex problems; (ii) their convergence is provided on a solution with minimum magnitude of gradient that is expensive to compute, though their results also imply a convergence on a random solution selected from all iterates with decreasing sampling probabilities. In contrast, these shortcomings do not exist in this paper. To the best of our knowledge, our result is the first one that explicitly shows that coordinate-wise adaptive step size could yield faster convergence than using non-adaptive step size for non-smooth non-convex problems, which is similar to that in the convex case and was observed in practice for deep learning (Dean et al., 2012).
The proposed stagewise algorithm is similar to several existing algorithms in design (Xu et al., 2017; Davis & Grimmer, 2017), which are originated from the proximal point algorithm (Rockafellar, 1976). I.e., at each stage a regularized convex subproblem is formed and then a stochastic algorithm is employed for optimizing the regularized subproblem inexactly with a number of iterations. Xu et al. (2017) used this idea for solving problems that satisfy a local error bound condition, aiming to achieve faster convergence than vanilla SGD. Davis & Grimmer (2017) are probably the first who analyzed this idea for solving non-smooth weakly convex problems. In these two papers SGD with decreasing step sizes for a strongly convex problem is employed at each stage for solving the regularized subproblem. Our stagewise algorithm is developed following the similar idea. The key differences from (Xu et al., 2017; Davis & Grimmer, 2017) are that (i) we focus on non-convex problems instead of convex problems considered in (Xu et al., 2017); (ii) we analyze a non-uniform sampling strategy with sampling probabilities increasing as the stage number to justify a heuristic approach that uses the last averaged solution for prediction, unlike the uniform sampling used in (Davis & Grimmer, 2017); (iii) we present a unified algorithmic framework and convergence analysis, which enable one to employ any suitable stochastic convex optimization algorithms at each stage. Importantly, it brings us several interesting variants including stagewise stochastic momentum methods and stagewise ADAGRAD.

3 PRELIMINARIES
The problem of interest in this paper is:

min (x) = E[(x; )],
x

(1)

where   Rd is a closed convex set,   U is a random variable, (x) and (x; ) are non-convex functions, with the basic assumptions on the problem given in Assumption 1.

To state the convergence property of an algorithm for solving the above problem. We need to in-
troduce some definitions. These definitions can be also found in related literature, e.g., Davis & Grimmer (2017); Davis & Drusvyatskiy (2018b). In the sequel, we let · denote an Euclidean norm, [S] = {1, . . . , S} denote a set, and (·) denote the indicator function of the set .

3

Under review as a conference paper at ICLR 2019

Definition 1. (Fre´chet subgradient) For a non-smooth and non-convex function f (·),

F f (x) = {v  Rd|f (y)  f (x) + v (y - x) + o( y - x ), y  Rd}

denotes the Fre´chet subgradient of f .
Definition 2. (First-order stationarity) For problem (1), a point x   is a first-order stationary point if 0  F ( + )(x), where  denotes the indicator function of . Moreover, a point x is said to be -stationary if dist(0, F ( + )(x))  , where dist denotes the Euclidean distance from a point to a set.

Definition 3. (Moreau Envelope and Proximal Mapping) For any function f and  > 0, the following function is called a Moreau envelope of f

1

f(x)

=

min
z

f (z)

+

2

z-x

2.

Further, the optimal solution to the above problem denoted by

(2)

1

proxf (x)

=

arg

min
z

f (z)

+

2

z-x

2

is called a proximal mapping of f .

Definition 4.

(Weakly

convex)

A

function

f

is

µ-weakly

convex

(µ

>

0),

if

f (x)

+

µ 2

x

2 is convex.

It is known that if f (x) is µ-weakly convex and  < µ-1, then its Moreau envelope f(x) is

C1-smooth with the gradient given by (see e.g., Davis & Drusvyatskiy (2018b))

f(x) = -1(x - proxf (x)).
The tool of Moreau envelope is introduced to measure the convergence for optimizing non-smooth and non-convex functions. A small norm of f(x) has an interpretation that x is close to a point that is nearly stationary. In particular for any x  Rd, let x = proxf (x), then we have (Davis & Drusvyatskiy, 2018b)

f (x)  f (x), x - x =  f(x) , dist(0, f (x))  f(x) .

(3)

This means that a point x satisfying f(x)  is close to a point in distance of O( ) that is -stationary.

It is notable that for a non-smooth function f (·), there could exist a sequence of solutions {xk} such that f(xk) converges while dist(0, f (xk)) may not converge (Drusvyatskiy & Paquette, 2018). A simple example is to consider minxR |x|. As long as x = 0, dist(0, f (x)) = 1 = 0 no matter how close is x to the stationary point 0. To handle such a challenging issue for non-smooth
non-convex problems, we will follow existing works (Davis & Drusvyatskiy, 2018a; Drusvyatskiy
& Paquette, 2018; Davis & Grimmer, 2017) to prove the near stationarity in terms of f(x). In the case when f is smooth, f(x) is closely related to the magnitude of the gradient , which
is used as standard a criterion for non-convex smooth optimization (Drusvyatskiy & Lewis, 2016).
Thus, the near stationarity in terms of f(x) implies the near stationarity in terms of f (x) for a smooth function f (·) for a properly chosen  > 0. In this work, we define (x) as the Moreau envelope of (x) + (x) as in (2) with f (x) replaced by (x) + (x) and study the convergence in terms of of (x).

Now, we are ready to state the basic assumptions of the considered problem (1).
Assumption 1. (i) There is a measurable mapping g :  × U  R such that E[g(x; )]  F (x) for any x  ; (ii) for any x  , E[ g(x; ) 2]  G2; (iii) there exists  > 0 such that (x) - minz (z)   for any x  ; (iv) the objective function  is µ-weakly convex;
Remark: Assumption 1(i), (ii) assume a stochastic subgradient is available for the objective function and its Euclidean norm square is bounded in expectation, which are standard assumptions for nonsmooth optimization. Assumption 1(iii) assumes that the objective value with respect to the optimal value is bounded. Assumption 1(iv) assumes weak convexity of the objective function, which is weaker than assuming smoothness. Below, we present some examples of objective functions in machine learning that are weakly convex.

Ex. 1: Smooth Non-Convex Functions. If (·) is a L-smooth function (i.e., its gradient is LLipschitz continuous), then it is L-weakly convex. This will include the objective function for

4

Under review as a conference paper at ICLR 2019

Algorithm 1 A Meta Stagewise Algorithm: Stagewise-SA

1: Initialize: a sequence of decreasing step size parameters {s}, x0  ,  < µ-1

2: for s = 1, . . . , S do

3:

Let

fs(·)

=

(·)

+

1 2

· -xs-1 2

4: xs = SA(fs, xs-1, s, Ts)

xs is usually an averaged solution

5: end for

neural networks with a smooth activation function (e.g., the sigmoid function) and a smooth loss function (e.g., softmax loss).

Ex. 2: Convex and Smooth Composition. Consider (x; ) = h(c(x; )) where h(·) : Rm  R is closed convex and M -Lipschitz continuous, and c(x; ) : Rd  Rm is nonlinear smooth mapping with L-Lipschitz continuous gradient. This class of functions has been considered in (Drusvyatskiy
& Paquette, 2018) and it was proved that (x; ) is M L-weakly convex. An interesting example is phase retrieval, where (x; a, b) = |(x a)2 - b|. Another example related to deep learning is
that if c(x; ) denotes the mapping function of a neural network parameterized by x with a smooth
activation function and h denotes a non-smooth Lipschitz continuous convex loss function (e.g.,
hinge loss), then the resulting loss h(c(x; )) is a weakly convex function of x. More examples of
this class can be found in (Davis & Drusvyatskiy, 2018a).

4 STAGEWISE OPTIMIZATION: ALGORITHMS AND CONVERGENCE
In this section, we will present the proposed stagewise algorithms and their convergence results. We will first present a Meta algorithmic framework highlighting the key features of the proposed algorithms and then present several variants of the Meta algorithm.

The Meta algorithmic framework is described in Algorithm 1. There are several key features that
differentiate Algorithm 1 from existing stochastic algorithms that come with theoretical guarantee.
First, the algorithm is run with multiple stages. At each stage, a basic stochastic algorithm (SA)
is called to optimize a regularized problem fs(x) inexactly that consists of the original objective function and a quadratic term, which is guaranteed to be convex due to the weak convexity of  and  < µ-1. The convexity of fs allows one to employ any suitable existing stochastic algorithms (cf. Theorem 1) that have convergence guarantee for convex problems. The returned solution from the (s - 1)-th stage is used as a reference point for constructing fs and as an initial solution for warm-start. It is notable that SA usually returns an averaged solution xs at each stage. Second, a decreasing sequence of step size parameters s is used. At each stage, the SA uses a constant step size parameter s and runs the updates for a number of Ts iterations. We do not initialize Ts as it might be adaptive to the data as in stagewise ADAGRAD. The setup of s and Ts will depend on the specific choice of SA, which will be exhibited later for different variants.

To illustrate that Algorithm 1 is a universal framework such that any suitable SA algorithm can be employed, we present the following result by assuming that SA has an appropriate convergence for a convex problem.

Theorem 1. Let f (·) be a convex function, x = arg minx f (x) and  denote some problem dependent constants. Suppose for x+ = SA(f, x0, , T ), we have

E[f (x+) - f (x)]  1(, T, )

x0 - x

2 2

+

2(,

T

,

)(f

(x0)

-

f

(x))

+

3(,

T,

).

(4)

Under Assumption 1(i), (iii) and (iv), by running Algorithm 1 with  = 1/(2µ), and with s, Ts satisfying 1(s, Ts, )  1/(48), 2(s, Ts, )  1/2, 3(s, Ts, )  c3/s for some c3 > 0, we
have

E

 (x ) 2



32( +

1)

+

48c3(

+ 1) ,

(S + 1)

(S + 1)

where  is randomly selected from {0, . . . , S} with probabilities p  ( + 1),   1.
Remark: It is notable that the convergence guarantee is provided on a stagewise average solution x . To justify a heuristic approach that returns the final average solution for prediction, we analyze a new sampling strategy that samples a solution among all stagewise average solutions with sampling probabilities increasing as the stage number increases. This sampling strategy is better than uniform sampling strategy or a strategy with decreasing sampling probabilities in the existing literature. The convergence upper bound in (4) of SA covers the results of a broad family of stochastic convex

5

Under review as a conference paper at ICLR 2019

optimization algorithms. When 2(s, Ts, ) = 0 (as in SGD), the upper bound can be improved by a constant factor. Moreover, we do not optimize the value of . Indeed, any  < 1/µ will work, which only has an effect on constant factor in the convergence upper bound.
Next, we present several variants of the Meta algorithm by employing SGD, ADAGRAD, and stochastic momentum methods as the basic SA algorithm, to which we refer as stagewise SGD, stagewise ADAGRAD, and stagewise stochastic momentum methods, respectively.

4.1 STAGEWISE SGD

In this subsection, we analyze the convergence of stagewise SGD, in which SGD shown in Algorithm 3 in the Appendix is employed in the Meta framework. Besides Assumption 1, we impose the following bounded domain assumption in this subsection.

Assumption 2. There exists D > 0 such that x - y  D for any x, y  .

It is worth mentioning that bounded domain assumption is imposed for simplicity, which is usu-
ally assumed in convex optimization. For machine learning problems, one usually imposes some
bounded norm constraint to achieve a regularization. Nevertheless, the bounded domain assumption
is not essential for the proposed algorithm. We present a more involved analysis in subsection 4.3 for unbounded domain  = Rd. The following is a standard basic convergence result of SGD (Zinkevich, 2003), which clearly satisfies the bound in (4).

Lemma 1. For Algorithm 3, assume that f (·) is convex and E gt 2  G2, t, then for any x  

E[f (xT ) - f (x)]



x - x0 2 2(T + 1)

+

G2 2

The following theorem exhibits the convergence of stagewise SGD.

Theorem 2. Suppose Assumption 1 and 2 hold. By setting  = 1/(2µ), s = 0/s, Ts = 12s/0 where 0 > 0 is a free parameter, then stagewise SGD (Algorithm 1 employing SGD) guarantees

that

E

 (x ) 2



16µ(

+

1)

+

24µ0G^2(

+

1) ,

S+1

(S + 1)

where G^2 = 2G2 + 2-2D2, and  is similarly defined as in Theorem 1.

Remark: To find a solution with E (x ) 2  2, we can set S = O(1/ 2) and the total

iteration complexity

S s=1

Ts

is

in

the

order

of

O(1/

4).

The

above

theorem

is

essentially

a

corol-

lary of Theorem 1 by applying Lemma 1 to fs(·) at each stage. We present a complete proof in the

appendix.

4.2 STAGEWISE ADAGRAD

One of the main contributions of the present work is to develop a variant of ADAGRAD with adaptive convergence to data for stochastic non-convex optimization. In this subsection, we analyze stagewise ADAGRAD and establish its adaptive complexity. In particular, we consider the Meta algorithm that employs ADAGRAD in Algorithm 2 to optimize each fs. The key difference of stagewise ADAGRAD from stagewise SGD is that the number of iterations Ts at each stage is adaptive to the history of learning. It is this adaptiveness that makes the proposed stagewise ADAGRAD achieve adaptive convergence. It is worth noting that such adaptive scheme has been also considered in (Chen et al., 2018b) for solving stochastic strongly convex problems. In contrast, we consider stochastic weakly convex problems. Similar to previous analysis of ADAGRAD (Duchi et al., 2011; Chen et al., 2018b), we assume g(x; )   G, x   in this subsection. Note that this is stronger than Assumption 1 (ii). We formally state this assumption required in this subsection below.

Assumption 3. g(x; )   G for any x  . The convergence analysis of stagewise ADAGRAD is build on the following lemma, which is attributed to Chen et al. (2018b) with a proof sketch provided in the Appendix.

Lemma 2. Let f (x) be a convex function, H0 = GI with G  maxt gt , and iteration number

T

satisfy T



M

max{

G+maxi 2c

g1:T ,i

,c

d i=1

g1:T ,i

} for some c > 0. Algorithm 2 returns an

averaged solution xT such that

E[f (xT )

-

f (x)]



c M

x0 - x

2+  , Mc

(5)

where x = arg minx f (x), g1:t = (g(x1), . . . , g(xt)) and g1:t,i denotes the i-th row of g1:t.

6

Under review as a conference paper at ICLR 2019

Algorithm 2 ADAGRAD(f, x0, , )

1: Initialize: x1 = x0, g1:0 = [], H0  Rd×d 2: while T does not satisfy the condition in Theorem 3 do

3: Compute a stochastic subgradient gt for f (xt)

4: Update g1:t = [g1:t-1, g(xt)], st,i = g1:t,i 2

5:

Set

Ht

=

H0

+

diag(st)

and

t(x)

=

1 2

(x

-

x1)

Ht(x - x1)

6:

Let

xt+1

=

arg

min
x

x

1 t

t 

=1

g

+

1 t

t

(x)

7: end while

8: Output: xT =

T t=1

xt/T

The convergence property of stagewise ADAGRAD is described by following theorem.

Theorem 3.
 = 1/(2µ), 0, c > 0 are

Suppose Assumption s = 0/ s, Ts  free parameters, and

1(i), (iii), (iv), Assumption 2 and Assumption 3 hold. By setting

MMsssmax2{4(G^c,+thmenasxtiagge1ws:Tisse,i

)/(2c), c ADAGRAD

d i=1

g1s:Ts,i } where

guarantees that

E[

 (x )

2]

 16µ( + S+1

1)

+

4µ202( + 1) c2(S + 1)

,

where G^ = G + -1D, g1s:t,i denotes the cumulative stochastic gradient of the i-th coordinate at the s-th stage, and  is similarly defined as in Theorem 1.

Remark: of Ts due

Note that the free parameter to that (G^ + maxi g1s:Ts,i

c )

is inG^trodTusceadndto

balance the

d i=1

g1s:Ts

,tiwoterdmG^sinTtshehalovwe edrifbfoeruenndt

orders when d is very large. One way to balance these two terms is to set c in the order of 1/d,

resulting O(d/(S + 1)) for the second term in the above convergence bound. Another way is to

choose c in the s-th stage such that the two terms in the max of the lower bound of Ts match each

other. One can derive a similar order of O(d/(S + 1)) for the second term in the above convergence

bound. It is obvious that the total number of iterations

S s=1

Ts

is

adaptive

to

the

data.

Next,

let us present more discussion on the iteration complexity. Note that Ms = O( s) by setting

c as a constant. By the boundness of stochastic gradient g1s:Ts,i  O( Ts), therefore Ts in

the order of O(s) will satisfy the condition in Theorem 3. Thus, in the worst case the iteration

complexity for finding E[ (x ) 2]  2 is in the order of

S s=1

O(s)



O(1/

4).

To show

the potential advantage of adaptive step size as in the convex case, let us consider a good case

when the cumulative growth of stochastic gradient is slow, e.g., assuming g1s:Ts,i  O(Ts)

with  < 1/2. Then Ts = O(s1/(2(1-))) will work, and then the total number of iterations

S s=1

Ts



S 1+1/(2(1-))



O(1/

2+1/(1-)), which is better than

O(1/

4).

4.3 STAGEWISE STOCHASTIC MOMENTUM (SM) METHODS
Finally, we present stagewise stochastic momentum (SM) methods and their convergence results. In the literature, there are two popular variants of stochastic momentum methods, namely, stochastic heavy-ball method (SHB) and stochastic Nesterov's accelerated gradient method (SNAG). Both methods have been used for training deep neural networks (Krizhevsky et al., 2012; Sutskever et al., 2013), and have been analyzed by (Yang et al., 2016) for non-convex optimization. To contrast with the results in (Yang et al., 2016), we will consider the same unified stochastic momentum methods that subsume SHB, SNAG and SGD as special cases when  = Rd. The updates are presented in Algorithm 4 in the Appendix. There are two additional parameters:   (0, 1) is the momentum parameter and  is a parameter that can vary between [0, 1/(1 - )]. By changing the value of , we can obtain the three variants, SHB ( = 0), SNAG ( = 1) and SGD ( = 1/(1 - )). Due to the limit of space, we only present the convergence of stagewise SM methods below.

Theorem 4. Suppose Assumption 1 holds. By setting  = 1/(2µ), s = (1 - )/(96s( + 1)), Ts  2304( + 1)s, with  similarly defined as in Theorem 1 stagewise SM methods guarantee

E[

 (x )

2] 

16µ( + 1) S+1

+

G2( + 48(2 + 1)(1 - ))( + 1) .
96( + 1)(1 - )(S + 1)

Remark: The bound in the above theorem is in the same order as that in Theorem 2. The total iteration complexity for finding a solution x with E (x ) 2  2 is O(1/ 4) similar to that
achieved in (Yang et al., 2016).

7

Under review as a conference paper at ICLR 2019

Test Error (%)

ResNet20 CIFAR-10
0.40
ADAGRAD (heuristic) 0.35 ADAGRAD (theory)
AMSGRAD 0.30 Stagewise-ADAGRAD
0.25
0.20
0.15
0.10
0 50 100 150 200 250 300 350 400
# of iterations (*200)
ResNet20 CIFAR-100
0.8
ADAGRAD (heuristic) ADAGRAD (theory) 0.7 AMSGRAD Stagewise-ADAGRAD
0.6

Test Error (%)

ResNet20 CIFAR-10
0.40
SGD (heuristic) 0.35 SGD (theory)
Stagewise-SGD
0.30
0.25
0.20
0.15
0.10
0 50 100 150 200 250 300 350 400
# of iterations (*200) ResNet20 CIFAR-100
0.8
SGD (heuristic) SGD (theory) 0.7 Stagewise-SGD
0.6

Test Error (%)

ResNet20 CIFAR-10
0.40
SHB (heuristic) 0.35 SHB (theory)
Stagewise-SHB
0.30
0.25
0.20
0.15
0.10
0 50 100 150 200 250 300 350 400
# of iterations (*200) ResNet20 CIFAR-100
0.8
SHB (heuristic) SHB (theory) 0.7 Stagewise-SHB
0.6

Test Error (%)

ResNet20 CIFAR-10
0.40
SNAG (heuristic) 0.35 SNAG (theory)
Stagewise-SNAG
0.30
0.25
0.20
0.15
0.10
0 50 100 150 200 250 300 350 400
# of iterations (*200) ResNet20 CIFAR-100
0.8
SNAG (heuristic) SNAG (theory) 0.7 Stagewise-SNAG
0.6

Test Error (%)

Test Error (%)

Test Error (%)

Test Error (%)

0.5 0.5 0.5 0.5

0.4 0.4 0.4 0.4

0.3 0 50 100 150 200 250 300 350 400
# of iterations (*200)

0.3 0 50 100 150 200 250 300 350 400
# of iterations (*200)

0.3 0 50 100 150 200 250 300 350 400
# of iterations (*200)

0.3 0 50 100 150 200 250 300 350 400
# of iterations (*200)

Figure 1: Comparison of Testing Error on CIFAR-10 (top) and CIFAR-100 (bottom).

5 EXPERIMENTS

In this section, we present some empirical results to verify the effectiveness of the proposed stagewise algorithms. We use two benchmark datasets, namely CIFAR-10 and CIFAR-100 (Krizhevsky et al.) for our experiments. We implement the proposed stagewise algorithms in TensorFlow. We compare different algorithms for learning ResNet-20 (He et al., 2016) with batch normalization (Ioffe & Szegedy, 2015) adopted after each convolution and before ReLu activation.
Baselines. We compare the proposed stagewise algorithms with their their variants implemented in TensorFlow. It is notable that ADAGRAD has a step size (aka learning rate) parameter 1, which is a constant in theory (Li & Orabona, 2018; Chen et al., 2018a; Zou & Shen, 2018). However, in the deep learning community a heuristic fixed frequency decay scheme for the step size parameter is commonly adopted (Ren et al., 2018; Wilson et al., 2017). We thus compare two implementations of ADAGRAD - one with a constant learning rate parameter and another one with a fixed frequency decay scheme, which are referred to as ADAGRAD (theory) and ADAGRAD (heuristic). For each baseline algorithms of SGD, SHB, SNAG, we also implement two versions - a theory version with iteratively decreasing size 0/ t suggested by previous theories and a heuristic approach with fixed frequency decay scheme used in practice, using (theory) and (heuristic) to indicate them. The fixed frequency decay scheme used in the heuristic variants is the same as that in (Ren et al., 2018), i.e., the step size parameter is decreased by 10 at 40k, 60k iterations. We also compare stagwise ADAGRAD with AMSGrad (Reddi et al., 2018) - a corrected version of Adam. Parameters. The stagewise step size s = 0/ s is used in stagwise ADAGRAD, the number of iterations Ts in stagwise ADAGRAD is set according to Theorem 3 with some simplifications for dealing with unknown G^, in particular we set Ts to the smallest value larger than
T0 s maxi g1s:Ts,i i g1s:Ts,i . For stagewise SGD, SHB, SNAG, the stagewise step size and
iteration number is set to s = 0/s and Ts = T0s, respectively. The involved parameters in the compared algorithms are tuned for the best performance, including the initial step size parameter 0 of all algorithms, the value of T0 and  for our stagewise algorithms.
Results. We consider two settings - with/without an 2 norm regularization on weights. For comparison, we evaluate the training error and testing error of obtained solutions in the process of training. For our stagewise algorithms, the evaluation is done based on the current averaged solution, and for other baselines the evaluation is done based on the current solution. Due to the limit of space, we only show the results of testing error on CIFAR-10 and CIFAR-100 for the setting without regularization in Figure 1. All results are included in the Appendix. From all results, we have several observations. (i) The proposed stagewise algorithms perform much better in terms of testing error than the existing theoretical versions reported in the literature (marked with theory in the legend). This indicates the proposed stagewise step size scheme is better than iteratively decreasing step size scheme. (ii) The proposed stagewise algorithms achieve similar and sometimes even better testing error than the heuristic approaches with a fixed frequency decay scheme used in practice. However, the heuristic approaches usually give smaller training error. This seems indicate that the proposed algorithms are less vulnerable to the overfitting. In another word, the proposed algorithms have smaller generalization error, i.e., the difference between the testing error and the training error.

1note it is not equivalent to the step size in SGD.

8

Under review as a conference paper at ICLR 2019
6 CONCLUSION & FUTURE WORK
In this paper, we have proposed a universal stagewise learning framework for solving stochastic nonconvex optimization problems, which employs well-known tricks in practice that have not been well analyzed theoretically. We provided theoretical convergence results for the proposed algorithms for non-smooth non-convex optimization problems. We also established an adaptive convergence of a stochastic algorithm using data adaptive coordinate-wise step size of ADAGRAD, and exhibited its faster convergence than non-adaptive stepsize when the growth of cumulative stochastic gradients is slow similar to that in the convex case. For future work, one may consider developing more variants of the proposed meta algorithm, e.g., stagewise AMSGrad, stagewise RMSProp, etc. We will also consider the empirical studies on the large-scale ImageNet data set.
REFERENCES
Mart´in Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S. Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Ian Goodfellow, Andrew Harp, Geoffrey Irving, Michael Isard, Yangqing Jia, Rafal Jozefowicz, Lukasz Kaiser, Manjunath Kudlur, Josh Levenberg, Dandelion Mane´, Rajat Monga, Sherry Moore, Derek Murray, Chris Olah, Mike Schuster, Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal Talwar, Paul Tucker, Vincent Vanhoucke, Vijay Vasudevan, Fernanda Vie´gas, Oriol Vinyals, Pete Warden, Martin Wattenberg, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. TensorFlow: Large-scale machine learning on heterogeneous systems, 2015. URL https://www.tensorflow.org/. Software available from tensorflow.org.
Le´on Bottou. Large-scale machine learning with stochastic gradient descent. In Proceedings of International Conference on Computational Statistics (COMPSTAT), pp. 177­187, 2010.
Xiangyi Chen, Sijia Liu, Ruoyu Sun, and Mingyi Hong. On the convergence of a class of adam-type algorithms for non-convex optimization. CoRR, abs/1808.02941, 2018a.
Zaiyi Chen, Yi Xu, Enhong Chen, and Tianbao Yang. Sadagrad: Strongly adaptive stochastic gradient methods. In Proceedings of the 35th International Conference on Machine Learning (ICML), 2018b.
Damek Davis and Dmitriy Drusvyatskiy. Stochastic model-based minimization of weakly convex functions. CoRR, abs/1803.06523, 2018a.
Damek Davis and Dmitriy Drusvyatskiy. Stochastic subgradient method converges at the rate o(k-1/4) on weakly convex functions. CoRR, /abs/1802.02988, 2018b.
Damek Davis and Benjamin Grimmer. Proximally guided stochastic subgradient method for nonsmooth, nonconvex problems. arXiv preprint arXiv:1707.03505, 2017.
Jeffrey Dean, Greg S. Corrado, Rajat Monga, Kai Chen, Matthieu Devin, Quoc V. Le, Mark Z. Mao, Marc'Aurelio Ranzato, Andrew Senior, Paul Tucker, Ke Yang, and Andrew Y. Ng. Large scale distributed deep networks. In NIPS, pp. 1223­1231, USA, 2012. Curran Associates Inc.
D. Drusvyatskiy and C. Paquette. Efficiency of minimizing compositions of convex functions and smooth maps. Mathematical Programming, Jul 2018.
Dmitriy Drusvyatskiy and Adrian S. Lewis. Error bounds, quadratic growth, and linear convergence of proximal methods. arXiv:1602.06661, 2016.
John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and stochastic optimization. Journal of Machine Learning Research, 12(Jul):2121­2159, 2011.
Saeed Ghadimi and Guanghui Lan. Stochastic first- and zeroth-order methods for nonconvex stochastic programming. SIAM Journal on Optimization, 23(4):2341­2368, 2013.
Saeed Ghadimi and Guanghui Lan. Accelerated gradient methods for nonconvex nonlinear and stochastic programming. Math. Program., 156(1-2):59­99, 2016.
9

Under review as a conference paper at ICLR 2019
Moritz Hardt, Ben Recht, and Yoram Singer. Train faster, generalize better: Stability of stochastic gradient descent. In Proceedings of the 33nd International Conference on Machine Learning (ICML), pp. 1225­1234, 2016.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770­778, 2016.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In Proceedings of the 32nd International Conference on Machine Learning, (ICML), pp. 448­456, 2015.
Yangqing Jia, Evan Shelhamer, Jeff Donahue, Sergey Karayev, Jonathan Long, Ross Girshick, Sergio Guadarrama, and Trevor Darrell. Caffe: Convolutional architecture for fast feature embedding. arXiv preprint arXiv:1408.5093, 2014.
Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. International Conference on Learning Representations, 2015.
Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton. Cifar-10 (canadian institute for advanced research).
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. Imagenet classification with deep convolutional neural networks. In Advances in Neural Information Processing Systems (NIPS), pp. 1106­1114, 2012.
Xiaoyu Li and Francesco Orabona. On the convergence of stochastic gradient descent with adaptive stepsizes. CoRR, abs/1805.08114, 2018.
Ilya Loshchilov and Frank Hutter. SGDR: stochastic gradient descent with restarts. In ICLR, volume abs/1608.03983, 2017.
Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in pytorch. 2017.
Sashank J Reddi, Satyen Kale, and Sanjiv Kumar. On the convergence of adam and beyond. In International Conference on Learning Representations, 2018.
Mengye Ren, Wenyuan Zeng, Bin Yang, and Raquel Urtasun. Learning to reweight examples for robust deep learning. arXiv preprint arXiv:1803.09050, 2018.
R. Tyrrell Rockafellar. Monotone operators and the proximal point algorithm. SIAM Journal on Control and Optimization, 14:877­898, 1976.
Ilya Sutskever, James Martens, George E. Dahl, and Geoffrey E. Hinton. On the importance of initialization and momentum in deep learning. In Proceedings of the 30th International Conference on Machine Learning (ICML), pp. 1139­1147, 2013.
Rachel Ward, Xiaoxia Wu, and Leon Bottou. Adagrad stepsizes: Sharp convergence over nonconvex landscapes. CoRR, abs/1806.01811, 2018.
Ashia C. Wilson, Rebecca Roelofs, Mitchell Stern, Nati Srebro, and Benjamin Recht. The marginal value of adaptive gradient methods in machine learning. In NIPS, pp. 4151­4161, 2017.
Yi Xu, Qihang Lin, and Tianbao Yang. Stochastic convex optimization: Faster local growth implies faster global convergence. In ICML, pp. 3821 ­ 3830, 2017.
Tianbao Yang, Qihang Lin, and Zhe Li. Unified convergence analysis of stochastic momentum methods for convex and non-convex optimization. volume abs/1604.03257, 2016.
Martin Zinkevich. Online convex programming and generalized infinitesimal gradient ascent. In ICML, pp. 928­936, 2003.
Fangyu Zou and Li Shen. On the convergence of adagrad with momentum for training deep neural networks. CoRR, abs/1808.03408, 2018.
10

Under review as a conference paper at ICLR 2019

Algorithm 3 SGD(f, x0, , T )

for t = 0, . . . , T do

Compute a stochastic subgradient gt for f (xt).

xt+1 = [xt - gt]

end for

Output: xT =

T t=0

xt/(T

+

1)

Algorithm 4 Unified Stochastic Momentum Methods: SUM(f, x0, , T )

Set parameters:   0 and   (0, 1).

for t = 0, . . . , T do

Compute a stochastic subgradient gt for f (xt).

yt+1 = xt - gt

yt+1 = xt - gt

xt+1 = yt+1 + (yt+1 - yt)

end for

Output: xT =

T t=0

xt/(T

+

1)

A MORE EXPERIMENTAL RESULTS
In this section, we present more experimental results. Comparison of training and testing error in the two settings (w/o regularization) on the two data sets are plotted in Figure 2, 3, 4, 5. We also report the final testing error (after running 80k iterations) of different algorithms in the two settings on the two datasets in Table 1. For parameter tuning, the initial step sizes of all algorithms are tuned in {0.1, 0.3, 0.5, 0.7, 0.9}. The value of  of stagewise algorithms is tuned in {1, 10, 100, 500, 1000, 1500, 2000}. The initial value T0 for stagewise SGD, SHB, SNAG is tuned in {10, 100, 1k, 5k, 6k, 7k, 10k}, and that for stagewise ADAGRAD is tuned in {1, 10, 15, 20, 25}. Note that the training error plotted in the Figure is evaluated on a randomly sampled subset of data around with size about 1/5 of the full size.

B PROOF OF THEOREM 1

Proof. Below, we use Es to denote expectation over randomness in the s-th stage given all history before s-th stage. Define

zs

=

arg

min
x

fs(x)

=

prox(+)(xs-1)

(6)

Then (xs-1) = -1(xs-1 - zs). By applying the convergence bound of SA to fs(x), we have

Es[fs(xs) - fs(zs)]  1(s, Ts, )

xs-1 - zs

2 2

+

2(s,

Ts,

)(fs(xs-1)

-

fs(zs))

+

3(s,

Ts,

)

.

Es

It then follows that

1 Es (xs) + 2

xs - xs-1 2

 fs(zs) + Es  fs(xs-1) + Es  (xs-1) + Es

(7)

On the other hand, we have that
xs - xs-1 2 = xs - zs + zs - xs-1 2 = xs - zs 2 + zs - xs-1 2 + 2 xs - zs, zs - xs-1 (1 - s-1) xs - zs 2 + (1 - s) xs-1 - zs 2

11

Under review as a conference paper at ICLR 2019

Table 1: Comparison of Final Testing Error (%) on CIFAR-10 and CIFAR-100 Datasets

CIFAR-10

CIFAR-100

Algorithms

with reg. without reg. with reg. without reg.

SGD (theory) SGD (heuristic) Stagewise-SGD

15.30 9.09 8.21

18.39 10.43 9.20

39.35 34.12 32.65

50.11 37.17 36.25

SHB (theory) SHB (heuristic) Stagewise-SHB

11.63 7.95 9.35

11.58 10.09 8.64

41.36 34.24 32.74

42.92 38.29 32.65

SNAG (theory) SNAG (heuristic) Stagewise-SNAG

12.31 8.14 8.67

12.10 9.84 9.02

39.61 34.06 33.12

43.66 37.94 33.89

AMSGrad AdaGrad (theory) AdaGrad (heuristic) Stagewise-AdaGrad

11.10 11.94 11.30 8.40

11.82 13.69 13.41 9.48

39.78 41.39 38.42 33.72

41.02 46.58 39.89 34.95

ResNet20 CIFAR-10
0.5

ResNet20 CIFAR-10
0.5

ResNet20 CIFAR-10
0.5

ResNet20 CIFAR-10
0.5

ADAGRAD (heuristic)

SGD (heuristic)

SHB (heuristic)

SNAG (heuristic)

ADAGRAD (theory)

0.4 AMSGRAD

0.4

SGD (theory) Stagewise-SGD

0.4

SHB (theory) Stagewise-SHB

0.4

SNAG (theory) Stagewise-SNAG

Stagewise-ADAGRAD

0.3 0.3 0.3 0.3

Train Error (%)

Train Error (%)

Train Error (%)

Train Error (%)

0.2 0.2 0.2 0.2

0.1 0.1 0.1 0.1

0.0 0 50 100 150 200 250 300 350 400
# of iterations (*200)

0.0 0 50 100 150 200 250 300 350 400
# of iterations (*200)

0.0 0 50 100 150 200 250 300 350 400
# of iterations (*200)

0.0 0 50 100 150 200 250 300 350 400
# of iterations (*200)

ResNet20 CIFAR-10
0.40

ResNet20 CIFAR-10
0.40

ResNet20 CIFAR-10
0.40

ResNet20 CIFAR-10
0.40

ADAGRAD (heuristic)

SGD (heuristic)

SHB (heuristic)

SNAG (heuristic)

0.35

ADAGRAD (theory)

0.35

SGD (theory)

0.35

SHB (theory)

0.35

SNAG (theory)

AMSGRAD

0.30

Stagewise-ADAGRAD

0.30

Stagewise-SGD

0.30

Stagewise-SHB

0.30

Stagewise-SNAG

0.25 0.25 0.25 0.25

Test Error (%)

Test Error (%)

Test Error (%)

Test Error (%)

0.20 0.20 0.20 0.20

0.15 0.15 0.15 0.15

0.10 0.10 0.10 0.10

0 50 100 150 200 250 300 350 400
# of iterations (*200)

0 50 100 150 200 250 300 350 400
# of iterations (*200)

0 50 100 150 200 250 300 350 400
# of iterations (*200)

0 50 100 150 200 250 300 350 400
# of iterations (*200)

Figure 2: Comparison of Training Error (Top) and Testing Error (bottom) on CIFAR-10 without Regularization.

ResNet20 CIFAR-10
0.5

ResNet20 CIFAR-10
0.5

ResNet20 CIFAR-10
0.5

ResNet20 CIFAR-10
0.5

ADAGRAD (heuristic)

SGD (heuristic)

SHB (heuristic)

SNAG (heuristic)

ADAGRAD (theory)

0.4 AMSGRAD

0.4

SGD (theory) Stagewise-SGD

0.4

SHB (theory) Stagewise-SHB

0.4

SNAG (theory) Stagewise-SNAG

Stagewise-ADAGRAD

0.3 0.3 0.3 0.3

Train Error (%)

Train Error (%)

Train Error (%)

Train Error (%)

0.2 0.2 0.2 0.2

0.1 0.1 0.1 0.1

0.0 0 50 100 150 200 250 300 350 400
# of iterations (*200)

0.0 0 50 100 150 200 250 300 350 400
# of iterations (*200)

0.0 0 50 100 150 200 250 300 350 400
# of iterations (*200)

0.0 0 50 100 150 200 250 300 350 400
# of iterations (*200)

ResNet20 CIFAR-10
0.40

ResNet20 CIFAR-10
0.40

ResNet20 CIFAR-10
0.40

ResNet20 CIFAR-10
0.40

ADAGRAD (heuristic)

SGD (heuristic)

SHB (heuristic)

SNAG (heuristic)

0.35

ADAGRAD (theory)

0.35

SGD (theory)

0.35

SHB (theory)

0.35

SNAG (theory)

AMSGRAD

0.30

Stagewise-ADAGRAD

0.30

Stagewise-SGD

0.30

Stagewise-SHB

0.30

Stagewise-SNAG

0.25 0.25 0.25 0.25

Test Error (%)

Test Error (%)

Test Error (%)

Test Error (%)

0.20 0.20 0.20 0.20

0.15 0.15 0.15 0.15

0.10 0.10 0.10 0.10

0 50 100 150 200 250 300 350 400
# of iterations (*200)

0 50 100 150 200 250 300 350 400
# of iterations (*200)

0 50 100 150 200 250 300 350 400
# of iterations (*200)

0 50 100 150 200 250 300 350 400
# of iterations (*200)

Figure 3: Comparison of Training Error (Top) and Testing Error (bottom) on CIFAR-10 with Regularization. The regularization parameter is set 5e - 4.

where the inequality follows from the Young's inequality with 0 < s < 1. Combining the above inequality with (7) we have that

Es

(1 - s) 2

xs-1 - zs

2

 Es

(xs-1)

-

(xs)

+

(s-1 - 2

1)

xs - zs

2 + Es

 Es

(xs-1)

-

(xs)

+

(s-1 (-1

- 1) - µ)

(fs

(xs)

-

fs(zs))

+

Es

 Es

(xs-1)

-

(xs)

+

s-1 - µ (1 - µ)

Es

 Es (xs-1) - (xs)

+ Es

s-1 - µ (1 - µ)

{1

(s,

Ts

,

)

xs-1 - zs

2 +122(s, Ts, )(fs(xs-1) - fs(zs)) + 3(s, Ts, )}

,

(8)

Under review as a conference paper at ICLR 2019

Train Error (%)

ResNet20 CIFAR-100
0.8
ADAGRAD (heuristic) 0.7 ADAGRAD (theory) 0.6 AMSGRAD
Stagewise-ADAGRAD
0.5
0.4
0.3
0.2
0.1
0.0 0 50 100 150 200 250 300 350 400
# of iterations (*200)
ResNet20 CIFAR-100
0.8
ADAGRAD (heuristic) ADAGRAD (theory) 0.7 AMSGRAD Stagewise-ADAGRAD
0.6

Train Error (%)

ResNet20 CIFAR-100
0.8
SGD (heuristic) 0.7 SGD (theory) 0.6 Stagewise-SGD
0.5
0.4
0.3
0.2
0.1
0.0 0 50 100 150 200 250 300 350 400
# of iterations (*200)
ResNet20 CIFAR-100
0.8
SGD (heuristic) SGD (theory) 0.7 Stagewise-SGD
0.6

Train Error (%)

ResNet20 CIFAR-100
0.8
SHB (heuristic) 0.7 SHB (theory) 0.6 Stagewise-SHB
0.5
0.4
0.3
0.2
0.1
0.0 0 50 100 150 200 250 300 350 400
# of iterations (*200)
ResNet20 CIFAR-100
0.8
SHB (heuristic) SHB (theory) 0.7 Stagewise-SHB
0.6

Train Error (%)

ResNet20 CIFAR-100
0.8
SNAG (heuristic) 0.7 SNAG (theory) 0.6 Stagewise-SNAG
0.5
0.4
0.3
0.2
0.1
0.0 0 50 100 150 200 250 300 350 400
# of iterations (*200)
ResNet20 CIFAR-100
0.8
SNAG (heuristic) SNAG (theory) 0.7 Stagewise-SNAG
0.6

Test Error (%)

Test Error (%)

Test Error (%)

Test Error (%)

0.5 0.5 0.5 0.5

0.4 0.4 0.4 0.4

0.3 0 50 100 150 200 250 300 350 400
# of iterations (*200)

0.3 0 50 100 150 200 250 300 350 400
# of iterations (*200)

0.3 0 50 100 150 200 250 300 350 400
# of iterations (*200)

0.3 0 50 100 150 200 250 300 350 400
# of iterations (*200)

Figure 4: Comparison of Training Error (Top) and Testing Error (bottom) on CIFAR-100 without Regularization.

Train Error (%)

ResNet20 CIFAR-100
0.8
ADAGRAD (heuristic) 0.7 ADAGRAD (theory) 0.6 AMSGRAD
Stagewise-ADAGRAD
0.5
0.4
0.3
0.2
0.1
0.0 0 50 100 150 200 250 300 350 400
# of iterations (*200)

Train Error (%)

ResNet20 CIFAR-100
0.8
SGD (heuristic) 0.7 SGD (theory) 0.6 Stagewise-SGD
0.5
0.4
0.3
0.2
0.1
0.0 0 50 100 150 200 250 300 350 400
# of iterations (*200)

Train Error (%)

ResNet20 CIFAR-100
0.8
SHB (heuristic) 0.7 SHB (theory) 0.6 Stagewise-SHB
0.5
0.4
0.3
0.2
0.1
0.0 0 50 100 150 200 250 300 350 400
# of iterations (*200)

Train Error (%)

ResNet20 CIFAR-100
0.8
SNAG (heuristic) 0.7 SNAG (theory) 0.6 Stagewise-SNAG
0.5
0.4
0.3
0.2
0.1
0.0 0 50 100 150 200 250 300 350 400
# of iterations (*200)

ResNet20 CIFAR-100
0.8

ResNet20 CIFAR-100
0.8

ResNet20 CIFAR-100
0.8

ResNet20 CIFAR-100
0.8

ADAGRAD (heuristic)

SGD (heuristic)

SHB (heuristic)

SNAG (heuristic)

ADAGRAD (theory)

0.7 AMSGRAD

0.7

SGD (theory) Stagewise-SGD

0.7

SHB (theory) Stagewise-SHB

0.7

SNAG (theory) Stagewise-SNAG

Stagewise-ADAGRAD

0.6 0.6 0.6 0.6

Test Error (%)

Test Error (%)

Test Error (%)

Test Error (%)

0.5 0.5 0.5 0.5

0.4 0.4 0.4 0.4

0.3 0 50 100 150 200 250 300 350 400
# of iterations (*200)

0.3 0 50 100 150 200 250 300 350 400
# of iterations (*200)

0.3 0 50 100 150 200 250 300 350 400
# of iterations (*200)

0.3 0 50 100 150 200 250 300 350 400
# of iterations (*200)

Figure 5: Comparison of Training Error (Top) and Testing Error (bottom) on CIFAR-100 with Regularization. The regularization parameter is set 5e - 4.

where the second inequality uses the strong convexity of fs(x), whose strong convexity parameter is -1 - µ. Next, we bound fs(xs-1) - fs(zs) given that xs-1 is fixed. According to the definition of fs(·), we have

fs(xs-1)

-

fs(zs)

=

(xs-1)

-

(zs)

-

1 2

zs - xs-1

2

1 = (xs-1) - (xs) + (xs) - (zs) - 2

zs - xs-1

2

= [(xs-1) - (xs)] +

1 fs(xs) - fs(zs) + 2

zs - xs-1

2- 1 2

xs - xs-1 2

-1 2

zs - xs-1 2

 [(xs-1) - (xs)] + [fs(xs) - fs(zs)].

Taking expectation over randomness in the s-th stage on both sides, we have

fs(xs-1) - fs(zs)  Es[(xs-1) - (xs)] + Es[fs(xs) - fs(zs)]

 E[(xs-1) - (xs)] + 1(s, Ts, )

xs-1 - zs

2 2

+

2(s,

Ts,

)(fs(xs-1)

-

fs(zs))

+

3(s,

Ts,

).

Thus,

(1 - 2(s, Ts, ))(fs(xs-1) - fs(zs))  E[(xs-1) - (xs)] + 1(s, Ts, )

xs-1 - zs

2 2

+

3(s,

Ts,

).

Assuming that 2(s, Ts, )  1/2, we have

2(s, Ts, )(fs(xs-1) - fs(zs))  Es[(xs-1) - (xs)] + 1(s, Ts, )

xs-1 - zs

2 2

+

3(s,

Ts,

).

13

Under review as a conference paper at ICLR 2019

Plugging this upper bound into (8), we have

Es

(1 - s) 2

xs-1 - zs

2

 Es (xs-1) - (xs)

+ Es

s-1 - µ (1 - µ)

{21(s

,

Ts,

)

xs-1 - zs

2 + (xs-1) - (xs) + 23(s, Ts, )}

By setting s = 1/2,  = 1/(2µ) and assuming 1(s, Ts, )  1/(48), we have

Es

1 8

xs-1 - zs 2

 4Es (xs-1) - (xs)

+ 63(s, Ts, )}

Define ws = s. Multiplying both sides by ws, we have that

(9)

wsEs[  (xs-1) 2]  Es 32wss + 483(s, Ts, )ws

By summing over s = 1, . . . , S + 1, we have

S+1
wsE[  (xs-1) 2]  E

32 S+1

48 S+1

 wss +  ws3(s, Ts, )

s=1

s=1

s=1

Taking the expectation w.r.t.   {0, . . . , S}, we have that

E[ (x ) 2]]  E 32 

S+1 s=1

ws

s

S+1 s=1

ws

+

48

For the first term on the R.H.S, we have that

S+1 s=1

ws3(s

,

Ts,

))



S+1 s=1

ws

S+1

S+1

S+1

S+1

wss = ws((xs-1) - (xs)) = (ws-1(xs-1) - ws(xs)) + (ws - ws-1)(xs-1)

s=1

s=1

s=1

s=1

S+1
= w0(x0) - wS+1(xS+1) + (ws - ws-1)(xs-1)

s=1

S+1

S+1

= (ws - ws-1)((xs-1) - (xS+1))   (ws - ws-1) = wS+1

s=1

s=1

Then,

E[

 (x )

2] 

32wS+1



S+1 s=1

ws

48 +

S+1 s=1

ws

3

(s,

Ts,

)



S+1 s=1

ws

The standard calculus tells that

S
s 

S
xdx =

1 S+1

s=1 0

+1

S
s-1  SS-1 = S,   1,
s=1

S
s-1 

S

x-1dx

=

S , 0

<



<

1

s=1

0



Combining these facts and the assumption 3(s, Ts, )  c/s, we have that


 

32 (+1) (S+1)

+

48c(+1) (S+1)

E[  (x ) 2] 

 

32 (+1) (S+1)

+

48c(+1) (S+1)

1 0<<1

In order to have E[ (x ) 2]  2, we can set S = O(1/ 2). The total number of iterations is

SS

Ts  12s  6S(S + 1) = O(1/ 4)

s=1

s=1

14

Under review as a conference paper at ICLR 2019

C PROOF OF THEOREM 2

Proof. The proof is almost a duplicate to that of Theorem 1. Define ws = s. We apply Lemma 1 to each call of SGD in stagewise SGD,

E[fs(xs) - fs(zs)] 

zs - xs-1 2 + sG^2 ,

2sTs

2

Es

where G^2 is the upper bound of E[ g(x; ) + -1(x - xs-1) 2], which exists and can be set to 2G2 + 2-2D2 due to the Assumption 1-(ii) and the bounded assumption of the domain. Then following the same analysis as that in the proof of Theorem 1, we have

Es

(1 - s) 2

xs-1 - zs

2

Es

(xs-1)

-

(xs)

+

(s-1 - 2

1)

xs - zs

2 + Es

E

(xs-1)

-

(xs)

+

(s-1 - 1) (-1 - µ)

(fs(xs)

-

fs(zs))

+

Es

E

(xs-1)

-

(xs)

+

s-1 - µ (1 - µ)

Es

(10)

Combining the above inequalities, we have that

(1

-

s)

-

2(s-1 - µ) (1 - µ)sTs

Es[

 (xs-1)

2]  Es

2s

+

(s-1 - µ)sG^2 (1 - µ)

Multiplying both sides by ws, we have that

ws

(1

-

s)

-

2(s-1 - µ) (1 - µ)sTs

Es[

 (xs-1)

2]  Es

2wss

+

(s-1

- (1

µ)wssG^2 - µ)

By setting s = 1/2 and  = 1/(2µ), Tss  12, we have

1 4 wsEs[

 (xs-1)

2]  Es[2wss + 3wssG^2]

By summing over s = 1, . . . , S + 1, we have

S+1

S+1

S+1

wsE[  (xs-1) 2]  E 16µ wss + 24µ wssG^2

s=1

s=1

s=1

Taking the expectation w.r.t.   {0, . . . , S}, we have that

E[ (x ) 2]]  E 16µ

S+1 s=1

wss

S+1 s=1

ws

+

24µ

By similar analysis, we have that

S+1 s=1

ws

s

G^2

S+1 s=1

ws


 

16µ (+1) S+1

+

240 µG^ 2 (+1) S+1

E[  (x ) 2] 

 

16µ (+1) S+1

+

240 µG^ 2 (+1) (S+1)

1 0<<1

In order to have E[ (x ) 2]  2, we can set S = O(1/ 2). The total number of iterations is

SS

Ts  12s  6S(S + 1) = O(1/ 4)

s=1

s=1

15

Under review as a conference paper at ICLR 2019

D PROOF OF THEOREM 4

We need the following lemma for the convergence bound of stochastic momentum methods for a strongly convex problem, whose proof is postponed to Section F.

Lemma 3.

For

Algorithm

4,

assume

f (x)

=

(x)

+

1 2

x - x0

2 is a -strongly convex function,

gt

=

g(xt; ) +

1 

(xt

-

x0)

where

g(x;

)



F (xt) such that E[

g(x; )

2]



G2,

and 



(1 - )2/(8 + 4), then we have that

E[f (xT ) - f (x)] 

(1 - ) x0 - x 2(T + 1)

2

+

(f (x0) - f (x)) (1 - )(T + 1)

+

2G2(2 + 1-

1)

+

4 + 4 (1 - )

 2

x0 - x

2

(11)

where xT =

T t=0

xt/(1

+

T)

and

x



arg

minxRd

f (x).

Remark: It is notable that in the above result, we do not use the bounded domain assumption since

we consider  = Rd for the unified momentum methods in this subsection. The key to get rid of

bounded

domain

assumption

is

by

exploring

the

strong

convexity

of

f (x)

=

(x)

+

1 2

x - x0

2.

Proof. of Theorem 4 According to the definition of zs in (6) and Lemma 3, we have that

Es

1 (xs) + 2

xs - xs-1

2



fs(zs)

+

(fs(xs-1) - (1 - )(Ts

fs(zs)) + 1)

+

(1

- ) xs-1 - zs 2s(Ts + 1)

2 + 2sG2(2 + 1) + 1

1-

24

xs-1 - zs

2

 (xs-1) + Es,

Es

where the last inequality uses the value of s = (1 - )/(96s( + 1))  (1 - )/(96( + 1)), which also satisfies the condition in Lemma 3 by noting that  = -1 - µ = 1/(2). Similar to the proof of Theorem 1, we have

(1 - s) 2

xs-1 - zs

2

Es[(xs-1)

-

(xs)]

+

s-1 - µ (1 - µ)

Es

(12)

Plugging the expression of Es and rearranging above inequality, we have that

(1

-

s)

-

2(s-1 - µ)(1 - ) (1 - µ)s(Ts + 1)

-

s-1 - µ (1 - µ)

 24

 (xs-1) 2

2Es[s]

+

2(s-1 - µ) (1 - µ)

(fs(xs-1) - fs(zs)) (1 - )(Ts + 1)

+

2sG^2(2 1-

+

1)

The definition of fs gives that

fs(xs-1)

-

fs(zs)

=

(xs-1)

-

(zs)

-

1 2

zs - xs-1

2

On the other hand, the µ-weakly convexity of  gives that

(zs)  (xs-1) +

g(xs-1), zs - xs-1

-µ 2

zs - xs-1

2,

where g(xs-1)  F (xs-1). Combing these two inequalities we have that

fs(xs-1) - fs(zs)  g(xs-1), xs-1 - zs

-µ 2

zs - xs-1

2

 G2 + µ - µ 2µ 2

zs - xs-1

2 = G2 2µ

16

Under review as a conference paper at ICLR 2019

where the second inequality follows from Jensen's inequality for · and Young's inequality. Combining above inequalities and multiplying both side by ws, we have that

ws

(1

-

s)

-

2(s-1 - µ)(1 - ) (1 - µ)s(Ts + 1)

-

s-1 - µ (1 - µ)

 24

 (xs-1) 2

2wsEs[s]

+

2ws(s-1 - µ) (1 - µ)

2µ(1

G2 - )(Ts

+

1)

+

2sG2(2 1-

+

1)

(13)

By setting s = 1/2, s(Ts + 1)  24(1 - ), we have that

ws 4

 (xs-1)

2

 2wsEs[s] +

wssG2 4(1 - )2

+

12wssG2(2 + 1) 1-

Summing over s = 1, . . . , S + 1 and rearranging, we have

S+1
ws

 (xs-1) 2 = E

S+1

8  wss

+

wssG2(

+ 48(2 + (1 - )2

1)(1

-

))

s=1

s=1

Following similar analysis as in the proof of Theorem 2, we can finish the proof.

E PROOF OF THEOREM 3

Proof.

Applying Lemma 2 with Ts



Ms

max{

G^+maxi g1s:Ts,i 2c

,c

d i=1

g1s:Ts,i } Ms > 0, and

the

fact

that

(xs-1)



(zs)

+

1 2

xs-1 - zs

2 in sth stage, we have that

1 Es (xs) + 2s

xs - xs-1 2



fs(zs)

+

c Mss

xs-1 - zs

2 + s Msc

 (xs) + Es

Es

According to (12), we have that

(1

- s 2

)

Es[

xs-1 - zs

2]

(xs-1)

-

(xs)

+

(s-1 - 2

1)

xs - zs

2 + Es

(xs-1)

-

(xs)

+

s-1 - µ (1 - µ)

c Mss

xs-1 - zs

2 + s Msc

Rearranging above inequality then multiplying both side by ws, we have that

ws

(1

-

s

)-

22c(s-1 - µ) (1 - µ)Mss

 (xs-1) 2

2wsEs[s]

+

2wss(s-1 cMs(1 -

- µ) µ)

By using Mss  24c and summing over s = 1, . . . , S + 1, we have that

S+1
ws

 (xs-1) 2  E

S+1

8wss 

+

wss2 c22

s=1

s=1

By the definition of  in the theorem, taking expectation of (x ) 2 w.r.t.   {0, . . . , S} we have that

E[

 (x )

2] =E

8 S+1 

s=1

wss

S+1 i=1

wi

+

02 c22

S+1

s=1

s-1

S+1 i=1

wi

 8( + 1) (S + 1)

+

02( + 1) c22(S + 1)I(<1)

,

where I( < 1) is 1 if  < 1 and 0 otherwise.

17

Under review as a conference paper at ICLR 2019

F PROOF OF LEMMA 3

Proof. Following the analysis in Yang et al. (2016), we directly have the following inequality,

E[ xk+1 + pk+1 - x 2] =

= E[ xk + pk - x

2]

-

2 1-

E[(xk

-

x)

f (xk)]

-

2 (1 - )2

E[(xk

-

xk-1)

f (xk)]

-

22 (1 - )2

E[gk-1f (xk)]

+

 1-

2
E[|gk 2]

We also note that

f (xk) - f (x)  (xk - x)

f (xk)

-

 2

xk - x

2

f (xk) - f (xk-1)  (xk - xk-1)

f (xk)

-

 2

xk - xk-1

2

- E[gk-1f (xk)]  E[ gk-1

2+ 2

f (xk) 2]  1 2

xk-1 - x0

2

+

1 2

xk - x0

2 + 2G2

Ek [

gk

2]



2 2

xk - x0

2 + 2G2

where the first two inequalities are due to the strong convexity of f (·) and the last three inequalities are due to the boundness assumption. Thus

E[

xk+1 + pk+1 - x

2]  E[

xk + pk - x

2]

-

1

2 -

E[(f (xk)

-

f (x))]

-

2 (1 - )2

E[(f (xk)

-

f (xk-1))]

+

 1-

2
(2 + 1)4G2

-  1-

xk - x

2

-

 (1 - )2

xk - xk-1

2

2 2 + (1 - )2 2

xk-1 - x0

2

+

2 + 2 (1 - )2

2 2

xk - x0

2

By summarizing the above inequality over k = 0, . . . , T , we have

2 1-E

T
(f (xk) - f (x))

 E[

x0 - x

2]

+

2 (1 - )2

E[f (x0)

-

f (x)]

k=0

+



2
(2 + 1)4G2(T + 1)

1-

-  T 1-

xk - x

2

+

4 (1 - )2

2 2

T

xk-1 - x

2

+

4 + 4 (1 - )2

2 2

T

xk - x 2

k=0

k=0

k=0

4 + 4 2 + (1 - )2 2 (T + 1)

x0 - x

2

When   (1 - )2/(8 + 4), we have

E (f (xT ) - f (x))



(1 - ) x0 - x 2(T + 1)

2

+

(1

 -

)

f (x0) T

- f (x) +1

+

1

 -



(2

+

1)2G2

4 + 4  + (1 - ) 2

x0 - x

2

18

Under review as a conference paper at ICLR 2019

G PROOF OF LEMMA 2

The proof is almost a duplicate of the proof of Proposition 1 in Chen et al. (2018b). For completeness, we present a proof here.



Proof. Let 0(x) = 0 and x H = x Hx. First, we can see that t+1(x)  t(x) for any

t  0. Define t =

t 

=1

gt

and



=

(F (xt) - gt)

(xt - x). Let t be defined by

t(g) = sup g
x

x

-

1 

t(x)

Taking the summation of objective gap in all iterations, we have

TT

TT

(f (xt) - f (x))  f (xt) (xt - x) = gt (xt - x) + t

t=1 t=1

t=1 t=1

T

T1

1

T

= gt xt - gt x -  T (x) +  T (x) + t

t=1 t=1

t=1



1  T (x)

+

T
gt
t=1

xt

+

T t=1

t

+

sup
x

-

T

gt

x

-

1  T (x)

t=1

1 =  T (x) +

T

gt xt + T (-T ) +

T

t

t=1 t=1

Note that

T (-T ) = -

T

gt

xT +1

-

1  T (xT +1)



-

T

gt

xT +1

-

1  T -1(xT +1)

t=1 t=1



sup
x

-T

x

-

1  T -1(x)

=

T -1(-T

)



T -1(-T -1)

-

gT

T -1(-T -1)

+

 2

gT

2 T -1

where the last inequality uses the fact that t(x) is 1-strongly convex w.r.t consequentially t(x) is -smooth wr.t. · t = · .Ht-1 Thus, we have

· t =

· Ht and

T

gt xt + T (-T ) 

T

gt

xt

+

T -1(-T -1) -

gT

T -1(-T -1) +

 2

gT

2 T -1

t=1 t=1

=

T -1
gt

xt

+ T -1(-T -1)

+

 2

gT

2 T -1

t=1

By repeating this process, we have

T

gt

xt

+

T (-T )



0(-0)

+

 2

T

gt

2 t-1

=

 2

T

g2
t t-1

t=1 t=1 t=1

Then

T 1 T (f (xt) - f (x))   T (x) + 2

T

gt

+2
t-1

t

t=1 t=1 t=1

Following the analysis in Duchi et al. (2011), we have

(14)

Td

gt

2 t-1



2

g1:T,i 2

t=1 i=1

19

Under review as a conference paper at ICLR 2019

Thus

T

(f (xt) - f (x)) 

G

x - x1 2

2 2

+

(x

-

x1)

diag(sT )(x - x1) +  2

d

t=1 i=1

 G + maxi g1:T,i 2 2

x - x1

2 2

+



d

T
g1:T,i 2 + t

i=1 t=1

T
g1:T,i 2 + t
t=1

Now by the value of T



M

max{

(G+maxi 2c

g1:T ,i )

,c

d i=1

g1:T,i }, we have

(G + maxi g1:T,i 2) 

c ,

2T M



d i=1

g1:T ,i

2



T Mc

Dividing by T on both sides and setting x = x, following the inequality (3) and the convexity of f (x) we have

f (x)

-

f



c M

x0 - x

2+



1 +

Mc T

T

t

t=1

Let {Ft} be the filtration associated with Algorithm 1 in the paper. Noticing that T is a random variable with respect to {Ft}, we cannot get rid of the last term directly. Define the Sequence {Xt}tN+ as

1t

1t

Xt = t i = t

gi - E[gi], xi - x

i=1 i=1

(15)

where E[gi]  f (xi). Since E [gt+1 - E[gt+1]] = 0 and xt+1 = arg min x
x

1 t

t 

=1

g

+

1 t

t(x),

which

is

measurable

with

respect

to

g1, .

.

.

,

gt

and

x1,

..

.

,

xt,

it

is

easy

to

see

{t}tN

is a martingale difference sequence with respect to {Ft}, e.g. E[t|Ft-1] = 0. On the other

hand, since gt 2 is upper bounded by G, following the statement of T in the theorem, T  N =

M2

max{

(G+1)2 4

,

d2

G2

}

<



always

holds.

Then

following

Lemma

1

in

(Chen

et

al.,

2018b)

we

have that E[XT ] = 0.

Now taking the expectation we have that

E[f (x)

-

f]



c M

x0 - x

2 + c M

Then we finish the proof.

20


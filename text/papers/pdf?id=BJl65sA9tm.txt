Under review as a conference paper at ICLR 2019
IMPROVING GENERATIVE ADVERSARIAL IMITATION LEARNING WITH NON-EXPERT DEMONSTRATIONS
Anonymous authors Paper under double-blind review
ABSTRACT
Imitation learning aims to learn an optimal policy from expert demonstrations and its recent combination with deep learning has shown impressive performance. However, collecting a large number of expert demonstrations for deep learning is time-consuming and requires much expert effort. In this paper, we propose a method to improve generative adversarial imitation learning by using additional information from non-expert demonstrations which are easier to obtain. The key idea of our method is to perform multiclass classification to learn discriminator functions where non-expert demonstrations are regarded as being drawn from an extra class. Experiments in continuous control tasks demonstrate that our method learns optimal policies faster and has more stable performance than the generative adversarial imitation learning baseline.
1 INTRODUCTION
The goal of sequential decision making problems is to learn an optimal policy that exhibits tasksolving behavior. Reinforcement learning (RL) is a powerful approach to find such a policy by maximizing rewards computed by a reward function (Puterman, 1994; Sutton & Barto, 1998). While RL has achieved great success in solving challenging tasks (Mnih et al., 2015; Silver et al., 2017), its performance depends heavily on a good reward function which well captures the concept of tasksolving behavior. Unfortunately, designing such a good reward function is a difficult trial-and-error process and often time-consuming. This difficulty is one of the major limitations of RL for many real-world applications.
Imitation learning (IL) (Schaal, 1999) is an alternative approach to learn an optimal policy. In contrast to RL, IL has access to expert demonstrations, which are task-solving trajectories collected from experts who have mastered the task, and IL finds a policy that generates trajectories similar to these expert demonstrations. IL has been a long-studied problem and is attracting more attention recently, particularly in robotics (Duan et al., 2017; Stadie et al., 2017) and games (Ross et al., 2011). However, traditional IL methods rely on extensive feature engineering which makes their applicability quite limited.
Many approaches were proposed to overcome the difficulty of feature engineering in IL. Among them, the most successful approach is to use deep neural networks to learn representative features in an end-to-end manner (Wulfmeier et al., 2015; Finn et al., 2016a; Ho & Ermon, 2016; Fu et al., 2018). In particular, generative adversarial imitation learning (GAIL) (Ho & Ermon, 2016) is a state-of-the-art method that uses generative adversarial training to perform IL with deep neural networks. While the combination of IL and deep learning led to impressive performance improvement, it has introduced a new limitation regarding sample efficiency as training a deep neural network generally requires a large amount of data. This is a severe limitation in IL since collecting a large number of expert demonstrations can be expensive and time-consuming, and requires much expert effort. Moreover, the generative adversarial training procedure is known to be highly unstable (Mescheder et al., 2018), and this issue becomes more severe when only a small amount of data is available.
While expert demonstrations may be expensive, non-expert demonstrations collected from nonexperts who have not mastered the task are often much cheaper to obtain. For instance, demonstrations from amateur-level players in the game of Go are much cheaper to obtain than those from master-level players. Leveraging additional information from a large number of non-expert demonstrations to improve IL is the key idea of semi-supervised inverse RL (SSIRL) (Valko et al., 2012)
1

Under review as a conference paper at ICLR 2019
and IRL from failure (IRLF) (Shiarlis et al., 2016). Both SSIRL and IRLF have shown to perform well for low-dimensional problems with proper feature engineering. However, they are extensions of traditional IL methods and are not capable of efficiently training deep neural networks which are needed for handling high-dimensional problems. Moreover, both of them rely on rather restrictive assumptions about data generating processes of non-expert demonstrations which we cannot control in practice.
In this paper, we propose a novel method to leverage non-expert demonstrations without the aforementioned weaknesses of SSIRL and IRLF. Our method is built upon the generative adversarial training procedure where we perform multiclass classification to learn discriminator functions with non-expert demonstrations regarded as being drawn from an extra class. Our method uses both expert and non-expert demonstrations in the discriminator learning objective, and this leads to a better feature representation of discriminator functions. We show that the minimax formulation commonly used in generative adversarial training does not guarantee the optimality of policies for our method, and we alternatively propose a modified optimization procedure which provides such a guarantee. We further show that our method learns an approximated solution to maximum entropy IRL with two reward functions. Experiments on benchmark continuous control tasks show that our method learns the expert policies faster and achieves more stable performances overall than GAIL when only a small number of expert demonstrations is available.
2 RELATED WORK
IL has been a long-studied problem and there are many approaches to solve this problem, including behavior cloning (Pomerleau, 1988), occupancy measure matching (Syed et al., 2008) and IRL (Russell, 1998; Ng & Russell, 2000). Recently, IL methods that use a generative adversarial training procedure (Goodfellow et al., 2016) have gained a great deal of interest thanks to its effectiveness at training deep neural networks (Ho & Ermon, 2016; Finn et al., 2016a; Fu et al., 2018). Despite such success, deep neural networks are well-known to have poor data efficiency and require a large amount of data to train. For this reason, these methods may not perform well when only a small number of expert demonstrations are available.
Semi-supervised learning (SSL) (Chapelle et al., 2010) improves sample efficiency by utilizing a large amount of unlabeled data and it has shown promising results in deep learning (Ranzato & Szummer, 2008; Weston et al., 2012). While mainly developed for supervised learning, SSL can be applied to improve some IL methods as well. In particular, semi-supervised IRL (SSIRL) (Valko et al., 2012) improves the IRL method of Abbeel & Ng (2004) by using semi-supervised support vector machines to classify between expert demonstrations and trajectories generated by the agent. However, this approach is not suitable due to the difference in data generating processes between SSL and IRL. More specifically, SSL methods generally assume that an unlabeled dataset is a mixture of positive and negative samples. For SSIRL, this implies that an unlabeled demonstration dataset is a mixture of expert demonstrations and the agent's trajectories. However, collecting such an unlabeled dataset is quite difficult in practice since we do not know the agent's policies beforehand. This issue has been remedied to some extent by Audiffren et al. (2015) where the authors proposed using a manifold regularization technique which relies on a milder assumption on the unlabeled dataset. However, manifold regularization requires an appropriate similarity function to perform well. Moreover, both methods are unsuitable in high-dimensional problems due to its dependent on the linearity of reward functions and good feature engineering.
IRL from failure (IRLF) (Shiarlis et al., 2016) also utilizes an additional demonstration dataset which is assumed to consist of demonstrations collected by non-expert who failed to solve the task. Using this dataset, the authors proposed an IRL method that encourages the agent to be dissimilar to non-expert while learning an expert policy. While IRLF is technically sounded, collecting strictly failure demonstrations can be expensive on tasks such as autonomous driving where failures are catastrophic. Moreover, this method is still restricted by the linear reward assumption and is not applicable to train deep neural networks.
Using additional datasets to improve generative adversarial training was explored in the context of semi-supervised generative modeling (Salimans et al., 2016; Li et al., 2017a) and multi-modal generative modeling (Liu & Tuzel, 2016). However, if we want to apply these methods to our IL setting, then we are required to generate trajectories to imitate non-experts. This is inefficient since
2

Under review as a conference paper at ICLR 2019

generating trajectories requires interactions with environment and we would like to keep the number of such interactions as small as possible. In contrast, our method only learns the expert policy and only generate trajectories to imitate the expert.
Our proposal of using an additional dataset as an extra class in multiclass classification resembles the idea of universum learning (Vapnik, 1998; 2006; Zhang & LeCun, 2017). So far, universum learning has been applied only to supervised learning problems, especially for discriminative learning with support vector machines. Thus, our contribution may be regarded as the first attempt to apply the idea of universum learning to IL and also to generative adversarial learning.

3 BACKGROUND

In this section, we provide backgrounds of RL, IL, and GAIL.

3.1 REINFORCEMENT LEARNING (RL)

An RL problem is formulated as a discrete-time Markov decision process (MDP) which is defined by a tuple M " pS, A, pps1|s, aq, p0psq, rps, aq, q, where S  Rds is the (continuous) state space, s P S is a state, A  Rda is the (continuous) action space, a P A is an action, pps1|s, aq is
the transition probability density from s to s1 when a is taken, p0psq is the initial state probability density, rps, aq is the reward function, and 0    1 is the discount factor1. In each discrete

time-step t  0, an agent in a state st chooses an action at according to a policy pa|stq, which is a conditional probability density. Then, the agent transits to a next state st`1 ,, pps1|st, atq and receives a reward rpst, atq. We call a sequence of states and actions a trajectory  . The goal of RL
is to find an optimal policy that maximizes the expected discounted cumulative rewards (also called

return) defined as

«8 ff Ep0ps0q,pat|stqt0,ppst`1|st,atqt0 ÿ trpst, atq " E rrps, aqs ,
t"0

(1)

where the expectation is taken over the probability densities for all time steps and we use the notation

E for brevity. When the policy is a parameterized function with parameter , a locally optimal policy can be found by optimization methods such as policy gradients (Williams, 1992).

We also use an equivalent formulation in terms of occupancy measures (Puterman, 1994; Alt-

man, 1999; Syed et al., 2008). A state-action occupancy measure defines the expected dis-

counted (unnormalized) visitation density of each state-action pair and is denoted by ps, aq "

E rp0ps0q,pat|stqt0,ppst`1|st,atqt0

T t"0

tpst

´

s, at

´

aqs,

where



is

the

Dirac

delta

function2.

An important property of the occupancy measure is that if it satisfies the Bellman flow con-

straints,  ps1, a1qda1 " p0ps1q `   pps1|s, aqps, aqdsda, then there is one-to-one correspon-

dence between the occupancy measure and a policy given as pa|sq " ps, aq{psq, where psq "  ps, aqda is a state occupancy measure. This property allows us to rewrite the RL objective to be maximized as E rrps, aqs "  ps, aqrps, aqdsda.

The optimal policy of the above MDP is deterministic (Puterman, 1994) and a stochastic policy should be reduced to a deterministic policy at an optimum. However, a deterministic policy suffers from an exploration issue. In many tasks, it is beneficial to consider maximum entropy RL (Ziebart et al., 2008; 2010) whose optimal policy is a stochastic policy maximizing

E rrps, aqs ` Hpq,

(2)

where   0 and Hpq " ´E rlog pa|sqs is a discounted causal entropy (Ziebart et al., 2010). The advantage of maximum entropy RL is that it encourages exploration and allows the agent to
find a better policy when compared to the standard RL formulation (Haarnoja et al., 2017).

3.2 IMITATION LEARNING (IL)
The goal of IL, or apprenticeship learning, is to learn a parameterized policy , with policy parameter , such that  exhibits the same behavior as an expert policy E. We assume that E is unknown.
1 " 1 is only allowed for finite horizon setting. 2For discrete S and A,  is replaced by the indicator function.

3

Under review as a conference paper at ICLR 2019

We instead have access to expert demonstrations DE " tpsi, aiquNi"1 which are trajectories generated by executing the expert policy under an MDP. IL methods can be categorized into interactive methods and non-interactive methods. Interactive methods such as structured prediction (Ross et al., 2011) allow the agent to query for expert demonstrations during learning. Despite their strong theoretical guarantees and great empirical performances, these methods require the expert to be available during learning which is not always possible in reality. On the other hand, non-interactive methods, such as occupancy measure matching (Syed et al., 2008) and IRL (Ng & Russell, 2000; Abbeel & Ng, 2004), only require a pre-collected demonstration dataset for learning. In this paper, we focus on the non-interactive IL setting due to its high practicality.

3.3 GENERATIVE ADVERSARIAL IMITATION LEARNING (GAIL)

GAIL (Ho & Ermon, 2016) is a state-of-the-art non-interactive IL method that performs occupancy
measure matching to learn the parameterized policy. In occupancy measure matching (Syed et al.,
2008), the policy parameter is learned to minimize a distance measure between occupancy measures of E and . More formally, occupancy measure matching methods solve an optimization problem min pE ,  q ´ Hpq, where H is the causal entropy regularizer with   0.

The key idea of GAIL is to use generative adversarial training to estimate the distance and to mini-

mize the estimated distance. Briefly speaking, the distance measure is the Jensen-Shannon (JS) di-

vergence

defined

as JSpE ,  q

"

1 2

pgKLpE ||pE

`  q{2q ` gKLp ||pE `  q{2qq ,

where gKLp||qq

"



ps,

aq

log

ps,aq qps,aq

dsda

´

 ps, aqdsda

`

 qps, aqdsda

is

the

gener-

alized Kullback-Leibler (gKL) divergence defined for unnormalized densities3 (Dikmen et al.,

2015). Since both occupancy measures are unknown, the divergence is approximated via a bi-

nary classification problem: max E rlog Dps, aqs ` EE rlogp1 ´ Dps, aqqs. The function D : S ^ A Ñ p0, 1q is called a discriminator and is often parameterized by a deep neural network as Dps, aq " exppdps, aqq{pexppdps, aqq ` 1q. It can be shown that if the discriminator has

infinite capacity, the global maximum of this binary classification problem corresponds to the JS

divergence up to a constant. Based on this fact, GAIL minimizes an approximated JS divergence by

solving the following minimax optimization problem:

min


max


E

rlog

Dps,

aqs

`

EE

rlogp1

´

Dps,

aqqs

´

 Hp q.

(3)

In practice, this optimization problem is solved by alternately performing gradient ascent for  and gradient descent for . Notice that the gradient descent step for  is equivalent to performing policy gradient ascent with reward function rps, aq " ´ logpDps, aqq and a causal entropy.

4 IMITATION LEARNING WITH NON-EXPERT DEMONSTRATIONS
GAIL is an effective method that achieves state-of-the-art performance for high-dimensional IL problems. However, the generative adversarial training procedure described above is known to be highly unstable (Mescheder et al., 2018). This instability issue becomes more apparent when only a small amount of data is available. One of the reasons is due to inaccuracy of estimating the divergence via a discriminator learned by using a small number of demonstrations. A classical approach to improve binary classification is to use an unlabeled dataset in the context of SSL. However, as discussed previously, SSL is not suitable for IL due to its assumption of unlabeled data.
Note that the discriminator in GAIL is learned using two sets of data samples; expert demonstrations and trajectories collected by agent. For this reason, even when the number of expert demonstrations is small, GAIL may still learn well after observing a sufficiently large number of agent's trajectories. However, this is not desirable since in practice we often prefer data efficient methods that can learn well even with a small number of agent's trajectories.
To improve discriminator learning with a milder assumption on an additional demonstration dataset, we propose to perform multiclass classification using the additional dataset as a new class. In the following, we first formally describe our problem setting, and then present our IL method that learns a multiclass classifier for generative adversarial training. Lastly, we show an interesting relation
3An occupancy measure is not a density since it is not summed to one. Therefore, gKL is used instead.

4

Under review as a conference paper at ICLR 2019

between our multiclass classification objective and maximum entropy IRL with two reward functions (Ziebart et al., 2010).

4.1 PROBLEM SETTING
We consider an IL problem with an additional non-expert demonstration dataset as follows. We assume that expert demonstrations DE " tpsi, aiquiN"1 are trajectories collected by executing expert policy E in an MDP with reward function rEps, aq. We assume that E is an optimal policy, i.e., E " argmax E rrEps, aqs. In addition, we also have non-expert demonstrations DN " tpsj, ajqujM"1 collected by executing non-expert policy N in the same MDP. The nonexpert is assumed to be sub-optimal and gives worse expected rewards than the expert policy, i.e., EN rrEps, aqs  EE rrEps, aqs. We assume that DN is easier to obtain than DE and thus M " N . Our goal is to learn parameterized policy  that generates trajectories similar to those from E using both DE and DN.
The main challenge for solving this problem is to efficiently leverage information brought by non-expert demonstrations to learn the expert policy. Under a very weak assumption that EN rrEps, aqs  EE rrEps, aqs, the non-expert policy can be totally unrelated to the expert policy and contains little to none of useful information that can improve learning. In the worst case, the non-expert policy could just be a random policy that randomly generates trajectories.
Therefore, to make efficient learning possible, we need a stronger but not too restrictive assumption on the non-expert demonstrations. Recall that both expert and non-expert demonstrations are collected under the same MDP, and this makes it intuitive to assume that state-action pairs of both the expert and non-expert lie on similar low-dimensional manifolds. To make this statement more explicit, we assume that there exists a feature map  : S ^ A ÑÞ Rb such that state-action pairs from the expert and non-expert are linearly separable in this feature space. While this assumption itself still does not allow us to always escape from the random policy case, it allows us to utilize reasonably generated non-expert demonstrations to improve representation learning when training a discriminator. Furthermore, this assumption also naturally implies that the quality of learned features is improved as the non-expert policy becomes more related to the expert policy since their low-dimensional manifolds becomes more similar to each other. This is the opposite to IRLF where the failure demonstrations should make performance as worse as possible.

4.2 MULTICLASS CLASSIFICATION FOR DISCRIMINATOR LEARNING

Our generative adversarial method alternates between the discriminator learning step and policy learning step. In the discriminator learning step, our goal is to learn a multiclass probabilistic classifier that classifies state-action pairs into three classes; the expert class with label y " E, the non-expert class with label y " N, and the agent class with label y " A. Let the following softmax models be estimates of the class posterior of the three classes:

Fps, aq

"

exppfps, aqq , Zps, aqq

Gps, aq

"

 exppgps, aqq , Zps, aqq

Hps, aq

"

expphps, aqq , Zps, aqq

(4)

where Fps, aq estimates the expert class posterior ppy"E|s, aq, Gps, aq estimates the nonexpert class posterior ppy"N|s, aq, Hps, aq estimates the agent class posterior ppy"A|s, aq, and Zps, aq " exppfps, aqq `  exppgps, aqq ` expphps, aqq is the normalization factor.  is the parameter to be learned and   0 is a weight parameter specified by the user to control the influence of non-expert demonstrations. To learn parameter  for given policy parameter , we maximize the
weighted log-likelihood objective defined as

Lp, q " EE rlog Fps, aqs ` EN rlog Gps, aqs ` E rlog Hps, aqs .

(5)

Notice that the same  is used for both Gps, aq and Lp, q, and this choice is particularly impor-

tant for our analyses in the following sections. We call the functions f, g, and h discriminators.

For smooth and differentiable discriminators, this weighted-log-likelihood objective can be maxi-

mized by stochastic gradient ascent or its adaptive step-size variants. The expectations over E and N are approximated using mini-batch samples from DE and DN, respectively, while the expectation over  is approximated using trajectories tpsk, akqukK"1 collected by executing policy .

We explicitly assume that the discriminators are represented by deep neural networks with shared hidden layers and three outputs where each output represents f, g, and h. This parameterization

5

Under review as a conference paper at ICLR 2019

allows us to explicitly make use of our assumption of using common feature map . That is, the shared hidden layers learns a feature map which allows the three classes to be linearly separable, or closed to be linearly separable. A large number of non-expert demonstrations allow us to accurately learn such a feature map which leads to more reliable discriminators and a better classification accuracy, as experimentally demonstrated in Section 5.

4.3 OCCUPANCY MEASURE MATCHING WITH MULTICLASS CLASSIFIER

Our remaining task is to learn the parametrized policy in the policy learning step. Here, we present our occupancy measure matching method where a divergence is approximated from discriminators. First, we show that the discriminator learning objective approximates a JS divergence among three occupancy measures. This actually also implies that a commonly used minimax formulation is unsuitable since we do not recover the expert policy even in an infinite capacity setting. To cope with this problem, we propose a modified objective which allows us to recover the expert policy.

Let E ps, aq, N ps, aq, and  ps, aq be occupancy measures of the expert, non-expert, and agent,

respectively. A JS divergence among the three occupancy measures with corresponding weights

´¯

1 2`

,

 2`

,

1 2`

is defined as

1 JSpE , N ,  q " 2 `  pgKLpE ||q¯q ` gKLpN ||q¯q ` gKLp ||q¯qq ,

(6)

where q¯ps, aq " pE ps, aq ` N ps, aq `  ps, aqq{p2 ` q and gKLp||q¯q is the generalized KL divergence. Note that Eq.(6) may be defined by either KL or gKL since the extra terms in gKL

cancel out. Then, under the assumption that the discriminators have infinite capacity, the optimal

parameter < which maximizes Lp, q satisfies

´¯ Lp<, q " p2 ` qJSpE , N ,  q ` log p2 ` q´p2`q .

(7)

The proof is given in Appendix A.1. This implies that minimizing Lp<, q is equivalent to min-
imizing JSpE , N ,  q up to a constant. This relation is similar to the one between binary classification and the JS divergence in GAIL, where a minimax formulation in Eq.(3) is used for

occupancy measure matching.

However, solving the minimax problem, min max Lp, q, is inappropriate in our method
since the JS divergence in Eq.(6) is minimized by  " pE ` N q{p1 ` q instead by  " E . This minimizer corresponds to a policy mixture, pa|sq " pEpa|sqE psq ` Npa|sqN psqq { pE psq ` N psqq, which has a positive probability depending on  to select non-expert actions instead of expert actions. Therefore, the above minimax
formulation does not lead to occupancy measure matching in our method.

Here, we propose to perform occupancy measure matching by directly minimizing an approxima-

tion of JSpE ,  q. we have JS0pE , N

We , 

can q"

approximate JSpE , 

this divergence based on the fact q. Since minimizing Lp<, q is

that with equivalent

 to

" 0, min-

immiinzinLg0JpS<p,qEw, ithN, "

q for 0. In

any , we can perform occupancy measure matching practice, we instead minimize an approximation of the

by solving divergence

based on an intermediate solution  since the discriminators do not have infinite capacity and find-

ing a global optimum < is not possible in practice. By ignoring constant terms in min L0p, q

and including a causal entropy regularization with   0, we obtain the following minimization

problem for learning :

min


E

rlog

Hps,

aqs

´

 Hp q.

(8)

This minimization problem is equivalent to solving a maximum entropy RL problem with reward

function rps, aq " ´ log Hps, aq. Eq.(8) also resembles the minimization problem of GAIL in Eq.(3). The main difference between them is that we learn discriminator Hps, aq by solving a multiclass classification problem, while GAIL learns discriminator Dps, aq by solving a binary classification problem. Due to this similarity, we name our method multiclass GAIL (M-GAIL).

The algorithmic summary of our method is given in Appendix B. Our method alternates between updating  for maximization using an estimated gradient of Lp, q with   0, and updating  for minimization using an estimated gradient of E rlog Hps, aqs ´ Hpq. The computation complexity of our method is similar to GAIL, with a small additional cost of computing gradients
of Gps, aq in the discriminator learning step.

6

Under review as a conference paper at ICLR 2019

4.4 RELATION TO MAXIMUM ENTROPY INVERSE REINFORCEMENT LEARNING

Recently, Finn et al. (2016a) and Fu et al. (2018) showed an interesting result that a log-likelihood objective for solving a binary classification problem in generative adversarial training is related to a maximum entropy IRL objective (Ziebart et al., 2008; 2010). Here, we show a similar result that the weighted log-likelihood objective for solving a multiclass classification problem is related to a maximum entropy IRL objective with two reward functions. However, this relation is not equivalence and there is small discrepancy between the two objectives.

We consider the goal of learning the expert reward function rEps, aq and the non-expert reward function rNps, aq by parameterized functions fps, aq, and gps, aq, respectively. rNps, aq is a reward function whose N is optimal4. The parameter  of the two reward functions may be learned independently using maximum entropy IRL. However, we are interested in jointly learn both reward
functions by maximizing the following objective:

Mpq " EpEp q rlog pf p qs ` EpNp q rlog pgp qs .

(9)

The first and second terms are the maximum entropy IRL objectives for learning the ex-

pert reward and the non-expert reward, respectively. The weight parameter   0 con-

trols the influence of the second objective. The trajectory density pf p q is defined as

pf p q

9

p0

ps0

q

T
t"0

ppst`1|st,

at

q

exppfpst,

at

qq

and

pgp q

is

also

defined

similarly.

The

fol-

lowing proposition shows that Lp, q is a surrogate of Mpq.

Proposition 1. Under the assumption that hps, aq " pa|sq for fixed , the gradient of Lp, q is a biased gradient of Mpq. The bias vanishes when  " 0 and  is an optimal maximum
entropy policy for reward function fps, aq.

The proof is based on the proof by Fu et al. (2018) for their adversarial IRL (AIRL) method, but with additional terms from non-expert demonstrations in our case. The detailed proof is given in Appendix A.2 and a proof sketch is as follow. We show that the gradient of Mpq with importance weights which depend on  has the same form as the gradient of Lp, q with incorrect importance weights. Then, we show that the importance weights of the latter are corrected only when  " 0 and the policy is optimal for reward fps, aq under the maximum entropy RL framework.
This proposition indicates that local maxima of Mpq are approximated by local maxima of Lp, q, and that approximation errors decrease as  decreases to zero. Due to approximation errors, this result is less attractive than the occupancy measure matching method which we proposed. However, it still suggests that we may use fps, aq and gps, aq as approximations of the reward functions which could be useful in the IRLF framework of Shiarlis et al. (2016) when the non-expert demonstrations are clearly failure demonstrations.

5 EXPERIMENTS
We compare M-GAIL against GAIL on four continuous control tasks from OpenAI gym (Brockman et al., 2016) with the PyBullet physics simulator (Coumans & Bai, 2018). These tasks are accompanied with true reward functions which are used for evaluation. We consider an episodic setting where a trajectory consists of 1000 transition samples. The discriminator and policy are neural networks with the same architecture used by GAIL (Ho & Ermon, 2016). We use Adam (Kingma & Ba, 2014) and TRPO (Schulman et al., 2016) to optimize the discriminator and policy, respectively. More details such as hyper-parameter settings are provided in Appendix C.
Setup: To collect expert demonstrations, we train a TRPO agent with the true reward function until convergence. Then, we treat a policy at the final iteration as expert and use it to collect two expert datasets with N " 1000 (i.e., 1 trajectory) and N " 10000. To collect non-expert demonstrations, we choose two intermediate policies which achieve approximately 50% and 70% performance of expert as non-expert. The 0% performance policy corresponds to initial random policy. Then, we use these non-expert policies to collect two non-expert datasets denoted by DN50% and DN70% with M " 100000. We train IL agents for 5 trials with different random seeds using fixed datasets. In each update iteration, the agents collect transition samples of size K " 2000 for learning.
4The optimal reward function always exists since for any trajectory there is at least one reward function that makes the trajectory optimal (Ng & Russell, 2000).

7

Under review as a conference paper at ICLR 2019

Average return Average return Average return

1e3

2

1

0 Expert

1

GAIL M-GAIL ( 7N0%, =0.1)

0 1000Up2d0a0t0e ite30ra0t0ion4000 5000

(a) HalfCheetah

1e3 2.0 1.5 1.0 0.5 0.0
0 1000Up2d0a0t0e ite30ra00tion4000 5000
(b) Ant

1e3 2.0

1.5

1.0

0.5 0

1000Up2d0a0t0e ite30ra00tion4000 5000

(c) Hopper

Figure 1: Returns averaged over 5 trials for N " 1000. The shaded area indicates standard error.

Table 1: The mean and standard error of total returns over 5 trials for N " 1000 (higher is better). The best and comparable methods according to paired t-test with p-value  0.01 are in boldface.

Tasks
Cheetah Ant Hopper Walker

GAIL
1621.6p22.6q 1158.0p15.2q 1744.1p8.8q 1036.9p5.6q

M-GAIL with DN50%

 " 0.1

 " 0.5

1644.1p21.3q 1725.0p21.2q

1180.3p14.6q 1342.8p15.1q

1702.3p9.9q 1821.4p8.3q

1056.4p5.3q 1046.9p5.3q

M-GAIL with DN70%

 " 0.1

 " 0.5

1788.2p20.7q 1787.2p19.7q

1351.5p14.9q 1281.1p15.5q

1834.8p7.9q 1844.0p7.4q

1067.7p5.3q 1069.6p5.2q

Results: We consider M-GAIL with  P t0.1, 0.5u. Three representative learning curves of GAIL and M-GAIL with N " 1000 are shown in Figure 1. The learning curves of four tasks with N " 1000 and N " 10000 are presented in Appendix D. This result shows that the proposed training procedure of M-GAIL indeed learns and converges to the expert policy without diverging. It also shows that M-GAIL learns faster than GAIL and requires collecting less agent's trajectories. While GAIL eventually learns the expert policy after collecting enough trajectories, it has quite unstable learning performance across trials as seen by large standard errors. We conjecture that a large number of non-expert demonstrations makes discriminator learning more stable and less prone to overfitting. On the other hand, discriminator learning in GAIL overfits to poor local optima in each trial, and it requires a sufficient amount of agent's trajectories to escape from these optima.
We also consider the total returns, computed as an area under learning curves divided by 5000, as an evaluation metric for comparing the overall performance given the same number of update iteration. Table 1 presents the total returns for N " 1000. We can see that M-GAIL with DN70% achieves higher total returns than GAIL on all tasks, with  " 0.1 performs the best overall. M-GAIL with DN50% also tends to outperform GAIL, and it performs slightly worse than M-GAIL with DN70%. This is consistent with our assumption that as N becomes more related to E, we can learn a better shared representation using non-expert demonstrations which leads to more accurate discriminator and faster learning. For M-GAIL with DN70% and N " 1000, the two values of  achieve similar performance, and this this shows that M-GAIL is quite robust against different values  for this setting. However, the choice of  becomes more crucial for N " 10000 as shown in Appendix D. Nonetheless, M-GAIL still achieves higher total returns than GAIL overall.
6 CONCLUSION
We presented M-GAIL, a method that improves GAIL by using non-expert demonstration as an extra class in discriminator learning. Compared to related methods that use an additional dataset for IL, M-GAIL relies on a less restrictive assumption on the dataset and can efficiently train deep neural networks. Our experiments showed that M-GAIL effectively uses non-expert demonstrations to achieve faster and more stable learning than GAIL.
We developed M-GAIL based on JS-divergence. However, recent research suggests that other distances such as Wasserstein distance are more suitable when learning from image data (Arjovsky et al., 2017; Li et al., 2017b). Extending our method to use such distances is an important future work for applying our idea to solve image-based tasks such as Atari games (Mnih et al., 2015).
8

Under review as a conference paper at ICLR 2019
REFERENCES
Pieter Abbeel and Andrew Y. Ng. Apprenticeship learning via inverse reinforcement learning. In Proceedings of the 21st International Conference on Machine Learning, 2004. doi: 10.1145/ 1015330.1015430.
Eitan Altman. Constrained Markov Decision Processes. Chapman and Hall, 1999.
Mart´in Arjovsky, Soumith Chintala, and Le´on Bottou. Wasserstein Generative Adversarial Networks. In Proceedings of the 34th International Conference on Machine Learning, pp. 214­223, 2017.
Julien Audiffren, Michal Valko, Alessandro Lazaric, and Mohammad Ghavamzadeh. Maximum entropy semi-supervised inverse reinforcement learning. In Proceedings of the Twenty-Fourth International Joint Conference on Artificial Intelligence, IJCAI, pp. 3315­3321, 2015.
Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba. OpenAI Gym. CoRR, abs/1606.01540, 2016.
Olivier Chapelle, Bernhard Schlkopf, and Alexander Zien. Semi-Supervised Learning. The MIT Press, 1st edition, 2010. ISBN 0262514125, 9780262514125.
Erwin Coumans and Yunfei Bai. Pybullet, a python module for physics simulation for games, robotics and machine learning. http://pybullet.org, 2018.
Onur Dikmen, Zhirong Yang, and Erkki Oja. Learning the information divergence. IEEE Transactions on Pattern Analysis and Machine Intelligence, 37(7):1442­1454, 2015.
Yan Duan, Marcin Andrychowicz, Bradly C. Stadie, Jonathan Ho, Jonas Schneider, Ilya Sutskever, Pieter Abbeel, and Wojciech Zaremba. One-shot imitation learning. In Advances in Neural Information Processing Systems 30, pp. 1087­1098, 2017.
Chelsea Finn, Paul F. Christiano, Pieter Abbeel, and Sergey Levine. A connection between generative adversarial networks, inverse reinforcement learning, and energy-based models. CoRR, 2016a.
Chelsea Finn, Sergey Levine, and Pieter Abbeel. Guided cost learning: Deep inverse optimal control via policy optimization. In Proceedings of the 33nd International Conference on Machine Learning, pp. 49­58, 2016b.
Justin Fu, Katie Luo, and Sergey Levine. Learning robust rewards with adversarial inverse reinforcement learning. In International Conference on Learning Representation, 2018.
Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep Learning. MIT Press, 2016.
Tuomas Haarnoja, Haoran Tang, Pieter Abbeel, and Sergey Levine. Reinforcement Learning with Deep Energy-Based Policies. In Doina Precup and Yee Whye Teh (eds.), Proceedings of the 34th International Conference on Machine Learning, 2017.
Jonathan Ho and Stefano Ermon. Generative Adversarial Imitation Learning. In Advances in Neural Information Processing Systems 29, pp. 4565­4573, 2016.
Diederik P. Kingma and Jimmy Ba. Adam: A Method for Stochastic Optimization. CoRR, abs/1412.6980, 2014.
Chongxuan Li, Taufik Xu, Jun Zhu, and Bo Zhang. Triple generative adversarial nets. In Advances in Neural Information Processing Systems 30, pp. 4091­4101, 2017a.
Yunzhu Li, Jiaming Song, and Stefano Ermon. Infogail: Interpretable imitation learning from visual demonstrations. In Advances in Neural Information Processing Systems 30, pp. 3815­3825, 2017b.
Ming-Yu Liu and Oncel Tuzel. Coupled generative adversarial networks. In Advances in Neural Information Processing Systems 29, pp. 469­477, 2016.
9

Under review as a conference paper at ICLR 2019
Lars M. Mescheder, Andreas Geiger, and Sebastian Nowozin. Which training methods for gans do actually converge? In Proceedings of the 35th International Conference on Machine Learning, pp. 3478­3487, 2018.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Bellemare, Alex Graves, Martin Riedmiller, Andreas K. Fidjeland, Georg Ostrovski, Stig Petersen, Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wierstra, Shane Legg, and Demis Hassabis. Human-Level Control Through Deep Reinforcement Learning. Nature, 518(7540):529­533, February 2015. ISSN 00280836.
Andrew Y. Ng and Stuart J. Russell. Algorithms for Inverse Reinforcement Learning. In Proceedings of the 17th International Conference on Machine Learning, pp. 663­670, 2000.
Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in pytorch. In NIPS-W, 2017.
Dean Pomerleau. ALVINN: an autonomous land vehicle in a neural network. In Advances in Neural Information Processing Systems 1, pp. 305­313, 1988.
Martin L. Puterman. Markov Decision Processes: Discrete Stochastic Dynamic Programming. John Wiley & Sons, Inc., New York, NY, USA, 1st edition, 1994. ISBN 0-471-61977-9.
Marc'Aurelio Ranzato and Martin Szummer. Semi-supervised learning of compact document representations with deep networks. In Proceedings of the Twenty-Fifth International Conference in Machine Learning, pp. 792­799, 2008.
Ste´phane Ross, Geoffrey J. Gordon, and Drew Bagnell. A reduction of imitation learning and structured prediction to no-regret online learning. In Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics, pp. 627­635, 2011.
Stuart J. Russell. Learning agents for uncertain environments (extended abstract). In Proceedings of the Eleventh Annual Conference on Computational Learning Theory, pp. 101­103, 1998.
Tim Salimans, Ian J. Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training gans. In Advances in Neural Information Processing Systems 29, pp. 2226­2234, 2016.
Stefan Schaal. Is imitation learning the route to humanoid robots? Trends in Cognitive Sciences, 3 (6):233­242, 1999.
John Schulman, Sergey Levine, Philipp Moritz, Michael Jordan, and Pieter Abbeel. Trust Region Policy Optimization. In Proceedings of the 32nd International Conference on Machine Learning, 2015.
John Schulman, Philipp Moritz, Sergey Levine, Michael I. Jordan, and Pieter Abbeel. HighDimensional Continuous Control Using Generalized Advantage Estimation. In International Conference on Learning Representation, 2016.
Kyriacos Shiarlis, Joa~o V. Messias, and Shimon Whiteson. Inverse reinforcement learning from failure. In Proceedings of the 2016 International Conference on Autonomous Agents & Multiagent Systems, pp. 1060­1068, 2016.
David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez, Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, Yutian Chen, Timothy Lillicrap, Fan Hui, Laurent Sifre, George van den Driessche, Thore Graepel, and Demis Hassabis. Mastering the Game of Go Without Human Knowledge. Nature, 550(7676):354­359, October 2017. ISSN 00280836.
Bradly C. Stadie, Pieter Abbeel, and Ilya Sutskever. Third person imitation learning. In International Conference on Learning Representation, 2017.
Richard S. Sutton and Andrew G. Barto. Reinforcement Learning - an Introduction. Adaptive computation and machine learning. MIT Press, 1998.
10

Under review as a conference paper at ICLR 2019
Umar Syed, Michael H. Bowling, and Robert E. Schapire. Apprenticeship learning using linear programming. In Proceedings of the 25th International Conference on Machine Learning, pp. 1032­1039, 2008. doi: 10.1145/1390156.1390286.
Michal Valko, Mohammad Ghavamzadeh, and Alessandro Lazaric. Semi-Supervised Apprenticeship Learning. In Proceedings of the 10th European Workshop on Reinforcement Learning, pp. 131­142, 2012.
Vladimir N. Vapnik. Statistical Learning Theory. Wiley-Interscience, 1998. Vladimir N. Vapnik. Estimation of Dependences Based on Empirical Data. Springer-Verlag, 2006. Jason Weston, Fre´de´ric Ratle, Hossein Mobahi, and Ronan Collobert. Deep learning via semi-
supervised embedding. In Neural Networks: Tricks of the Trade - Second Edition, pp. 639­655. 2012. Ronald J. Williams. Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning. Machine Learning, 8(3):229­256, 1992. doi: 10.1007/BF00992696. Markus Wulfmeier, Peter Ondruska, and Ingmar Posner. Maximum Entropy Deep Inverse Reinforcement Learning. CoRR, 2015. Xiang Zhang and Yann LeCun. Universum prescription: Regularization using unlabeled data. In Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence, pp. 2907­2913, 2017. Brian D. Ziebart, Andrew L. Maas, J. Andrew Bagnell, and Anind K. Dey. Maximum entropy inverse reinforcement learning. In Proceedings of the Twenty-Third AAAI Conference on Artificial Intelligence, pp. 1433­1438, 2008. Brian D. Ziebart, J. Andrew Bagnell, and Anind K. Dey. Modeling Interaction via the Principle of Maximum Causal Entropy. In Proceedings of the 27th International Conference on Machine Learning, 2010.
11

Under review as a conference paper at ICLR 2019

A PROOFS

A.1 MAXIMUM OF WEIGHT LOG-LIKELIHOOD OBJECTIVE APPROXIMATES JENSEN-SHANNON DIVERGENCE

If we assume that the functions F, G, and H are non-parametric and have infinite capacity, we can rewrite the multiclass classification objective as

LpF, G, Hq " EE rlog F ps, aqs ` EN rlog Gps, aqs ` E rlog Hps, aqs .

(10)

Recall that E rrps, aqs "  ps, aqrps, aqdsda where  is the occupancy measure of . This objective is a concave function and we can find its global maximizer under constraints F ps, aq  0, Gps, aq  0, Hps, aq  0 and F ps, aq ` Gps, aq ` Hps, aq " 1, @s@a, by using the method of Lagrange multipliers. The global maximizer corresponds to

F <ps, aq

"

E ps, aq , qps, aq

G<ps, aq

"

N ps, aq , qps, aq

H<ps, aq

"

 ps, aq , qps, aq

(11)

where qps, aq

"

E ps, aq ` N ps, aq `  ps, aq.

Let q¯ps, aq

"

qps,aq 2`

,

by

substituting

these

quantities back to L, we obtain the maximum objective value as

LpF

<,

G<,

H<q

"

EE

,, log

E ps, aq  qps, aq

`

EN

,, log

N ps, aq qps, aq



`

E

,, log

 ps, aq qps, aq



"

EE

,, log

E ps, aq  q¯ps, aq

`

EN

,, log

N ps, aq q¯ps, aq



`

E

,, log

 ps, aq q¯ps, aq



`  log  ´ p2 ` q logp2 ` q.

(12)

Notice that the first, second, and third terms look similar to Kullback-Leibler (KL) divergences

which

are

defined

as

KLpp1||p2q

"



p1ps,

aq

log

p1 p1

ps,aq ps,aq

dsda,

for

probability

densities

p1

and

p2. However, the occupancy measures are unnormalized densities and theoretically we cannot use

KL divergences to rewrite Eq.(12). For unnormalized densities ps, aq and q¯ps, aq, we consider a

generalized KL divergence defined as

gKLp||q¯q

"



ps, aq log

ps, q¯ps,

aq aq

dsda

´



 ps, aqdsda `

q¯ps, aqdsda



" KLp||q¯q ´ ps, aqdsda ` q¯ps, aqdsda.

(13)

We can show that for Eq.(12), gKL and KL are equivalent since the extra terms cancel out:

gKLpE ||q¯q`gKLpN ||q¯q ` gKLp ||q¯q " KLpE ||q¯q ` KLpN ||q¯q ` KLp ||q¯q  ´ pE ps, aq ` N ps, aq `  ps, aqq dsda ` p2 ` q q¯ps, aqdsda

" KLpE ||q¯q ` KLpN ||q¯q ` KLp ||q¯q 
´ pE ps, aq ` N ps, aq `  ps, aqq dsda ` qps, aqdsda

" KLpE ||q¯q ` KLpN ||q¯q ` KLp ||q¯q. Then, the optimal value in Eq.(12) can be rewritten in terms of gKL divergences as

(14)

LpF <, G<, H<q " gKLpE ||q¯q ` gKLpN ||q¯q ` gKLp ||q¯q `  log  ´ p2 ` q logp2 ` q

(15)

Next, we consider a JS divergence among E , N , and  with corresponding weights 1{p2 ` q, {p2 ` q, and 1{p2 ` q, which can be defined based on (generalized) KL divergence as

1 JSpE , N ,  q " 2 `  pgKLpE ||q¯q ` gKLpN ||q¯q ` gKLp ||q¯qq .

(16)

12

Under review as a conference paper at ICLR 2019

Note that we use the fact that the sum of gKL and the sum of KL are equivalent to define the

JS divergence. Alternatively, we may use a definition of JS in terms of a Shannon entropy,

´¯

JSpE , N ,  q

"

´

1 2`

pHpE q ` HpN q ` Hp qq

`

H

1 2`

pE

`

N

`

 q

,

where Hpq " ´  ps, aq log ps, aqdsda, to obtain the same definition of JS divergence.

By comparing Eq.(15) and Eq.(16), we have that the maximum value of L corresponds to JS with a positive scaling and a constant shift:

´¯ LpF <, G<, H<q " p2 ` qJSpE , N ,  q ` log p2 ` q´p2`q .

(17)

Notice that JSpE , N ,  q is a convex function of  . By setting its derivative to zero, we have

that

the

minimum

of

JSpE , N ,  q

is

obtained

when



"

E `N 1`

.

Then,

the

correspond-

ing mixture policy can be obtained by using the one-to-one correspondence between occupancy

measure

and

policy

given

as

pa|sq

"

 ps,aq  psq

.

A.2 PROPOSITION 1

Here, we give the proof of Proposition 1. We follows the proof for AIRL by Fu et al. (2018), but with additional terms from non-expert demonstrations in our case. That is, we show that the gradient of a maximum entropy IRL objective proposed by Finn et al. (2016b) with specific importance weights only differs from the gradient of the discriminator learning objective in the importance weights. Similar to the case of AIRL, we only consider the case of non-discounted, finite horizon MDP.

Let rEps, aq be the expert reward function whose E is the optimal policy. Similarly, let rNps, aq be the non-expert reward function whose N is the optimal policy. The maximum entropy IRL objective for estimating the two rewards by f and g is

Mpq " EpEp q rlog pf p qs ` EpNp q rlog pgp qs ,

(18)

where

pf p q

9

p0ps0q

T
t"0

ppst`1

|st

,

atq

exppfpst

,

atqq

and

pgp q

is

defined

similarly

by

gps, aq. pEp q and pNp q denotes (unknown) trajectory densities determined by expert and non-

expert policies, respectively. The gradient of this objective w.r.t.  is

«T ff  T

ÿ

Mpq " EpEp q

fpst, atq ´  log p0ps0q ppst`1|st, atq exppfpst, atqqd

t"0 t"0

«T ff  T

ÿ

` EpNp q

gpst, atq ´  log p0ps0q ppst`1|st, atq exppgpst, atqqd

t"0 t"0

«T ff «T ff

ÿÿ

" EpEp q

fpst, atq ´ Epf p q

fpst, atq

t"0 t"0

«T ff «T ff

ÿÿ

` EpNp q

gpst, atq ´ Epgp q

gpst, atq .

(19)

t"0 t"0

Let ptEps, aq "  pEp qd t1t be a state-action marginal density at time t obtained by marginalizing pEp q over states and actions at all time-step except at t. The state-action marginal densities pNt ps, aq, ptf ps, aq, and ptgps, aq are defined similarly. We can rewrite the expectations over trajectories as a sum of expectations over state-action marginals as follows:

T

ÿ! "

i"

i

Mpq "

EptEps,aq fps, aq ´ Epft ps,aq fps, aq

t"0

"i

" i)

` EpNt ps,aq gps, aq ´ Eptgps,aq gps, aq .

(20)

Ltreibtutµiotnps,wahqere"pt

pEt ps,

ps, aq

aq is

` a

ptNps, aq ` pt ps, aq state-action marginal

be of

a a

(unnormalized) sampling trajectory density pp q

dis"

13

Under review as a conference paper at ICLR 2019

p0ps0q

T
t"0

expectations

ppst`1|st, over ptf ps,

atqpat|stq. By rearranging terms and using aq and pgt ps, aq, we can rewrite the gradient as

importance

weights

for

the

T

ÿ!

"

i

"i

Mpq "

EptEps,aq fps, aq ` EpNt ps,aq gps, aq

t"0

´

Eµt

ps,aq

"

pft ps, µt ps,

aq aq



f

ps,

aq

`



pgt ps, µt ps,

aq aq



g

ps,

i) aq .

(21)

Next, we show that Lp, q has the same form as Mpq but with incorrect importance weights. First, we rewrite Lp, q using the state-action marginal densities as

Lp, q " EE rlog Fps, aqs ` EN rlog Gps, aqs ` E rlog Hps, aqs

«T ff «T ff «T ff

ÿ ÿÿ

" EpEp q

log Fpst, atq ` EpNp q

log Gpst, atq ` Epp q

log Hpst, atq

t"0 t"0 t"0

T

ÿ!

)

" EpEt ps,aq rlog Fps, aqs ` EptNps,aq rlog Gps, aqs ` Ept ps,aq rlog Hps, aqs ,

t"0

(22)

where in the second line we use the definition of the expected return for non-discounted finite-
horizon case. Replacing F, G, and H by their parameterization in Eq.(4) with hps, aq " log pa|sq gives

T

ÿ!

Lp, q "

EpEt ps,aq rfps, aqs ` EptNps,aq rgps, aq ` log s ` Eptps,aq rlog pa|sqs

t"0

) ´ EpEt ps,aq`pNt ps,aq`ptps,aq rlog pexppfps, aqq `  exppgps, aqq ` pa|sqqs

T
ÿ! " EptEps,aq rfps, aqs ` EpNt ps,aq rgps, aq ` log s ` Ept ps,aq rlog pa|sqs
t"0

)

´ Eµt ps,aq rlog pexppfps, aqq `  exppgps, aqq ` pa|sqqs ,

(23)

where we use logp exppgps, aqqq " gps, aq ` log  in the first equality and use µt ps, aq defined above in the second equality. Then, the gradient of Lp, q w.r.t.  is given by

T

ÿ!

Lp, q "

EpEt ps,aq rfps, aqs ` EptNps,aq rgps, aqs

t"0

) ´ Eµt ps,aq r log pexppfps, aqq `  exppgps, aqq ` pa|sqqs

T
ÿ! " EptEps,aq rfps, aqs ` EptNps,aq rgps, aqs
t"0

´

Eµt

ps,aq

,,

exppfps, aqq Zps, aq



f

ps,

aq

`



exppgps, aqq Zps, aq



g

ps,

 aq

) ,

(24)

where Zps, aq " exppfps, aqq `  exppgps, aqq ` pa|sq. Let pt psq "  pt ps, aqda be a state marginal of the policy trajectory density. By multiplying both the nominators and denominators of
the third and fourth terms by ptpsq, we obtain

T

ÿ!

Lp, q "

EptEps,aq rfps, aqs ` EptNps,aq rgps, aqs

t"0

´

Eµt

ps,aq

,,

pt

psq exppfps, aqq ptpsqZps, aq



f

ps,

aq

`



pt

psq exppgps, aqq pt psqZps, aq



g

ps,

 aq

) ,

(25)

14

Under review as a conference paper at ICLR 2019
By comparing Eq.(21) and Eq.(25), we can see that Lp, q and Mpq have the same form and they only differs in the importance weights in the third and fourth terms. The importance weights used in Mpq are correct and performing gradient ascent with Mpq leads to a local maxima of Mpq (assuming exact computation of the expectations). On the other hand, the importance weights used in Lp, q lead to bias in terms of the objective Mpq, and performing gradient ascent with Lp, q does not lead to a local maxima of Mpq. Thus, the global maximizer of Lp, q does not recover the true reward functions regardless of .
The bias can be corrected only when  " 0 and  is the optimal maximum entropy policy of a reward fps, aq. This is because when  " 0, M0pq is the maximum entropy IRL objective for learning the expert reward function and L0p, q us the binary classification objective of GAIL and AIRL. As shown by Fu et al. (2018), the gradients of these two objectives are equivalent under the assumption that  is the optimal maximum entropy policy for reward rps, aq " fps, aq.
We emphasize that for our case it is insufficient to only assume that  is the optimal maximum entropy policy without assuming that  " 0. This is because to make the nominators equal, we need that the two equalities, ptpsq exppfps, aqq " ptf ps, aq and ptpsq exppgps, aqq " pgt ps, aq, hold. The former holds when  is (maximum entropy) optimal for a reward fps, aq, while the latter holds when  is (maximum entropy) optimal for a reward gps, aq. Both cannot hold at the same time unless fps, aq " gps, aq with a positive scaling , which implies non-expert is expert himself. Moreover, to have that the partition functions are equivalent, pt psqZps, aq " µt ps, aq, we also need the policy  to be optimal for a reward mixture. These contradictions imply that the two gradients can be made identical only when  " 0.
B PSEUDOCODE
Algorithm 1 Multiclass Generative Adversarial Imitation Learning (M-GAIL)
1: Input: Demonstration datasets DE and DN, initial parameters  and , weight parameter . 2: while not converge do 3: Collect trajectory samples tpsk, akquKk"1 using pa|sq. 4: Sample mini-batches data from DE and DN. 5: Update  to maximize Lp, q in Eq.(5), by e.g., Adam (Kingma & Ba, 2014). 6: Update  to solve the RL problem in Eq.(8), by e.g., TRPO (Schulman et al., 2015). 7: end while 8: Output: Parameterized policy .
C MORE EXPERIMENT DETAILS
We implement GAIL and M-GAIL with PyTorch deep learning framework (Paszke et al., 2017) (Our code will be publicly available.). Both GAIL and M-GAIL use a common setting as follows. The discriminator and policy are represented by neural networks with two hidden layers and 100 hyperbolic tangent units in each layer, as proposed by the original GAIL paper (Ho & Ermon, 2016). We use a Gaussian policy with state-independent and diagonal covariance for all tasks. The network parameters are initialized randomly. The discriminator is optimized by Adam (Kingma & Ba, 2014) with step-size 3 ^ 10´4, 1 " 0.9, and 2 " 0.999. In each update iteration, we sample mini-batch of size 64 from DE and DN to update the discriminator.
We tried to make the implementation of GAIL and M-GAIL as close as possible. In particular, we let fps, aq " 0 be a constant function in M-GAIL (This makes exppfps, aqq " 1). That is, the discriminator networks give two outputs; g and h. This choice is valid since using three estimators to estimate three class posteriors is over-parameterize and we only need two estimators to compute an estimate of the third class posterior. With this implementation choice, M-GAIL with  " 0 is exactly the same as GAIL where h in M-GAIL is the same as d in GAIL.
The policy is optimized by trust-region policy optimization (TRPO) with generalized advantage estimation (GAE) (Schulman et al., 2016) with KL bound " 0.01 and damping coefficient for the Fisher information matrix 0.1. We do not add causal entropy and set  " 0 for GAIL and MGAIL. For GAE, we use the above network architecture to learn the value function with  " 0.995
15

Under review as a conference paper at ICLR 2019

Return

1e3
2

1

0 TRPO

Random Return: -1589.78 Iter. 0

1

50 % Return: 351.08 Iter. 700 70 % Return: 742.83 Iter. 800

Expert Return: 2145.46 Iter. 10000

0 2000 4U0p0d0ate iter6a0ti0o0n 8000 10000

(a) HalfCheetah

Return

2.25 1e3 2.00 1.75 1.50 1.25 1.00 0.75 0.50
0

TRPO Random 50 % 70 % Expert

Return: 394.13 Iter. 0 Return: 1275.55 Iter. 2400 Return: 1628.29 Iter. 2900 Return: 2155.87 Iter. 10000

2000 4U0p0d0ate iter6a0ti0o0n 8000 10000

(b) Ant

Return

1e3
2.0
1.5
1.0
0.5
0.0 0

TRPO Random 50 % 70 % Expert

Return: 13.15 Iter. 0 Return: 984.22 Iter. 600 Return: 1454.17 Iter. 3700 Return: 2062.65 Iter. 10000

2000 4U0p0d0ate iter6a0ti0o0n 8000 10000

(c) Hopper

Return

1e3
1.2 1.0 0.8 0.6 0.4 0.2 0.0
0

TRPO Random 50 % 70 % Expert

Return: 10.44 Iter. 0 Return: 655.11 Iter. 300 Return: 894.43 Iter. 1400 Return: 1273.56 Iter. 10000

2000 4U0p0d0ate iter6a0ti0o0n 8000 10000

(d) Walker2D

Figure 2: The return in each update iteration of TRPO expert agent. The return is computed using Gaussian policy with exploration noise. "Random" is the initial random policy. "Iter." denotes the update iteration to achieve the corresponding return values.

and GAE " 0.97. In each update iteration, we use Adam, with step-size 3 ^ 10´4, 1 " 0.9, 2 " 0.999, and 2 regularizer of 10´3, to update the value function network for 3 epochs with minibatch size 128. The GAE value is standardized to have zero mean and unit variance for TRPO update. In each update iteration, we collect transition samples of size K " 2000 using the current policy. This hyper-parameter setting is chosen for TRPO since it works well for learning the expert policy, as described below.
We consider four tasks; Half-Cheetah, Hopper, Walker2D, and Ant, simulated by PyBullet physics simulator (Coumans & Bai, 2018). (At the time of submission, OpenAI gym with Pybullet physics does not have the Swimmer task, and we could not successfully train a Humanoid expert agent. The two inverted-pendulum tasks are too simple with all methods perform equally well.) To train the expert policy, we use TRPO with GAE with the above setting, except for Walker2D where we require an entropy regularizer of  " 0.0001 to learn a walking policy. Without this regularizer, the agent simply stands still to receive a reward of 1 in each time-step. The same scenario does not happen when we train the IL agents and we do not include the entropy regularizer for GAIL and M-GAIL. For each task, we train a TRPO agent for 5 trials with different random seeds and choose the best policy among 5 trials that gives the highest return at the 10000-th iteration as expert. The return of the chosen TRPO agent, as well as the returns at approximately 50% and 70% of the expert's return are shown in Figure 2. We can clearly see that non-expert policies can be learned using much less samples than the expert policy.
After obtaining the expert and non-expert policies, we use the learned Gaussian policies to collect demonstration trajectories with exploration noise. The same datasets are used in all 5 trials of GAIL and M-GAIL training.
16

Under review as a conference paper at ICLR 2019

Table 2: The mean and standard error of total returns over 5 trials for N " 1000 and N " 10000. The best and comparable methods according to paired t-test with p-value  0.01 are in boldface.

Tasks (N )
Cheetah (1000) Ant (1000) Hopper (1000) Walker (1000)
Cheetah (10000) Ant (10000) Hopper (10000) Walker (10000)

GAIL
1621.6p22.6q 1158.0p15.2q 1744.1p8.8q 1036.9p5.6q
1502.0p21.3q 1240.6p15.3q 1679.3p8.9q 1031.7p5.1q

M-GAIL with DN50%

 " 0.1

 " 0.5

1644.1p21.3q 1180.3p14.6q 1702.3p9.9q 1056.4p5.3q

1725.0p21.2q 1342.8p15.1q
1821.4p8.3q 1046.9p5.3q

1444.0p21.0q 1295.2p15.7q 1786.2p8.5q 992.6p5.5q

1477.4p20.5q 1308.0p15.2q 1775.8p8.1q 1041.6p5.2q

M-GAIL with DN70%

 " 0.1

 " 0.5

1788.2p20.7q 1351.5p14.9q 1834.8p7.9q 1067.7p5.3q

1787.2p19.7q 1281.1p15.5q 1844.0p7.4q 1069.6p5.2q

1503.7p20.6q 1194.6p14.4q 1824.3p8.0q 1038.0p5.3q

1391.4p21.5q 1412.0p15.4q 1849.8p7.9q 1060.8p5.0q

D MORE EXPERIMENTAL RESULTS
We train GAIL and M-GAIL agents for 5 trials with different random seeds. All trials use the same datasets collected by the TRPO agent as explained previously. We evaluate the learned policies by generating 10 test trajectories using the learned policies to select actions without exploration noise. Then, we compute the averaged return over 10 trajectories using the true reward function without discount factor, and this gives us a learning curve for each trial. The mean and standard error of these learning curves over 5 trials for N " 1000 and N " 10000 are shown in Figure 3. Table 2 shows the total return (area under learning curves divided by 5000) of GAIL and M-GAIL with N " 1000 and N " 10000. The comparison between GAIL and M-GAIL for N " 10000 is similar to that for N " 1000 in the main text, except that GAIL performs quite well on HalfCheetah and achieves comparable performance.
We can also see that M-GAIL with  " 0.5 performs better overall than M-GAIL with  " 0.1 for N " 10000. A possible explanation of this phenomena is that  can be regarded as a class prior of the non-expert class which determines the ratio of non-expert samples against samples from the other two classes. This ratio becomes close to one when the number of expert demonstrations increases while the number of non-expert demonstrations remains constant. This suggests that an appropriate value of the weight parameter should depend on the number of demonstrations. Developing a systematic procedure to select this tuning parameter is one of our future work.
Figure 4 shows the learning curves after we let the agents train for more update iteration. This result confirms that all methods converge to the expert policy without diverging. We only report the results with N " 1000 until the 5000-th update iteration of GAIL and the best performing M-GAIL in the main text for better clarity.
We can also see that for HalfCheetah, the total returns of GAIL and M-GAIL with N " 10000 is significantly worse than those with N " 1000. This is quite unintuitive since using more samples should yield faster learning. A possible explanation is that for HalfCheetah the expert policy may generate relatively poor trajectories which have poor performance, and the expert dataset with N " 10000 includes these trajectories which makes learning more difficult. Nonetheless, both GAIL and M-GAIL converge to the expert policy as seen in Figure 4b.

17

Under review as a conference paper at ICLR 2019

Average return

Average return

1e3
2.5 2.0 1.5 1.0 0.5 0.0 0.5 1.0 1.5
0

Expert GAIL M-GAIL ( N50%, =0.1) M-GAIL ( 5N0%, =0.5) M-GAIL ( 7N0%, =0.1) M-GAIL ( 7N0%, =0.5) 1000 U2p0d0a0te iter3a0ti0o0n 4000 5000

(a) HalfCheetah (N " 1000)
1e3

2.0

Average return

1e3
2.5 2.0 1.5 1.0 0.5 0.0 0.5 1.0 1.5 0

Expert GAIL M-GAIL ( N50%, =0.1) M-GAIL ( N50%, =0.5) M-GAIL ( N70%, =0.1) M-GAIL ( 7N0%, =0.5) 1000 U2p0d0a0te iter3a0ti0o0n 4000 5000

(b) HalfCheetah (N " 10000)
1e3

2.0

Average return

1.5 1.5

1.0 0.5 0.0
0

Expert GAIL M-GAIL ( 5N0%, =0.1) M-GAIL ( N50%, =0.5) M-GAIL ( 7N0%, =0.1) M-GAIL ( 7N0%, =0.5) 1000 U20p0d0ate iter3a0ti0o0n 4000 5000

1e3
2.00 1.75 1.50 1.25 1.00 0.75 0.50 0.25
0

(c) Ant (N " 1000)
Expert GAIL M-GAIL ( 5N0%, =0.1) M-GAIL ( N50%, =0.5) M-GAIL ( N70%, =0.1) M-GAIL ( 7N0%, =0.5) 1000 U20p0d0ate iter3a0ti0o0n 4000 5000

(e) Hopper (N " 1000)
1e3

1.2

1.0

0.8

0.6 0.4 0.2
0

Expert GAIL M-GAIL ( N50%, =0.1) M-GAIL ( N50%, =0.5) M-GAIL ( 7N0%, =0.1) M-GAIL ( 7N0%, =0.5) 1000 U20p0d0ate iter3a0ti0o0n 4000 5000

(g) Walker2D (N " 1000)

Average return

Average return

1.0 0.5 0.0
0

Expert GAIL M-GAIL ( 5N0%, =0.1) M-GAIL ( 5N0%, =0.5) M-GAIL ( 7N0%, =0.1) M-GAIL ( N70%, =0.5) 1000 U20p0d0ate iter3a0ti0o0n 4000 5000

1e3
2.00 1.75 1.50 1.25 1.00 0.75 0.50 0.25
0

(d) Ant (N " 10000)
Expert GAIL M-GAIL ( N50%, =0.1) M-GAIL ( N50%, =0.5) M-GAIL ( 7N0%, =0.1) M-GAIL ( N70%, =0.5) 1000 U20p0d0ate iter3a0ti0o0n 4000 5000

(f) Hopper (N " 10000)
1e3
1.2

1.0

0.8

0.6 0.4 0.2
0

Expert GAIL M-GAIL ( N50%, =0.1) M-GAIL ( N50%, =0.5) M-GAIL ( N70%, =0.1) M-GAIL ( 7N0%, =0.5) 1000 U20p0d0ate iter3a0ti0o0n 4000 5000

(h) Walker2D (N " 10000)

Figure 3: Returns averaged over 5 trials where the shaded area indicates standard error. The results for N " 1000 are in the left column and for N " 10000 in the right column. Returns are computed without exploration noise and discount factor.

18

Average return

Average return

Under review as a conference paper at ICLR 2019

Average return

Average return

1e3
2.5 2.0 1.5 1.0 0.5 0.0 0.5 1.0 1.5
0

Expert GAIL M-GAIL ( N50%, =0.1) M-GAIL ( N50%, =0.5) M-GAIL ( N70%, =0.1) M-GAIL ( 7N0%, =0.5) 2000 U4p0d0a0te iter6a0ti0o0n 8000 10000

(a) HalfCheetah (N " 1000)
1e3

2.0

Average return

1e3
2.5 2.0 1.5 1.0 0.5 0.0 0.5 1.0 1.5 0

Expert GAIL M-GAIL ( 5N0%, =0.1) M-GAIL ( N50%, =0.5) M-GAIL ( 7N0%, =0.1) M-GAIL ( N70%, =0.5) 2000 U4p0d0a0te iter6a0ti0o0n 8000 10000

(b) HalfCheetah (N " 10000)
1e3

2.0

Average return

1.5 1.5

1.0 0.5 0.0
0

Expert GAIL M-GAIL ( N50%, =0.1) M-GAIL ( N50%, =0.5) M-GAIL ( N70%, =0.1) M-GAIL ( 7N0%, =0.5) 2000 U4p0d0a0te iter6at0i0o0n 8000 10000

1e3
2.00 1.75 1.50 1.25 1.00 0.75 0.50 0.25
0

(c) Ant (N " 1000)
Expert GAIL M-GAIL ( 5N0%, =0.1) M-GAIL ( 5N0%, =0.5) M-GAIL ( N70%, =0.1) M-GAIL ( 7N0%, =0.5) 2000 U4p0d0a0te iter6at0i0o0n 8000 10000

(e) Hopper (N " 1000)
1e3

1.2

1.0

0.8 0.6 0.4 0.2
0

Expert GAIL M-GAIL ( N50%, =0.1) M-GAIL ( N50%, =0.5) M-GAIL ( N70%, =0.1) M-GAIL ( 7N0%, =0.5) 2000 U4p0d0a0te iter6at0i0o0n 8000 10000

(g) Walker2D (N " 1000)

Average return

Average return

1.0 0.5 0.0
0

Expert GAIL M-GAIL ( N50%, =0.1) M-GAIL ( 5N0%, =0.5) M-GAIL ( N70%, =0.1) M-GAIL ( N70%, =0.5) 2000 U4p0d0a0te iter6at0i0o0n 8000 10000

1e3
2.00 1.75 1.50 1.25 1.00 0.75 0.50 0.25
0

(d) Ant (N " 10000)
Expert GAIL M-GAIL ( 5N0%, =0.1) M-GAIL ( N50%, =0.5) M-GAIL ( N70%, =0.1) M-GAIL ( 7N0%, =0.5) 2000 U4p0d0a0te iter6at0i0o0n 8000 10000

(f) Hopper (N " 10000)
1e3

1.2

1.0

0.8 0.6 0.4 0.2
0

Expert GAIL M-GAIL ( 5N0%, =0.1) M-GAIL ( N50%, =0.5) M-GAIL ( N70%, =0.1) M-GAIL ( N70%, =0.5) 2000 U4p0d0a0te iter6at0i0o0n 8000 10000

(h) Walker2D (N " 10000)

Figure 4: The results in Figure 3 with a longer update iteration.

19

Average return

Average return


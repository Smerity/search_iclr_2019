Under review as a conference paper at ICLR 2019
INITIALIZED EQUILIBRIUM PROPAGATION FOR BACKPROP-FREE TRAINING
Anonymous authors Paper under double-blind review
ABSTRACT
Deep neural networks are almost universally trained with reverse-mode automatic differentiation (a.k.a. backpropagation). Biological networks, on the other hand, appear to lack any mechanism for sending gradients back to their input neurons, and thus cannot be learning in this way. In response to this, Scellier & Bengio (2017) proposed Equilibrium Propagation - a method for gradient-based training of neural networks which uses only local learning rules and, crucially, does not rely on neurons having a mechanism for back-propagating an error gradient. Equilibrium propagation, however, has a major practical limitation: inference involves doing an iterative optimization of neural activations to find a fixed-point, and the number of steps required to closely approximate this fixed point scales poorly with the depth of the network. In response to this problem, we propose Initialized Equilibrium Propagation, which trains a feedforward network to initialize the iterative inference procedure for Equilibrium propagation. This feed-forward network learns to approximate the state of the fixed-point using a local learning rule. After training, we can simply use this initializing network for inference, resulting in a learned feedforward network. Our experiments show that this network appears to work as well or better than the original version of Equilibrium propagation. This shows how we might go about training deep networks without using backpropagation.
1 INTRODUCTION
Deep neural networks are almost always trained with gradient descent, and gradients are almost always computed with backpropagation. For those interested in understanding the working of the brain in the context of machine learning, it is therefore distressing that biological neurons appear not to send signals backwards.
Biological neurons communicate by sending a sequence of pulses to downstream neurons along a one-way signaling pathway called an "axon". If neurons were doing backpropagation, one would expect a secondary signalling pathway wherein gradient signals travel backwards along axons. This appears not to exist, so it seems that biological neurons cannot be doing backpropagation.
Moreover, backpropagation may not be the ideal learning algorithm for efficient implementation in hardware, because it involves buffering activations for each layer until an error gradient returns. This requirement becomes especially onerous when we wish to backpropagate through many steps of time, or through many layers of depth. For these reasons, researchers are looking into other means of neural credit assignment - mechanisms for generating useful learning signals without doing backpropagation.
Recently, Scellier & Bengio (2017) proposed a novel algorithm called Equilibrium Propagation, which enables the computation of parameter gradients in a deep neural network without backpropagation. Equilibrium Propagation defines a neural network as a dynamical system, whose dynamics follow the negative-gradient of an energy function. The "prediction" of this network is the fixedpoint of the dynamics - the point at which the system settles to a local minimum energy given the input, and ceases to change. Because of this inference scheme, Equilibrium Propagation is impractically slow for large networks - the network has to iteratively converge to a fixed point at every training iteration.
1

Under review as a conference paper at ICLR 2019

In this work, we take inspiration from Hinton et al. (2015) and distill knowledge from a slow, energy based equilibrating network into a fast feedforward network by training the feedforward network to predict the fixed-points of the equilibrating network with a local loss. At the end of training, we can then discard the equilibrating network and simply use our feedforward network for testtime inference. We thus have a way to train a feedforward network without backpropagation. The resulting architecture loosely resembles a Conditional Generative Adversarial Network (Mirza & Osindero, 2014), where the feedforward network produces a network state which is evaluated by the energy-based equilibrating network.
To aid the reader, this paper contains a glossary of symbols in Appendix A.
1.1 EQUILIBRIUM PROPAGATION
Equilibrium Propagation (Scellier & Bengio, 2017) is a method for training a Continuous Hopfield Network Hopfield (1984) for classification. The network performs inference by iteratively converging to a fixed-point, conditioned on the input data, and taking the state of the output neurons at the fixed point to be the output of the network. The network's dynamics are defined by an energy function over neuron states s and parameters  = (w, b):

1 E(s, x) = 2

s2i -

bi(si) -

wij(si)(sj) -

xiwij (sj )

iS iS

jS,ij S

jS,ij I

(1)

Where I is the set of input neurons, S is the set non-input neurons; s  R|S| is the vector of neuron states;  = {j : j  S} denotes the network architecture, where j  {i : i  I  S, i = j, (j  ii  S)} is the set of neurons connected to neuron j with the no self-connections and symmetry
constraints; x denotes the input vector; and  is some nonlinearity; w is a weight matrix with a symmetric constraint: wij = wji1, and entries only defined for {wij : i  j} The state-dynamics
for non-input neurons, derived from Equation 1, are:



sj t

= - E(s, x) sj

= -sj +  (sj) bj +

wij (si) +

wijxi j  S

jS,ij S

jS,ij I

(2)

The network is trained using a two-phase procedure, with a negative and then a positive phase. In the negative phase, the network is allowed to settle to an energy minimum s- := arg mins E(s, x) conditioned on a minibatch of input data x. In the positive phase, a target y is introduced, and

the energy function is augmented to "perturb" the fixed-point of the state towards the target with a

"clamping

factor"

:

E (s,

x,

y)

=

E (s,

x)

+



 C (sO ,y) s

,

where



is

a

small

positive

number.

The

network is allowed to settle to the perturbed state s+ := arg mins(E(s, x, y)).

Finally, the parameters of the network are learned based on a contrastive loss between the negative-
phase and positive-phase energy, which can be shown to be proportional to the gradient of the output loss in the limit of   0:

 = -  E(s+, x) - E(s-, x)  - C(s-O, y)

 





(3)

Where  is some learning rate; O  S is the subset of output neurons. This results in a local learning rule, where parameter changes only depend on the activities of the pre- and post-synaptic neurons:

1However as described in Scellier et al. (2018), the symmetry requirement can be relaxed without significantly impacting performance. In this case there is no energy function, but one simply defines the network in terms of the state dynamics. The network nevertheless seems to learn to settle to fixed-points rather than falling into limit cycles or chaotic dynamics. The reason for this phenomenon is not well understood.

2

Under review as a conference paper at ICLR 2019

 wij = 
 bi = 

(s+i )(s+j ) - (si-)(sj-) (s+i ) - (s-i )

(4) (5)

Intuitively, the algorithm works by adjusting  to pull arg mins E(s, x) closer to arg mins E(s, x, y) so that the network will gradually learn to naturally minimize the output loss.
Because inference involves an iterative settling process, it is an undesirably slow process in Equilibrium propagation. In their experiments, Scellier & Bengio (2017) indicate that the number of settling steps required scales super-linearly with the number of layers. This points to an obvious need for a fast inference scheme.

2 METHOD
We propose training a feedforward network f(x)  sf  R|S| to predict the fixed-point of the equilibrating network. This allows the feedforward network to achieve two things: First, it initializes the state of the equilibrating network, so that the settling process starts in the right regime. Second, the feedforward network can be used to perform inference at test-time, since it learns to approximate the minimal-energy state of the equilibrating network, which corresponds to the prediction. f(x) is defined as follows:

f(x) := (fj (x) : j  S)

xj fj (x) := h

ifj ij fi (x)

+ cj

if j  I if j  S

 R|S| R

(6)

Where h is some nonlinearity; jf = (i : (i  j)  (i < j)) is the set of feedforward connections to neuron j (which is a subset of j - the full set of connections to neuron j from the equilibrium network from Equation 1); j := (j,j  R|jf |, cj  R) is the set of parameters connected to neuron j. This feedforward network produces the initial state of the negative phase of equilibrium
propagation network, given the input data - i.e., instead of starting at a zero-state, the equilibriumpropagation network is initialized in a state sf := f(x). We train the parameters  to approximate the minimal energy state s- of the equilibrating network. In other words, we seek:

 := arg min L(f(x), s-)

L(f(x), s-) := Li(fi (x), si-) := (fi (x) - s-i )2
iS iS
The derivative of the forward parameters of the i'th neuron, i, can be expanded as:

(7) (8)

local

distant

L i

:=

iS

Li(fi (x), s-i ) i

=

Li sif

sif i

+

j>i

Lj sjf

sjf sif

sfi i

(9)

The distant term is problematic, because computing

 sfj  sfi

would require backpropagation, and the

entire purpose of this exercise is to train a neural network without backpropagation. However, we

find

that

only

optimizing

the

local

term

Li i

does

not

noticeably

harm

performance.

In

Section

2.2

we go into more detail on why it appears to be sufficient to minimize local losses.

Over the course of training, parameters  will learn until our feedforward network is a good predictor of the minimal-energy state of the equilibrating network. This feedforward network can then be used

3

Under review as a conference paper at ICLR 2019

to do inference: we simply take the state of the output neurons to be our prediction of the target data. The full training procedure is outlined in Algorithm 1. At the end of training, inference can be done either by taking the output activations from the forward pass of the inference network f (Algorithm 2), or by initializing with a forward pass and then iteratively minimizing the energy (Algorithm 3). Experiments in Section 3 indicate that the forward pass performs just as well as the full energy minimization.

Algorithm 1 Training

Algorithm 2 Feedforward Inference

1: Input: Dataset (x, y), Step Size , 1: Input: Input Data x, Inference Parame-

Learning Rate , Network Architecture ters 

, Number of negative-phase steps T -, 2: s  f(x)

Number of positive-phase steps T +

3: return (si : i  O) # Output unit states

2:   InitializeFeedforwardParameters()

3:   InitializeEquilibriumParameters()

4: while not converged do

5: xm, ym  SampleMinibatch(x, y) 6: s  sf  f(xm) 7: for t  1..T - do # Neg. Phase

8:

ss-

E (s,xm) s

9: s-  s

10: for t  1..T + do # Pos. Phase

11:

ss-

E (s,xm,ym) s

12: s+  s

13:





-

 

E (s+,x) 

-

E (s-,x) 

Algorithm 3 Iterative Inference

1: Input: Input Data x, Initialization Pa-

rameters , Equilibriating Parameters ,

Number of Negative Steps T -

2: s  f(x) 3: for t  1..T - do # Neg. Phase

4:

ss-

E (s,xm) s

5: return (si : i  O) # Output unit states

14:

i



i

-



Li

(sfi ,si- i

)

i

15: Return: ,  # Parameters

2.1 INCLUDING THE FORWARD STATES IN THE ENERGY FUNCTION
Because the fixed point of a neuron of the equilibrating network is a nonlinear function of all the parameters of the network, it is possible for the equilibrating network to create targets that are not achievable by the neurons in the feedforward network. Neurons in the feedforward network simply learn a linear mapping from the previous layer's activations to the targets provided by the equilibrating network. In order to encourage the equilibrating network to stay in the regime that is reachable by the forward network, we can add a loss encouraging the fixed points to stay in the regime of the forward pass.

E(s, x) = E(s, x) +  (sjf - sj)2
jS

(10)

Where  is a hyperparameter which brings the fixed-points of the equilibrating network closer to
the states of the forward pass. This trick has a secondary benefit: It allows faster convergence in the negative phase by pulling the minimum of E(s, x) closer to the feedforward prediction, so we can learn with fewer convergence steps.

2.2 WHY THE LOCAL LOSS IS SUFFICIENT: GRADIENT ALIGNMENT
In Equation 9 we decompose the loss-gradient of parameters  into a local and a global component. Empirically (see Appendix B), we find that using the local loss and simply ignoring the global loss led to equally good convergence.
To understand why this is the case, let use consider a problem where we learn the mapping from an input x to a set of neuron-wise targets: s. Assume these targets are generated by some (unknown)

4

Under review as a conference paper at ICLR 2019

set of ideal parameters , so that s = f (x). To illustrate, we consider a two layer network with  = (w1, w2) and  = (w1, w2):

s1 = h(x · w1) s1 = h(x · w1) L1 = s1 - s1 s2 = h(s1 · w2) s2 = h(s1 · w2) L2 = s2 - s2

(11)

It may come as a surprise that, given random parameter initializations, the term

,L1 L2
w1 w1

> 0 is

almost always true (where ·, · denotes an inner product), i.e. the local and distant gradients tend to

be aligned. This is a pleasant surprise because it means the local loss will at least initially guide us in the right direction. The reason becomes apparent when we define w := w - w, and write out

the expression for the gradient in the limit of w  0

L1 T w1

= (x
w0

·

w1

f (x · w1)

f (x · w1))T · x

L2 T w1

=
w0

x · w1

f (x · w1)·w2

G1
f (s1 · w2)2 · w2T

T
f (x · w1) · x

- s1 · w2

G2
f (s1 · w2)2 · w2T

T
f (x · w1) · x

(12)

When the term w2

f

(s1 · w2)2 · w2T

is

proportional

to

an

identity

matrix,

we

can

see

that

L1 w1

and G1 are perfectly aligned. This will be the case when w2 is orthogonal and layer 2 has a linear

activation. However, even for randomly sampled parameters, w2 f (s1 · w2)2 · w2T has a strong

diagonal component and the terms thus tend to be positively aligned. Figure 1 demonstrates this

effect.

10 1 CS ,( wL11 wL11 )

10 1 CS ,( wL11 wL21 )

10 1 CS ,( wL11 wL31 )

10 1 CS ,( wL11 wL41 )

10 1 CS ,( wL11 wL51 )

10 1 CS ,( wL11 wL61 )

Figure 1: In a randomly initialized 6-layer network, with randomly generated targets s, we observe that the local parameter gradient tends to align to the distant components of the parameter gradient. Here, the i th panel shows the histogram of gradient alignments between local loss L1 and the loss of layer i, Li, w.r.t. the first layer's parameters w1, over random network initializations. CS(a, b) is the cosine similarity between vectors a and b.
This explains the empirical observation in Appendix 2.2 that optimizing the local, as opposed to the global, loss for the feedforward network does not appear to slow down convergence: Layer layers do not have to "wait" for earlier layers to converge before they themselves converge - earlier layers optimize the loss of later layers right from the beginning of training.
3 EXPERIMENTS
We base our experiments off of those of Scellier & Bengio (2017): We use (x) = max(0, min(1, x)) as our nonlinearity. We clip the state of si to the range (0, 1) because, since  (x) = 0 : x < 0  x > 1, if the system in Equation 2 were run in continuous time, it should never reach states outside this range. Like Scellier & Bengio (2017), to avoid instability problems arising from incomplete negative-phase convergence, we randomly sample   U ({-base, +base}), where base is a small positive number, for each minibatch and use this for both the positive phase
5

Under review as a conference paper at ICLR 2019

Classification Test Error Classification Test Error

100 80 60 40 20 0 10 8 6 4 2 00

[784-500-10], 4-step
Eq Prop: s Init Eq Prop: sf Init Eq Prop: s

[784-500-500-500-10], 20-step

[784-500-10], 20-step

[784-500-500-500-10], 50-step

10 20Epoch 30 40 50 0

50 Ep1o0c0h 150 200

Figure 2: Learning Curves on MNIST comparing the performance of Equilibrium Propagation (Eq Prop: s-), the Forward-Pass in Initialized Equilibrium Propagation (Fwd Eq Prop: sf ) (Algorithm 2) and the Negative Phase in Initialized Equilibrium Propagation (Fwd Eq Prop: s-) (Algorithm 3). Left Column: A shallow network with a single hidden layer of 500 units. Right Column: A deeper network with 3 layers of [500, 500, 500] hidden units. Top Row: Training with a smallnumber of negative-phase steps (4 for the shallow network, 20 for the deeper) shows that feedfoward initialization makes training more stable by providing a good starting point for the negative phase optimization. The Eq Prop s- lines on the upper plots are shortened because we terminate training when the network fails to converge. Bottom Row: Training with more negative-phase steps shows that when the baseline Equilibrium Propagation network is given sufficient time to converge, it performs comparably with our feedforward network (Note that the y-axis scale differs from the top).

and for multiplying the learning rate in Equation 3 (for simplicity, this is not shown in Algorithm 1). Unlike Scellier & Bengio (2017), we do not use the trick of caching and reusing converged states for each data point between epochs. Our feedforward network (described in Equation 6) has activation function h(x) = (x) + 0.01x, where the 0.01 "leak" is added to prevent the feed-forward neurons from getting "stuck" due to zero-gradients in the saturated regions.
3.1 MNIST
We verify that the our algorithm works on the MNIST dataset. The learning curves can be seen in Figure 2. We find, somewhat surprisingly, that the forward pass of our network performs almost indistinguishably from the performance of the negative-phase of Equilibrium Propagation. This encouraging result shows that this approach for training a feedforward network without backprop does indeed work. We also see from from the top-two panels of Figure 2 that our approach can stabilize EquilibriumProp learning when we run the network for fewer steps than are needed for full convergence. By initializing the negative phase in a close-to-optimal regime, the network is able to learn when the number of steps is so low that plain Equilibrium Propagation cannot converge.
6

Under review as a conference paper at ICLR 2019

4 RELATED WORK

4.1 RELATION TO ADVERSARIAL LEARNING

Several authors (Kim & Bengio, 2016), (Finn et al., 2016), (Zhai et al., 2016) have pointed out the connection between Energy Based Models and Generative Adversarial Networks (GANs). In these works, a feedforward generator network proposes synthetic samples to be evaluated by an energybased discriminator, which learns to push down the energy of real samples and push up the energy of synthetic ones. In these models, both the generator/sample proposer and the discriminator/energybased-model are deep feedforward networks trained with backpropagation.

In our approach, we have a similar scenerio. The inference network f can be thought of as a
conditional generator which produces a network state sf given a randomly sampled input datum x: sf = f(x). Parameters  are trained to approximate the minimal-energy states of the energy
function: min f(x) - arg mins E(s, x) . However, in our model, the Energy-Based network E(s, x) does not directly evaluate the energy of the generated data sf , but of the minimizing state s- = arg mins E(s, x) which is produced by performing T - energy-minimization steps on sf (see Algorithm 1). Like a discriminator, the energy-based model parameters  learn based on a contrastive loss which pushes up the energy of the of the "synthetic" network state s- while pushing down the energy of the "real" state s+.

If we were to set T - = 0 so that s- = sf (see Algorithm 1) and train our forward parameters  to

minimize E(sf ), we would have a model that is very similar to a conditional GAN. However doing

this naively does not work empirically (the network fails to converge), and the reason for this failure

is clear: As stated in Equation 3, the Equilibrium Propagation only accurately computes the output

loss

gradient

 C (sO ,y) 

if

s

is

at

an

minimum

of

E(s, x).

Although

the

feed-forward

network

does

attempt to find this minimizing state, we empirically find that this is not close enough to the true

minimum to provide for stable learning, and the network fails to converge. This approach - using a

recurrent, energy-based model as a discriminator which provides local losses to train a feedforward

inference network - seems like an interesting direction of future research.

4.2 RELATION TO DISTILLATION
Hinton et al. (2015) proposed the idea of "distilling" the knowledge of a large neural network or ensemble of neural networks into a smaller network which is designed to run efficiently at inference time. In this work, we use this same idea of distillation to turn a slow, training-time network in to a fast inference network, but in a different way: Our equilibrating network has a slow, iterative inference phase, but is able to provide layerwise targets for a fast feedforward network. We "distill" the knowledge out of the equilibrating network into the feedforward network. In addition, we use our feedforward network to speed the equilibrating network by initializing its inference at a good starting point.

4.3 RELATION TO OTHER WORK IN CREDIT-ASSIGNMENT
The idea of initializing an iterative settling process with a forward pass was proposed by Bengio et al. (2016). In this work, they propose using the parameters of the Equilibriating network to do a forward pass, and describe the conditions under which this provides a good approximation the the energy-minimizing state. Their conclusion is that this criterion is met when consecutive layers of the energy-based model form a good autoencoder. Their work differs from ours in that the parameters of the forward model are tied to the parameters of the energy-based model. The effects of this assumption are unclear, and the authors do not demonstrate a training algorithm using this idea.
Another interesting approach to shortening the inference phase in Equilibrium propagation was proposed by Kohan et al. (2018). The authors propose a model that is almost a feedforward network, except that the output layer projects back to the input layer. In the negative phase, the authors make several feedforward passes through the network, reprojecting the output back to the input with each pass. Although the resulting inference model is not a feedforward network, the authors claim that this approach allows them to dramatically shorten convergence time of the negative phase.

7

Under review as a conference paper at ICLR 2019
More broadly, other approaches to backprop-free credit assignment have been tried. DifferenceTarget propagation (Lee et al., 2015) proposes a mechanism to send back targets to each layer, such that locally optimizing targets also optimizes the true objective. Feedback-Alignment (Lillicrap et al., 2014) shows that, surprisingly, it is possible to train while using random weights for the backwards pass in backpropagation, because the forward pass parameters tends to "align" to the backwards-pass parameters so that the pseudogradients tend to be within 90 of the true gradients. A similar phenomenon was observed in Equilibrium Propagation by Scellier et al. (2018), who showed that when one removed the constraint of symmetric weight in Equilibrium propagation, the weights would evolve towards symmetry through training.
5 DISCUSSION
In this paper we describe how to use a recurrent, energy-based model to provide layerwise targets with which to train a feedforward network without backpropagation. This work helps us understand how the brain might be training fast inference networks. In this view, neurons in the inference network learn to predict local targets, which correspond to the minimal energy states, which are found by the iterative settling of a separate, recurrently connected equilibrating network. An interesting direction for future work would be to more closely connect this to adversarial learning. We can think of the feedforward network as generating "synthetic" network states, conditioned on input data, and the equilibrating network as being a discriminator, which aims to distinguish these "synthetic" states from the "real" energy minima which arise when knowledge of the target is present.
8

Under review as a conference paper at ICLR 2019
REFERENCES
Yoshua Bengio, Benjamin Scellier, Olexa Bilaniuk, Joao Sacramento, and Walter Senn. Feedforward initialization for fast inference of deep generative networks is biologically plausible. arXiv preprint arXiv:1606.01651, 2016.
Chelsea Finn, Paul Christiano, Pieter Abbeel, and Sergey Levine. A connection between generative adversarial networks, inverse reinforcement learning, and energy-based models. arXiv preprint arXiv:1611.03852, 2016.
Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531, 2015.
John J Hopfield. Neurons with graded response have collective computational properties like those of two-state neurons. Proceedings of the national academy of sciences, 81(10):3088­3092, 1984.
Taesup Kim and Yoshua Bengio. Deep directed generative models with energy-based probability estimation. arXiv preprint arXiv:1606.03439, 2016.
Adam A Kohan, Edward A Rietman, and Hava T Siegelmann. Error forward-propagation: Reusing feedforward connections to propagate errors in deep learning. arXiv preprint arXiv:1808.03357, 2018.
Dong-Hyun Lee, Saizheng Zhang, Asja Fischer, and Yoshua Bengio. Difference target propagation. In Machine Learning and Knowledge Discovery in Databases, pp. 498­515. Springer, 2015.
Timothy P Lillicrap, Daniel Cownden, Douglas B Tweed, and Colin J Akerman. Random feedback weights support learning in deep neural networks. arXiv preprint arXiv:1411.0247, 2014.
Mehdi Mirza and Simon Osindero. Conditional generative adversarial nets. arXiv preprint arXiv:1411.1784, 2014.
Benjamin Scellier and Yoshua Bengio. Equilibrium propagation: Bridging the gap between energybased models and backpropagation. Frontiers in computational neuroscience, 11:24, 2017.
Benjamin Scellier, Anirudh Goyal, Jonathan Binas, Thomas Mesnard, and Yoshua Bengio. Extending the framework of equilibrium propagation to general dynamics, 2018. URL https: //openreview.net/forum?id=SJTB5GZCb.
Shuangfei Zhai, Yu Cheng, Rogerio Feris, and Zhongfei Zhang. Generative adversarial networks as variational training of energy based models. arXiv preprint arXiv:1611.01799, 2016.
9

Under review as a conference paper at ICLR 2019
A GLOSSARY

Here we we have a reference of symbols used in the paper, in (Greek, Latin) alphabetical order.

i  {j : j  S, j = i}: The set of neurons in the Equilibrating Network that connect to neuron i if = {j : j  i, j < i}: The set of neurons in the Feedforward Network connected to neuron i.   R: The perturbation factor, which modulates how much the augmented energy E is affected by the output loss.

  R+: The learning rate.

: The set of parameters (all wij's and bj's, in the Equilibrating network)

: a neuron nonlinearity. In all experiments it is (x) = max(0, min(1, x))

 = (, c): The set of parameters (all ij's and cj's), in the feedforward network

j: The set of parameters (all ·j's and cj') belonging to a neuron j

(, c): The (weights, biases) of the feedforward network. Collectively called 

C(sO, y)  R: The output loss function, defined on the states of the output units.
E(s, x)  R: The energy function of the Equilibrating network (Equation 1) produces a scalar energy given a set of states s and input x

E(s, x, y)

=

E (s,

x)

+



 C (sO ,y) s



R:

The augmented energy function of the Equilibrating

network, when it has been perturbed by a factor  by target data y

f(x)  sf : The initialization function: A feedforward network which initializes the state of the Equilibrating network.

I: The set of input neurons.

h: The nonlinearity of the Feedforward network.

L: The total loss in the Feedforward network's prediction. Defined in Equation 8

Li: The local loss on the i th neuron in the feedforward network. Defined in Equation 8 O: The set of output neurons, a subset of S

S: The set of non-input neurons.

s: The set of neuron states. s := {si : i  S}  Rdim(S)
s- := arg mins E(s, x)  Rdim(S): The minimizing state of the Energy function. s+ := arg mins E(s, x, y)  Rdim(S) The minimizing state of the augmented energy function. sf := f(x)z  Rdim(S): The state output by the feedforward network.
sO  Rdim(O): the states of the output units T -, T +: Hyperparameters for Equilibrium Prop defining the number of steps of convergence of the negative/positive phase.

w, b: the parameters of the Equilibrating network (collectively called ) x  Rdim I : The input data y  Rdim O: The target data

10

Under review as a conference paper at ICLR 2019

B LOCAL VS GLOBAL LOSSES

Classification Test Error

10 8 6 4 2 00

Local: s^f Local: s^Global: s^f Global: s^i 5 10 Epoch 15 20 25

Figure 3: We find that only optimizing the Local Loss term (solid lines) performs about as well as when we optimize the global loss term (dashed lines) in Equation 9. Green lines show the initialization pass error and Orange lines show the error after the end of the negative phase.

11


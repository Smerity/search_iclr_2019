Under review as a conference paper at ICLR 2019
TRANSFERRING SLU MODELS IN NOVEL DOMAINS
Anonymous authors Paper under double-blind review
ABSTRACT
Spoken language understanding (SLU) is a critical component in building dialogue systems. When building models for novel natural language domains, a major challenge is the lack of data in in the new domains, no matter whether the data is annotated or not. Recognizing and annotating "intent" and "slot" of natural languages is a time-consuming process. Therefore, spoken language understanding in low resource domains remains a crucial problem to address. In this paper, we address this problem by proposing a transfer-learning method, whereby an SLU model is transferred to a novel but data-poor domain via a deep neural network framework. We also introduce meta-learning in our work to bridge the semantic relations between seen and unseen data, allowing new intents to be recognized and new slots to be filled with much lower new training effort. We show the performance improvement with extensive experimental results for spoken language understanding in low resource domains. We show that our method can also handle novel intent recognition and slot-filling tasks. Our methodology provides a feasible solution for alleviating data shortages in spoken language understanding.
1 INTRODUCTION
The recent surge of artificial intelligence motivates the technical and applicable exploration of novel human-computer interactions. Spoken dialogue systems are widely studied and used in various mobile devices. Well-known commercial applications driven by dialogue systems include intelligent personal assistants and intelligent robots for customer services. As more and more products and scenarios integrate dialogue systems in intelligent services, the ability for model adaptation in dialogue systems is critically needed. Many state-of-the art dialogue systems follow a learning pipeline that includes components such as spoken language understanding (SLU), dialogue management (DM) and spoken language generation or retrieval (SLG). Since both dialogue management and spoken language generation systems rely on knowledge or information learned from spoken language understanding, research and industry community have paid much attention to the SLU area. In this paper, we focus on the problem of data shortage for SLU in a new domain.
Spoken language understanding typically defines and represents user utterances in terms of semantic frames comprised of domains, intents and slots (Tur & Mori, 2011). A spoken language understanding task involves classifying the domain, detecting the intent of user utterance and identifying token sequences corresponding to slot values in the semantic frame. A learning model is often designed to handle the above subtasks separately and sometimes simultaneously. Deep learning models are the cutting-edge techniques for achieving impressive performance in spoken language understanding compared with conventional machine learning models.
However, deep neural-network models often require the collection and manual annotation of data, which is a time and labor intensive process. These problems pose as a major obstacle to building accurate learning models in new domains. To address this problem, we propose to perform domain adaption of SLU models by transferring the SLU knowledge and model parameters learned in an auxiliary domain where a model is already built. A related problem is that only few examples of some output intent and slot classes are available during the training phrase in a new domain, because training sets for a new domain are small. To deal with this limitation, we propose a novel few shot learning model in a meta learning paradigm, we call it the Transfer Semantic Similarity Model (TSSM). The TSSM achieves few shot learning by exploiting the semantic embeddings of both seen and unseen intent and slots.
1

Under review as a conference paper at ICLR 2019
In the related work section, we argue that the current spoken language understanding tasks often do not take the relationship between intents and slots into consideration, even when these works classify the intents and slots in a novel SLU domain simultaneously in a single model. We propose a "intent2slot" mechanism which consider such relationship and show that the TSSM can indeed achieve better performance in natural language understanding.
In summary, the challenges of spoken language understanding include:
· Expanding SLUs to novel domains is necessary as the development of artificial intelligent products extends to more areas in our society. But it is difficult to get sufficient data or spoken language sentence in novel domains.
· Without sufficient labeled data, deep learning methods are difficult to train and thus it is hard to obtain satisfactory performance in a new domain.
· Unseen intents and slots exist because small annotated training set might not cover all intents and slots. Thus, the generalization ability of the models are crucial for dealing with unseen classes.
Our contributions are summarized as follows:
· We design a novel and efficient SLU model, the TSSM. It incorporates transfer learning and meta learning for SLU tasks in low resource domains.
· The proposed TSSM can improve model performance with only a small amount of training data in new domains and it can handle unseen classes easily. The experiments illustrate that our proposed methods outperform the state-of-the-art baselines in many experimental domains.
· Our work can treat intent and slot detection as a structured multi-objective optimization problems, which further increases the accuracy of learning tasks in new domains.
2 RELATED WORK
Spoken language understanding is usually defined as a supervised learning problem, involving conventional machine learning models (Young, 2002; Hahn et al., 2011; Wang et al., 2005) and deep learning models (Mesnil et al., 2015; Kurata et al., 2016; Sarikaya et al., 2011) on massive amount of annotated training data. Although unsupervised learning and semi-supervised learning based approaches have been proposed as well (Chen et al., 2013; 2015b;a), deep learning approaches were shown to outperform most others. Besides the requirement of a large amount of annotated data being available, the intents and slots are usually predefined. Thus they are inflexible to expand to new domains. Some researchers have paid attention to alleviate this limitation. An example is Korpusik et al. , which work retrains its models to cover new intents and associated slots while redesigning a semantic schema (Korpusik et al., 2014).
Considering that models for different domains are usually trained independently, some attempts have been made to learn shared latent semantic representations or shared model parameters for multiple domains via transfer learning. A survey fnor transfer learning with applications is Pan & Yang (2010). In general, the goal of transfer learning is to improve performance in low data domains and reduce model re-training efforts by bridging the knowledge from data-rich source tasks to new target tasks. Transfer learning in deep learning setting has been applied to recommendation systems and computer vision domains. In natural language processing (NLP), researchers have studied crossdomain NLP problems but have mainly focused in cross-lingual problem settings (Mou et al., 2016; Buys & Botha, 2016; Kim et al., 2017a;b). In Hakkani-Tr et al. (2016), multi-task and transfer learning approaches are proposed to learn shared implicit feature representation across various domains for spoken language understanding. Another study Jaech et al. (2016) follows the similar idea in cross-domain NLP and showed significant performance improvement. Researchers in Amazon have shown in a recent study that DNN-based natural language engines can re-use the existing models through transfer learning (Goyal et al., 2018).
When certain classes do not show up in the training data, one can exploit meta learning to use fewshot learning to help with knowledge transfer Fei-Fei et al. (2006); Palatucci et al. (2009); Vinyals et al. (2016); Ravi & Larochelle (2018). For example, Vinyals et al. (2016) learns a network that
2

Under review as a conference paper at ICLR 2019

maps a small labeled support set and unlabeled examples to its labels, saving the effort in fine-tuning to adapt to new class types. Besides image object recognition tasks, in SLU some experiments have been conducted. The objective of Ferreira et al. (2015) is to predict the semantic tag sequences of a user query without using any target-domain user utterances and thus in-context semantic tags. The research studies build a statistical model to predict the SLU for unseen data in Yazdani & Henderson (2015); Chen et al. (2016). An Action Matching algorithm is proposed by Zhao & Eske´nazi (2018) to learn a cross-domain embedding space that models the semantics of dialog responses, which, in turn, lets a neural dialog generation model generalize to new domains. Recent publication by Google (Bapna et al., 2017) explores semi-supervised slot-filling based on deep learning that can use the slot labels without the need for any labeled or unlabeled data in domain examples or the need for any explicit schema alignment, to quickly bootstrap the model in a new domain.

3 OUR PROPOSED APPROACH FOR MODEL TRANSFER

In this section, we introduce our transfer learning approach to address the cross-domain SLU prob-
lem. We consider a source domain Ds and a target domain Dt. Typically, we have a large annotated dataset for Ds, while only a few labeled data for Dt. The tasks of interest in both Ds and Dt are of the same types, namely they are both slot filling and intent detection tasks. However, Ds and Dt have different intent label spaces Ts and Tt, because the target domain might have new intent values (slot face the same situation).

We propose the TSSM with an attention component, where the architecture is illustrated in Fig. 1. The model contains: (1) a word hashing layer obtained by converting one-hot word representations into embedding vectors, (2) a bi-directional Long Short Term Memory networks(LSTMs) (Hochreiter & Schmidhuber, 1997) layer to capture features from word embeddings, (3) a intent specific bi-LSTMs layer combined with an semantic network to detect intent, (4) a slot specific bi-LSTMs layer powered with an attention component and semantic network to conduct slot filling. We first pre-train this model in a source domain with large amount of annotated data. We wish to allow the model to extend to new slot names, slot values and intent names in the target domain.

The number of possible intents/slots grows rapidly as we get into new domains with new intents and slots, making this task one of multi-label classification with a very large number of labels. One natural approach to this task is to train one binary classifier for each possible label, to decide whether or not to include it in the output. However, this would require training a large number of classifiers. It would be impossible to generalize to target domains that include intents and slots that do not show up in the training set, since there will not be any parameter sharing among label classifiers. Instead of encoding slot names and intent names as discrete features in the target domain, we encode them using a continuous representation. To be specific, we learn a representation of slot names, slot values and intent names from their constituent words using a semantic network. We then check to see if these representations match the representations of utterances. In the following, we explain in details how this representation works in our transfer learning algorithm.

... ...

Slot Value 1 Slot Value 2 Slot Value n
...


Semantic Similarity Attention
... t

Slot Name
Intent 1 Intent 2 ...... Intent k

 softmax
Semantic Similarity

Slot task

LSTM

LSTM

LSTM

LSTM

LSTM

intent

Intent Task

LSTM

LSTM

LSTM

LSTM

LSTM

Common Representation LSTM

LSTM

LSTM

LSTM

LSTM

Embeddings
0 ... t

... +1

Figure 1: TSSM archiecture for joint SLU. The orange lines are inputs for semantic similarity network and the green lines represent the input for attention network.
3

Under review as a conference paper at ICLR 2019

Let w = (w0, w1, w2, . . . , wT +1) represent the input word sequence with w0 and wT +1 being the beginning-of-sequence ( bos ) and end-of-sequence ( eos ) tokens, respectively. T is the number of words in the utterance. Let s = (s0, s1, . . . , sT ) be the slot label sequence, where s0 is a padded slot label that maps to the beginning-of-sequence token bos and si  S. Let   T be the intent class. S and T are the slot and intent label space, respectively.
Let sn = (sn1, sn2, . . . , snm) represent the m slot names. sni = (w1, . . . , wmi ) is a vector of the words for the i-th slot name, where mi is its number of words. Each slot si has a list of slot values svi = (svi,1, svi,2, . . . , svi,ni ), where ni is the number of slot values for the i-th slot. A slot value is allowed to be a phrase like `fried chicken' for slot 'Food'. Let svi,j = (w1, . . . , wni,j ) be the words of the j-th slot value for i-th slot and ni,j is the number of words of the phrase.

3.1 BASE NETWORKS

We use a Pretrained DNN model as our base model (Goyal et al., 2018). The model has three layers of bi-LSTMs. The first layer is the common bi-LSTMs layer, which learns task related representations from input utterances. The learned representations are taken as input to the upper two bi-LSTMs layers, which are optimized separately for the slot-filling task and intent-detection task. There are no connections between the upper two layers in Pretrained DNN (Goyal et al., 2018). However, we believe that the output of intent specific bi-LSTMs layer could benefit the slot detection task. As one of our contributions, in our base network, we concatenate the output of common bi-LSTMs layer and intent bi-LSTMs layer to feed to the slot bi-LSTMs layer.

For the intent specific bi-LSTMs layer, we concatenate the last hidden state of each direction to acquire the global representation of the whole utterance:

hintent = [-h Tin+te1nt, h-i0ntent]

(1)

Such an operation converts utterances with variable lengths into a fixed-length vector, with which the information throughout the entire utterance can be captured. Representation hintent will then be
used to calculate the similarity scores with representations of candidate intents.

3.2 SEMANTIC NETWORKS

We wish to build a representation-learning mechanism for the SLU task that can generalize to unseen words and labels. In order to represent words, we use word embeddings, which are a form of vector space model. In various NLP tasks (Baroni et al., 2014; Collobert & Weston, 2008; Mikolov et al., 2013), word embeddings have proven to be effective models of semantic representation of words. Because word embeddings trained on large corpora of unlabeled data reflect the similarity between words, they help the model generalize from the words in the original training domains to the words in the new target domains. For every utterance we learn how to compose the word embeddings to reflect the semantics of that utterance. Furthermore, we learn how to compose the semantic representation of each intent/slot from the embeddings of the words used to name that intent/slot. This enables us to generalize to unseen intents/slots.

For slot names, slot values and intent names, we use the same semantic network and transform it to
a fixed-length representation vector. Take slot name sni = (w1, w2, . . . , wmi ) as an example, the semantic network first extracts the word embedding E(wi) for each word. E(wi) is then fed to a feed forward layer to get the non-linear semantic features. The last is a Mean-Pooling layer, which
applies the mean operations over each dimension of semantic features across all words. The output
is treated as the semantic representation for sni.

rislot name =

1 mi

m
relu (E(wj)
j=1

Wslot transf rom + bslot transf rom)

(2)

where Wslot transfrom and bslot transfrom are the learned linear projection matrix and bias for slot name semantic network respectively.
In the same way, we could get the semantic representation ris,ljot value for slot value svi,j and riintent name for the intent name i.

4

Under review as a conference paper at ICLR 2019

More sophisticated vector-space semantic representations of the slots/intents are an area for future work, but we believe the simple Mean-Pooling would be a proper choice as intent name, slot name and slot values are typically very short and composed by entity names.

3.3 SLOT VALUE ATTENTION NETWORKS

With previously introduced semantic networks, we can formulate a semantic representation rsi,ljot value of each slot value svi,j from the i-th candidate slot si. The whole slot value repre-
sentation list is ris,l1ot value, . . . , ris,lnoit value .

At each decoding step t, the slot bi-LSTMs layer output state htslot is connected to the representation

of slot values to calculate the similarity scores. We compute an attention weight t,i,j for slot value

svi,j via a bilinear operator, which reflects how relevant or important slot value svi,j is to the current

slot hidden states htslot:

t,i,j  exp((ris,ljot value) Wv hstlot)

(3)

where Wv is a parameter matrix to be learned. The overall slot value representation would be:

ni

rts,liot values =

t,i,j ris,ljot value

j

(4)

3.4 TRAINING PROCEDURE
The seen data containing utterances and associated intents/slots is used for training the model. The idea behind this model is to learn the representation for both utterances and intents/slots such that utterances with the same intents/slots are close to each other in the continuous semantic space. Below we define a semantic score between an utterance and an intent/slot using the similarity between their representation embeddings, h and r:

Sim(h, r) = h Wsr where Ws is the corresponding similarity matrix for slot or intent task.

(5)

The posterior probability of a possible intent given an utterance is computed based on the semantic score through a softmax function

P ( = i|hintent, w) =

exp Sim(hintent, riintent name) j inT exp Sim(hintent, rjintent name)

(6)

where i is an intent candidate.

Similarly, the posterior probability of a possible slot at each time is also computed based on the semantic score through a softmax function. Since we have the representation of both slot names and slot values, we first add the semantic score from slot name and slot value and then feed to the softmax function.

Sim(si, hstlot) = Sim(hstlot, rsi lot name) + Sim(htslot, rsi lot values)

(7)

P (st = si|hstlot, w) = where si is an slot candidate.

exp Sim(si, htslot) sj S exp Sim(sj , htslot)

(8)

For model training, we maximize the likelihood of the correctly associated intents/intents given all training utterances. The parameters W of the model are optimized by an joint negative log likelihood objective:

l
L = - log P
k=1

 = i|hwinktent, wk

l Tk
- log P
k=1 t=0

st = si|hts,lwotk , wk

(9)

where l is the total number of training utterances, wk is the k-th input utterance. The model is optimized using mini-batch stochastic gradient descent(SGD) (Huang et al., 2013).

5

Under review as a conference paper at ICLR 2019

3.5 TRANSFER TO A NEW DOMAIN
We train our TSSM using labeled data from source domain Ds. This stage is called the pre-training, where the networks learn to extract semantic relationship between utterance representations and intent/slot representations. The model is able to adapt to new domains, since the seen and unseen intents/slots representations are in the same semantic space, use the same shared composition semantic network with the same unsupervised word embeddings as input. It has been shown that such word embeddings can capture word similarities and hence the classifier is no longer ignorant about any new intent or slot. This can help overcome the data sparsity issue in the SLU training set by transferring knowledge between similar utterances and similar intents/slots.
After the pre-training, we fine-tune our model on the target domain Dt with less annotated data and with unseen intents/slots. The model for the new domain Dt are the same as that for Ds and are initialized by the pre-trained network parameters.
For zero-shot learning where no fine-tune data is avaiable, in order to predict new intents/slots on new utterances, the TSSM calculate semantic similarites. We transform each input utterance w into a vector hintent using the base network, and then estimate its semantic similarity with all intents including seen and unseen intents. The vector representations for new intents can be generated from the trained semantic network by feeding the word embedding vectors of new intents as the input. For the utterance w, the estimated semantic score of the i-th intent is defined as Sim(hintent, riintent) in Equation 5. Then predicted intent for each utterance is decided according to the estimated semantic scores.
Predicting slot sequences is similar to predicting intents except that the calculation is conducted step by step at each position.

4 EXPERIMENTS AND RESULTS

In this section, we first conduct extensive experiments to verify the transfer ability of the TSSM with different number of training data in the target domain. Then we verify the generalization ability of the TSSM with different number of new labels unseen in the target domain training set. Finally we verify the zeros-shot learning ability of the TSSM model and verify the effectiveness of the proposed "intent2slot" mechanism.

4.1 DATASET
In this experiment, we use five popular public dialogue datasets from five domains. The CamHotel dialogue dataset (Hotel) (Wen et al., 2015) records some dialogues about booking a hotel room in Cambridge. The CamRestaurant dialogue dataset (Restaurant) (Wen et al., 2016c;a) contains dialogue records about booking a restaurant in Cambridge. The Laptop and TV dialogue dataset (Wen et al., 2016b) are dialogues about buying Laptops and TVs. The Airline Travel Information System dataset (Atis) (Price, 1990) contains dialogues about airline travel, such as the fligh time, the destination, etc.

4.1.1 SETTING

In order to thoroughly evaluate the transfer learning ability on more diversified domains, we make three different pairs of source and target domains, as shown in table 1. In case when we have multiple source domains, all source domains will be combined into a large source domain.

Table 1: Data Sizes of the source and target domains.

Source Domain

Target Domain #Src Sentence #Tgt Sentence

Restaurant, Laptop, TV, Atis Hotel

48566

2168

Restaurant, Hotel, Laptop, TV Atis

45756

4978

Atis

Hotel

4978

2168

For each pair of the source and target domain pair, we first pretrain the model on the source domain for 50 epoch. Then we fine-tune the model on the target domain training data, finally we evaluate the fine-tuned model on the target domain testing dataset.

6

Under review as a conference paper at ICLR 2019
4.2 EVALUATION METRICS
In this experiment, we use the accuracy to evaluate the intent classification problem and we use the accuracy and F1 score to evaluate the slot filling task.
4.3 BASELINES
We compare the proposed model with the Pretrained DNN (Goyal et al., 2018), which can transfer low-level word embedding features across different domains and different tasks. Besides the transfer-learning model, we also compare with some popular baselines that do not use transfer learning.
1. The Pretrained DNN (Goyal et al., 2018) (DNN), which shares low-level word features between the intent classifiation task and the slot filling task.
2. The Max Entropy model (MaxEntropy) is a popular baseline for intent classification, the feature of each sentence is the bag-of-words features. This model cannot benefit from the source domain since the source and the target domain have different intents.
3. The Conditional Random Field model (CRF) is another popular baseline for the slot filling task. It leverage the pretrained word-embedding models and it can model the transformation probability between each sequential labels. This model cannot benefit from the source domain since the source and the target domain have different slots.
4.4 RESULTS UNDER DIFFERENT SIZES IN TARGET-DOMAIN DATA SETS
In order to evaluate the transfer learning performance for the proposed model, we compare the proposed model with the baselines when we have different number of target domain training data. After pretraining on the source domain, we fine-tune all models on different percentage of target domain data, and we evaluate the performance of the models on the test set. We use 50% of the target domain data as testset, and we vary the percentage of trainset used from 5% to 46%.
The results are shown in Figure 2. As a higher percentage of target domain data is used to fine-tune the model, the performance of all models improves. Generally, transfer learning models perform better than non-transfer models, which implies that transfer learning can help improve the performance in the target domain. The proposed TSSM model outperforms the baseline model significantly in almost every pair of source and target domains, and for both intent classification task and slot filling task. As the amount of target domain training data increases, the improvement of the TSSM model remain very significant, which demonstrates the TSSM model can effectively adapt to new domains.
4.5 RESULTS UNDER DIFFERENT NUMBERS OF NEW LABELS IN THE TARGET DOMAIN
In order to evaluate the generalization ability of the proposed TSSM model, we test the model on previously unseen testing labels. More specifically, we evaluate the proposed model under situations when the target domain test set have different number of new labels that is not seen in the target domain training set. We remove zero, one and two labels and their corresponding training instances from the target domain training set.
The results are shown in Figure 3. Here we use 10% of all target domain data as the training set, when the percentage of target domain training data increases to 20% or 30%, the results are similar. As the number of unseen label increases in the target domain testset, the performance of all model drops significantly. Transfer learning methods again out-performans none-transfer learning baselines, which demonstrate that transferring knowledge from source domain is generally beneficial to the target domain. As the number of unseen label increases, the performance drop of the TSSM model is significantly slower than the other baselines models. Compared with a softmax classification layer, the semantic similarity module works better since it can still computes semantic similarity for unseen labels. With a pretrained word embeddings, the TSSM can theoretically compute semantic similarity for arbitrary label as long as the label has a word embedding. The experimental results demonstrate that the TSSM model can effectively deal with cold start labels that is not seen in the target domain training data.
7

Under review as a conference paper at ICLR 2019

F1 F1 F1

Accuracy

0.90

0.88

0.86

0.84

0.82

0.80

0.78

MaxEntropy

TSSM

0.76 DNN

0.10 0.15 0.20 0.25 0.30 0.35 0.40

Accuracy

0.96 0.94 0.92 0.90 0.88 0.86
0.84 CRF TSSM 0.82 DNN
0.10 0.15 0.20 0.25 0.30 0.35 0.40

0.96 0.94 0.92 0.90 0.88 0.86
0.84 CRF TSSM 0.82 DNN
0.10 0.15 0.20 0.25 0.30 0.35 0.40

Accuracy

0.92 0.90 0.88 0.86 0.84 0.82 0.80
0.1

MaxEntropy DNN
0.2 0.3

TSSM
0.4

Accuracy

0.975 0.950 0.925 0.900 0.875 0.850 0.825 0.800
0.1

CRF DNN
0.2 0.3

TSSM
0.4

0.950 0.925 0.900 0.875 0.850 0.825 0.800 0.775 0.1

CRF DNN
0.2 0.3

TSSM
0.4

Accuracy

0.88

0.86

0.84

0.82

0.80

0.78

0.76

0.74 0.72

MaxEntropy DNN

TSSM

0.10 0.15 0.20 0.25 0.30 0.35 0.40

Accuracy

0.94 0.92 0.90 0.88 0.86
0.84 CRF TSSM 0.82 DNN
0.10 0.15 0.20 0.25 0.30 0.35 0.40

0.94 0.92 0.90 0.88 0.86
0.84 CRF TSSM 0.82 DNN
0.10 0.15 0.20 0.25 0.30 0.35 0.40

Figure 2: Performance comparison under different percentage of training data used in the target

domain fine-tuning. In each figure, X-axis is the percentage of data used in the target domain, yaxis is the performance (higher is better). The three columns corresponds to three different metrics, that is the intent accuracy (left column), the slot accuracy (middle column) and the slot f1 score (right column). The three rows corresponds to different source and target domain pairs. The first row is obtained when transferring from "Restaurant, Laptop, TV, Atis" to "Hotel". The second row

are the results transferring from "Restaurant, Hotel, Laptop, TV" to "Atis", and the third row are transferring from "Atis" to "Hotel".

4.6 ZERO-SHOT TRANSFER LEARNING

In the extreme case, there could be no training instances in the target domain at all. In order to test the proposed TSSM model on such extreme case, we report the performances of the TSSM model when there is no taraget domain training instances. The result is shown in Table 2. The random guess performance is 0.05 for intent classification accuracy, 0.02 for slot accuracy and 0.02 for slot f1. The TSSM have an performance much better than random guess. In such case, both the Pretrained DNN, the Max Entropy model and the CRF model could not work, since all of them require target domain training instances to initialize their softmax layer.

Table 2: The zeros-shot performance of TSSM

Src and Tgt Intent Accuracy Slot Accuracy Slot F1

nohotel2hotel 0.787076271 0.871391063 0.869830213

atis2hotel

0.477754237 0.756110818 0.758700696

noatis2atis 0.220323741 0.61739509 0.616158269

4.7 ABLATION EXPERIMENT
One of the improvements we have made is to use the intent bi-LSTMs output states as input features to the slot bi-LSTMs, we call this mechanism "intent2slot". In order to evaluate whether the "intent2slot" mechanism can actually improve the performance of the model, we compare the proposed TSSM with its ablation variant without "intent2slot", and the result is shown in Table 3 (improvements for transferring from Restaurant, Laptop, TV, Atis to Hotel is not obvious and are not reported). As shown in the result, the TSSM outperforms the TSSM-abl without "intent2slot" in most cases. The results demonstrate that the "intent2slot" mechanism helps to improve model performance.
8

Under review as a conference paper at ICLR 2019

F1 F1 F1

Accuracy

0.86

MaxEntropy DNN

TSSM

0.84

0.82

0.80

0.78

0.76

0.74 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00

Accuracy

0.95 CRF TSSM 0.90 DNN
0.85 0.80 0.75 0.70
0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00

0.95 CRF TSSM DNN
0.90 0.85 0.80 0.75 0.70
0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00

Accuracy

0.92

MaxEntropy

TSSM

0.90 DNN

0.88

0.86

0.84

0.82

0.80 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00

Accuracy

0.95 CRF TSSM 0.90 DNN
0.85 0.80 0.75 0.70 0.65
0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00

CRF TSSM 0.90 DNN
0.85 0.80 0.75 0.70 0.65 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00

Accuracy

0.86

MaxEntropy

TSSM

0.84 DNN

0.82

0.80

0.78

0.76

0.74

0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00

Accuracy

0.95 CRF TSSM 0.90 DNN
0.85 0.80 0.75 0.70
0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00

0.95 CRF TSSM 0.90 DNN
0.85 0.80 0.75 0.70
0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00

Figure 3: Performance vs the number of unseen intent/slot in the target domain testset, where 10% of the total target domain data is used. In each figure, X-axis is the number of unseen intent/slots in the target domain testset, y-axis is the performance (higher is better).

Src and Tgt
atis2hotel noatis2atis

Table 3: The TSSM with/without "intent2slot"

Intent Accuracy

Slot Accuracy

Slot F1

TSSM-abl TSSM

TSSM-abl TSSM

TSSM-abl

0.94473938 0.9524614 0.93722848 0.9379564 0.94492515

0.80719112 0.8404923 0.89069569 0.8963195 0.89931065

TSSM 0.9450242 0.887933

5 CONCLUSIONS
In this paper, we introduce a deep semantic similarity network to transfer from domains with sufficient labeled data to low resource domains. The experiments illustrate that our proposed methods outperform the state-of-the-art baselines in several experimental settings.
REFERENCES
Ankur Bapna, Gokhan Tur, Dilek Hakkani-Tur, and Larry Heck. Towards zero shot frame semantic parsing for domain scaling. In Interspeech 2017, 2017.
Marco Baroni, Georgiana Dinu, and Germa´n Kruszewski. Don't count, predict! a systematic comparison of context-counting vs. context-predicting semantic vectors. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), volume 1, pp. 238­247, 2014.
Jan Buys and Jan A. Botha. Cross-lingual morphological tagging for low-resource languages. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1954­1964. Association for Computational Linguistics, 2016. doi: 10. 18653/v1/P16-1184. URL http://www.aclweb.org/anthology/P16-1184.
Y. Chen, W. Y. Wang, and A. I. Rudnicky. Unsupervised induction and filling of semantic slots for spoken dialogue systems using frame-semantic parsing. In 2013 IEEE Workshop on Automatic Speech Recognition and Understanding, pp. 120­125, Dec 2013. doi: 10.1109/ASRU.2013. 6707716.
Y. Chen, D. Hakkani-Tr, and X. He. Zero-shot learning of intent embeddings for expansion by convolutional deep structured semantic models. In 2016 IEEE International Conference

9

Under review as a conference paper at ICLR 2019
on Acoustics, Speech and Signal Processing (ICASSP), pp. 6045­6049, March 2016. doi: 10.1109/ICASSP.2016.7472838.
Yun-Nung Chen, William Yang Wang, Anatole Gershman, and Alexander Rudnicky. Matrix factorization with knowledge graph propagation for unsupervised spoken language understanding. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pp. 483­494. Association for Computational Linguistics, 2015a. doi: 10.3115/v1/P15-1047. URL http://www.aclweb.org/anthology/P15-1047.
Yun-Nung Chen, William Yang Wang, and Alexander Rudnicky. Jointly modeling inter-slot relations by random walk on knowledge graphs for unsupervised spoken language understanding. In Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 619­629. Association for Computational Linguistics, 2015b. doi: 10.3115/v1/N15-1064. URL http://www.aclweb.org/ anthology/N15-1064.
Ronan Collobert and Jason Weston. A unified architecture for natural language processing: Deep neural networks with multitask learning. In Proceedings of the 25th international conference on Machine learning, pp. 160­167. ACM, 2008.
Li Fei-Fei, R. Fergus, and P. Perona. One-shot learning of object categories. IEEE Transactions on Pattern Analysis and Machine Intelligence, 28(4):594­611, April 2006. ISSN 0162-8828. doi: 10.1109/TPAMI.2006.79.
E. Ferreira, B. Jabaian, and F. Lefvre. Online adaptative zero-shot learning spoken language understanding using word-embedding. In 2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 5321­5325, April 2015. doi: 10.1109/ICASSP. 2015.7178987.
Anuj Kumar Goyal, Angeliki Metallinou, and Spyros Matsoukas. Fast and scalable expansion of natural language understanding functionality for intelligent agents. CoRR, abs/1805.01542, 2018. URL http://arxiv.org/abs/1805.01542.
S. Hahn, M. Dinarelli, C. Raymond, F. Lefevre, P. Lehnen, R. De Mori, A. Moschitti, H. Ney, and G. Riccardi. Comparing stochastic approaches to spoken language understanding in multiple languages. IEEE Transactions on Audio, Speech, and Language Processing, 19(6):1569­1583, 2011. doi: 10.
Dilek Hakkani-Tr, Gokhan Tur, Asli Celikyilmaz, Yun-Nung Vivian Chen, Jianfeng Gao, Li Deng, and Ye-Yi Wang. Multi-domain joint semantic frame parsing using bi-directional rnn-lstm. ISCA, June 2016. URL https://www.microsoft.com/en-us/research/publication/ multijoint/.
Sepp Hochreiter and Ju¨rgen Schmidhuber. Long short-term memory. Neural computation, 9(8): 1735­1780, 1997.
Po-Sen Huang, Xiaodong He, Jianfeng Gao, Li Deng, Alex Acero, and Larry Heck. Learning deep structured semantic models for web search using clickthrough data. In Proceedings of the 22nd ACM international conference on Conference on information & knowledge management, pp. 2333­2338. ACM, 2013.
Aaron Jaech, Larry P. Heck, and Mari Ostendorf. Domain adaptation of recurrent neural networks for natural language understanding. CoRR, abs/1604.00117, 2016. URL http://arxiv. org/abs/1604.00117.
Joo-Kyung Kim, Young-Bum Kim, Ruhi Sarikaya, and Eric Fosler-Lussier. Cross-lingual transfer learning for pos tagging without cross-lingual resources. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pp. 2832­2838. Association for Computational Linguistics, 2017a. URL http://aclweb.org/anthology/D17-1302.
10

Under review as a conference paper at ICLR 2019
Young-Bum Kim, Karl Stratos, and Dongchan Kim. Domain attention with an ensemble of experts. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 643­653. Association for Computational Linguistics, 2017b. doi: 10.18653/v1/P17-1060. URL http://www.aclweb.org/anthology/P17-1060.
M. Korpusik, N. Schmidt, J. Drexler, S. Cyphers, and J. Glass. Data collection and language understanding of food descriptions. In 2014 IEEE Spoken Language Technology Workshop (SLT), pp. 560­565, Dec 2014. doi: 10.1109/SLT.2014.7078635.
Gakuto Kurata, Bing Xiang, Bowen Zhou, and Mo Yu. Leveraging sentence-level information with encoder LSTM for natural language understanding. CoRR, abs/1601.01530, 2016. URL http://arxiv.org/abs/1601.01530.
G. Mesnil, Y. Dauphin, K. Yao, Y. Bengio, L. Deng, D. Hakkani-Tur, X. He, L. Heck, G. Tur, D. Yu, and G. Zweig. Using recurrent neural networks for slot filling in spoken language understanding. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 23(3):530­539, March 2015. ISSN 2329-9290. doi: 10.1109/TASLP.2014.2383614.
Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig. Linguistic regularities in continuous space word representations. In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 746­751, 2013.
Lili Mou, Zhao Meng, Rui Yan, Ge Li, Yan Xu, Lu Zhang, and Zhi Jin. How transferable are neural networks in nlp applications? In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pp. 479­489. Association for Computational Linguistics, 2016. doi: 10.18653/v1/D16-1046. URL http://www.aclweb.org/anthology/D16-1046.
Mark Palatucci, Dean Pomerleau, Geoffrey Hinton, and Tom M. Mitchell. Zero-shot learning with semantic output codes. In Proceedings of the 22Nd International Conference on Neural Information Processing Systems, NIPS'09, pp. 1410­1418, USA, 2009. Curran Associates Inc. ISBN 9781-61567-911-9. URL http://dl.acm.org/citation.cfm?id=2984093.2984252.
S. J. Pan and Q. Yang. A survey on transfer learning. IEEE Transactions on Knowledge and Data Engineering, 22(10):1345­1359, Oct 2010. ISSN 1041-4347. doi: 10.1109/TKDE.2009.191.
Patti J Price. Evaluation of spoken language systems: The atis domain. In Speech and Natural Language: Proceedings of a Workshop Held at Hidden Valley, Pennsylvania, June 24-27, 1990, 1990.
Sachin Ravi and Hugo Larochelle. Optimization as a model for few-shot learning. In Proceedings of the 6th International Conference on Learning Representations, ICLR'18, 2018.
R. Sarikaya, G. E. Hinton, and B. Ramabhadran. Deep belief nets for natural language call-routing. In 2011 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 5680­5683, May 2011. doi: 10.1109/ICASSP.2011.5947649.
Gokhan Tur and Renato De Mori. Spoken language understanding: Systems for extracting semantic information from speech. 2011.
Oriol Vinyals, Charles Blundell, Timothy P. Lillicrap, Koray Kavukcuoglu, and Daan Wierstra. Matching networks for one shot learning. CoRR, abs/1606.04080, 2016. URL http://arxiv. org/abs/1606.04080.
Ye-Yi Wang, Li Deng, and Alex Acero. Spoken language understanding an introduction to the statistical framework. IEEE Signal Processing Magazine, 22/5:16­31, January 2005. URL https://www.microsoft.com/en-us/research/publication/ spoken-language-understanding-an-introduction-to-the-statistical-framework/.
Tsung-Hsien Wen, Milica Gasic, Nikola Mrksic, Pei-Hao Su, David Vandyke, and Steve Young. Semantically conditioned lstm-based natural language generation for spoken dialogue systems. arXiv preprint arXiv:1508.01745, 2015.
11

Under review as a conference paper at ICLR 2019
Tsung-Hsien Wen, Milica Gasic´, Nikola Mrksic´, Lina M. Rojas-Barahona, Pei-Hao Su, Stefan Ultes, David Vandyke, and Steve Young. Conditional generation and snapshot learning in neural dialogue systems. arXiv preprint: 1606.03352, June 2016a.
Tsung-Hsien Wen, Milica Gasic´, Nikola Mrksic´, Lina Rojas-Barahona, Pei-Hao Su, David Vandyke, and Steve Young. Multi-domain neural network language generation for spoken dialogue systems. In Proceedings of NAACL-HLT, 2016b.
Tsung-Hsien Wen, David Vandyke, Nikola Mrksic´, Milica Gasic´, Lina M. Rojas-Barahona, Pei-Hao Su, Stefan Ultes, and Steve Young. A network-based end-to-end trainable task-oriented dialogue system. arXiv preprint: 1604.04562, April 2016c.
Majid Yazdani and James Henderson. A model of zero-shot learning of spoken language understanding. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pp. 244­249. Association for Computational Linguistics, 2015. doi: 10.18653/v1/D15-1027. URL http://www.aclweb.org/anthology/D15-1027.
Steve Young. Talking to machines (statistically speaking). 2002. Tiancheng Zhao and Maxine Eske´nazi. Zero-shot dialog generation with cross-domain latent ac-
tions. CoRR, abs/1805.04803, 2018. URL http://arxiv.org/abs/1805.04803.
12


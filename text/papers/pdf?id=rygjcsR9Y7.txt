Under review as a conference paper at ICLR 2019
DEEP SELF-ORGANIZATION: INTERPRETABLE DISCRETE REPRESENTATION LEARNING ON TIME SERIES
Anonymous authors Paper under double-blind review
ABSTRACT
1 High-dimensional time series are common in many domains. Since human cognition is 2 not optimized to work well in high-dimensional spaces, these areas could benefit from 3 interpretable low-dimensional representations. However, most representation learning 4 algorithms for time series data are difficult to interpret. This is due to non-intuitive mappings 5 from data features to salient properties of the representation and non-smoothness over time. 6 To address this problem, we propose a new representation learning framework building on 7 ideas from interpretable discrete dimensionality reduction and deep generative modeling. 8 This framework allows us to learn discrete representations of time series, which give rise to 9 smooth and interpretable embeddings with superior clustering performance. We introduce 10 a new way to overcome the non-differentiability in discrete representation learning and 11 present a gradient-based version of the traditional self-organizing map algorithm that is 12 more performant than the original. Furthermore, to allow for a probabilistic interpretation of 13 our method, we integrate a Markov model in the representation space. This model uncovers 14 the temporal transition structure, improves clustering performance even further and provides 15 additional explanatory insights as well as a natural representation of uncertainty. 16 We evaluate our model in terms of clustering performance and interpretability on static 17 (Fashion-)MNIST data, a time series of linearly interpolated (Fashion-)MNIST images, a 18 chaotic Lorenz attractor system with two macro states, as well as on a challenging real world 19 medical time series application on the eICU data set. Our learned representations compare 20 favorably with competitor methods and facilitate downstream tasks on the real world data.
121 INTRODUCTION
22 Interpretable representation learning on time series is a seminal problem for uncovering the latent structure 23 in complex systems, such as chaotic dynamical systems or medical time series. In areas where humans 24 have to make decisions based on large amounts of data, interpretability is fundamental to ease the human 25 task. Especially when decisions have to be made in a timely manner and rely on observing some chaotic 26 external process over time, such as in finance or medicine, the need for intuitive interpretations is even stronger. 27 However, many unsupervised methods, such as clustering, make misleading i.i.d. assumptions about the data, 28 neglecting their rich temporal structure and smooth behaviour over time. This poses the need for a method 29 of clustering, where the clusters assume a topological structure in a lower dimensional space, such that the 30 representations of the time series retain their smoothness in that space. In this work, we present a method with 31 these properties.
32 We choose to employ deep neural networks, because they have a very successful tradition in representation 33 learning (Bengio et al., 2012). In recent years, they have increasingly been combined with generative modeling 34 through the advent of generative adversarial networks (GANs) (Goodfellow et al., 2014) and variational 35 autoencoders (VAEs) (Kingma and Welling, 2013). However, the representations learned by these models are 36 often considered cryptic and do not offer the necessary interpretability (Chen et al., 2016). A lot of work has 37 been done to improve them in this regard, in GANs (Chen et al., 2016) as well as VAEs (Higgins et al., 2017; 38 Esmaeili et al., 2018). Alas, these works have focused entirely on continuous representations, while discrete 39 ones are still underexplored.
40 In order to define temporal smoothness in a discrete representation space, the space has to be equipped with 41 a topological neighborhood relationship. One type of representation space with such a structure is induced 42 by the self-organizing map (SOM) (Kohonen, 1998). The SOM allows to map states from an uninterpretable 43 continuous space to a lower-dimensional space with a predefined topologically interpretable structure, such as
1

Under review as a conference paper at ICLR 2019

44 an easily visualizable two-dimensional grid. However, while yielding promising results in static patient state 45 visualization (Tirunagari et al., 2015), the classical SOM formulation does not offer a notion of time. The 46 time component can be incorporated using a probabilistic transition model, e.g. a Markov model, such that the 47 representations of a single time point are enriched with information from the adjacent time points in the series. 48 It is therefore potentially fruitful to apply the approaches of probabilistic modeling alongside representation 49 learning and discrete dimensionality reduction in an end-to-end model.
50 In this work, we propose a novel deep architecture that learns topologically interpretable discrete representa51 tions in a probabilistic fashion. Moreover, we introduce a new method to overcome the non-differentiability 52 in discrete representation learning architectures and develop a gradient-based version of the classical self53 organizing map algorithm with improved performance. We present extensive empirical evidence for the 54 model's performance on synthetic and real world time series from benchmark data sets, a synthetic dynamical 55 system with chaotic behavior and real world medical data.
56 Our main contributions are to
57 · Devise a novel framework for interpretable discrete representation learning on time series.
58 · Show that the latent probabilistic model in the representation learning architecture improves clustering 59 and interpretability of the representations on time series.
60 · Show superior clustering performance of the model on benchmark data and a real world medical data 61 set, on which it also facilitates downstream tasks.

262 PROBABILISTIC SOM-VAE
63 Our proposed model combines ideas from self-organizing maps (Kohonen, 1998), variational autoencoders 64 (Kingma and Welling, 2013) and probabilistic models. In the following, we will lay out the different 65 components of the model and their interactions.
66 2.1 INTRODUCING TOPOLOGICAL STRUCTURE IN THE LATENT SPACE
67 A schematic overview of our proposed model is depicted in Figure 1. An input x  Rd is mapped to a latent 68 encoding ze  Rm (usually m < d) by computing ze = f(x), where f(·) is parameterized by the encoder 69 neural network. The encoding is then assigned to an embedding zq  Rm in the dictionary of embeddings 70 E = {e1, . . . , ek | ei  Rm} by sampling zq  p(zq|ze). The form of this distribution is flexible and can be a 71 design choice. In order for the model to behave similarly to the original SOM algorithm (see below), in our 72 experiments we choose the distribution to be categorical with probability mass 1 on the closest embedding to 73 ze, i.e. p(zq|ze) = 1[zq = arg mineE ze - e 2], where 1[·] is the indicator function. A reconstruction x^ of 74 the input can then be computed as x^ = g(z), where g(·) is parameterized by the decoder neural network. 75 Since the encodings and embeddings live in the same space, one can compute two different reconstructions, 76 namely x^e = g(ze) and x^q = g(zq).

input

encoder latent encodings

Markov model decoder

self-organizing map

zqt

z t+1 e
z t+1 q

zet P(zqt+1|zqt)

reconstruction

Figure 1: Schematic overview of our model architecture. Time series from the data space [green] are encoded by a neural network [black] time-point-wise into the latent space. The latent data manifold is approximated with a self-organizing map (SOM) [red]. In order to achieve a discrete representation, every latent data point (ze) is mapped to its closest node in the SOM (zq). A Markov transition model [blue] is learned to predict the next discrete representation (zqt+1) given the current one (zqt). The discrete representations can then be decoded by another neural network back into the original data space.
2

Under review as a conference paper at ICLR 2019

77 To achieve a topologically interpretable neighborhood structure, the embeddings are connected to form 78 a self-organizing map. A self-organizing map consists of k nodes V = {v1, . . . , vk}, where every node 79 corresponds to an embedding in the data space ev  Rd and a representation in a lower-dimensional discrete 80 space mv  M , where usually M  N2. During training on a data set D = {x1, . . . , xn}, a winner node 81 v~ is chosen for every point xi according to v~ = arg minvV ev - xi 2. The embedding vector for every 82 node u  V is then updated according to eu  eu + N (mu, mv~)(xi - eu), where  is the learning rate and 83 N (mu, mv~) is a neighborhood function between the nodes defined on the representation space M . There can 84 be different design choices for N (mu, mv~).

85 We choose to use a two-dimensional SOM because it facilitates visualization similar to Tirunagari et al. (2015).
86 Since we want the architecture to be trainable end-to-end, we cannot use the standard SOM training algorithm
87 described above. Instead, we devise a loss function term whose gradient corresponds to a weighted version of 88 the original SOM update rule (see below). We implement it in such a way that any time an embedding ei,j 89 at position (i, j) in the map gets updated, it also updates all the embeddings in its immediate neighborhood 90 N (ei,j). The neighborhood is defined as N (ei,j) = {ei-1,j, ei+1,j, ei,j-1, ei,j+1} for a two-dimensional 91 map.

92 The loss function for a single input x looks like LSOM-VAE(x, x^q, x^e) = Lreconstruction(x, x^q, x^e) +  Lcommitment(x) +  LSOM(x)
93 where x, ze, zq, x^e and x^q are defined as above and  and  are weighting hyperparameters.

(1)

94 Every term in this function is specifically designed to optimize a different model component. The first term is 95 the reconstruction loss Lreconstruction(x, x^q, x^e) = x-x^q 2 + x-x^e 2. The first subterm of this is the discrete 96 reconstruction loss, which encourages the assigned SOM node zq(x) to be an informative representation of 97 the input. The second subterm encourages the encoding ze(x) to also be an informative representation. This 98 ensures that all parts of the model have a fully differentiable credit assignment path to the loss function, which
99 facilitates training. Note that the reconstruction loss corresponds to the evidence lower bound (ELBO) of the
100 VAE part of our model (Kingma and Welling, 2013). Since we assume a uniform prior over zq, the KL-term in 101 the ELBO is constant w.r.t. the parameters and can be ignored during optimization.

102 The term Lcommitment encourages the encodings and assigned SOM nodes to be close to each other and is 103 defined as Lcommitment(x) = ze(x) - zq(x) 2. Closeness of encodings and embeddings should be expected 104 to already follow from the Lreconstruction term in a fully differentiable architecture. However, due to the non105 differentiability of the embedding assignment in our model, the Lcommitment term has to be explicitly added to
106 the objective in order for the encoder to get gradient information about zq.

107 The SOM loss LSOM is defined as LSOM(x) = e~N(zq(x)) e~ - sg[ze(x)] 2, where N (·) is the set of 108 neighbors in the discrete space as defined above and sg[·] is the gradient stopping operator that does not 109 change the outputs during the forward pass, but sets the gradients to 0 during the backward pass. It encourages 110 the neighbors of the assigned SOM node zq to also be close to ze, thus enabling the embeddings to exhibit 111 a self-organizing map property, while stopping the gradients on ze such that the encoding is not pulled in 112 the direction of the neighbors. This term enforces a neighborhood relation between the discrete codes and 113 encourages all SOM nodes to ultimately receive gradient information from the data. The gradient stopping in 114 this term is motivated by the observation that the data points themselves do not get moved in the direction of 115 their assigned SOM node's neighbors in the original SOM algorithm either (see above). We want to optimize 116 the embeddings based on their neighbors, but not the respective encodings, since any single encoding should be 117 as close as possible to its assigned embedding and not receive gradient information from any other embeddings 118 that it is not assigned to. Note that the gradient update of a specific SOM node in this formulation depends on 119 its distance to the encoding, while the step size in the original SOM algorithm is constant. It will be seen that 120 this offers some benefits in terms of optimization and convergence (see Sec. 4.1).

121 2.2 OVERCOMING THE NON-DIFFERENTIABILITY
122 The main challenge in optimizing our architecture is the non-differentiability of the discrete cluster assignment 123 step. Due to this, the gradients from the reconstruction loss cannot flow back into the encoder. A model with a 124 similar problem is the recently proposed vector-quantized VAE (VQ-VAE) (van den Oord et al., 2017). It can 125 be seen as being similar to a special case of our SOM-VAE model, where one sets  = 0, i.e. disables the 126 SOM structure.
127 In order to mitigate the non-differentiability, the authors of the VQ-VAE propose to copy the gradients from 128 zq to ze. They acknowledge that this is an ad hoc approximation, but observed that it works well in their

3

Under review as a conference paper at ICLR 2019

129 experiments. Due to our smaller number of embeddings compared to the VQ-VAE setup, the average distance 130 between an encoding and its closest embedding is much larger in our case. The gradient copying (see above) 131 thus ceases to be a feasible approximation, because the true gradients at points in the latent space which are 132 farther apart will likely be very different.
133 In order to still overcome the non-differentiability issue, we propose to add the second reconstruction subterm 134 to Lreconstruction, where the reconstruction x^e is decoded directly from the encoding ze. This adds a fully 135 differentiable credit assignment path from the loss to the encoder and encourages ze to also be an informative 136 representation of the input, which is a desirable model feature. Most importantly, it works well in practice (see 137 Sec. 4.1).
138 Note that since ze is continuous and therefore much less constrained than zq, this term is optimized easily and 139 becomes small early in training. After that, mostly the zq-term contributes to Lreconstruction. One could therefore 140 view the ze-term as an initial encouragement to place the data encodings at sensible positions in the latent 141 space, after which the actual clustering task dominates the training objective.

142 2.3 ENCOURAGING SMOOTHNESS OVER TIME

143 Our ultimate goal is to predict the development of time series in an interpretable way. This means that not only 144 the state representations should be interpretable, but so should be the prediction as well. To this end, we use a 145 temporal probabilistic model.

146 Learning a probabilistic model in a high-dimensional continuous space can be challenging. Thus, we exploit 147 the low-dimensional discrete space induced by our SOM to learn a temporal model. For that, we define a 148 system state as the assigned node in the SOM and then learn a Markov model for the transitions between those 149 states. The model is learned jointly with the SOM-VAE, where the loss function becomes

L(xt-1, xt, x^qt , x^et ) = LSOM-VAE(xt, x^qt , x^et ) +  Ltransitions(xt-1, xt) +  Lsmoothness(xt-1, xt)

(2)

150 with weighting hyperparameters  and  .

151 The term Ltransitions encourages the probabilities of actually observed transitions to be high. It is defined as 152 Ltransitions(xt-1, xt) = - log PM (zq(xt-1)  zq(xt)), with PM (zq(xt-1)  zq(xt)) being the probability 153 of a transition from state zq(xt-1) to state zq(xt) in the Markov model.
154 The term Lsmoothness encourages the probabilities for transitions to nodes that are far away from the current 155 data point to be low or respectively the nodes with high transition probabilities to be proximal. It achieves this 156 by taking large values only for transitions to far away nodes that have a high probability under the model. It is 157 defined as Lsmoothness(xt-1, xt) = EPM (zq(xt-1)e~) e~ - ze(xt) 2 . The probabilistic model can inform the 158 evolution of the SOM through this term which encodes our prior belief that transitions in natural data happen 159 smoothly and that future time points will therefore mostly be found in the neighborhood of previous ones. In a 160 setting where the data measurements are noisy, this improves the clustering by acting as a temporal smoother.

3161 RELATED WORK
162 From the early inception of the k-means algorithm for clustering (Lloyd, 1982), there has been much method163 ological improvement on this unsupervised task. This includes methods that perform clustering in the latent 164 space of (variational) autoencoders (Aljalbout et al., 2018) or use a mixture of autoencoders for the clustering 165 (Zhang et al., 2017; Locatello et al., 2018). The method most related to our work is the VQ-VAE (van den 166 Oord et al., 2017), which can be seen as a special case of our framework (see above). Its authors have put 167 a stronger focus on the discrete representation as a form of compression instead of clustering. Hence, our 168 model and theirs differ in certain implementation considerations (see Sec. 2.2). All these methods have in 169 common that they only yield a single number as a cluster assignment and provide no interpretable structure of 170 relationships between clusters.
171 The self-organizing map (SOM) (Kohonen, 1998), however, is an algorithm that provides such an interpretable 172 structure. It maps the data manifold to a lower-dimensional discrete space, which can be easily visualized in 173 the 2D case. It has been extended to model dynamical systems (Barreto and Araújo, 2004) and combined with 174 probabilistic models for time series (Sang et al., 2008), although without using learned representations. There 175 are approaches to turn the SOM into a "deeper" model (Dittenbach et al., 2000), combine it with multi-layer 176 perceptrons (Furukawa et al., 2005) or with metric learning (Plon´ski and Zaremba, 2014). However, it has (to

4

Under review as a conference paper at ICLR 2019
Figure 2: Images generated from a section of the SOM-VAE's latent space with 512 embeddings trained on MNIST. It yields a discrete two-dimensional representation of the data manifold in the higher-dimensional latent space.
177 the best of our knowledge) not been proposed to use SOMs in the latent space of (variational) autoencoders or 178 any other form of unsupervised deep learning model. 179 Interpretable models for clustering and temporal predictions are especially crucial in fields where humans 180 have to take responsibility for the model's predictions, such as in health care. The prediction of a patient's 181 future state is an important problem, particularly on the intensive care unit (ICU) (Harutyunyan et al., 2017; 182 Badawi et al., 2018). Probabilistic models, such as Gaussian processes, have been successfully applied in this 183 domain (Colopy et al., 2016; Schulam and Arora, 2016). Recently, deep generative models have been proposed 184 (Esteban et al., 2017), sometimes even in combination with probabilistic modeling (Lim and van der Schaar, 185 2018). To the best of our knowledge, SOMs have only been used to learn interpretable static representations of 186 patients (Tirunagari et al., 2015), but not dynamic ones.
4187 EXPERIMENTS
188 We performed experiments on MNIST handwritten digits (LeCun et al., 1998), Fashion-MNIST images of 189 clothing (Xiao et al., 2017), synthetic time series of linear interpolations of those images, time series from 190 a chaotic dynamical system and real world medical data from the eICU Collaborative Research Database 191 (Goldberger et al., 2000). If not otherwise noted, we use the same architecture for all experiments, sometimes 192 including the latent probabilistic model (SOM-VAE_prob) and sometimes excluding it (SOM-VAE). For model 193 implementation details, we refer to the appendix (Sec. A). 194 We found that our method achieves a superior clustering performance compared to other methods. We also 195 show that we can learn a temporal probabilistic model concurrently with the clustering, which is on par 196 with the maximum likelihood solution, while improving the clustering performance. Moreover, we can learn 197 interpretable state representations of a chaotic dynamical system and discover patterns in real medical data.
198 4.1 CLUSTERING ON MNIST AND FASHION-MNIST
199 In order to test the clustering component of the SOM-VAE, we performed experiments on MNIST and Fashion200 MNIST. We compare our model (including different adjustments to the loss function) against k-means (Lloyd, 201 1982) (sklearn-package (Pedregosa et al., 2011)), the VQ-VAE (van den Oord et al., 2017), a standard 202 implementation of a SOM (minisom-package (Vettigli, 2017)) and our version of a GB-SOM (gradient-based 203 SOM), which is a SOM-VAE where the encoder and decoder are set to be identity functions. 204 The results of the experiment in terms of purity and normalized mutual information (NMI) are shown in Table 205 1. The SOM-VAE outperforms the other methods w.r.t. the clustering performance measures. It should be 206 noted here that while k-means is a strong baseline, it is not density matching, i.e. the density of cluster centers 207 is not proportional to the density of data points. Hence, the representation of data in a space induced by the 208 k-means clusters can be misleading. 209 As argued in the appendix (Sec. B), NMI is a more balanced measure for clustering performance than purity. 210 If one uses 512 embeddings in the SOM, one gets a lower NMI due to the penalty term for the number of 211 clusters, but it yields an interpretable two-dimensional representation of the manifolds of MNIST (Fig. 2, Supp. 212 Fig. S1) and Fashion-MNIST (Supp. Fig. S2). 213 The experiment shows that the SOM in our architecture improves the clustering (SOM-VAE vs. VQ-VAE) 214 and that the VAE does so as well (SOM-VAE vs. GB-SOM). Both parts of the model therefore seem to be 215 beneficial for our task. It also becomes apparent that our reconstruction loss term on ze works better in practice 216 than the gradient copying trick from the VQ-VAE (SOM-VAE vs. gradcopy), due to the reasons described in 217 Section 2.2. If one removes the ze reconstruction loss and does not copy the gradients, the encoder network
5

Under review as a conference paper at ICLR 2019

Table 1: Performance comparison of our method and some baselines in terms of purity and normalized mutual information on different benchmark data sets. The methods marked with an asterisk are variants of our proposed method. The values are the means of 10 runs and the respective standard errors. Each method was used to fit 16 embeddings/clusters.

Method
k-means minisom GB-SOM VQ-VAE no_grads* gradcopy* SOM-VAE*

MNIST

Purity

NMI

0.690 ± 0.000 0.406 ± 0.006 0.653 ± 0.007 0.538 ± 0.067 0.114 ± 0.000 0.583 ± 0.004 0.731 ± 0.004

0.541 ± 0.001 0.342 ± 0.012 0.519 ± 0.005 0.409 ± 0.065 0.001 ± 0.000 0.436 ± 0.004 0.594 ± 0.004

Fashion-MNIST

Purity

NMI

0.654 ± 0.001 0.413 ± 0.006 0.606 ± 0.006 0.611 ± 0.006 0.110 ± 0.009 0.556 ± 0.008 0.678 ± 0.005

0.545 ± 0.000 0.475 ± 0.002 0.514 ± 0.004 0.517 ± 0.002 0.018 ± 0.016 0.444 ± 0.005 0.590 ± 0.003

218 does not receive any gradient information any more and the learning fails completely (no_grads). Another 219 interesting observation is that stochastically optimizing our SOM loss using Adam (Kingma and Ba, 2015) 220 seems to discover a more performant solution than the classical SOM algorithm (GB-SOM vs. minisom). 221 This could be due to the dependency of the step size on the distance between embeddings and encodings, 222 as described in Section 2.1. Since k-means seems to be the strongest competitor, we are including it as a 223 reference baseline in the following experiments as well.
224 4.2 MARKOV TRANSITION MODEL ON THE DISCRETE REPRESENTATIONS
225 In order to test the probabilistic model in our architecture and its effect on the clustering, we generated 226 synthetic time series data sets of (Fashion-)MNIST images being linearly interpolated into each other. Each 227 time series consists of 64 frames, starting with one image from (Fashion-)MNIST and smoothly changing 228 sequentially into four other images over the length of the time course.
229 After training the model on these data, we constructed the maximum likelihood estimate (MLE) for the Markov 230 model's transition matrix by fixing all the weights in the SOM-VAE and making another pass over the training 231 set, counting all the observed transitions. This MLE transition matrix reaches a negative log likelihood of 0.24, 232 while our transition matrix, which is learned concurrently with the architecture, yields 0.25. Our model is 233 therefore on par with the MLE solution.
234 Comparing these results with the clustering performance on the standard MNIST and Fashion-MNIST test sets, 235 we observe that the performance in terms of NMI is not impaired by the inclusion of the probabilistic model 236 into the architecture (Tab. 2). On the contrary, the probabilistic model even slightly increases the performance 237 on Fashion-MNIST. Note that we are using 64 embeddings in this experiment instead of 16, leading to a 238 higher clustering performance in terms of purity, but a slightly lower performance in terms of NMI compared 239 to Table 1. This shows again that the measure of purity has to be interpreted with care when comparing 240 different experimental setups and that therefore the normalized mutual information should be preferred to 241 make quantitative arguments.
242 This experiment shows that we can indeed fit a valid probabilistic transition model concurrently with the 243 SOM-VAE training, while at the same time not hurting the clustering performance. It also shows that for 244 certain types of data the clustering performance can even be improved by the probabilistic model (see Sec. 245 2.3).
246 4.3 INTERPRETABLE REPRESENTATIONS OF CHAOTIC TIME SERIES
247 In order to assess whether our model can learn an interpretable representation of more realistic chaotic time 248 series, we train it on synthetic trajectories simulated from the famous Lorenz system (Lorenz, 1963). The 249 Lorenz system is a good example for this assessment, since it offers two well defined macro-states (given by 250 the attractor basins) which are occluded by some chaotic noise in the form of periodic fluctuations around the 251 attractors. A good interpretable representation should therefore learn to largely ignore the noise and model the 252 changes between attractor basins. For a review of the Lorenz system and details about the simulations and the 253 performance measure, we refer to the appendix (Sec. C.1).
6

Under review as a conference paper at ICLR 2019

Table 2: Performance comparison of the SOM-VAE with and without latent Markov model (SOM-VAEprob) against k-means in terms of purity and normalized mutual information on different benchmark data sets. The values are the means of 10 runs and the respective standard errors. Each method is used to fit 64 embeddings/clusters.

Method
k-means SOM-VAE SOM-VAE-prob

MNIST

Purity

NMI

0.791 ± 0.005 0.537 ± 0.001 0.868 ± 0.003 0.595 ± 0.002 0.858 ± 0.004 0.596 ± 0.001

Fashion-MNIST

Purity

NMI

0.703 ± 0.002 0.492 ± 0.001 0.739 ± 0.002 0.520 ± 0.002 0.724 ± 0.003 0.525 ± 0.002

254 In order to compare the interpretability of the learned representations, we computed entropy distributions over 255 simulated subtrajectories in the real system space, the attractor assignment space and the representation spaces 256 for k-means and our model. The computed entropy distributions over all subtrajectories in the test set are 257 depicted in Figure 3.

1.2 1.0 0.8 0.6 0.4 0.2 0.0
1.5 2.0 2.5 3.0 3.5 4.0 4.5

10 8 6 4 2 0 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7

2.5 2.0 1.5 1.0 0.5 0.0
0.0 0.5 1.0 1.5 2.0

(a) Lorenz attrac- (b) Real space (c) Attractor assig- (d) SOM-VAE tor ment

0.7 0.6 0.5 0.4 0.3 0.2 0.1 0.0
0.0 0.5 1.0 1.5 2.0 2.5 3.0
(e) k-means

1.2 1.0 0.8 0.6 0.4 0.2 0.0
0.0 0.5 1.0 1.5 2.0
(f) Simulated trajectories

Figure 3: Histograms of entropy distributions (entropy on the x-axes) over all Lorenz attractor subtrajectories [a] of 100 time steps length in our test set. Subtrajectories without a change in attractor basin are colored in blue, the ones where a change has taken place in green.

258 The experiment shows that the SOM-VAE representations (Fig. 3d) are much closer in entropy to the ground259 truth attractor basin assignments (Fig. 3c) than the k-means representations (Fig. 3e). For most of the 260 subtrajectories without attractor basin change they assign a very low entropy, effectively ignoring the noise, 261 while the k-means representations partially assign very high entropies to those trajectories. In total, the 262 k-means representations' entropy distribution is similar to the entropy distribution in the noisy system space 263 (Fig. 3b). The representations learned by the SOM-VAE are therefore more interpretable than the k-means 264 representations with regard to this interpretability measure. As could be expected from these figures, the 265 SOM-VAE representation is also superior to the k-means one in terms of purity with respect to the attractor 266 assignment (0.979 vs. 0.956) as well as NMI (0.577 vs. 0.249).
267 Finally, we use the learned probabilistic model on our SOM-VAE representations to sample new latent system 268 trajectories and compute their entropies. The distribution looks qualitatively similar to the one over real 269 trajectories (Fig. 3), but our model slightly overestimates the attractor basin change probabilities, leading to a 270 heavier tail of the distribution.

271 4.4 LEARNING REPRESENTATIONS OF REAL MEDICAL TIME SERIES
272 In order to demonstrate interpretable representation learning on a complex real world task, we trained our 273 model on vital sign time series measurements of intensive care unit (ICU) patients. We analyze the performance 274 of the resulting clustering w.r.t. the patients' future physiology states in Table 3. This can be seen as a way to 275 assess the representations' informativeness for a downstream prediction task. For details regarding the data 276 selection and processing, we refer to the appendix (Sec. C.2).
277 Our full model (including the latent Markov model) performs best on the given tasks, i.e. better than k-means 278 and also better than the SOM-VAE without probabilistic model. This could be due to the noisiness of the 279 medical data and the probabilistic model's smoothing tendency (see Sec. 2.3).
280 In order to qualitatively assess the interpretability of the probabilistic SOM-VAE, we analyzed the average 281 physiology score per cluster (Fig. 4). Our model exhibits clusters where higher scores are enriched compared

7

Under review as a conference paper at ICLR 2019

Table 3: Performance comparison of our method with and without probabilistic model (SOM-VAE-prob and SOM-VAE) against k-means in terms of normalized mutual information on a challenging unsupervised prediction task on real eICU data. The dynamic endpoints are the maximum of the physiology score within the next 6, 12 or 24 hours (physiology_6_hours, physiology_12_hours, physiology_24_hours). The values are the means of 10 runs and the respective standard errors. Each method is used to fit 64 embeddings/clusters.

Method
k-means SOM-VAE SOM-VAE-prob

physiology_6_hours
0.0411 ± 0.0007 0.0407 ± 0.0005 0.0474 ± 0.0006

physiology_12_hours
0.0384 ± 0.0006 0.0376 ± 0.0004 0.0444 ± 0.0006

physiology_24_hours
0.0366 ± 0.0005 0.0354 ± 0.0004 0.0421 ± 0.0005

(a) k-means

(b) VQ-VAE

(c) SOM-VAE-prob (d) Patient trajectories

Figure 4: Comparison of the patient state representations learned by different models. The clusters are colored by degree of patient abnormality as measured by a variant of the APACHE physiology score (more yellow means "less healthy"). White squares correspond to unused clusters, i.e. clusters that contain less than 0.1 percent of the data points. Subfigure (d) shows two patient trajectories in the SOM-VAE-prob representation over their respective whole stays in the ICU. The dots mark the ICU admission, the stars the discharge from the ICU (cured [green] or dead [red]). It can be seen that our model is the only one that learns a topologically interpretable structure.

282 to the background level. Moreover, these clusters form compact structures, facilitating interpretability. We do 283 not observe such interpretable structures in the other methods.
284 As an illustrative example for data visualization using our method, we show the trajectories of two patients 285 that start in the same state. One patient (green) stays in the regions of the map with low average physiology 286 score and eventually gets discharged from the hospital healthily. The other one (red) moves into map regions 287 with high average physiology score and ultimately dies. Such knowledge could be helpful for doctors, who 288 could determine the risk of a patient for certain deterioration scenarios from a glance at their trajectory in the 289 SOM-VAE representation.
5290 CONCLUSION
291 The SOM-VAE can recover topologically interpretable state representations on time series and static data. It 292 provides an improvement to standard methods in terms of clustering performance and offers a way to learn 293 discrete two-dimensional representations of the data manifold in concurrence with the reconstruction task. 294 It introduces a new way of overcoming the non-differentiability of the discrete representation assignment 295 and contains a gradient-based variant of the traditional self-organizing map that is more performant than the 296 original one. On a challenging real world medical data set, our model learns more informative representations 297 with respect to medically relevant prediction targets than competitor methods. The learned representations can 298 be visualized in an interpretable way and could be helpful for clinicians to understand patients' health states 299 and trajectories more intuitively.
300 It will be interesting to see in future work whether the probabilistic component can be extended to not 301 just improve the clustering and interpretability of the whole model, but also enable us to make predictions. 302 Promising avenues in that direction could be to increase the complexity by applying a higher order Markov 303 Model, a Hidden Markov Model or a Gaussian Process. Another fruitful avenue of research could be to 304 find more theoretically principled ways to overcome the non-differentiability and compare them with the 305 empirically motivated ones. Lastly, one could explore deviating from the original SOM idea of fixing a latent 306 space structure, such as a 2D grid, and learn the neighborhood structure as a graph directly from data.
8

Under review as a conference paper at ICLR 2019
307 REFERENCES
308 Martín Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S. Corrado, 309 Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Ian Goodfellow, Andrew Harp, Geoffrey 310 Irving, Michael Isard, Yangqing Jia, Rafal Jozefowicz, Lukasz Kaiser, Manjunath Kudlur, Josh Levenberg, 311 Dan Mane, Rajat Monga, Sherry Moore, Derek Murray, Chris Olah, Mike Schuster, Jonathon Shlens, Benoit 312 Steiner, Ilya Sutskever, Kunal Talwar, Paul Tucker, Vincent Vanhoucke, Vijay Vasudevan, Fernanda Viegas, 313 Oriol Vinyals, Pete Warden, Martin Wattenberg, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. TensorFlow: 314 Large-Scale Machine Learning on Heterogeneous Distributed Systems. arXiv preprint arXiv:1603.04467, 315 2016. URL http://arxiv.org/abs/1603.04467.
316 Elie Aljalbout, Vladimir Golkov, Yawar Siddiqui, and Daniel Cremers. Clustering with Deep Learn317 ing: Taxonomy and New Methods. arXiv preprint arXiv:1801.07648, 2018. ISSN 1607-551X. doi: 318 10.1227/01.NEU.0000255452.20602.C9.
319 Omar Badawi, Xinggang Liu, Erkan Hassan, Pamela J. Amelung, and Sunil Swami. Evaluation of ICU Risk 320 Models Adapted for Use as Continuous Markers of Severity of Illness Throughout the ICU Stay. Critical 321 Care Medicine, 46(3):361­367, 2018. doi: 10.1097/CCM.0000000000002904.
322 Guilherme A. Barreto and Aluizio F. R. Araújo. Identification and Control of Dynamical Systems Using the 323 Self-Organizing Map. IEEE Transactions on Neural Networks, 15(5):1244­1259, 2004. ISSN 1045-9227. 324 doi: 10.1109/TNN.2004.832825. URL http://www.ncbi.nlm.nih.gov/pubmed/18238091.
325 Yoshua Bengio, Aaron Courville, and Pascal Vincent. Representation Learning: A Re326 view and New Perspectives. IEEE Transactions on Pattern Analysis and Machine Intelli327 gence, 35(8):1798­1828, 2012. ISSN 1939-3539. doi: 10.1109/TPAMI.2013.50. URL 328 http://www.ncbi.nlm.nih.gov/pubmed/23459267.
329 Xi Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya Sutskever, and Pieter Abbeel. InfoGAN: In330 terpretable Representation Learning by Information Maximizing Generative Adversarial Nets. arXiv 331 preprint arXiv:1606.03657, 2016. ISSN 978-3-319-16807-4. doi: 10.1007/978-3-319-16817-3. URL 332 http://arxiv.org/abs/1606.03657.
333 Glen Wright Colopy, Marco A F Pimentel, Steven J Roberts, and David A Clifton. Bayesian Gaussian 334 Processes for Identifying the Deteriorating Patient. Annual International Conference of the Engineering in 335 Medicine and Biology Society, pages 5311­5314, 2016. doi: 10.1109/EMBC.2016.7591926.
336 M. Dittenbach, D. Merkl, and A. Rauber. The growing hierarchical self-organizing map. In Proceedings of the 337 International Joint Conference on Neural Networks, pages 15­19. IEEE, 2000. ISBN 0-7695-0619-4. doi: 338 10.1109/IJCNN.2000.859366. URL http://ieeexplore.ieee.org/document/859366/.
339 Babak Esmaeili, Hao Wu, Sarthak Jain, N. Siddharth, Brooks Paige, and Jan-Willem van de 340 Meent. Hierarchical Disentangled Representations. arXiv preprint arXiv:1804.02086, 2018. URL 341 http://arxiv.org/abs/1804.02086.
342 Cristóbal Esteban, Stephanie L. Hyland, and Gunnar Rätsch. Real-valued (Medical) Time Series Generation 343 with Recurrent Conditional GANs. arXiv preprint arXiv:1706.02633, 2017. doi: 10.1002/fut.
344 Tetsuo Furukawa, Kazuhiro Tokunaga, Kenji Morishita, and Syozo Yasui. Modular Network SOM (mnSOM): 345 From Vector Space to Function Space. Proceedings of the International Joint Conference on Neural 346 Networks, 3:1581­1586, 2005. doi: 10.1109/IJCNN.2005.1556114.
347 Ary L Goldberger, Luis A N Amaral, Leon Glass, Jeffrey M Hausdorff, Plamen Ch Ivanov, Roger G Mark, 348 Joseph E Mietus, George B Moody, Chung-Kang Peng, and H Eugene Stanley. PhysioBank, PhysioToolkit, 349 and PhysioNet. Components of a New Research Resource for Complex Physiologic Signals. Circulation, 350 101(23), 2000.
351 Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, 352 Aaron Courville, and Yoshua Bengio. Generative Adversarial Nets. In Advances in Neu353 ral Information Processing Systems, pages 2672­2680, 2014. ISBN 1406.2661. URL 354 http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf.
9

Under review as a conference paper at ICLR 2019

355 Klaus Greff, Aaron Klein, Martin Chovanec, Frank Hutter, and Jürgen Schmidhuber. The Sacred Infrastructure 356 for Computational Research. In Proceedings of the 15th Python in Science Conference, 2017. URL 357 http://ml.informatik.uni-freiburg.de/papers/17-SciPy-Sacred.pdf.

358 Hrayr Harutyunyan, Hrant Khachatrian, David C Kale, and Aram Galstyan. Multitask Learning and Bench359 marking with Clinical Time Series Data. arXiv preprint arXiv:1703.07771, 2017. ISSN 16130073. doi: 360 10.1145/nnnnnnn.nnnnnnn.

361 Irina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick, Shakir 362 Mohamed, and Alexander Lerchner. beta-VAE: Learning Basic Visual Concepts with a Constrained 363 Variational Framework. In Proceedings of the International Conference on Learning Representations, 2017. 364 URL https://openreview.net/forum?id=Sy2fzU9gl.

365 Diederik P. Kingma and Jimmy Ba. Adam: A Method for Stochastic Optimization. In 366 Proceedings of the International Conference on Learning Representations, 2015. ISBN 367 9781450300728. doi: http://doi.acm.org.ezproxy.lib.ucf.edu/10.1145/1830483.1830503. URL 368 http://arxiv.org/abs/1412.6980.

369 Diederik P Kingma and Max Welling. Auto-Encoding Variational Bayes. arXiv preprint arXiv:1312.6114, 370 2013. ISSN 1312.6114v10. URL http://arxiv.org/abs/1312.6114.

371 Aaron Klein, Stefan Falkner, Numair Mansur, and Frank Hutter. RoBO: A Flexible and Robust 372 Bayesian Optimization Framework in Python. In NIPS Bayesian Optimization Workshop, 2017. URL 373 https://bayesopt.github.io/papers/2017/22.pdf.

374 W A Knaus, E A Draper, D P Wagner, and J E Zimmerman. APACHE II: a sever375 ity of disease classification system. Critical Care Medicine, 13(10):818­29, 1985. URL 376 http://www.ncbi.nlm.nih.gov/pubmed/3928249.

377 Teuvo Kohonen. The self-organizing map. Neurocomputing, 21(1-3):1­6, nov 1998. ISSN 0925-2312. doi: 378 10.1016/S0925-2312(98)00030-7.

379 Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-Based Learning Applied to 380 Document Recognition. Proceedings of the IEEE, 86(11):2278­2324, 1998. ISSN 00189219. doi: 381 10.1109/5.726791.

382 Bryan Lim and Mihaela van der Schaar. Disease-Atlas: Navigating Disease Trajectories using Deep Learning. 383 arXiv preprint arXiv:1803.10254, 2018. URL http://arxiv.org/abs/1803.10254.

384 Stuart Lloyd. Least squares quantization in PCM. IEEE Transactions on Information Theory, 28(2):129­137, 385 1982. doi: 10.1109/TIT.1982.1056489.

386 Francesco Locatello, Damien Vincent, Ilya Tolstikhin, Gunnar Rätsch, Sylvain Gelly, and Bernhard Schölkopf. 387 Clustering Meets Implicit Generative Models. arXiv preprint arXiv:1804.11130, 2018.

388 Edward N. Lorenz. Deterministic Nonperiodic Flow. Journal of the Atmospheric Sciences,

389 20(2):130­141, 1963.

doi: 10.1175/1520-0469(1963)020<0130:DNF>2.0.CO;2.

URL

390 http://journals.ametsoc.org/doi/abs/10.1175/1520-0469%281963%29020%3C0130%3ADNF%3E2.0.

391 Christopher D. Manning, Prabhakar Raghavan, and Hinrich Schütze. Introduction to Information Retrieval. 392 Cambridge University Press, 2008. ISBN 0521865719.

393 Fabian Pedregosa, Gaël Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier Grisel, 394 Mathieu Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, Jake Vanderplas, Alexandre Passos,
395 David Cournapeau, Matthieu Brucher, Matthieu Perrot, and Édouard Duchesnay. Scikit-learn: Machine 396 Learning in Python. Journal of Machine Learning Research, 12(Oct):2825­2830, 2011. ISSN 1533-7928.

397 Piotr Plon´ski and Krzysztof Zaremba. Improving Performance of Self-Organising Maps with Distance 398 Metric Learning Method. arXiv preprint arXiv:1407.1201, 2014. doi: 10.1007/978-3-642-29347-4_20. URL 399 http://arxiv.org/abs/1407.1201%0Ahttp://dx.doi.org/10.1007/978-3-642-29347-4_20.

400 Huiyan Sang, Alan E. Gelfand, Chris Lennard, Gabriele Hegerl, and Bruce Hewitson. Interpreting self401 organizing maps through space-time data models. Annals of Applied Statistics, 2(4):1194­1216, 2008. ISSN 402 19326157. doi: 10.1214/08-AOAS174.

10

Under review as a conference paper at ICLR 2019
403 Peter Schulam and Raman Arora. Disease Trajectory Maps. arXiv preprint arXiv:1606.09184, 2016. ISSN 404 10495258. URL http://arxiv.org/abs/1606.09184. 405 Santosh Tirunagari, Norman Poh, Guosheng Hu, and David Windridge. Identifying Similar Patients Using 406 Self-Organising Maps: A Case Study on Type-1 Diabetes Self-care Survey Responses. arXiv preprint 407 arXiv:1503.0631, 2015. 408 Warwick Tucker. The Lorenz attractor exists. Comptes Rendus de l'Académie des Sciences-Series I409 Mathematics, 328(12):1197­1202, 1999. ISSN 0764-4442. 410 Aaron van den Oord, Oriol Vinyals, and Koray Kavukcuoglu. Neural Discrete Representation Learning. arXiv 411 preprint arXiv:1711.00937, 2017. URL http://arxiv.org/abs/1711.00937. 412 Giuseppe Vettigli. MiniSom: a minimalistic implementation of the Self Organizing Maps, 2017. URL 413 https://github.com/JustGlowing/minisom. 414 Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-MNIST: a Novel Image Dataset for 415 Benchmarking Machine Learning Algorithms. arXiv preprint arXiv:1708.07747, 2017. URL 416 http://arxiv.org/abs/1708.07747. 417 Dejiao Zhang, Yifan Sun, Brian Eriksson, and Laura Balzano. Deep Unsupervised Clus418 tering Using Mixture of Autoencoders. arXiv preprint arXiv:1712.07788, 2017. URL 419 http://arxiv.org/abs/1712.07788.
11

Under review as a conference paper at ICLR 2019

420 APPENDIX
A421 IMPLEMENTATION DETAILS
422 The hyperparameters of our model were optimized using Robust Bayesian Optimization with the packages 423 sacred and labwatch (Greff et al., 2017) for the parameter handling and RoBo (Klein et al., 2017) for 424 the optimization, using the mean squared reconstruction error as the optimization criterion. Since our model 425 defines a general framework, some competitor models can be seen as special cases of our model, where 426 certain parts of the loss function are set to zero or parts of the architecture are omitted. We used the same 427 hyperparameters for those models. For external competitor methods, we used the hyperparameters from 428 the respective publications where applicable and otherwise the default parameters from their packages. The 429 models were implemented in TensorFlow (Abadi et al., 2016) and optimized using Adam (Kingma and Ba, 430 2015).

B431 CLUSTERING PERFORMANCE MEASURES

432 Given that one of our most interesting tasks at hand is the clustering of data, we need some performance 433 measures to objectively compare the quality of this clustering with other methods. The measures that 434 we decided to use and that have been used extensively in the literature are purity and normalized mutual 435 information (NMI) (Manning et al., 2008). We briefly review them in the following.

436 Let the set of ground truth classes in the data be C = {c1, c2, . . . , cJ } and the set of clusters that result from

437

the

algorithm



=

{1, 2, . . . , K }.

The

purity



is

then

defined

as

(C, )

=

1 N

K k=1

maxj

|k



cj |

438 where N is the total number of data points. Intuitively, the purity is the accuracy of the classifier that assigns

439 the most prominent class label in each cluster to all of its respective data points.

440 While the purity has a very simple interpretation, it also has some shortcomings. One can for instance easily 441 observe that a clustering with K = N , i.e. one cluster for every single data point, will yield a purity of 1.0 442 but still probably not be very informative for most tasks. It would therefore be more sensible to have another 443 measure that penalizes the number of clusters. The normalized mutual information is one such measure.

444

The

NMI

is

defined

as

NMI(C, )

=

2 I(C,) H (C )+H ()

where

I(C, )

is

the

mutual

information

between

C

and



445 and H(·) is the Shannon information entropy. While the entropy of the classes is a data-dependent constant,

446 the entropy of the clustering increases with the number of clusters. It can therefore be seen as a penalty term

447 to regularize the trade-off between low intra-cluster variance and a small number of clusters. Both NMI and

448 purity are normalized, i.e. take values in [0, 1].

C449 EXPERIMENTAL METHODS

450 C.1 INTERPRETABLE REPRESENTATIONS OF CHAOTIC TIME SERIES

The Lorenz system is the system of coupled ordinary differential equations defined by

dX = a(Y - X) dt

dY = X(b - Z) - Y dt

dZ = XY - cZ dt

451

with tuning parameters a, b and c.

For parameter choices a

=

10,

b

=

28 and c

=

8 3

,

the system shows

452 chaotic behavior by forming a strange attractor (Tucker, 1999) with the two attractor points being given by

453 p1,2 = [± c(b - 1), ± c(b - 1), b - 1]T .

454 We simulated 100 trajectories of 10,000 time steps each from the chaotic system and trained the SOM-VAE as 455 well as k-means on it with 64 clusters/embeddings respectively. The system chaotically switches back and 456 forth between the two attractor basins. By computing the Euclidian distance between the current system state 457 and each of the attractor points p1,2, we can identify the current attractor basin at each time point.

458 In order to assess the interpretability of the learned representations, we have to define an objective measure of 459 interpretability. We define interpretability as the similarity between the representation and the system's ground 460 truth macro-state. Since representations at single time points are meaningless with respect to this measure, we 461 compare the evolution of representations and system state over time in terms of their entropy.

12

Under review as a conference paper at ICLR 2019

462 We divided the simulated trajectories from our test set into spans of 100 time steps each. For every subtrajectory, 463 we computed the entropies of those subtrajectories in the real system space (macro-state and noise), the assigned 464 attractor basin space (noise-free ground-truth macro-state), the SOM-VAE representation and the k-means 465 representation. We also observed for every subtrajectory whether or not a change between attractor basins has 466 taken place. Note that the attractor assignments and representations are discrete, while the real system space is 467 continuous. In order to make the entropies comparable, we discretize the system space into unit hypercubes 468 for the entropy computation. For a representation R with assignments Rt at time t and starting time tstart of 469 the subtrajectory, the entropies are defined as

H (R, tstart) = H ({Rt | tstart  t < tstart + 100})

(3)

470 with H(·) being the Shannon information entropy of a discrete set.

471 C.2 LEARNING REPRESENTATIONS OF ACUTE PHYSIOLOGICAL STATES IN THE ICU
472 All experiments were performed on dynamic data extracted from the eICU Collaborative Research Database 473 (Goldberger et al., 2000). Irregularly sampled time series data were extracted from the raw tables and then 474 resampled to a regular time grid using a combination of forward filling and missing value imputation using 475 global population statistics. We chose a grid interval of one hour to capture the rapid dynamics of patients in 476 the ICU.
477 Each sample in the time-grid was then labeled using a dynamic variant of the APACHE score (Knaus et al., 478 1985), which is a proxy for the instantaneous physiological state of a patient in the ICU. Specifically, the 479 variables MAP, Temperature, Respiratory rate, HCO3, Sodium, Potassium, and Creatinine 480 were selected from the score definition, because they could be easily defined for each sample in the eICU 481 time series. The value range of each variable was binned into ranges of normal and abnormal values, in line 482 with the definition of the APACHE score, where a higher score for a variable is obtained for abnormally high 483 or low values. The scores were then summed up, and we define the predictive score as the worst (highest) 484 score in the next t hours, for t  {6, 12, 24}. Patients are thus stratified by their expected pathology in the 485 near future, which corresponds closely to how a physician would perceive the state of a patient. The training 486 set consisted of 7000 unique patient stays, while the test set contained 3600 unique stays.

13

Under review as a conference paper at ICLR 2019
Figure S1: Images generated from the SOM-VAE's latent space with 512 embeddings trained on MNIST. It yields an interpretable discrete two-dimensional representation of the data manifold in the higher-dimensional latent space.
14

Under review as a conference paper at ICLR 2019
Figure S2: Images generated from the SOM-VAE's latent space with 512 embeddings trained on MNIST. It yields an interpretable discrete two-dimensional representation of the data manifold in the higher-dimensional latent space.
15


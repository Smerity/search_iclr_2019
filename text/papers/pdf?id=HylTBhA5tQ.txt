Under review as a conference paper at ICLR 2019

THE LIMITATIONS OF ADVERSARIAL TRAINING AND THE BLIND-SPOT ATTACK
Anonymous authors Paper under double-blind review

ABSTRACT
The adversarial training procedure proposed by Madry et al. (2018) is one of the most effective methods to defend against adversarial examples on deep neuron networks (DNNs). Despite being very effective on MNIST, adversarial training on larger datasets like CIFAR and ImageNet achieves much worse results. In our paper, we shed some lights on the practicality and hardness of adversarial training by first showing that the effectiveness of adversarial training procedure on test set has a strong correlation with the distance between the test point and the manifold of training data. The test examples that are relatively far away from the manifold of training dataset are more likely to be vulnerable to adversarial attacks. Consequentially, adversarial training based defense is susceptible to a new class of attacks ("blind-spot attack") where the input image resides in a "blind-spot" in the empirical distribution of training data but is still on the ground-truth data manifold. For MNIST, we found that these blind-spots can be easily found by simply scaling and shifting image pixel values. Most importantly, for large datasets with high dimensional and complex data manifold (CIFAR, ImageNet, etc), the existence of blind-spots in adversarial training makes the defense on any valid test examples almost impossible due to the curse of dimensionality.

1 INTRODUCTION

Since the discovery of adversarial examples in deep neural networks (DNNs) (Szegedy et al., 2013), adversarial training with the robustness optimization framework (Madry et al., 2018; Sinha et al., 2018) has become one of the most effective methods to defend against adversarial examples. A recent study by Athalye et al. (2018) showed that adversarial training does not cause obfuscated gradient and delivers promising results for defending adversarial examples on small datasets. Adversarial training approximately solves the following min-max optimization problem:

min E max L(x + ; y; ) ,
 (x,y)X S

(1)

where X is the set of training data, L is the loss function,  is the parameter of the network, and S is
usually a norm constrained p ball centered at 0. Madry et al. (2018) propose to use projected gradient descent (PGD) to approximately solve the maximization problem within S = { |    }, where = 0.3 for MNIST dataset on a 0 - 1 pixel scale, and = 8 for CIFAR-10 dataset on a
0 - 255 pixel scale. This approach achieves impressive defending results on the MNIST test set:
so far the best available white-box attacks by Zheng et al. (2018) can only decrease the test accuracy from approximately 98% to 88%1. However, on CIFAR-10 dataset, a simple 20-step PGD can decrease the test accuracy from 87% to less than 50%2.

Currently, the effectiveness of adversarial training is measured on the test dataset. However, the adversarial training process itself is done on the training set. Suppose we can optimize (1) perfectly, then certified robustness may be obtained on those training data points. However, if the empirical distribution of training dataset differs from the true data distribution, drawing a test point from the true data distribution might be far away from the empirical distribution of training dataset and is not "covered" by the adversarial training procedure. For datasets that are relatively simple and has low intrinsic dimensions (MNIST, Fashion MNIST, etc), we can obtain enough training examples

1 https://github.com/MadryLab/mnist challenge 2 https://github.com/MadryLab/cifar10 challenge

1

Under review as a conference paper at ICLR 2019
to make sure adversarial training covers most part of the data distribution. For high dimensional datasets (CIFAR, ImageNet), adversarial training have been shown difficult (Kurakin et al., 2016; Trame`r et al., 2018) and only limited success was obtained.
A recent attack proposed by Song et al. (2018) shows that adversarial training can be defeated when the input image is produced by a generative model (for example, a generative adversarial network) rather than selected directly from the test examples. The generated images are well recognized by humans and thus valid images in the ground-truth data distribution. In our interpretation, this attack effective finds the "blind-spots" in the input space that the training data do not well cover.
For higher dimensional datasets, we hypothesize that many test images already fall into these blindspots of training data and thus adversarial training only obtains a moderate level of robustness. It is interesting to see that for those test images that adversarial training fails to defend, if their distances (in some metrics) to the training dataset are indeed larger. In our paper, we try to explain the success of robust optimization based adversarial training and show the limitations of this approach when the test points are slightly off the empirical distribution of training data. Our main contributions are:
· We show that on the original set of test images, the effectiveness of adversarial training is highly correlated with the distance (in different distance metrics) from the test image to the distribution of training images. For MNIST and Fashion MNIST dataset, most test images are close the training data and very good robustness is observed on these points. For CIFAR, there is a clear trend that the adversarially trained network gradually loses its robustness property when the test images are further away from training data.
· We propose a method to estimate the distance between training set and test set using the K-L divergence, which is a good indicator of the success of adversarial training. On datasets where adversarial training does not perform well, this divergence tends to be larger.
· We identify a new class of attacks, "blind-spot attacks", where the input image resides in a "blind-spot" in the empirical distribution of training data (far enough from any training examples) but is still in the ground-truth data distribution (well recognized by humans and correctly classified by the model). Adversarial training cannot provide good robustness on these blindspots. GAN based attacks by Song et al. (2018) is a successful attack in this class.
· We show that blind-spots can be easily found without using an more involved procedure like GAN. We propose a few simple transformations (changing contrast and background), that do not significantly affect MNIST and Fashion MNIST model's accuracy but make the adversarially trained models model vulnerable to adversarial attacks on these sets of transformed input images. These transformations effectively move the test images slightly out of the domain of training images, which does not affect generalization but poses a challenge for robust learning.
· Our results imply that current adversarial training procedures cannot scale to datasets with a large (intrinsic) dimension, where any practical amount of training data cannot cover all the blind-spots. This explains the limited success for applying adversarial training on ImageNet dataset, where many test images can be sufficiently far away from the empirical distribution of training dataset.
2 RELATED WORKS
2.1 DEFENDING AGAINST ADVERSARIAL EXAMPLES
Adversarial examples in DNNs have brought great threats to the deep learning-based AI applications such as autonomous driving and face recognition. Therefore, defending against adversarial examples is an urgent task before we can safely deploy deep learning models to a wider range of applications. Following the emergence of adversarial examples, various defense methods have been proposed recently, such as defensive distillation in Papernot et al. (2016) and feature squeezing in Xu et al. (2017). Some of these defense methods have been proven vulnerable or ineffective under strong attack methods such as C&W in Carlini & Wagner (2017). Another category of recent defense is gradient masking or obfuscated gradient (Buckman et al. (2018); Ma et al. (2018); Guo et al. (2017); Song et al. (2017); Samangouei et al. (2018)), but these methods are also successfully evaded by the stronger BPDA attack (Athalye et al. (2018)). Randomization in DNNs (Dhillon et al., 2018; Xie et al., 2017; Liu et al., 2018) is also used to reduce the success rate of adversarial attacks, however, it usually incurs additional computational costs and still cannot fully defend against an adaptive attacker (Athalye et al., 2018; Athalye & Sutskever, 2017).
2

Under review as a conference paper at ICLR 2019
Another effective defense method is adversarial training, which trains the model with adversarial examples freshly generated during the entire training process. First introduced by Goodfellow et al., adversarial training demonstrates the state-of-the-art defending performance. Madry et al. (2018) formulated the adversarial training procedure into a min-max robust optimization problem and has achieved state-of-the-art defending performance on MNIST and CIFAR datasets. Several attacks have been proposed to attack the model release by Madry et al. (2018). On the MNIST testset, so far the best attack by Zheng et al. (2018) can only reduce the test accuracy from 98% to 88%. Analysis by Athalye et al. (2018) shows that this adversarial training framework does not obfuscate gradient and truly increases model robustness; gradient based attacks with random starts can only achieve less than 10% success rate with given distortion constraints and are unable to penetrate this defense. On the other hand, attacking adversarial training using generative models have also been investigated; both Xiao et al. (2018) and Song et al. (2018) propose to use GANs to produce adversarial examples in black-box and white-box settings, respectively. The attack proposed by Song et al. (2018) is also effective to attack other "certified" defense methods such as Kolter & Wong (2018) and Raghunathan et al. (2018), as these methods have non-trivial robustness certifications only on training examples.
2.2 ANALYZING ADVERSARIAL EXAMPLES
Along with the attack-defense arms race, some insightful findings have been discovered to understand the natural of adversarial examples, both theoretically and experimentally. Schmidt et al. (2018) show that even for a simple data distribution of two class-conditional Gaussians, robust generalization requires significantly larger number of samples than standard generalization. Cullina et al. (2018) extend the well-known PAC learning theory to the case with adversaries, and derive the adversarial VC-dimension which can be either larger or smaller than the standard VC-dimension. Bubeck et al. (2018) conjecture that a robust classifier can be computationally intractable to find, and proved the computation hardness under statistical query (SQ) model. Additionally, finding the safe area approximately is computationally hard according to Katz et al. (2017) and Weng et al. (2018). Mahloujifar et al. (2018) explain the prevalence of adversarial examples by making a connection to the "concentration of measure" phenomenon in metric measure spaces. Tsipras et al. (2018) discover that data examples consist of robust and non-robust features and adversarial training tends to find robust features that have strongly-correlations with the labels. Su et al. (2018) conduct large scale experiments on ImageNet and find a negative correlation between robustness and accuracy.
Our paper complements those existing findings by showing the strong correlation between the effectiveness of adversarial training and the distance between training data and test points. Additionally, we show that a tiny shift in input domain (which may or may not be detectable in feature space) can easily destroy the robustness property of an adversarially trained network.
3 METHODOLOGY
3.1 MEASURING THE DISTANCE BETWEEN TRAINING DATASET AND A TEST DATA POINT
To verify the correlation between the effectiveness of adversarial training and how close a test point is to the training dataset, we need to propose a reasonable distance metric between a test example and a set of training examples. However, defining a meaningful distance metric for high dimensional image data is a challenging problem. Naively using an Euclidean distance metric in the input space of images works poorly as it does not reflect the true distance between the images on their ground-truth manifold. One strategy is to use (kernel-)PCA, t-SNE (Maaten & Hinton, 2008), or UMAP (McInnes & Healy, 2018) to reduce the dimension of training data to a low dimensional space, and then define distance in that space. These methods are sufficient for small and simple datasets like MNIST, but for more general and complicated dataset like CIFAR, extracting a meaningful low-dimensional manifold directly on the input space can be really challenging.
On the other hand, using a DNN to extract features of input images and measuring the distance in the deep feature embedding space has demonstrated better performance in many applications (Hu et al., 2014; 2015), since DNN models can capture the manifold of image data much better than simple methods such as PCA or t-SNE. Although we can form an empirical distribution using kernel density estimation (KDE) on the deep feature embedding space and then obtain probability densities for test
3

Under review as a conference paper at ICLR 2019

points, our experiments showed that KDE work poorly because the features extracted by DNNs are still high dimensional (hundreds or thousands dimensions).

Taking the above considerations into account, we propose a simple and intuitive distance metric

using deep feature embeddings and k-nearest neighbour. Given a feature extraction neural network

h(x), points

a set Xtest

of n training = {xt1est, xt2est,

·d·a·ta, xptmeosti}ntfsroXmtratihn e=tru{exd1taratian,dxi2tsrtarinib, ·u·ti·o,nx, ntfroairn

}, and each j

a 

set of m test data [m], we define the

following distance between xjtest and Xtrain:

D(xtjest, Xtrain)

:=

1 k

k

h(xtjest) - h(xtraji(ni)) p

i=1

(2)

where j : [n]  [n] is a permutation that {j(1), j(2), · · · , j(n)} is an ascending ordering of training data based on the p distance between xtjest and xtirain in the deep embedding space, i.e.,

i < i , h(xtjest) - h(xtraji(ni)) p  h(xjtest) - h(xtraji(ni )) p.
In other words, we average the embedding space distance of k nearest neighbors of xj in the training dataset. This simple metric is non-parametric and we found that the results are not sensitive to the selection of k; also, for naturally trained and adversarially trained feature extractors, the distance metric obtained by them have the same correlation with the effectiveness of adversarial training.

3.2 MEASURING THE DISTANCE BETWEEN TRAINING AND TEST DATASETS

We are also interested to investigate the "distance" between the training dataset and the test dataset, as it will become a good indicator for how adversarial training performs on the test set. Unlike the setting in Section 3.1, this requires to compute a divergence between two data distributions.

Given {xt1est,

n training data points xt2est, · · · , xmtest}, we first

Xtrain apply

a

= {x1train, xt2rain, · · · , xntrain} neural feature extractor h to

and m test them, which

data is the

points Xtest = same as in Sec-

tion 3.1. Then, we apply a non-linear projection (in our case, we use t-SNE) to project both h(xitrain)

and The

h(xjtest) dataset

to a low dimensional space, after feature extraction and

and obtain projection

xi¯stirdaienn=otepdroaj(shX(¯xtratirinaina)n)danX¯dtexs¯t.tjesBt =ecapursoej(xh¯ti(raxinjtesat)n)d.

x¯tjest are low dimensional, we can use kernel density estimation (KDE) to form empirical distribu-

tions p¯train and p¯test for them. Then, we approximate the K-L divergence between p¯train and p¯test via a

numerical integration of the following integral:

DKL(Ptrain||Ptest) 

V

p¯train(x) log

p¯train(x) dx p¯test(x)

(3)

where

p¯train(x)

=

1 n

n i=1

K (x

-

x¯itrain;

H)

and

p¯test(x)

=

1 m

m j=1

K (x

-

x¯jtest;

H)

are

the

KDE

density functions. K is the kernel function (specifically, we use the Gaussian kernel) and H is the

bandwidth parameter automatically selected by Scott's rule (Scott, 2015). V is chosen as a box

bounding all training and test data points. For a multi-class dataset, we compute the aforementioned

KDE and K-L divergence for each class separately.

As we will show in experiments, this K-L divergence is a good indicator on the effectiveness of adversarial training ­ a large divergence between training and test data points is a strong indication that adversarial training is likely to fail on many test points.

3.3 THE BLIND-SPOT ATTACK: A NEW CLASS OF ADVERSARIAL ATTACKS
Inspired by our findings of the negative correlation between the effectiveness of adversarial training and the distance between a test image and training dataset, we identify a new class of adversarial attacks called "blind-spot attacks", where we find input images that are far enough from any existing training examples such that:
· They are still drawn from the ground-truth data distribution (i.e. well recognized by humans) and classified correctly by the model (within the generalization capability of the model);

4

Under review as a conference paper at ICLR 2019
· Adversarial training cannot provide good robustness properties on these images, and we can easily find their adversarial examples using a simple gradient based attack.
We find that these blind-spots are prevalent and can be easily found without resorting to complex generative models like in Song et al. (2018). For the MNIST dataset which Madry et al. (2018) demonstrate the strongest defense results so far, we propose a simple transformation to find the blind-spots in this model. We simply scale and shift each pixel value. Suppose the input image x  [-0.5, 0.5]d, we scale and shift each test data example x to form a new example x :
x = x + , s.t. x  [-0.5, 0.5]d
where  is a constant close to 1 and  is a constant close to 0. We make sure that the selection of  and  will result in a x that is still in the valid input range [-0.5, 0.5]d. This transformation effectively adjusts the contrast of the image, and/or adds a gray background to the image. It is important that the blind-spot images are still valid images; for example, a digit that is slightly darker than the one in test set is still considered as a valid digit and can be well recognized by humans. Also, we found that with appropriate  and  the accuracy of MNIST and Fashion-MNIST models barely decreases; the model has enough generalization capability for this set of slightly transformed images, yet their adversarial examples can be easily found.
Although the blind-spot attack is beyond the threat model considered in adversarial training (e.g.  norm constrained perturbations), our argument is that adversarial training (and probably other defense methods with certifications only on training examples such as Kolter & Wong (2018)) are unlikely to scale well to datasets that lie in a high dimensional manifold, as the limited training data only guarantees robustness near these training examples. The blind-spots are almost inevitable in high dimensional case. For example, in CIFAR-10, about 50% of test images are already in blindspots and their adversarial examples can be trivially found by projected gradient descent despite adversarial training. Using data augmentation may eliminate some blind-spots, however for high dimensional data it is impossible to enumerate all possible inputs of one class due to the curse of dimensionality.
4 EXPERIMENTS
4.1 SETUP
We conduct experiments on adversarially trained models by Madry et al. (2018) on three datasets: MNIST, Fashion MNIST and CIFAR-10. For MNIST, we use the "secret" model release for the MNIST attack challenge3. For CIFAR-10, we use the public "adversarially trained" model4. For Fashion MNIST, we train our own model with the same model structure and parameters as the robust MNIST model, except that the iterative adversary is allowed to perturb each pixel by at most
= 0.1 as a larger will significantly reduce model accuracy.
We attack each model with Carlini & Wagner's  attack (Carlini & Wagner (2017)). We found that this method generally finds adversarial examples with smaller perturbations than projected gradient descent (PGD). To avoid gradient masking, we initial our attacks using two schemes: (1) from the original image plus a random Gaussian noise with a standard deviation of 0.2; (2) from a blank gray image where all pixels are initialized as 0. A successful attack is defined as finding an perturbed example that changes the model's classification and the  distortion is less than a given used for robust training. For MNIST, = 0.3; for Fashion-MNIST, = 0.1; and for CIFAR, = 8/255. All input images are normalized to [-0.5, 0.5].
4.2 EFFECTIVENESS OF ADVERSARIAL TRAINING AND THE DISTANCE TO TRAINING SET
In this set of experiments, we build a connection between attack success rate on adversarially trained models and the distance between a test example and the whole training set. We use the metric defined in Section 3.1 to measure this distance. For MNIST and Fashion-MNIST, the output of the first fully connected layer and for CIFAR, the output of the last average pooling layer of both naturally and adversarially trained networks are used as the neural feature extractor h(x). p is set as 2 and k is set as 5. The results are shown in Figure 1, 2 and 3. For each test set, after obtaining the distance of
3 https://github.com/MadryLab/mnist challenge 4 https://github.com/MadryLab/cifar10 challenge
5

Under review as a conference paper at ICLR 2019

percentage success rate

percentage success rate

each test point, we bin the test data points based on their distances to the training set and show them in the histogram at the bottom half of each figure (red). The top half of each figure (blue) represents the attack success rate for the test images in the corresponding bin. Note that we only attack images that are correctly classified and only calculate success rate on those images.

1.00 0.75 0.50 0.25 0.00 0 2 4 6 8 10 12 14
10
5
0 0 2 4 d6istance 8 10 12 14
(a) Using adversarially trained model's hidden layer

percentage success rate

1.00 0.75 0.50 0.25 0.00 5
10
5
05

10 15 20
10 d1i5stance 20

25 25

(b) Using naturally trained model's hidden layer

Figure 1: Attack success rate and distance distribution of MNIST. Upper: The C&W  attack success rate, = 0.3. Lower: The distribution of the average 2 (embedding space) distance between the images in test set and the top-5 nearest images in training set.

0.2
0.1
02.00 2 4 6 8 10 12 15 10 5
0 2 4 6distance8 10 12
(a) Using adversarially trained model's hidden layer

percentage success rate

0.15

0.10

0.05

0.0200 5

10 15 20 25 30 35 40

15

10

5

0 5 10 15 20distan2c5e 30 35 40

(b) Using naturally trained model's hidden layer

Figure 2: Attack success rate and distance distribution of Fashion MNIST. Upper: The C&W  attack success rate, = 0.1. Lower: The distribution of the average 2 (embedding space) distance between the images in test set and the top-5 nearest images in training set.

1.00

0.75

0.50

0.25

0.00 12.5

1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5

10.0

7.5

5.0

2.5

0.0 1.0 1.5 2.0 dis2t.5ance 3.0 3.5 4.0 4.5

(a) Using adversarially trained model's hidden layer

percentage success rate

1.00

0.75

0.50

0.25

0.00 20

1

2

3

4

5

6

7

15

10

5

0 1 2 3 d4istance 5 6 7

(b) Using naturally trained model's hidden layer

Figure 3: Attack success rate and distance distribution of CIFAR. Upper: The C&W  attack success rate, = 8/255. Lower: The distribution of the average 2 (embedding space) distance between the images in test set and the top-5 nearest images in training set.

As we can observe in all three figures, most successful attacks in test sets for adversarially trained networks concentrate on the right hand side of the distance distribution, and the success rates tend to grow when the distance is increasing. The trend is independent of the feature extractor being used. The strong correlation between attack success rates and the distance from a test point to the

6

percentage success rate

Under review as a conference paper at ICLR 2019

training dataset supports our hypothesize that adversarial training tends to fail on test points that are far enough from the training data distribution.

4.3 K-L DIVERGENCE BETWEEN TRAINING AND TEST SETS VS ATTACK SUCCESS RATE
To quantify the overall distance between the training and the test set, we calculate the K-L divergence between training set distribution and test set distribution for each class according to Eq. (3). Then, for each dataset we take the average K-L divergence across all classes, as shown in Table 1. We also report the attack success rates and test accuracy for each adversarially trained model.
Clearly, Fashion-MNIST is the dataset with strongest defense as measured by the attack success rates on test set, and its K-L divergence is also the smallest. For CIFAR, the divergence between training and test sets is significantly larger, and adversarial training only has limited success. The hardness of training a robust model for MNIST is in between Fashion-MNIST and CIFAR. Another important observation is that the effectiveness of adversarial training does not depend on the accuracy; for Fashion-MNIST, classification is harder as the data is more complicated than MNIST, but training a robust Fashion-MNIST model is easier as the data distortion is more concentrated and adversarial training has less "blind-spots".

Table 1: Average K-L divergence between training and test set across all classes. Note that we only attack images that are correctly classified and calculate success rate on those images.

Dataset Fashion-MNIST
MNIST CIFAR

Average K-L divergence 0.046 0.119 0.571

Success Rates on Test Set 6.4% 9.7% 37.9%

Test Accuracy 86.1% 98.2% 87.0%

4.4 BLIND-SPOT ATTACK ON MNIST AND FASHION MNIST
In this section we focus on applying the proposed blind-spot attack to MNIST and Fashion MNIST. For MNIST, = 0.3 so we set the scaling factor to  = {1.0, 0.9, 0.8, 0.7}. For Fashion-MNIST,
= 0.1 so we set the scaling factor to  = {1.0, 0.95, 0.9}. We set  to either 0 or a small constant. The case  = 1.0,  = 0.0 represents the original test set images. We report the model's accuracy and attack success rates for each choice of  and  in Table 2 and Table 3. Because we scale the image by a factor of , we also set a stricter criterion of success ­ the  perturbation must be less than  to be counted as a successful attack. For MNIST, = 0.3 and for Fashion-MNIST, = 0.1. We report both success criterion, and  in Tables 2 and 3.
We first observe that for all pairs of  and  the transformation does not affect the models' test accuracy at all. The adversarially trained model classifies these slightly scaled and shifted images very well, with test accuracy equivalent to the original test set. Visual comparisons in Figure 4 show that when  is close to 1 and  is close to 0, it is hard to distinguish the transformed images from the original images. On the other hand, according to Tables 2 and 3, the attack success rates for those transformed test images are significantly higher than the original test images, for both the original criterion and the stricter criterion  . In Figure 4, we can see that the  adversarial perturbation required is much smaller than the original image after the transformation. Thus, the proposed scale and shift transformations indeed move test images into blind-spots.

, 

 = 1.0 =0

acc 98.2%

th. 0.3

 = 0.9

=0

 = 0.05

98.3%

98.5%

0.3 0.27 0.3 0.27

 = 0.8

=0

 = 0.1

98.4%

98.5%

0.3 0.24 0.3 0.24

 = 0.7

=0

 = 0.15

98.4%

98.1%

0.3 0.21 0.3 0.21

suc. rate

9.70%

75.20% 15.20% 93.65% 82.50% 94.85% 52.30% 99.55% 95.45% 98.60% 82.45% 99.95% 99.95%

Table 2: The attack success rate (suc. rate) and test accuracy (acc) of scaled and shifted MNIST. An attack is considered as successful if its  distortion is less than given thresholds (th.) 0.3 or 0.3.

7

Under review as a conference paper at ICLR 2019

, 

 = 1.0 =0

acc 86.1%

th. 0.1

 = 0.95

=0

 = 0.025

86.1%

86.4%

0.1 0.095 0.1 0.095

 = 0.9

=0

 = 0.05

86.1%

86.2%

0.1 0.09 0.1 0.09

suc. rate

6.40%

11.25% 9.05% 22.55% 18.55% 25.70% 19.15% 62.60% 55.95%

Table 3: The attack success rate (suc. rate) and test accuracy (acc) of scaled and shifted Fashion-
MNIST. An attack is considered as successful if its  distortion is less than a given thresholds (th.)0.1 or 0.1.

(a) (b) (c) (d) (e) (f)  = 1.0  = 0.9  = 0.9  = 1.0  = 0.8  = 0.8  = 0.0  = 0.0  = 0.05  = 0.0  = 0.0  = 0.1 dist= 0.218 dist= 0.099 dist= 0.070 dist= 0.338 dist= 0.229 dist= 0.129
Figure 4: Blind-spot attacks on Fashion-MNIST and MNIST data with scaling and shifting. First row contains input images and the second row contains the found adversarial examples.
One might think that we can generally detect blind-spot attacks by observing their distances to the training dataset, using a metric similar to Eq. (2). Thus, we plot histograms for the distances between tests points and training dataset, for both original test images and those slightly transformed ones in Figure 5. We set  = 0.7,  = 0 for MNIST and  = 0.9,  = 0 for Fashion-MNIST. Unfortunately, the differences in distance histograms for these blind-spots is so tiny that we cannot reliably detect the change, yet the robustness property drastically changes on these transformed images.
5 CONCLUSION
In this paper, we observed that the effectiveness of adversarial training is highly correlated with the characteristics of the dataset, and data points that are far enough from the distribution of training data are still vulnerable to adversarial attacks despite adversarial training. Following this observation, we defined a new class of attacks called "blind-spot attack" and proposed a simple scale-andshift scheme for conducting blind-spot attacks on adversarially trained MNIST and Fashion MNIST

20.0 image 40.0 image

0.7 scale image

0.9 scale image

30.0

percentage percentage

10.0 20.0

10.0

0.0 0 2 4 d6istanc8e 10 12 14 (a) MNIST

0.0 2 4 6distanc8e 10 12 14 (b) Fashion-MNIST

Figure 5: The distribution of the 2 distance between the original and scaled images in test set and the top-5 nearest images (k=5) in training set using the distance metric defined in Eq. (2)

8

Under review as a conference paper at ICLR 2019
datasets with high success rates. Our findings suggest that adversarial training can be challenging due to the prevalence of blind-spots in high dimensional datasets.
REFERENCES
Anish Athalye and Ilya Sutskever. Synthesizing robust adversarial examples. arXiv preprint arXiv:1707.07397, 2017.
Anish Athalye, Nicholas Carlini, and David Wagner. Obfuscated gradients give a false sense of security: Circumventing defenses to adversarial examples. International Conference on Machine Learning (ICML), 2018.
Se´bastien Bubeck, Eric Price, and Ilya Razenshteyn. Adversarial examples from computational constraints. arXiv preprint arXiv:1805.10204, 2018.
Jacob Buckman, Aurko Roy, Colin Raffel, and Ian Goodfellow. Thermometer encoding: One hot way to resist adversarial examples. ICLR, 2018.
Nicholas Carlini and David Wagner. Towards evaluating the robustness of neural networks. In 2017 IEEE Symposium on Security and Privacy (SP), pp. 39­57. IEEE, 2017.
Daniel Cullina, Arjun Nitin Bhagoji, and Prateek Mittal. Pac-learning in the presence of evasion adversaries. arXiv preprint arXiv:1806.01471, 2018.
Guneet S Dhillon, Kamyar Azizzadenesheli, Zachary C Lipton, Jeremy Bernstein, Jean Kossaifi, Aran Khanna, and Anima Anandkumar. Stochastic activation pruning for robust adversarial defense. arXiv preprint arXiv:1803.01442, 2018.
Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples (2014). arXiv preprint arXiv:1412.6572.
Chuan Guo, Mayank Rana, Moustapha Cisse, and Laurens van der Maaten. Countering adversarial images using input transformations. arXiv preprint arXiv:1711.00117, 2017.
Junlin Hu, Jiwen Lu, and Yap-Peng Tan. Discriminative deep metric learning for face verification in the wild. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 1875­1882, 2014.
Junlin Hu, Jiwen Lu, and Yap-Peng Tan. Deep transfer metric learning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 325­333, 2015.
Guy Katz, Clark Barrett, David L Dill, Kyle Julian, and Mykel J Kochenderfer. Reluplex: An efficient smt solver for verifying deep neural networks. In International Conference on Computer Aided Verification, pp. 97­117. Springer, 2017.
J Zico Kolter and Eric Wong. Provable defenses against adversarial examples via the convex outer adversarial polytope. ICML, 2018.
Alexey Kurakin, Ian Goodfellow, and Samy Bengio. Adversarial machine learning at scale. arXiv preprint arXiv:1611.01236, 2016.
Xuanqing Liu, Minhao Cheng, Huan Zhang, and Cho-Jui Hsieh. Towards robust neural networks via random self-ensemble. ECCV, 2018.
Xingjun Ma, Bo Li, Yisen Wang, Sarah M Erfani, Sudanthi Wijewickrema, Michael E Houle, Grant Schoenebeck, Dawn Song, and James Bailey. Characterizing adversarial subspaces using local intrinsic dimensionality. arXiv preprint arXiv:1801.02613, 2018.
Laurens van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of machine learning research, 9(Nov):2579­2605, 2008.
Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep learning models resistant to adversarial attacks. ICLR, arXiv preprint arXiv:1706.06083, 2018.
9

Under review as a conference paper at ICLR 2019
Saeed Mahloujifar, Dimitrios I Diochnos, and Mohammad Mahmoody. The curse of concentration in robust learning: Evasion and poisoning attacks from concentration of measure. arXiv preprint arXiv:1809.03063, 2018.
Leland McInnes and John Healy. Umap: Uniform manifold approximation and projection for dimension reduction. arXiv preprint arXiv:1802.03426, 2018.
Nicolas Papernot, Patrick McDaniel, Xi Wu, Somesh Jha, and Ananthram Swami. Distillation as a defense to adversarial perturbations against deep neural networks. In 2016 IEEE Symposium on Security and Privacy (SP), pp. 582­597. IEEE, 2016.
Aditi Raghunathan, Jacob Steinhardt, and Percy Liang. Certified defenses against adversarial examples. arXiv preprint arXiv:1801.09344, 2018.
Pouya Samangouei, Maya Kabkab, and Rama Chellappa. Defense-GAN: Protecting classifiers against adversarial attacks using generative models. arXiv preprint arXiv:1805.06605, 2018.
Ludwig Schmidt, Shibani Santurkar, Dimitris Tsipras, Kunal Talwar, and Aleksander Madry. Adversarially robust generalization requires more data. arXiv preprint arXiv:1804.11285, 2018.
David W Scott. Multivariate density estimation: theory, practice, and visualization. John Wiley & Sons, 2015.
Aman Sinha, Hongseok Namkoong, and John Duchi. Certifying some distributional robustness with principled adversarial training. ICLR, 2018.
Yang Song, Taesup Kim, Sebastian Nowozin, Stefano Ermon, and Nate Kushman. Pixeldefend: Leveraging generative models to understand and defend against adversarial examples. arXiv preprint arXiv:1710.10766, 2017.
Yang Song, Rui Shu, Nate Kushman, and Stefano Ermon. Generative adversarial examples. arXiv preprint arXiv:1805.07894, 2018.
Dong Su, Huan Zhang, Hongge Chen, Jinfeng Yi, Pin-Yu Chen, and Yupeng Gao. Is robustness the cost of accuracy?­a comprehensive study on the robustness of 18 deep image classification models. In Proceedings of the European Conference on Computer Vision (ECCV), pp. 631­648, 2018.
Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus. Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199, 2013.
Florian Trame`r, Alexey Kurakin, Nicolas Papernot, Ian Goodfellow, Dan Boneh, and Patrick McDaniel. Ensemble adversarial training: Attacks and defenses. In International Conference on Learning Representations (ICLR), 2018. https://arxiv.org/abs/1705.07204.
Dimitris Tsipras, Shibani Santurkar, Logan Engstrom, Alexander Turner, and Aleksander Madry. There is no free lunch in adversarial robustness (but there are unexpected benefits). arXiv preprint arXiv:1805.12152, 2018.
Tsui-Wei Weng, Huan Zhang, Hongge Chen, Zhao Song, Cho-Jui Hsieh, Duane Boning, Inderjit S Dhillon, and Luca Daniel. Towards fast computation of certified robustness for relu networks. ICML, 2018.
Chaowei Xiao, Bo Li, Jun-Yan Zhu, Warren He, Mingyan Liu, and Dawn Song. Generating adversarial examples with adversarial networks. arXiv preprint arXiv:1801.02610, 2018.
Cihang Xie, Jianyu Wang, Zhishuai Zhang, Zhou Ren, and Alan Yuille. Mitigating adversarial effects through randomization. arXiv preprint arXiv:1711.01991, 2017.
Weilin Xu, David Evans, and Yanjun Qi. Feature squeezing: Detecting adversarial examples in deep neural networks. arXiv preprint arXiv:1704.01155, 2017.
Tianhang Zheng, Changyou Chen, and Kui Ren. Distributionally adversarial attack. arXiv preprint arXiv:1808.05537, 2018.
10


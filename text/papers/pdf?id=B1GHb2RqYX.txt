Under review as a conference paper at ICLR 2019
POLYCNN: LEARNING SEED CONVOLUTIONAL FILTERS
Anonymous authors Paper under double-blind review
ABSTRACT
In this work, we propose the polynomial convolutional neural network (PolyCNN), as a new design of a weight-learning efficient variant of the traditional CNN. The biggest advantage of the PolyCNN is that at each convolutional layer, only one convolutional filter is needed for learning the weights, which we call the seed filter, and all the other convolutional filters are the polynomial transformations of the seed filter, which is termed as an early fan-out. Alternatively, we can also perform late fan-out on the seed filter response to create the number of response maps needed to be input into the next layer. Both early and late fan-out allow the PolyCNN to learn only one convolutional filter at each layer, which can dramatically reduce the model complexity by saving 10× to 50× parameters during learning. While being efficient during both training and testing, the PolyCNN does not suffer performance due to the non-linear polynomial expansion which translates to richer representational power within the convolutional layers. By allowing direct control over model complexity, PolyCNN provides a flexible trade-off between performance and efficiency. We have verified the on-par performance between the proposed PolyCNN and the standard CNN on several visual datasets, such as MNIST, CIFAR-10, SVHN, and ImageNet.
1 INTRODUCTION
Applications of deep convolutional neural networks (CNNs) have been overwhelmingly successful in all aspect of perception tasks, ranging from computer vision to speech recognition and understanding, from biomedical data analysis to quantum physics. In the past couple of years, we have seen the evolution of many successful CNN architectures such as AlexNet (Krizhevsky et al., 2012), VGG (Simonyan & Zisserman, 2015), Inception (Szegedy et al., 2015), and ResNet (He et al., 2016b;a). However, training these networks end-to-end with fully learnable convolutional filters (as is standard practice) is still very computationally expensive and is prone to over-fitting due to the large number of parameters. To alleviate this issue, we have come to think about this question: can we arrive at a more efficient CNN in terms of learnable parameters, without sacrificing the high CNN performance?
In this paper, we present an alternative approach to reducing the computational complexity of CNNs while performing as well as standard CNNs. We introduce the polynomial convolutional neural networks (PolyCNN). The core idea behind the PolyCNN is that at each convolutional layer, only one convolutional filter is needed for learning the weights, which we call the seed filter, and all the other convolutional filters are the polynomial transformations of the seed filter, which is termed as an early fan-out. Alternatively, we could also perform late fan-out on the seed filter response to create the number of response maps desired to be input into the next layer. Both early and late fan-out allow the PolyCNN to learn only one convolutional filter at each layer, which can dramatically reduce the model complexity. Parameter savings of at least 10×, 26×, 50×, etc. can be realized during the learning stage depending on the spatial dimensions of the convolutional filters (3 × 3, 5 × 5, 7 × 7 etc. sized filters respectively). While being efficient during both training and testing, the PolyCNN does not suffer performance due to the non-linear polynomial expansion which translates to richer representational power within the convolutional layers. We have verified the on-par performance between the proposed PolyCNN and the standard CNN on several visual datasets, such as MNIST, CIFAR-10, SVHN, and ImageNet.
1

Under review as a conference paper at ICLR 2019

xl
CNN Module

Wl

(a)

xl+1

xl
PolyCNN Module
(Early Fan-Out, Single-Seed)

Wl

Wl

xl
PolyCNN Module
(Early Fan-Out, Multi-Seed)

(b) (c)

xl+1 xl+1

xl
PolyCNN Module
(Late Fan-Out, Single-Seed)

Wl

(d)

xl+1

Wl xl

xl+1

PolyCNN Module
(Late Fan-Out, Multi-Seed)

(e)

Figure 1: Basic module in (a) CNN and (b-e) PolyCNN (both early fan-out and late fan-out). Wl and Wl
(encircled in red dashed line) are the learnable weights for CNN and PolyCNN respectively. (b-c) Early fan-out PolyCNN, single-seed and multi-2s3ee2d cases. (d-e) Late fan-out PAoldyvCanNcNed, scionrgrleel-asteioedn afinltderms ulti-seed cases.

Convolutional Filter
1
=
2
=
···
i
=

Response Map

1
=

2
=
···
i
=

Input x image

x1 x2 x3
xN
Point nonlinearities

h1 h2 h3 hN

gx
Nonlinear filter output

Figure 2: (L) Polynomial expansionFoifguthree s6e.1e0d Nfiltthe-roarsdewreplloalysntohme siaeledcofirlrteelratrieosnpofinltseermap, with exponent parameters 1, . . . , i, (R) Image taken from Vijaya Kumar et al. (2005): polynomial correlation filter.
The corresponding filter structure is shown in Figure 6.10.

2 6.4.1 Derivation of the solution

The objective is to find the filters hi(m, n) such that structure shown in Figure 6.10

Under review as a conference paper at ICLR 2019

2 PROPOSED METHOD
2.1 POLYNOMIAL CONVOLUTIONAL NEURAL NETWORKS
Two decades ago, Mahalanobis & Vijaya Kumar (1997) generalized the traditional correlation filter and created the polynomial correlation filter (PCF), whose fundamental difference is that the correlation output from a PCF is a nonlinear function of the input. As shown in Figure 2(R), the input image x undergoes a set of point-wise nonlinear transformation (polynomial) for augmenting the input channels. Based on some pre-defined objective function, usually in terms of simultaneously maximizing average correlation peak and minimizing some correlation filter performance criterion such as average similarity measure (ASM) (Mahalanobis et al., 1994), output noise variance (ONV) (Vijaya Kumar et al., 2005), the average correlation energy (ACE) (Vijaya Kumar et al., 2005), or any combination thereof, the filters h1, h2, . . . , hN can be solved in closed-form (Mahalanobis & Vijaya Kumar, 1997; Vijaya Kumar et al., 2005; Alkanhal & Vijaya Kumar, 2003).
We draw inspiration from the design principles of the polynomial correlation filter and propose the polynomial convolutional neural network (PolyCNN) as a weight-learning efficient variant of the traditional convolutional neural networks. The core idea of PolyCNN is that at each convolutional layer, only one convolutional filter (seed filter) needs to be learned, and we can augment other filters by taking point-wise polynomials of the seed filter. The weights of these augmented filters need not to be updated during the network training. When convolved with the input data, the learnable seed filter and k non-learnable augmented filters result in (k + 1) response maps. We call this procedure: early fan-out. Similarly, one can instead fan-out the response map from the seed filter to create (k + 1) response maps for the subsequent layers. We call this procedure: late fan-out. The details of both early and late fan-out are shown in the following sections. The PolyCNN pipelines are depicted in Figure 1 with distinctions between early and late fan-out, as well as single-seed vs. multi-seed cases. Figure 2(L) shows the polynomial expansion of seed filters as well as seed filter response maps.

2.2 EARLY FAN-OUT: FILTER WEIGHTS

At any given layer, given the seed weights wi for that layer, we generate many new filter weights. The weights are generated via a non-linear transformation v = f (wi) of the weights. The convolutional
outputs are computed as follows (1-D signals for simplicity):

CC

y = f wij  xj = y[ ] =

xj[ - k]f wij[k]

j=1

j=1 k

(1)

where xj is the jth channel of the input image and wij is the jth channel of the ith filter. During the forward pass weights are generated from the seed convolutional kernel and are then convolved with the inputs i.e.,

z[i] = f (w[i]) = sign(w[i])|w[i]|

v[i] =

z[i]

-

1 n

i

z[i]

-

1 n

i z[i]
1
i z[i] 2 2

(2) (3)

where we normalize the response maps to prevent the responses from vanishing or exploding and the

normalized response map is now called v. f (·) is a non-linear function that operates on each element

of

w.

Backpropagating

through

this

filter

transformation

necessitates

the

computation

of

l w

and

l x

.

l w[i]

=

l y[j]

y[j] w[i]

=

l y[j]

y[j] f (w[i])

f (w[i]) w[i]

=

l y[j]

x[j

-

i]f

(w[i])

jj

j

l w

=

l y

x

f (w)

(4) (5)

3

Under review as a conference paper at ICLR 2019

Plug in the normalized response map, we have:

v[i] w[i]

=

v[i] z[i]

z[i] w[i]

v[i] z[i]

=

1

-

1 n

1-

1

-

1 n

z[i]

-

1 n

j z[j]
3

i

z[i]

-

1 n

i z[i] 2 2

i

z[i]

-

1 n

i z[i] 2 2

z[i] w[i]

=

sign (w[i]) jw[i]j-1

(6) (7) (8)

Similarly, we can compute the gradient with respect to input x as follows:

l x[i]

=

l y[j]

y[j] x[i]

=

l y[j]

f

(w[j

-

i])

jj

l x

=

l y



f (w)

(9) (10)

2.3 LATE FAN-OUT: RESPONSE MAPS

At any given layer, we compute the new feature maps from the seed feature maps via non-linear
transformations of the feature maps. The forward pass for this layer involves the application of the following non-linear function s[i] = f (x[i]),

s[i] = f (x[i]) = sign(x[i])|x[i]|

t[i] =

s[i]

-

1 n

i

s[i]

-

1 n

i s[i]
1
i s[i] 2 2

(11) (12)

where we normalize the response maps to prevent the responses from vanishing or exploding and

the normalized response map is now called t. Backpropagating through such a transformation of the

response

maps

requires

the

computation

of

l x

.

l x[i]

=

l t[i]

t[i] s[i]

s[i] x[i]

(13)

t[i] s[i]

=

1

-

1 n

1-

1

-

1 n

s[i]

-

1 n

j s[j]
3

i

s[i]

-

1 n

i s[i] 2 2

i

s[i]

-

1 n

i s[i] 2 2

s[i] x[i]

=

sign (x[i]) jx[i]j-1

(14) (15)

2.4 DESIGN OF THE BASIC POLYCNN MODULE
The core idea of the PolyCNN1 is to restrict the network to learn only one (or a few) convolutional filter at each layer, and through polynomial transformations we can augment the convolutional filters, or the response maps. The gist is that the augmented filters do not need to be updated or learned during the network back-propagation. As shown in Figure 1, the basic module of PolyCNN (early fan-out, single-seed) starts with just one learnable convolutional filter Wl, which we call the seed filter. If we desire m filters in total for one layer, the remaining m - 1 filters are non-learnable and are the polynomial transformation of the seed filter Wl. The input image xl is filtered by these convolutional filters and becomes m response maps, which are then passed through a non-linear activation gate, such as ReLU, and become m feature maps. Optionally, these m feature maps can be further lineally combined using m learnable weights, which is essentially another convolution operation with filters of size 1 × 1.
1In this paper we assume convolutional filters do not have bias terms.

4

Under review as a conference paper at ICLR 2019

Compared to the CNN module under the same structure (with 1 × 1 convolutions), the number of
learnable parameters is significantly smaller in PolyCNN. Let us assume that the number of input and output channels are p and q. Therefore, the size of each 3D filter in both CNN and PolyCNN is p · h · w, where h and w are the spatial dimensions of the filter, and there are m such filters. The 1 × 1 convolutions act on the m filters and create the q-channel output. For standard CNN, the number of learnable weights is p · h · w · m + m · q. For PolyCNN, the number of learnable weights is p · h · w · 1 + m · q. For simplicity let us assume p = q, which is usually the case for multi-layer
CNN architecture. Then we have the parameter saving ratio:



=

#

# parameters in CNN parameters in PolyCNN

=

p·h·w·m+m·q p·h·w·1+m·q

=

h·w·m+m h·w+m

(16)

and when the spatial filter size h = w = 3 and the number of convolutional filters desired for each

layer m

32, we have the parameter saving ratio 

=

10m m+9



10.

Similarly for spatial filter

size h = w = 5 and m

52,

the

parameter

saving

ratio



=

26m m+25



26.

For

spatial

filter

size

h = w = 7 and m

72,

the

parameter

saving

ratio



=

50m m+49



50.

If we do not include the 1 × 1 convolutions for both standard CNN and PolyCNN, and thus make m = q = p, readers can verify that the parameter saving ratio  becomes m. Numerically, PolyCNN saves around 10×, 26×, and 50× parameters during learning for 3 × 3, 5 × 5, and 7 × 7 convolutional
filters respectively. The aforementioned calculation also applies to late fan-out of the PolyCNN.

2.5 TRAINING OF THE POLYCNN
The training of the PolyCNN is quite straightforward, where the back-propagation is the same for the learnable weights and the augmented weights that do not update. Gradients get propagated through the polynomial augmented filters just like they would with learnable filters. This is similar to propagating gradients through layers without learnable parameters e.g., ReLU, Max Pooling etc.). However, we do not compute the gradient with respect to the fixed filters nor update them during the training process.
The non-learnable filter banks (tensor) of size p × h × w × (m - 1) (assuming a total of m filters in each layer) in the PolyCNN can be generated by taking polynomial transformations from the seed filter, by raising to some exponents, which can either be integer exponents, or fractional exponents that are randomly sampled from a distribution. Strictly speaking, to qualify for polynomials, only non-negative integer powers are allowed. In this work, without violating the forward and backward pass derivation, we allow the exponents to take negative numbers (relating to Laurent series), and even fractional numbers (relating to Puiseux series).

3 TOWARDS A GENERAL CONVOLUTIONAL LAYER

In this section, we will first analyze the PolyCNN layer and how the early fan-out and late fan-out can very well approximate the standard convolutional layer. Then, we will extend the formulation to a generalized convolutional layer representation.

3.1 UNDERSTANDING POLYCNN LAYER

At layer l, let x  R(p·h·w)×1 be a vectorized single patch from the p-channel input maps at location , where h and w are the spatial sizes of the convolutional filter. Let w  R(p·h·w)×1 be a vectorized single convolution filter from the convolutional filter tensor W  Rp×h×w×m which contains a total of m fan-out convolutional filters at layer l. We drop the layer subscription l for brevity.

In a standard CNN, this patch x is taken dot-product with (projected onto) the filter w, followed by the non-linear activation resulting in a single output feature value d, at the corresponding location  on the feature map. Similarly, each value of the output feature map is a direct result of convolving the
entire input map x with a convolutional filter w. This microscopic process can be expressed as:

d = relu(w x)

(17)

Without loss of generality, we assume single-seed PolyCNN case for the following analysis. For an early fan-out PolyCNN layer, a single seed filter wS is expanded into a set of m convolutional filters

5

Under review as a conference paper at ICLR 2019

W  Rm×p×h×w where wi = wSi , and the power terms 1, 2, . . . , m are pre-defined and are not updated during training. The  is again the Hadamard power.

The corresponding output feature map value dearly for early fan-out PolyCNN layer is a linear combination of multiple elements from the intermediate response maps (implemented as 1 × 1
convolution). Each slice of this response map is obtained by convolving the input map x with W , followed by a non-linear activation. The corresponding output feature map value dearly is thus obtained by linearly combining the m response maps via 1 × 1 convolution with parameters 1, 2, . . . , m. This entire process can be expressed as:

dearly = relu(W x)  = crelu

(18)

1×m m×1

where W is now a 2D matrix of size m × (p · h · w) with m filters vec(wi) stacked as rows, with a slight abuse of notation.  = [1, . . . , m]  Rm×1. Comparing Equation 17 and 18, we consider the following two cases (i) d = 0: since crelu = relu(W x)  0, a vector   Rm×1 always exists such that dearly = d. However, when (ii) d > 0: it is obvious that the approximation does not hold when crelu = 0. Therefore, under the mild assumption that crelu is not an all-zero vector, the approximation dearly  d will hold.

For the late fan-out PolyCNN layer, a single response map is a direct result of convolving the input

map x with the seed convolutional filter wS . Then, we obtain a set of m response maps by expanding

the response map with Hadamard power coefficients 1, 2, . . . , m which are pre-defined and not

updated during training, just like in the early fan-out case. The corresponding output feature map

value dlate is also a linear combination of the corresponding elements from the m response maps via 1 × 1 convolution with parameters 1, 2, . . . , m. This process follows:

 (wS x)1 

dlate

=

relu



(wS

x ···

)2



 1 (wS ) 1 (x) 



=

relu



2 (wS ) ·

·

2 (x) ·



 = crelu

(19)

(wS x)m m×1

m (wS ) m (x)

1×m

where i (·) is the point-wise polynomial expansion with Hadamard power i. With similar reasoning and the mild assumption that crelu is not an all-zero vector, the approximation dlate  d will hold.

3.2 GENERAL CONVOLUTIONAL LAYER

The formulation of PolyCNN discussed in the previous sections has facilitated the forming of a general convolution layer. To simplify the notation, we use 1D convolution as an example. The idea can be extended to 2D and higher dimensional convolution as well. Here is a description of a general convolutional layer:

KL

y = kl k(w)  l(x)

(20)

k=1 l=1

where k(·) and l(·) are kernel functions, (·) is a non-linearity and kl are linear weights. If both k(·) and l(·) are linear functions then the expression reduces to a module consisting of convolutional layer, non-linear activation and 1 × 1 convolutions. Under this general setting, the
learnable parameters are  and w. Setting the parameters w to fixed sparse binary values would

allow us to arrive a general version of local binary CNN (Juefei-Xu et al., 2017). Now we consider some special cases. If k(·) is a linear function, then the expression reduces to:

L
y = l w  l(x)
l=1

(21)

Similarly if l(·) is a linear function, then the expression reduces to:
K
y = k (k(w)  x)
k=1

(22)

6

Under review as a conference paper at ICLR 2019

At location  of the input x, the convolutions can be reduced to:

KL

KL

y[] =

kl k(w) l(x) =

kl (Kkl(w, x))

k=1 l=1

k=1 l=1

(23)

where Kkl is a base kernel function defined between w and image patch x at the  location. The base kernel can take many forms, including polynomials, random Fourier features, Gaussian radial
basis functions, etc., that adhere to the Mercer's theorem (Schölkopf et al., 2002). The weights kl can be learned via 1 × 1 convolutions. In general kl > 0 must hold for a valid overall kernel function, but perhaps this can be relaxed or imposed during the learning process. We can also think of Equation 23 as a generalized learnable activation function if we get rid of (·). So (·) and  together can approximate any desired activation function. There are several related work on the
combination of kernels and convolutional neural networks such as Mairal et al. (2014); Mairal (2016);
Zhang et al. (2016b; 2017). As can be seen, the early fan-out PolyCNN layer can be related to
Equation 22 and the late fan-out PolyCNN layer can be related to Equation 21 and 23.

4 EXPERIMENTAL RESULTS
4.1 DATASETS
We have experimented with 4 publicly available visual datasets, MNIST (LeCun et al., 1998), SVHN (Netzer et al., 2011), CIFAR-10 (Krizhevsky & Hinton, 2009), and ImageNet ILSVRC-2012 classification dataset (Russakovsky et al., 2015). The MNIST dataset contains a training set of 60K and a testing set of 10K 32 × 32 gray-scale images showing hand-written digits from 0 to 9. SVHN is also a widely used dataset for classifying digits, house number digits from street view images in this case. It contains a training set of 604K and a testing set of 26K 32 × 32 color images showing house number digits. CIFAR-10 is an image classification dataset containing a training set of 50K and a testing set of 10K 32 × 32 color images, which are across the following 10 classes: airplanes, automobiles, birds, cats, deer, dogs, frogs, horses, ships, and trucks. The ImageNet ILSVRC-2012 classification dataset consists of 1000 classes, with 1.28 million images in the training set and 50K images in the validation set, where we use for testing as commonly practiced. For faster roll-out, we first randomly select 100 classes with the largest number of images (1300 training images in each class, with a total of 130K training images and 5K testing images.), and report top-1 accuracy on this subset. Full ImageNet experimental results are also reported in the subsequent section.
4.2 IMPLEMENTATION DETAILS
Conceptually PolyCNN can be easily implemented in any existing deep learning framework. Since the convolutional weights are fixed, we do not have to compute the gradients nor update the weights. This leads to savings both from a computational point of view and memory as well. We have used a custom implementation of backpropagation through the PolyCNN layers that is 3x-5x more efficient than autograd-based back propagation in PyTorch.
We base the model architectures we evaluate in this paper on ResNet (He et al., 2016a), with default 3 × 3 filter size. Our basic module is the PolyCNN module shown in Figure 1 along with an identity connection as in ResNet. We experiment with different numbers of PolyCNN layers, 10, 20, 50, and 75, which is equivalent to 20, 40, 100, and 150 convolutional layers (1 × 1 convolution counted).
For PolyCNN, the convolutional weights are generated following the procedure described in Section 2.5. We use 511 randomly sampled fractional exponents for creating the polynomial filter weights (512 convolutional filters in total at each layer), for all of our MNIST, SVHN, and CIFAR-10 experiments. Spatial average pooling is adopted after the convolution layers to reduce the spatial dimensions of the image to 6 × 6. We use a learning rate of 1e-3 and following the learning rate decay schedule from He et al. (2016a). We use ReLU nonlinear activation and batch normalization (Ioffe & Szegedy, 2015) after PolyCNN convolutional module.
For our experiments with ImageNet-1k, we experiment with ad hoc CNN architectures such as the AlexNet and the native ResNet family, including ResNet-18, ResNet-34, ResNet-50, ResNet-101, and ResNet-152. Standard CNN layers are thus replaced with the proposed PolyCNN layers.

7

Under review as a conference paper at ICLR 2019

Table 1: Classification accuracy (%). PolyCNN rows only show the best performing (single-seed) model and the Baseline row shows the particular CNN counterpart.

PolyCNN (early fan-out) PolyCNN (late fan-out)
Baseline BC (Courbariaux et al., 2015) BNN (Courbariaux & Bengio, 2016) ResNet (He et al., 2016b) Maxout (Goodfellow et al., 2013) NIN (Lin et al., 2014)

MNIST
99.37 98.77
99.48 98.99 98.60
/ 99.55 99.53

SVHN
93.29 90.11
95.21 97.85 97.49
/ 97.53 97.65

CIFAR-10
90.56 85.98
92.95 91.73 89.85 93.57 90.65 91.19

100 60 5

90 50 4.5

80 40 4

Accuracy Accuracy (%)
Loss

70 AlexNet (accuracy)

30

PolyCNN (accuracy) AlexNet (loss)

3.5

60 PolyCNN (loss)

20 3 50

PolyCNN (early) train accuracy

40

PolyCNN (early) test accuracy

10

PolyCNN (late) train accuracy

PolyCNN (late) test accuracy

2.5

30 0 2 0 20 40 60 80 100 0 10 20 30 40 50 60

Epoch

Epoch

Figure 3: (L) Accuracy of the best performing single-seed PolyCNN (early fan-out) and single-seed PolyCNN

(late fan-out) on CIFAR-10. (R) Accuracy and loss on full ImageNet classification.

4.3 RESULTS ON MNIST, SVHN, AND CIFAR-10
For a fair comparison and to quantify the exact difference between our PolyCNN approach and traditional CNN, we compare ours against the exact corresponding network architecture with dense and learnable convolutional weights. We also use the exact same data and hyper-parameters in terms of the number of convolutional weights, initial learning rate and the learning rate schedule. In this sense, PolyCNN enjoys 10×, 26×, 50×, etc. savings in the number of learnable parameters because the baseline CNNs also have the 1 × 1 convolutional layer. The best performing single-seed PolyCNN models in terms of early fan-out are:
· For MNIST: 75 PolyCNN layers, m = 512, q = 256, 128 hidden units in the fc layer.
· For SVHN: 50 PolyCNN layers, m = 512, q = 256, 512 hidden units in the fc layer.
· For CIFAR-10: 50 PolyCNN layers, m = 512, q = 384, 512 hidden units in the fc layer.
Table 1 consolidates the images classification accuracies from our experiments. The best performing PolyCNNs are compared to their particular baselines, as well as the state-of-the-art methods such as BinaryConnect (Courbariaux et al., 2015), Binarized Neural Networks (BNN) (Courbariaux & Bengio, 2016), ResNet (He et al., 2016b), Maxout Network (Goodfellow et al., 2013), Network in Network (NIN) (Lin et al., 2014). The network structure for the late fan-out follows that of the early fan-out. As can be seen, performance from late fan-out is slightly inferior, but early fan-out reaches on-par performance while enjoying huge parameter savings.
4.3.1 EARLY FAN-OUT VS. LATE FAN-OUT
Table 2 compares the accuracy on CIFAR-10 achieved by various single-seed PolyCNN architectures (both early and late fan-out) as well as their standard CNN counterparts. We can see that for a fixed number of convolution layers and filters, the more output channels q leads to higher performance. Also, PolyCNN (early fan-out) is on par with the CNN counterpart, while saves 10× parameters. As can be seen from Table 2 and Figure 3(L), the early fan-out version of the PolyCNN is quite comparable to the standard CNN, and is better than its late fan-out counterpart.
8

Under review as a conference paper at ICLR 2019

Table 2: Classification accuracy (%) on CIFAR-10 with 20 convolution layers and 512 filters in each layer.

q
Baseline PolyCNN (early fan-out) PolyCNN (late fan-out)

32
86.30 83.49 79.23

64
88.77 86.11 81.77

128
90.86 88.60 84.01

192
91.69 89.47 85.36

256
92.15 90.01 85.44

384
92.93 90.06 85.50

Table 3: Classification accuracy (%) on CIFAR-10 with 20 convolution layers and 512 filters in each layer.

# Seed Filters
PolyCNN (early fan-out) PolyCNN (late fan-out)

1
87.24 81.73

2
88.06 83.42

4
88.76 85.50

8
88.98 86.95

16
89.35 88.91

32
90.02 90.34

64
90.78 91.48

128
91.89 92.09

256
92.28 92.21

512
92.48 92.33

4.3.2 VARYING THE NUMBER OF SEED FILTERS
Here we report CIFAR-10 accuracy by varying the number of seed filters in Table 3. The network has 20 PolyCNN layers, and the total number of filters per layer is set to 512. We now vary the number of seed filters from 1 to 512, by a factor of 2. So when the number of seed filters is approaching 512, PolyCNN reduces to standard CNN. As can be seen, as we increase the number of seed filters, we are essentially increase the model complexity and the performance is rising monotonically. This experiment will provide insight into trading-off between performance and model complexity.

4.4 RESULTS ON 100-CLASS IMAGENET SUBSET
We report the top-1 accuracy on 100-Class subset of ImageNet 2012 classification challenge dataset in Table 4. The input images of ImageNet is much larger than those of MNIST, SVHN, and CIFAR-10, which allows us to experiments with various convolutional filter sizes. Both the PolyCNN and our baseline share the same architecture: 20 PolyCNN layers, 512 convolutional filters, 512 output channels, 4096 hidden units in the fully connected layer. For this experiment, we omit the late fan-out and only use the better performing early fan-out version of the PolyCNN.

4.5 RESULTS ON FULL IMAGENET
The first ad hoc network architecture we experiment with is the AlexNet (Krizhevsky et al., 2012). We train a PolyCNN version of the AlexNet to take on the full ImageNet classification task. The AlexNet architecture is comprised of five consecutive convolutional layers, and two fully connected layers, mapping from the image (224×224×3) to the 1000-dimension feature for the classification purposes in the forward pass. The number of convolutional filters used and their spatial sizes are tabulated in Table 5. For this experiment, we create a single-seed PolyCNN (early fan-out) counterpart following the AlexNet architecture. For each convolutional layer in AlexNet, we keep the same input and output channels. Replacing the traditional convolution module with PolyCNN, we are allowed to specify another hyper-parameter, the fan-out channel m. Table 5 shows the comparison of the number of learnable parameters in convolutional layers in both AlexNet and its PolyCNN counterpart, by setting fan-out channel m = 256. As can be seen, PolyCNN saves about 6.4873× learnable parameters in the convolutional layers. What's important is that, by doing so, PolyCNN does not suffer the performance as can be seen in Figure 3(R) and Table 6. We have plotted accuracy curves and loss curves after 55 epochs for both the AlexNet and its PolyCNN counterpart.
The second ad hoc network architecture we experiment with is the native ResNet family. We create a single-seed PolyCNN (early fan-out) counterpart following the ResNet-18, ResNet-34, ResNet-50, ResNet-101, and ResNet-152 architectures, with the same number of input and output channels. The number of convolutional filters in each layer is equivalent for both models. The two baselines are the CNN ResNet implemented by ourselves and by Facebook (Facebook, 2016). Table 7 shows the top-1 accuracy on the two baselines as well as the PolyCNN. Since ResNet is primarily composed of 3 × 3

Table 4: Classification accuracy (%) on 100-class ImageNet with varying convolutional filter sizes.

Filter Size
Baseline PolyCNN

3×3
65.74 60.47

5×5
64.90 60.21

7×7
66.53 60.76

9×9
65.91 61.16

11×11
65.22 60.98

13×13
64.94 60.32

9

Under review as a conference paper at ICLR 2019

Table 5: Comparison of the number of learnable parameters in convolutional layers in AlexNet and AlexNet with PolyCNN modules. The proposed method saves 6.4873× learnable parameters in the convolutional layers.

Layers
Layer 1 Layer 2 Layer 3 Layer 4 Layer 5
Total

AlexNet (Krizhevsky et al., 2012)
96*(11*11*3)=34,848 256*(5*5*48)=307,200 384*(3*3*256)=884,736 384*(3*3*192)=663,552 256*(3*3*192)=442,368
2, 332, 704 ( 2.333M )

PolyCNN (AlexNet)
(11*11*3)+96*256=24,576 (5*5*48)+256*256=65,536 (3*3*256)+384*256=98,304 (3*3*192)+384*256=98,304 (3*3*192)+256*256=65,536
359, 579 ( 0.3596M )

Table 6: Top-1 classification accuracy (%) on full ImageNet with AlexNet.

ImageNet

PolyCNN 51.9008

AlexNet (ours) 56.7821

AlexNet (BLVC) 56.9

convolutional filter, the PolyCNN enjoys around 10x parameters savings while achieving competitive performance.
4.6 DISCUSSIONS
We have shown the effectiveness of the proposed PolyCNN. Not only can it achieve on-par performance with the state-of-the-art, but also enjoy a significant utility savings. The PyTorch implementation of the PolyCNN will be made publicly available.

5 RELATED WORK
Given the proliferation and success of deep convolutional neural networks, there is growing interest in improving the efficiency of such models both in terms computational and memory requirements. Multiple approaches have been proposed to compress existing models as well as to directly train efficient neural networks. Approaches include pruning unnecessary weights in exiting models, sharing of parameters, binarization and more generally quantization of model parameters, transferring the knowledge of high-performance networks into a smaller more more compact network by learning a student network to mimic a teacher network.
The weights of existing networks can be pruned away using the magnitude of weights (Pratt, 1989), or the Hessian of the loss function (Hassibi et al., 1993; LeCun et al., 1989). Ba & Caruana (2014) showed that it is possible to train a shallow but wider student network to mimic a teacher network, performing almost as well as the teacher. Similarly Hinton et al. (2015) proposed Knowledge Distillation to train a student network to mimic a teacher network. Among recent approaches for training high-performance CNNs, PolyNet (Zhang et al., 2016a) shares similar names to our proposed PolyCNN. PolyNet considers higher-order compositions of learned residual functions while PolyCNN considers higher-order polynomials of the weights and response maps.

6 CONCLUSIONS

Inspired by the polynomial correlation filter, in this paper, we have proposed the PolyCNN as an alternative to the standard convolutional neural networks. The PolyCNN module enjoys significant savings in the number of parameters to be learned at training, at least 10× to 50×. PolyCNN have much lower model complexity compared to traditional CNN with standard convolutional layers. The proposed PolyCNN demonstrates performance on par with the state-of-the-art architectures on several image recognition datasets.

Table 7: Top-1 classification accuracy (%) on full ImageNet with native ResNet family.

Baseline (Facebook, 2016) Baseline (ours) PolyCNN

ResNet-18
69.57 67.69 62.26

ResNet-34
73.27 71.38 65.46

ResNet-50
75.99 74.02 67.98

ResNet-101
77.56 75.43 69.49

ResNet-152
77.84 75.48 69.69

10

Under review as a conference paper at ICLR 2019
REFERENCES
Mohamed Alkanhal and B.V.K. Vijaya Kumar. Polynomial distance classifier correlation filter for pattern recognition. Applied optics, 42(23):4688­4708, 2003.
Jimmy Ba and Rich Caruana. Do deep nets really need to be deep? In Advances in neural information processing systems, pp. 2654­2662, 2014.
Matthieu Courbariaux and Yoshua Bengio. BinaryNet: Training deep neural networks with weights and activations constrained to +1 or -1. arXiv preprint arXiv:1602.02830, 2016.
Matthieu Courbariaux, Yoshua Bengio, and Jean-Pierre David. BinaryConnect: Training Deep Neural Networks with binary weights during propagations. In Advances in Neural Information Processing Systems, pp. 3105­3113, 2015.
Facebook. Resnet training in torch, 2016. https://github.com/facebook/fb.resnet. torch.
Ian J Goodfellow, David Warde-Farley, Mehdi Mirza, Aaron Courville, and Yoshua Bengio. Maxout networks. arXiv preprint arXiv:1302.4389, 2013.
Babak Hassibi, David G Stork, and Gregory J Wolff. Optimal brain surgeon and general network pruning. In Neural Networks, 1993., IEEE International Conference on, pp. 293­299. IEEE, 1993.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity Mappings in Deep Residual Networks. arXiv preprint arXiv:1603.05027, 2016a.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep Residual Learning for Image Recognition. IEEE International Conference on Computer Vision and Pattern Recognition, 2016b.
Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531, 2015.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. arXiv preprint arXiv:1502.03167, 2015.
F. Juefei-Xu, V. N. Boddeti, and M. Savvides. Local Binary Convolutional Neural Networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 19­28. IEEE, July 2017.
Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. CIFAR Dataset, 2009.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. In Advances in neural information processing systems, pp. 1097­1105, 2012.
Yann LeCun, John S Denker, Sara A Solla, Richard E Howard, and Lawrence D Jackel. Optimal brain damage. In NIPs, volume 2, pp. 598­605, 1989.
Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278­2324, 1998.
Min Lin, Qiang Chen, and Shuicheng Yan. Network in network. ICLR, 2014.
Abhijit Mahalanobis and B.V.K. Vijaya Kumar. Polynomial filters for higher order correlation and multi-input information fusion. In Optoelectronic Information Processing: Invited Contributions from a Workshop Held 2-5 June 1997, Barcelona, Spain, pp. 1221. SPIE Press, 1997.
Abhijit Mahalanobis, BVK Vijaya Kumar, Sewoong Song, SRF Sims, and JF Epperson. Unconstrained correlation filters. Applied Optics, 33(17):3751­3759, 1994.
Julien Mairal. End-to-end kernel learning with supervised convolutional kernel networks. In Advances in neural information processing systems, pp. 1399­1407, 2016.
11

Under review as a conference paper at ICLR 2019
Julien Mairal, Piotr Koniusz, Zaid Harchaoui, and Cordelia Schmid. Convolutional kernel networks. In Advances in neural information processing systems, pp. 2627­2635, 2014.
Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y Ng. Reading digits in natural images with unsupervised feature learning. In NIPS workshop on deep learning and unsupervised feature learning, 2011.
Lorien Y Pratt. Comparing biases for minimal network construction with back-propagation, volume 1. Morgan Kaufmann Pub, 1989.
Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual recognition challenge. International Journal of Computer Vision, 115(3):211­252, 2015.
Bernhard Schölkopf, Alexander J Smola, et al. Learning with kernels: support vector machines, regularization, optimization, and beyond. MIT press, 2002.
Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. International Conference on Learning Representations (ICLR), 2015.
Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 1­9, 2015.
B.V.K. Vijaya Kumar, Abhijit Mahalanobis, and Richard D Juday. Correlation pattern recognition. Cambridge University Press, 2005.
Shuai Zhang, Jianxin Li, Pengtao Xie, Yingchun Zhang, Minglai Shao, Haoyi Zhou, and Mengyi Yan. Stacked kernel network. arXiv preprint arXiv:1711.09219, 2017.
Xingcheng Zhang, Zhizhong Li, Chen Change Loy, and Dahua Lin. Polynet: A pursuit of structural diversity in very deep networks. arXiv preprint arXiv:1611.05725, 2016a.
Yuchen Zhang, Percy Liang, and Martin J Wainwright. Convexified convolutional neural networks. arXiv preprint arXiv:1609.01000, 2016b.
12


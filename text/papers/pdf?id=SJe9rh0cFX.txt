Under review as a conference paper at ICLR 2019
ON THE UNIVERSAL APPROXIMABILITY AND COMPLEXITY BOUNDS OF QUANTIZED RELU NEURAL NETWORKS
Anonymous authors Paper under double-blind review
ABSTRACT
Compression is a key step to deploy large neural networks on resource-constrained platforms. As a popular compression technique, quantization constrains the number of distinct weight values and thus reducing the number of bits required to represent and store each weight. In this paper, we study the representation power of quantized neural networks. First, we prove the universal approximability of quantized ReLU networks on a wide class of functions. Then we provide upper bounds on the number of weights and the memory size for a given approximation error bound and the bit-width of weights for function-independent and functiondependent structures. Our results reveal that, to attain an approximation error bound of , the number of weights needed by a quantized network is no more than O log5(1/ ) times that of an unquantized network. This overhead is of much lower order than the lower bound of the number of weights needed for the error bound, supporting the empirical success of various quantization techniques. To the best of our knowledge, this is the first in-depth study on the complexity bounds of quantized neural networks.
1 INTRODUCTION
Various deep neural networks deliver state-of-the-art performance on many tasks such as object recognition and natural language processing using new learning strategies and architectures (Chen et al., 2017; He et al., 2016; Kumar et al., 2016; Ioffe & Szegedy, 2015; Vaswani et al., 2017). Their prevalence has extended to embedded or mobile devices for edge intelligence, where security, reliability or latency constraints refrain the networks from running on servers or in clouds. However, large network sizes with the associated expensive computation and memory consumption make edge intelligence even more challenging (Cheng et al., 2018; Sandler et al., 2018).
In response, as will be more detailed in Section 2, substantial effort has been made to reduce the memory consumption of neural networks while minimizing the accuracy loss. The memory consumption of neural networks can be reduced by either directly reducing the number of weights or decreasing the number of bits (bit-width) needed to represent and store each weight, which can be employed on top of each other (Choi et al., 2016). The number of weights can be reduced by pruning (Han et al., 2015b), weight sparsifying (Liu et al., 2015), structured sparsity learning (Wen et al., 2016) and low rank approximation (Denton et al., 2014). The bit-width is reduced by quantization that maps data to a smaller set of distinct levels (Sze et al., 2017). Note that while quantization may stand for linear quantization only (Li et al., 2017; Gysel et al., 2016) or nonlinear quantization only (Han et al., 2015a; Choi et al., 2016) in different works, our discussion will cover both cases.
However, as of today quantization is still only empirically shown to be robust and effective to compress various neural network architectures (Hubara et al., 2016; Zhou et al., 2017b; Zhuang et al., 2017). Its theoretical foundation still remains mostly missing. Specifically, many important questions remain unanswered. For example:
и Why even binarized networks, those most extremely quantized with bit-width down to one, still work well in some cases?
1

Under review as a conference paper at ICLR 2019
и To what extent will quantization decrease the expressive power of a network? Alternatively, what is the overhead induced by weight quantization in order to maintain the same accuracy?
In this paper, we provide some insights into these questions from a theoretical perspective. We focus on ReLU networks, which is among the most widely used in deep neural networks (Xu et al., 2015). We follow the idea from Yarotsky (2017) to prove the complexity bound by constructing a network. Specifically, given the number of distinct weight values , we construct a network that can achieve an arbitrarily small error bound to prove the universal approximability . The memory size of this network then naturally serves as an upper bound for the minimal network size.
When constructing the network to approximate a target function, we derive the bound in two cases, function-dependent structure and function-independent structure. In the case of functionindependent structure, the topology of the network is fixed for any function considered. In contrast, the topology of function-dependent structure can be adapted to a specific target function and thus enables a tighter bound.
We consider two types of quantization, linear quantization and nonlinear quantization (Sze et al., 2017). Linear quantization does mapping with a same distance between contiguous quantization levels and is usually implemented by storing weights as fixed-point numbers with reduced bit-width (Li et al., 2017; Gysel et al., 2016). Nonlinear quantization maps the data to quantization levels that are not uniformly distributed and can be either preselected or learned from training. Then the weights are stored using lossless binary coding (the index to a lookup table) instead of the actual values (Han et al., 2015a; Choi et al., 2016). The most important difference between linear quantization and nonlinear quantization is that the latter does not require weights to be low precision values.
Based on the bounds derived, we compare them with the available results on unquantized neural networks and discuss its implications. In particular, the main contributions of this paper include:
и We prove that even the most extremely quantized ReLU networks using two distinct weight values are capable of representing a wide class of functions with arbitrary accuracy.
и Given the number of distinct weights and the desired approximation error bound, we provide upper bounds on the number of weights and the memory size. We further show that our upper bounds have good tightness by comparing them with the lower bound of unquantized ReLU networks established in the literature.
и We show that, to attain the same approximation error bound , the number of weights needed by a quantized network is no more than O log5(1/ ) times that of an unquantized network. This overhead is of much lower compared with even the lower bound of the number of weights needed for the error bound. This partially explains why many state-ofthe-art quantization schemes work well in practice.
и We demonstrate that how a theoretical complexity bound can be used to estimate an optimal bit-width, which in turn enables the best cost-effectiveness for a given task.
The remainder of the paper is organized as follows. Section 2 reviews related works. Section 3 lays down the models and assumptions of our analysis. We prove the universal approximability and the upper bounds with function-independent structure in Section 4 and extend it to function-dependent structure in Section 5. We analyze the bound-based optimal bit-width in Section 6. Finally, Section 7 discusses the results and gets back to the questions raised above.
2 RELATED WORKS
Quantized Neural Networks: There are rich literatures on how to obtain quantized networks, either by linear quantization or nonlinear quantization (Zhou et al., 2017a; Leng et al., 2017; Shayar et al., 2017). It is reported that a pruned AlexNet can be quantized to eight bits and five bits in convolutional layers and fully connected layers, respectively, without any loss of accuracy. Similar results are also observed in LENET-300-100, LENET-5, and VGG-16 (Han et al., 2015a). One may argue that some of these benchmark networks are known to have redundancy. However, recent works show that quantization works well even on networks that are designed to be extremely small and compact. SqueezeNet, which is a state-of-the-art compact network, can be quantized to 8-bit while preserving
2

Under review as a conference paper at ICLR 2019

the original accuracy (Gysel et al., 2016; Iandola et al., 2016). There are some representative works that can achieve little accuracy loss on ImageNet classification even using binary or ternary weights (Courbariaux et al., 2015; Rastegari et al., 2016; Li et al., 2016; Zhu et al., 2016). More aggressively, some works also reduce the precision of activations, e.g. (Hubara et al., 2016; Rastegari et al., 2016; Faraone et al., 2018). Although the classification accuracy loss can be minimized, the universal approximation property is apparently lost, as with limited output precision the network cannot achieve arbitrary accuracy. Accordingly, we do not include them in the discussion of this paper. The limit of quantization is still unknown while the state-of-the-art keeps getting updated. For example, VGG-16 is quantized to 3-bit while maintaining the original accuracy (Leng et al., 2017). Motivated by the great empirical success, the training of quantized neural networks has been analyzed theoretically, but not the network capacity (Li et al., 2017; Choi et al., 2016).
Universal Approximability and Complexity Bounds: The universal approximability of ReLU networks is proved in Mhaskar & Micchelli (1992) and revisited in (Sonoda & Murata, 2017). Recently, Hanin (2017) discusses the expressive power of ReLU networks with bounded width and proves that a ReLU network with width d+1 can approximate any continuous convex function of d variables arbitrarily well. Shaham et al. (2016) construct a sparsely-connected depth-4 ReLU network and prove its error bound. Liang & Srikant (2016) prove that, for a large class of piecewise smooth functions, a network with ReLU and binary step units can provide an error bound with O(1/ ) layers and O(poly log(1/ )) neurons. The universal approximation property of low displacement rank (LDR) neural networks has been proved by Zhao et al. (2017) under a mild condition on the displacement operator, which is the result of another effective technique of neural network compression. In this work, we use the bound-by-construction idea for unquantized networks from Yarotsky (2017), but with new and additional construction components essential for quantized networks. Moreover, we prove the error bound of the constructed network with the new error terms imposed by quantization constraints.

3 MODELS AND ASSUMPTIONS

Throughout this paper, we define ReLU networks as feedforward neural networks with the ReLU activation function (x) = max(0, x). The ReLU network considered includes multiple input units, a number of hidden units, and one output unit. Without loss of generality, each unit can only connect to units in the next layer. Our conclusions on ReLU networks can be extended to any other networks that use piecewise linear activation functions with finite breakpoints such as leaky ReLU and ReLU6 immediately, as one can replace a ReLU network by an equivalent one using these activation functions while only increasing the number of units and weights by constant factors (Yarotsky, 2017).
We denote the number of distinct weight values as  (  2), for both linear quantization and nonlinear quantization. To store each weight, we only need log() 1 bits to encode the index, i.e. the bit-width is log(). The overhead to store sparse structures can be ignored because it varies depending on the implementation and can be easily reduced to the same order as the weight storage using techniques such as compressed sparse row (CSR) for nonlinear quantization. The number of bits needed to store the codebook can also be ignored because it has lower order of complexity.
We consider any function f in the Sobolev space: f  Wn,([0, 1]d) and

max ess sup |Dnf (x)|  1.
n:|n|n x[0,1]d

(1)

The space Wn, consists of all locally integrable function f :   R such that Dnf  L(), where |n|  n and  is an open set in Rd. We denote this function space as Fd,n in this paper.
Note that we only assume weak derivatives up to order n exist where n can be as small as 1 where the function is non-differentiable. We also only assume the Lipschitz constant to be no greater than 1 for the simplicity of the derivation. When the Lipschitz constant is bigger than 1, as long as it is bounded, the whole flow of the proof remains the same though the bound expression will scale accordingly.

1Throughout this paper, we omit base 2 for clarity of presentation

3

Under review as a conference paper at ICLR 2019

One might consider that we can transform an unquantized network within the error bound to a quantized one in a straightforward way by approximating every continuous-value weight with a combination of discrete weights with arbitrary accuracy. However, the complexity of such approximation (number of discrete weights needed) depends on the distribution of those continuous-value weights (e.g., their min and max), which may vary depending on the training data and network structure and a closed-form expression for the upper bounds is not possible. As such, a more elegant approach is needed. Below we will establish a constructive approach which allows us to bound the approximation analytically. Throughout this paper, we use "sub-network" to denote a network that is used as a part of the final network that approximates the target function.

4 FUNCTION-INDEPENDENT STRUCTURE

We start our analysis with function-independent structure, where the network topology is fixed for any function considered. We first present the approximation of some basic functions by subnetworks in Section 4.1. We then present the sub-network that approximates any weight in Section 4.2, and finally the approximation of general functions and our main results are in Section 4.3.

4.1 APPROXIMATION OF SQUARING/MULTIPLICATION

Proposition 1.

Let fsr

be a ReLU sub-network with only two weight values

1 2

and

-

1 2

.

The function

fs(x) = x2 on the segment [0, 1] can be approximated by fsr, such that (i) if x = 0, fsr(x) = 0; (ii)

the approximation error s  2-2(r+1); (iii) the depth is O (r); (iv) the width is a constant; (v) the

number of weight is O (r).

The proof and the details of the sub-network constructed are included in Appendix A.1. Once
the approximation to squaring function is obtained, we get Proposition 2 by the fact that 2xy = (x + y)2 - x2 - y2.

Proposition 2.

Given x



[-1, 1],

y



[-1, 1], and only two weight values

1 2

and

-

1 2

,

there

is

a ReLU sub-network with two input units that implements a function О : R2  R, such that (i) if

x = 0 or y = 0, then О (x, y) = 0; (ii) for any x, y, the error О = | О (x, y) - xy|  6 и 2-2(r+1);

(iii) the depth is O (r); (iv) the width is a constant; (v) the number of weights is O (r).

Proof. Build three sub-networks fsr as described in Proposition 1 and let О (x, y) = 2 (fsr (|x + y|/2) - fsr (|x|/2) - fsr (|y|/2)) .

(2)

Then the statement (i) is followed by property (i) of Proposition 1. Using the error bound in Proposition 1 and Equation (2), we get the error bound of О :

О  6 и 2-2(r+1).

(3)

Since a sub-network Babs that computes (x) + (-x) can be constructed to get the absolute value

of x trivially, we can construct О (x, y) as a linear combination of three parallel fsr and feed them

with

|x| 2

,

|y| 2

,

and

|x+y| 2

.

Then

claims

of

statement

(iii),

(iv),

and

(v)

are

also

obtained.

4.2 APPROXIMATION OF WEIGHTS

Proposition 3. A connection with any weight w  [-1, 1] can be approximated by a ReLU sub-

network that has only   2 distinct weights, such that (i) the sub-network is equivalent to a connection with weight w while the approximation error is bounded by 2-t i.e., |w - w| < 2-t;

(ii) the depth is O

1
t -1

; (iii) the width is O(t); (iv) the number of weights is O

t

1 -1

+1

.

Proof. Consider that we need a weight w to feed the input x to a unit in the next layer as wx. With a limited number of distinct weight values, we can construct the weight we need by cascade and combination.
For clarity, we first consider w  0 and x  0, and relax these assumptions later. The connections with w = 0 can be seen as an empty sub-network while w = 1 can be easily implemented by 4 units

4

Under review as a conference paper at ICLR 2019

with

weight

1 2

.

Now

we

show

how

to

represent

all

integral

multiples

of

2-t

from

2-t

to

1-

2-t,

which will lead to the statement (i) by choosing the nearest one from w as w . Without loss of

generality,

we

assume

1
t -1

is

an

integer.

We

use



weights

that

include

-

1 2

and

W:

12

-2

W {2-1, 2-t -1 , 2-t -1 , и и и , 2-t -1 }.

(4)

We first construct all w from Wc which is defined as

Wc {2-1, 2-2, и и и , 2-(t-1)}.

(5)

Similar

to

a

numeral

system

with

radix

equal

to

1
t -1

,

any

wi



Wc

can

be

obtained

by

concatenating

weights

from

W

while

every

weights

in

W

is

used

no

greater

than

1
t -1

-

1

times.

After that, all integral multiples of 2-t from 2-t to 1 - 2-t can be represented by a binary expansion

on Wc.

Note that connections in the last layer for binary expansion use weight

1 2

,

thus

additional

2-1 is multiplied to scale the resolution from 2-(t-1) to 2-t. Since for any weight in Wc we need

to concatenate no more than 

1
t -1

-1

weights in a straight line, the sub-network has no greater

than 

1
t -1

-1

+ 1 layers, and no greater than 4t

1
t -1

-1

+ 8t + 4 weights.

We now relax the assumption w  0. When w < 0, the sub-network can be constructed as w = |w|,

while

we

use

-

1 2

instead of

1 2

in the last layer.

To relax the assumption x



0, we can make a

duplication

of

the

sub-network.

Let

all

the

weights

in

the

first

layer

of

the

sub-network

be

1 2

for

one

and

-

1 2

for

the

other.

Here

we

are

utilizing

the

gate

property

of

ReLU.

In

this

way,

one

sub-network

is activated only when x > 0 and the other is activated only when x < 0. The sign of the output

can be adjusted by flipping the sign of weights in the last layer. Note that the configuration of the

sub-network is solely determined by w and works for any input x.

The efficiency of the weight approximation is critical to the overall complexity. Compared with the

weight

selection

as

{2-1,

2-t

1 -1

,

2-t

2 -1

,

.

.

.

,

2-t

(-2) -1

},

our

approximation

reduces

the

number

-2
of weights by a factor of t -1 .

4.3 APPROXIMATION OF GENERAL FUNCTIONS

With the help of Proposition 2 and Proposition 3, we are able to prove the upper bound for general functions.

Theorem 1. For any f  Fd,n , given  distinct weights, there is a ReLU network with fixed structure that can approximate f with any error  (0, 1), such that (i) the depth is

O

1
 log -1 (1/

) + log (1/

)

; (ii) the number of weights is O



log

1 -1

+1

(1/

) (1/

d
)n

; (iii)

the number of bits needed to store the network is O



log

()

log

1 -1

+1

(1/

) (1/

d
)n

.

The complete proof and the network constructed can be found in Appendix A.2. We first approxi-

mate f by f2 using the Taylor polynomial of order n - 1 and prove the approximation error bound.

Note that even when f is non-differentiable (only first order weak derivative exists), the Taylor

polynomial

of order

0 at x

=

m N

can still be

used,

which takes the

form of

Tm

=

f

(

m N

).

Then

we

approximate f2 by a ReLU network that is denoted as f with bounded error. After that, we present

the ReLU implementation the network f and the complexity.

The discussion above focuses on nonlinear quantization which is a more general case compared to

linear quantization. For linear quantization, which strictly determines the available weight values

once  is given, we can use the same proof for nonlinear quantization except for a different sub-

network for weight

approximation

with

width t

and depth

t log



+1.

Here

we

give

the theorem

and

the proof is included in Appendix A.3.

Theorem 2.

For any f



Fd,n

, given weight maximum precision

1 

,

there

is

a

ReLU

network

with fixed structure that can approximate f with any error  (0, 1), such that (i) the depth is

O (log (1/ )); (ii) the number of weights is O

log (1/

)+

log2 (1/ log 

)

d
(1/ ) n

; (iii) the number

of bits needed to store the network is O

log() log (1/ ) + log2 (1/ )

d
(1/ ) n

.

5

Under review as a conference paper at ICLR 2019

5 FUNCTION-DEPENDENT STRUCTURE

The network complexity can be reduced if the network topology can be set according to a specific target function, i.e. function-dependent structure. In this section, we provide an upper bound for function-dependent structure when d = 1 and n = 1, which is asymptotically better than that of a
fixed structure. Specifically, we first define an approximation to f (x) as f (x) that has special properties to match the peculiarity of quantized networks. Then we use piecewise linear interpolation
and "cached" functions (Yarotsky, 2017) to approximate f (x) by a ReLU network.

5.1 FUNCTION TRANSFORMATION

While simply using piecewise linear interpolation at the scale of can satisfy the error bound with O (1/ ) weights, the complexity can be reduced by first doing interpolation at a coarser scale and
then fill the details in the intervals to make the error go down to . By assigning a "cached" function
to every interval depending on specific function and proper scaling, the number of weights is reduced to O log-1 (1/ ) 1/ when there is no constraint on weight values (Yarotsky, 2017).

The key difficulty in applying this approach to quantized ReLU networks is that the required linear

interpolation at

i T

exactly where i = 1, 2, и и и

,T

is not feasible because of the constraint on weight

selection. To this end, we transform f (x) to f (x) such that the approximation error is bounded;

the Lipschitz constant is preserved; f

i T

are reachable for the network under the constraints of

weight selection without increasing the requirement on weight precision. Then we can apply the

interpolation and cached function method on f (x) and finally approximate f (x) with a quantized

ReLU network.

Formally, we get the following proposition and the proof can be found in Appendix A.4.

Proposition 4. For any f  F1,1, there exists a function f (x) such that (i)f (x) is a continuous

function

with

Lipschitz

constant

1;

(ii)

f

(

i T

)

=

Tf

i T

/2-t

2-t T

;

(iii)

|f (x) - f (x)|

<

2-t T

.

5.2 APPROXIMATION BY RELU NETWORKS

With the help of Proposition 4 and the weight construction method described in Section 4.2, we are able to apply the interpolation and cached function approach. Denoting the output of the network as
f (x), we have |f (x)-f (x)| = |f (x)-f (x)|+|f (x)-f (x)|  by choosing appropriate hyperparameters which are detailed in Appendix A.5 and the network complexity is obtained accordingly.

Theorem 3. For any f  F1,1 , given  distinct weights, there is a ReLU network with function-dependent structure that can approximate f with any error  (0, 1), such
1
that (i) the depth is O  (log log (1/ )) -1 + log (1/ ) ; (ii) the number of weights is

O

 (log log (1/

))

1 -1

+1

+ (1/

)

(iii) the number of bits needed to store the network is

O

log 

 (log log (1/

))

1 -1

+1

+

(1/

)

.

Using the different weight construction approach as in the case of function-independent structure,

we have the result for linear quantization:

Theorem 4.

For any f



F1,1

, given weight maximum precision

1 

,

there

is

a

ReLU

network

with

function-dependent structure that can approximate f with any error  (0, 1), such that (i) the

depth is O (log (1/ )); (ii) the number of weights is O (1/ ); (iii) the number of bits needed to store

the network is O (log()/ ).

6 BOUND-BASED OPTIMAL BIT-WIDTH
In this section, we first introduce the optimal bit-width problem and then show how a theoretical bound could potentially be used to estimate the optimal bit-width of a neural network.
Because of the natural need and desire of comparison with competitive approaches, most quantization techniques are evaluated on some popular reference networks, without modification of the

6

Under review as a conference paper at ICLR 2019

20 4 256x256x166 (ADNI MRI)
10 3.5 256x256x3 (ImageNet)
0 3 32x32x3 (CIFAR-10)
28x28 (MNIST) -10 2.5

-20 2

-30 1.5 100 101 102 103 100 102 104 106

(a) Scaled derivative of M ()

(b) Optimal bit-width

Figure 1: Quantitative evaluation of the derivative of M () and the optimal bit-width log(opt).

The

derivative

is

scaled

by

log

- -1

(3n2d

)

to

fit

in

the

same

range.

log

- -1

(3n2d

)

is

a

positive

monotonically increasing function and thus does not affect the trends too much. Note that  is the

number of distinct weight values and thus log() is the corresponding bit-width. It can be seen that

and n only matter log(opt) when d is small (< 102). We mark the input dimension d of various

image data set and their corresponding log(opt). It shows that the optimal bit-width increase very

slowly with d.

network topology. On the one hand, the advancement of lossless quantization almost stalls at a bit-width between two and six (Han et al., 2015a; Choi et al., 2016; Sze et al., 2017; Blott et al., 2017; Su et al., 2018; Faraone et al., 2018). A specific bit-width depends on the compactness of the reference network and the difficulty of the task. On the other hand, the design space, especially the different combinations of topology and bit-width, is largely underexplored because of the complexity, resulting in sub-optimal results. A recent work by Su et al. (2018) empirically validates the benefit of exploring flexible network topology during quantization. That work adds a simple variable of network expanding ratio, and shows that a bit-width of four achieves the best cost-accuracy trade-off among limited options in {1, 2, 4, 8, 16, 32}. Some recent effort on using reinforcement learning to optimize the network hyper-parameters (He et al., 2018) could potentially be used to address this issue. But the current design space is still limited to a single variable per layer (such as the pruning ratio based on a reference network). How to estimate an optimal bit-width for a target task without training could be an interesting research direction in the future.

The memory bound expression as derived in this paper helps us to determine whether there is an

optimal  that would lead to the lowest bound and most compact network (which can be translated

to computation cost in a fully connected structure) for a given target function. For example, by drop-

ping the lower-order term and ignoring the rounding operator, our memory bound can be simplified

as

M ()

=

1



log()

log

1 -1

+1

(3n2d

/

)

(6)

where 1 is a constant determined by , n, and d. We can find an optimal  that minimizes M ():

opt = argmin M ()


(7)

As is detailed in Appendix B, we prove that there exists one and only one local minimum (hence

global minimum) in the range of [2, ) whenever

<

1 2

.

We also

show that

opt

is

determined by

log 3n2d/ , which can be easily dominated by d. Based on such results, we quantitatively evaluate

the derivative of M (), and based on which the optimal bit-width log(opt) under various settings in Figure 1a and Figure 1b, respectively. In Figure 1b, we also mark the input dimension of a few

image data sets. It is apparent to see that the optimal bit width derived from M () is dominated by

d and lies between one and four for a wide range of input size. This observation is consistent with

most existing empirical research results, hence showing the potential power of our theoretical bound

derivation.

7

Under review as a conference paper at ICLR 2019

Since the bounds are derived for fully connected networks and depend on the construction approach, the interesting proximity between log(opt) and the empirical results can not be viewed as a strict theoretical explanation. Regardless, we show that the complexity bound may be a viable approach to understand the optimal bit-width problem, thus potentially accelerating the hyper-parameter optimization of deep neural networks. We defer such a thorough investigation of the optimal bit-width or optimal hybrid bit-width configuration across the network to our future work.

7 DISCUSSION

In this section, we further discuss the bound of nonlinear quantization with a function-independent structure as the generality of nonlinear quantization. The availability of unquantized functionindependent structures in literature also makes it an excellent reference for comparison.

Comparison with the Upper Bound: The quality of an upper bound lies on its tightness. Compared
with the most recent work on unquantized ReLU networks (Yarotsky, 2017), where the upper bound
d
on the number of weights to attain an approximation error is given by O log(1/ ) (1/ ) n , our

result for a quantized ReLU network is given by O



log

1 -1

+1

(1/

)

d
(1/ ) n

, which translates

1
to an increase by a factor of  log -1 (1/ ) . Loosely speaking, this term reflects the loss of expressive power because of weight quantization, which decreases quickly as  increases.

Comparison with the Lower Bound: We also compare our bound with the lower bound of the

number of weights needed to attain an error bound of to have a better understanding on the tightness

of the bound. We use the lower bound for unquantized ReLU networks from (Yarotsky, 2017), as it

is also a natural lower bound for quantized ReLU networks. Under the same growth rate of depth, the

lower bound is given by (log-3(1/ ) (1/ )d/n), while our upper bound is, within a polylog factor

when



is

a

constant,

O(

log

1 -1

+1

(1/

)(1/

)d/n).

The

comparison

validates

the

good

tightness

of our upper bound.

The Upper Bound of Overhead: More importantly, the above comparison yields an upper bound on the possible overhead induced by quantization. By comparing the expressions of two bounds while treating  as a constant, we can show that, to attain the same approximation error bound , the number of weights needed by a quantized ReLU network is no more than O(log5(1/ )) times that needed by an unquantized ReLU network. Note that this factor is of much lower order than the lower bound (log-3(1/ ) (1/ )d/n). This little overhead introduced by weight quantization explains in part the empirical success on network compression and acceleration by quantization and also answers in part the questions as raised in Section 1. Given the significant benefits of quantization in term of memory and computation efficiency, we anticipate that the use of quantization networks will continue to grow, especially on resource-limited platforms.

Future Work: There remain many other avenues for future investigation. For example, although we derived the first upper bound of quantized neural networks, the lower bound is still missing. If a tight lower bound of the network size is established, it could be combined with the upper bound to give a much better estimation of required resources and the optimal bit-width. We believe the trends associated with the bounds can also be useful and deserve some further investigation. For example, the trend may help hardware designers in their early stage of design exploration without the need of lengthy training. While we assume a uniform bit-width across all layers, another area of research is to allow different bit-widths in different layers, which could achieve better efficiency and potentially provide theoretical justifications on the emerging trend of hybrid quantization (Zhang et al., 2017; Wang et al., 2018).

REFERENCES
Michaela Blott, Thomas B Preu▀er, Nicholas Fraser, Giulio Gambardella, Kenneth O'Brien, Yaman Umuroglu, and Miriam Leeser. Scaling neural network performance through customized hardware architectures on reconfigurable logic. In Computer Design (ICCD), 2017 IEEE International Conference on, pp. 419Г422. IEEE, 2017.

8

Under review as a conference paper at ICLR 2019
Guobin Chen, Wongun Choi, Xiang Yu, Tony Han, and Manmohan Chandraker. Learning efficient object detection models with knowledge distillation. In Advances in Neural Information Processing Systems, pp. 742Г751, 2017.
Jian Cheng, Pei-song Wang, Gang Li, Qing-hao Hu, and Han-qing Lu. Recent advances in efficient computation of deep convolutional neural networks. Frontiers of Information Technology & Electronic Engineering, 19(1):64Г77, 2018.
Yoojin Choi, Mostafa El-Khamy, and Jungwon Lee. Towards the limit of network quantization. arXiv preprint arXiv:1612.01543, 2016.
Matthieu Courbariaux, Yoshua Bengio, and Jean-Pierre David. Binaryconnect: Training deep neural networks with binary weights during propagations. In Advances in Neural Information Processing Systems, pp. 3123Г3131, 2015.
Emily L Denton, Wojciech Zaremba, Joan Bruna, Yann LeCun, and Rob Fergus. Exploiting linear structure within convolutional networks for efficient evaluation. In Advances in Neural Information Processing Systems, pp. 1269Г1277, 2014.
Julian Faraone, Nicholas Fraser, Michaela Blott, and Philip HW Leong. Syq: Learning symmetric quantization for efficient deep neural networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 4300Г4309, 2018.
Philipp Gysel, Mohammad Motamedi, and Soheil Ghiasi. Hardware-oriented approximation of convolutional neural networks. arXiv preprint arXiv:1604.03168, 2016.
Song Han, Huizi Mao, and William J Dally. Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding. arXiv preprint arXiv:1510.00149, 2015a.
Song Han, Jeff Pool, John Tran, and William Dally. Learning both weights and connections for efficient neural network. In Advances in Neural Information Processing Systems, pp. 1135Г1143, 2015b.
Boris Hanin. Universal function approximation by deep neural nets with bounded width and relu activations. arXiv preprint arXiv:1708.02691, 2017.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual networks. In European Conference on Computer Vision, pp. 630Г645. Springer, 2016.
Yihui He, Ji Lin, Zhijian Liu, Hanrui Wang, Li-Jia Li, and Song Han. Amc: Automl for model compression and acceleration on mobile devices. In Proceedings of the European Conference on Computer Vision (ECCV), pp. 784Г800, 2018.
Itay Hubara, Matthieu Courbariaux, Daniel Soudry, Ran El-Yaniv, and Yoshua Bengio. Quantized neural networks: Training neural networks with low precision weights and activations. arXiv preprint arXiv:1609.07061, 2016.
Forrest N Iandola, Song Han, Matthew W Moskewicz, Khalid Ashraf, William J Dally, and Kurt Keutzer. Squeezenet: Alexnet-level accuracy with 50x fewer parameters andА 0.5 mb model size. arXiv preprint arXiv:1602.07360, 2016.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. arXiv preprint arXiv:1502.03167, 2015.
Ankit Kumar, Ozan Irsoy, Peter Ondruska, Mohit Iyyer, James Bradbury, Ishaan Gulrajani, Victor Zhong, Romain Paulus, and Richard Socher. Ask me anything: Dynamic memory networks for natural language processing. In International Conference on Machine Learning, pp. 1378Г1387, 2016.
Cong Leng, Hao Li, Shenghuo Zhu, and Rong Jin. Extremely low bit neural network: Squeeze the last bit out with admm. arXiv preprint arXiv:1707.09870, 2017.
Fengfu Li, Bo Zhang, and Bin Liu. Ternary weight networks. arXiv preprint arXiv:1605.04711, 2016.
9

Under review as a conference paper at ICLR 2019
Hao Li, Soham De, Zheng Xu, Christoph Studer, Hanan Samet, and Tom Goldstein. Training quantized nets: A deeper understanding. In Advances in Neural Information Processing Systems, pp. 5811Г5821, 2017.
Shiyu Liang and R Srikant. Why deep neural networks for function approximation? arXiv preprint arXiv:1610.04161, 2016.
Baoyuan Liu, Min Wang, Hassan Foroosh, Marshall Tappen, and Marianna Pensky. Sparse convolutional neural networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 806Г814, 2015.
Hrushikesh N Mhaskar and Charles A Micchelli. Approximation by superposition of sigmoidal and radial basis functions. Advances in Applied mathematics, 13(3):350Г373, 1992.
Mohammad Rastegari, Vicente Ordonez, Joseph Redmon, and Ali Farhadi. Xnor-net: Imagenet classification using binary convolutional neural networks. In European Conference on Computer Vision, pp. 525Г542. Springer, 2016.
Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen. Inverted residuals and linear bottlenecks: Mobile networks for classification, detection and segmentation. arXiv preprint arXiv:1801.04381, 2018.
Uri Shaham, Alexander Cloninger, and Ronald R Coifman. Provable approximation properties for deep neural networks. Applied and Computational Harmonic Analysis, 2016.
Oran Shayar, Dan Levi, and Ethan Fetaya. Learning discrete weights using the local reparameterization trick. arXiv preprint arXiv:1710.07739, 2017.
Sho Sonoda and Noboru Murata. Neural network with unbounded activation functions is universal approximator. Applied and Computational Harmonic Analysis, 43(2):233Г268, 2017.
Jiang Su, Nicholas J Fraser, Giulio Gambardella, Michaela Blott, Gianluca Durelli, David B Thomas, Philip HW Leong, and Peter YK Cheung. Accuracy to throughput trade-offs for reduced precision neural networks on reconfigurable logic. In Applied Reconfigurable Computing. Architectures, Tools, and Applications: 14th International Symposium, ARC 2018, Santorini, Greece, May 2-4, 2018, Proceedings, volume 10824, pp. 29. Springer, 2018.
Vivienne Sze, Yu-Hsin Chen, Tien-Ju Yang, and Joel Emer. Efficient processing of deep neural networks: A tutorial and survey. arXiv preprint arXiv:1703.09039, 2017.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Information Processing Systems, pp. 6000Г6010, 2017.
Junsong Wang, Qiuwen Lou, Xiaofan Zhang, Chao Zhu, Yonghua Lin, and Deming Chen. Design flow of accelerating hybrid extremely low bit-width neural network in embedded fpga. arXiv preprint arXiv:1808.04311, 2018.
Wei Wen, Chunpeng Wu, Yandan Wang, Yiran Chen, and Hai Li. Learning structured sparsity in deep neural networks. In Advances in Neural Information Processing Systems, pp. 2074Г2082, 2016.
Bing Xu, Naiyan Wang, Tianqi Chen, and Mu Li. Empirical evaluation of rectified activations in convolutional network. arXiv preprint arXiv:1505.00853, 2015.
Dmitry Yarotsky. Error bounds for approximations with deep ReLU networks. Neural Networks, 94:103Г114, 2017.
Xiaofan Zhang, Xinheng Liu, Anand Ramachandran, Chuanhao Zhuge, Shibin Tang, Peng Ouyang, Zuofu Cheng, Kyle Rupnow, and Deming Chen. High-performance video content recognition with long-term recurrent convolutional network for fpga. In Field Programmable Logic and Applications (FPL), 2017 27th International Conference on, pp. 1Г4. IEEE, 2017.
10

Under review as a conference paper at ICLR 2019 Liang Zhao, Siyu Liao, Yanzhi Wang, Jian Tang, and Bo Yuan. Theoretical properties for neural
networks with weight matrices of low displacement rank. arXiv preprint arXiv:1703.00144, 2017. Aojun Zhou, Anbang Yao, Yiwen Guo, Lin Xu, and Yurong Chen. Incremental network quantiza-
tion: Towards lossless cnns with low-precision weights. arXiv preprint arXiv:1702.03044, 2017a. Yiren Zhou, Seyed-Mohsen Moosavi-Dezfooli, Ngai-Man Cheung, and Pascal Frossard. Adaptive
quantization for deep neural network. arXiv preprint arXiv:1712.01048, 2017b. Chenzhuo Zhu, Song Han, Huizi Mao, and William J Dally. Trained ternary quantization. arXiv
preprint arXiv:1612.01064, 2016. Bohan Zhuang, Chunhua Shen, Mingkui Tan, Lingqiao Liu, and Ian Reid. Towards effective low-
bitwidth convolutional neural networks. arXiv preprint arXiv:1711.00205, 2017.
11

Under review as a conference paper at ICLR 2019

A PROOFS

A.1 THE PROOF OF PROPOSITION 1

Proposition 1. The function fs(x) = x2 on the segment [0, 1] can be approximated by a ReLU

sub-network

fsr

with

only

two

weight

values

1 2

and

-

1 2

,

such

that

(i)

if

x

=

0,

fsr (x)

=

0;

(ii)

the

approximation error s  2-2(r+1); (iii) the depth is O (r); (iv) the width is a constant; (v) the

number of weight is O (r).

Proof. For fs(x) = x2, let fsr be the piecewise linear interpolation of f with 2r + 1 uniformly

distributed

breakpoints

k 2r

,

k

=

0, . . . , 2r.

We have fsr

k 2r

=

k 2r

2,k

=

0, . . . , 2r

and the

approximation error s = ||fsr(x)-fs(x)||  2-2(r+1). We can use the function g : [0, 1]  [0, 1] to obtain fsr:

g(x) =

2x 2(1 - x)

x

<

1 2

x



1 2

,

(8)

r
fsr(x) = x - 2-2igi(x)

(9)

i=1

where gi(x) is the i-th iterate of g(x). Since g(x) can be implemented by a ReLU network as

g(x)

=

2(x)

-

4(x

-

1 2

),

g r (x)

can

be

obtained

by

concatenating

such

implementation

of

g(x)

for r times.

Now, to implement fsr(x) based on gr which can be easily constructed with

(x), all we additional

need are weights {2-2, 2-4, и

2r

layers

and

the

weight

1 2

.

и и , 2-2(r-1), Then, fsr(x)

2-2r }, can be

implemented by a ReLU sub-network as in Figure 2. A connection from or to a block indicates

connections

to

all

units

in

the

block.

Some

units

are

copied

to

compensate

the

scaling

caused

by

1 2

and to facilitate the construction of higher level networks.

Figure 2: ReLU Implementation of fsr(x) where b and w denote bias and weight respectively. Details of block Bm and block Bg are depicted on the left.

A.2 THE PROOF OF THEOREM 1

Theorem 1. For any f  Fd,n , given  distinct weights, there is a ReLU network with fixed structure that can approximate f with any error  (0, 1), such that (i) the depth is

O

1
 log -1 (1/

) + log (1/

)

; (ii) the number of weights is O



log

1 -1

+1

(1/

) (1/

d
)n

; (iii)

the number of bits needed to store the network is O



log

()

log

1 -1

+1

(1/

) (1/

d
)n

.

Proof. The proof is composed of four steps. We first approximate f by f2 using the Taylor poly-

nomial of order n - 1 and prove the approximation error bound. Note that even when f is non-

differentiable (only still be used, which

first order weak derivative takes the form of Tm = f (

exist), the Taylor polynomial

m N

).

Then

we

approximate

f2

of by

order

0

at

x

=

m N

a ReLU network

can that

is denoted as f with bounded error. After that, we present the ReLU implementation the network

f and the complexity of the network.

12

Under review as a conference paper at ICLR 2019

We use a partition of unity on [0, 1]d: m m(x)  1, x  [0, 1]d where m = (m1, и и и , md)  {0, 1, и и и , N }d, and h(x) is defined as follows:

where N is a constant and

d
m(x) = h(3N xk - 3mk),
k=1
1 |x|  1  h(x) = 2 - |x| 1 < |x| < 2 . 0 |x|  2

(10) (11)

Note that supp m  {x : polynomial for the function f

axtkx-=mNNmk

< as

1 N

k}.

For all m, we have the order n - 1 Taylor

Dnf

mn

Tm(x) =

n!

n:|n|<n

x=

m N

x- N

.

(12)

To get a more realizable approximation for quantized networks, we define Tm(x) =

n:|n|<n m,n

x

-

m N

n

where m,n

is

Dnf

n!

x=

m N

rounded to the nearest integral multiple of

1 n

d N

n-|n|. Then we get an approximation to f using Tm and m as f2

m{0,иии ,N }d mTm.

Then the approximation error of f2 is bounded by Equation (13).

|f (x) - f2(x)| = | m(x)(f (x) - Tm(x))|
m

 |f (x) - Tm(x)|

m:|xk

-

mk N

|<

1 N

k



2d

max

m:|xk

-

mk N

|<

1 N

k

|f (x)

-

Tm(x)|

+

2d

max

m:|xk

-

mk N

|<

1 N

k

|Tm(x)

-

Tm(x)|

 2ddn n!

1 N

n

max ess sup |Dnf (x)| + 2d

max

n:|n|=n x[0,1]d

n:|n|<n

|m,n

-

Dnf n!

|

(x - m )n N

 2ddn

1 n 2d +

n! N

n

d

n
+иии+

d1

d n-1

N NN

 2d

dn

1

1+

N n!

(13)

The second step follows m(x) = 0 when x / suppm. In the third step we turn the sum to multiplication, because for any x there are up to 2d terms m(x) that are not equal to zero. The

fourth step uses a Lagrange's form of the Taylor remainder. The fifth step follows different round

precision of m,n in different order and the fact that the number of terms with order i is not greater than di.

We rewrite f2 as

f2(x) =

m,nfm,n(x),

(14)

m{0,иии ,N }d n:|n|<n

where

fm,n(x) = m

x- m N

n
.

(15)

Note that m,n is a constant and thus f2 is a linear combination of at most dn(N + 1)d terms of

fm,n(x). Note that when d = 1, the number of terms should be n(N + 1)d instead; but for simplicity

of presentation we loosely use the same expression as they are on the same order.

We define an approximation to fm,n(x) as fm,n(x). The only difference between fm,n(x) and fm,n(x) is that all multiplication operations are approximated by О as discussed in Proposition 2. Consider

that if we construct our function О with | О (x, y) - xy| < О = 2-2(r+1), then

| О (x, y) - xz|  |x(y - z)| + О .

(16)

13

Under review as a conference paper at ICLR 2019

Applying Equation (16) to |fm,n(x) - fm,n(x)| repeatedly, we bound it to Equation (17).

fm,n(x) - fm,n(x) =

О h(3N x1 - 3m1), и и и , О h(3N xd - 3md), О

xi1

-

m1 N

,О

иии ,

xi|n|

-

m|n| N

- h(3N x1 - 3m1) h(3N x2 - 3m2) и и и h(3N xd - 3md)

xi1

-

mi1 N

иии

xi|n|

-

mi|n| N

 (d + |n|) О i1, и и и , i|n|  {1, 2, и и и , d}

(17)

Finally, we define our approximation to f (x) as f (x):

f (x)

m,nfm,n(x).
m{0,иии ,N }d n:0<|n|<n

(18)

Using Equation (17), we get the error bound of the approximation to f2(x) as in Equation (19).

|f (x) - f2(x)| = |

m,n fm,n(x) - fm,n(x) |

m{0,иии ,N }d n:|n|<n

= | m,n fm,n(x) - fm,n(x) |
m:xsuppm n:|n|<n

(19)



2d

max
m:xsuppm

n:|n|<n

|fm,n(x)

-

fm,n(x)|

 2ddn (d + n - 1) О .

The second line follows again the support property and statement (i) of Proposition 2. The third line uses the bound |m,n|  1. The fourth line is obtained by inserting Equation (17).

Then the final approximation error bound is as follows:

|f (x) - f (x)|  |f (x) - f2(x)| + |f (x) - f2(x)|

 2ddn (d + n - 1) О + 2d+1

d N

n
.

(20)

Using

statement

(ii)

of

Proposition

2

and

choosing

r

as

r

=

log(6N n(d+n-1)) 2

- 1,

the

approximation

error turns to

|f (x) - f (x)|  3 и 2d

dn .

(21)

N

Therefore, for any f  Fd,n and  (0, 1), there is a ReLU network f that approximate f with

error bound if we choose N  3 и 2ddn

)

1 n

.

We now present the construction of the network for f (x). If every fm,n(x) can be computed by a

sub-network, then f (x) is simply a weighted sum of all outputs of fm,n(x). By Proposition 3, we

can implement the needed weights m,n

by choosing t

=

log

nN n dn

.

Then we simplify the task to

constructing fm,n(x).

О can h(x) =

be implemented as discussed h(-x), we can first compute

in |xi

Proposition 2.

-

mi N

|

as

(xi

For

-

mi N

h( 3N ) + (

xi -

mi N

-

mi
N
xi)

, noticing that and then scale it

to 3N

|xi -

mi N

|

.

The implementation of h(x) can thus be simplified as 1 - (x - 1) + (x - 2)

since the input is nonnegative.

Furthermore, by choosing N

as

cd2

where c



N and c

>

1,

1 N

is an

integral

multiple

of

1 n

d N

n

if

n

>

1.

When

n

=

1,

1 N

is an integral multiple of

1 n

d N

n

2
. As

discussed in Proposition 3, we build a weight construction network Bw in the way that all integral

multiples

of

the

minimal

precision

can

be

obtained.

Therefore,

all

mi N

can

be

obtained

in

the

same

way as m,n, except that we need to concatenate two weight construction sub-networks.

14

Under review as a conference paper at ICLR 2019

Figure 3: ReLU implementation of f (x). The connections of all Bfmn are the same. Every connection from BN to other blocks has no greater than two weights.

Table 1: Block configuration for function-independent structure

block
Bw
BN Babs Bs Bh BО

function

construct weights

construct

1 N

,

и

и

и

,

N -1 N

get absolute values

scale by 3N

implement h(x) implement О (x, y)

width
t
2N 4 4 12 60

depth
1
 t -1 - 1 + 1

1

1

log

3N 2

1

3r + 1

Now we analyze the complexity of the network. The implementation of f (x) is shown in Figure 3.

The function and size of blocks are listed in Table 1. Then we are able to obtain the complexity of

the network. While we can write the complexity of the network in an explicit expression, here we

use the O notation for clarity. Let Nd, Nw, Nb be the depth, the number of weights, and the number

of bits required respectively. The weight construction blocks Bw have the highest order of number

of weights and we have Nw = O

t

1 -1

+1

N

d

. Meanwhile, we get Nd = O

1
t -1 + log N

.

Inserting

t

=

log

nN n dn

and

N

=

O

1
(1/ ) n

, we get Nd = O

1
 log -1 (1/ ) + log (1/ )

and

Nw = O



log

1 -1

+1

(1/

) (1/

d
)n

. Multiplying Nw by log , Nb is obtained. This concludes

the proof of Theorem 1.

A.3 THE PROOF OF THEOREM 2

Theorem 2.

For any f



Fd,n

, given weight maximum precision

1 

,

there

is

a

ReLU

network

with fixed structure that can approximate f with any error  (0, 1), such that (i) the depth is

O (log (1/ )); (ii) the number of weights is O

log (1/

)+

log2 (1/ log 

)

d
(1/ ) n

; (iii) the number

of bits needed to store the network is O

log() log (1/ ) + log2 (1/ )

d
(1/ ) n

.

With



distinct

values,

a

linearly

quantized

network

has

a

minimal

resolution

of

1 

.

The

proof

for

the approximability of linear quantization can be done in the same way as Theorem 1 except for a

different sub-network for weight approximation. We still construct Wc in Proposition 3 first and any

weight

value

from

Wc

can

be

obtained

by

multiply

at

most

t log 

weights.

Thus

the

width

and

depth

of

the

weight

approximation

network

will

be

t

and

t log 

+1

respectively.

Updating

the

Bw

in

Table

1, we obtain the complexity accordingly.

15

Under review as a conference paper at ICLR 2019

Figure 4: An example to illustrate the relationship between f (x), f + and f (x).

A.4 THE PROOF OF PROPOSITION 4

Proposition 4. For any f  F1,1, the f (x) defined above has the following properties. (i)f (x) is a

continuous

function

with

Lipschitz

constant

1;

(ii)

f

(

i T

)

=

Tf

i T

/2-t

2-t T

;

(iii)

|f

(x)-f

(x)|

<

2-t T

.

Proof. We first divide [0, 1] into T intervals uniformly and define f +(x) as

f +(x) f (x) +

Tf

Tx

/2-t

2-t -f

Tx

T TT

,

(22)

where t



N.

Note

that

f

+(

i T

)

=

Tf

i T

/2-t

2-t T

and

df +(x) dx

=

df (x) dx

on(

i T

,

i+1 T

)

where

, i = 1, 2, и и и , T . Then, we define f (x):

f (0)

T f (0) 2-t

2-t

, T

(23)

df (x) dx

sgn(f +(x) - f (x)) f (x) = f +(x)

df (x) dx

f (x) = f +(x).

(24)

f +(x)

is

parallel

with

f (x)

on

any

given

interval

(

i T

,

i+1 T

]

and

is

shifted

to

let

f+(

i T

)

be

an

inte-

gral multiple of

2-t T

.

f

(

i T

)

is

f

(

i T

)

rounded

up

to

an

integral

multiple

of

2-t T

.

Meanwhile, f (x)

approaches f (x) with fixed slope whenever they are not equal. An example is shown in Figure 4.

Statement (i) follows the definition of f (x) directly. We give a proof for statement (ii) by contra-

diction and induction.

If

f

(

i T

)

=

f

+

(

i T

),

then

f (x)

=

f +(x)

on

(

i T

,

i+1 T

].

The proof idea for

f

(

i T

)

>

f

+

(

i T

)

and

f

(

i T

)

<

f

+

(

i T

)

are

basically

the

same

thus

we

present

the

proof

of

the

latter

for

brevity.

We

first

assume

f

(

i T

)

=

Tf

i T

/2-t

2-t T

,

which

is

satisfied

by

Equation

(23)

when

i

=

0.

Since f (x)

=

f +(x)

after

the

first

intersection

on

(

i T

,

i+1 T

],

if

f (x)

and

f +(x)

have

no

intersection

on

(

i T

,

i+1 T

],

then

f

(

i+1 T

)

<

f

+

(

i+1 T

)

because

f

(

i T

)

<

f

+

(

i T

).

Meanwhile,

we

have

df (x) dx

=

1,

and

i+1

i+1

i T df (x)

f =f +

dx

T T i dx

T

= Tf

i

/2-t

2-t 1 +

T TT

=T f

i

1 +

/2-t 2-t

TT

T

(25)

 T f i + 1 /2-t 2-t TT

 f+ i + 1 . T

16

Under review as a conference paper at ICLR 2019

This

contradicts

the

fact

that

f

(

i+1 T

)

<

f

+

(

i+1 T

).

Thus

f (x)

and

f +(x)

intersect

on

(0,

i T

]

and

in

turn

guarantee

that

f

(

i+1 T

)

=

Tf

i+1 T

/2-t

2-t T

.

By

induction,

we

prove

that

f (x)

and

f +(x)

intersect

on

every

interval

(

i T

,

i+1 T

].

This

implies

statement

(ii).

Now we prove the statement (iii).

Note that we have 0



f(

i T

)

-

f

(

i T

)



2-t T

by statement

(ii).

In

every

interval

(

i T

,

i+1 T

),

f (x)

=

f +(x)

after

the

their

intersection.

Therefore

we

have

0



|f +(x) - f (x)|



2-t T

by

Equation

(22).

Before

the

intersection,

if

f (0)

<

f +(0),

d(f (x)-f +(x)) dx



0.

Since

df +(x) dx

=

df (x) dx

on

(

i T

,

i+1 T

),

we

have

d(f (x)-f (x)) dx



0, thus 0 

f

(

i T

)

-

f(

i T

)



f (x) - f (x)



f +(x) - f (x)



2-t T

.

If

f

(

i T

)



f

+

(

i T

),

apply

the

same

logic

and

we

obtain

0



f +(x) - f (x)



f (x) - f (x)



f

(

i T

)

-

f

(

i T

)



2-t T

.

This implies statement (iii) and

concludes the proof.

A.5 THE PROOF OF THEOREM 3

Theorem 3. For any f  F1,1 , given  distinct weights, there is a ReLU network with function-dependent structure that can approximate f with any error  (0, 1), such
1
that (i) the depth is O  (log log (1/ )) -1 + log (1/ ) ; (ii) the number of weights is

O

 (log log (1/

))

1 -1

+1

+ (1/

)

(iii) the number of bits needed to store the network is

O

log 

 (log log (1/

))

1 -1

+1

+

(1/

)

.

Proof. We first transform f to f with Proposition 4. Then we apply the interpolation and cached

function method from [35] while using the weight construction method described in Proposition

3. Denoting the output of the network as f (x), we have |f (x) - f (x)| = |f (x) - f (x)| +

|f (x) - f (x)| 

by choosing the hyper-parameters as m

=

1 2

log

(1/

), t

=

log m,



=

1 8m

,

T=

8 log(1/

).

The approximation network is shown in Figure 5. The sizes of blocks are given in Table 2 where f T is the uniform linear interpolation function of f with T - 1 breakpoints, f  is the sum of the selected cached functions, (x) is a filtering function. The inputs connections to Bf1 and the connections inside Bm have higher order to the number of weights than others. Then the complexity can be obtained accordingly.

Figure 5: ReLU implementation of f (x). Illustration only, details are omitted. For example, arrows between blocks can represent more than one connection in the network and there are shortcut connections that allow scaling only part of the inputs.

A.6 THE PROOF OF THEOREM 4

Theorem 4.

For any f



F1,1

, given weight maximum precision

1 

,

there

is

a

ReLU

network

with

function-dependent structure that can approximate f with any error  (0, 1), such that (i) the

depth is O (log (1/ )); (ii) the number of weights is O (1/ ); (iii) the number of bits needed to store

the network is O (log()/ ).

17

Under review as a conference paper at ICLR 2019

Table 2: Block configuration for function-dependent structure. Terms with lower order are omitted.

block BT
B
Bm Bf T B Bs1 Bs2 Bs3 Bf1 Bf2 Bf3 Bf

function construct 1, 2, и и и , T

construct 

construct

1 m

,

и

и

и

,

m-1 m

implement f T (x)

implement (x)

scale by T

scale scale

by by

1 T1 

first layer of

f



second layer of f 

third layer of f 

implement f (x)

width log T
1
t T 4T 4 1 4 T 3m m3m 4

depth log T
1
(t -1 )
1
(t -1 )
1 1 log T log T log m 1 1 1
1

Proof. The proof for the approximability of linear quantization can be done in the same way as

Theorem 3 except for a different sub-network for weight approximation. We still construct Wc in

Proposition

3

first

and

any

weight

value

from

Wc

can

be

obtained

by

multiply

at

most

t log 

weights.

Thus

the

width

and

depth

of

the

weight

approximation

network

will

be

t

and

t log 

+

1

respectively.

Updating the B and Bm in Table 2, we obtain the complexity accordingly.

B THE EXISTENCE OF AN OPTIMAL BIT-WIDTH

In this section, we show that there exists one and only one local minimum on [2, ) for M (). Denote log 3n2d/ as 2 and we get the derivative of M () as:

dM d



=

 -1
2

log()

+

1 ln 2

-

 log() ln(2) ( - 1)2

(26)

Let Ms()

=

log() +

1 ln 2

-

ln(2)

 log() (-1)2

.



Since

 -1
2

>

0, we have sgn(Ms)

=

sgn

dM d

.

We have

dMs d

=

1 

+

1 -  + log() +  log() ( - 1)3

> 0,

  2

(27)

and Ms(2)

=

1

+

1 ln 2

-

2 ln(2)

<

0 and lim Ms()

=

.

It is clear that there exist a

opt such that sgn(Ms()) < 0 on [2, opt) and sgn(Ms()) > 0 on (opt, ). Remember that

sgn(Ms) = sgn

dM d

, then  = opt is the one and only one local minimum of M () on [2, ).

18


Under review as a conference paper at ICLR 2019
DOUBLE NEURAL COUNTERFACTUAL REGRET MINIMIZATION
Anonymous authors Paper under double-blind review
ABSTRACT
Counterfactual regret minimization (CRF) is a fundamental and effective technique for solving imperfect information games. However, the original CRF algorithm only works for discrete state and action spaces, and the resulting strategy is maintained as a tabular representation. Such tabular representation limits the method from being directly applied to large games and continuing to improve from a poor strategy profile. In this paper, we propose a double neural representation for the Imperfect Information Games, where one neural network represents the cumulative regret, and the other represents the average strategy. Furthermore, we adopt the counterfactual regret minimization algorithm to optimize this double neural representation. To make neural learning efficient, we also developed several novel techniques including a robust sampling method, mini-batch Monte Carlo counterfactual regret minimization (MCCFR) and Monte Carlo counterfactual regret minimization plus (MCCFR+) which may be of independent interests. Experimentally, we demonstrate that the proposed double neural algorithm converges significantly better than the reinforcement learning counterpart.
1 INTRODUCTION
In Imperfect Information Games (IIG), a player only has partial access to the knowledge of her opponents before making a decision. This is similar to the real world scenarios, such as trading, traffic routing, and public auction. Thus designing methods for solving IIG is of great economic and social benefits. Due to the hidden information, a player has to reason under the uncertainty about her opponents' information, and she also needs to act so as to take advantage of her opponents' uncertainty about her own information.
Nash equilibrium is a typical solution concept for a two-player extensive-form game. Many algorithms have been designed over years to approximately find Nash equilibrium for large games. One of the most effective approaches is CFR (Zinkevich et al., 2008). In this algorithm, the authors proposed to minimize overall counterfactual regret and prove that the average of the strategies in all iterations would converge to a Nash equilibrium. However, the original CFR only works for discrete state and action spaces, and the resulting strategy is maintained as a tabular representation. Such tabular representation limits the method from being directly applied to large games and continuing to improve if starting from a poor strategy profile.
To alleviate CFR's large memory requirement in large games such as heads-up no-limit Texas Hold'em, Moravk et al. (2017) proposed a seminal approach called DeepStack which uses fully connected neural networks to represent players' counterfactual values and obtain a strategy online as requested. However, the strategy is still represented as a tabular form and the quality of this solution depends a lot on the initial quality of the counterfactual network. Furthermore, the counterfactual network is estimated separately, and it is not easy to continue improving both counterfactual network and the tabular strategy profile in an end-to-end optimization framework.
Heinrich et al. (2015); Heinrich & Silver (2016) proposed end-to-end fictitious self-play approaches (XFP and NFSP respectively) to learn the approximate Nash equilibrium with deep reinforcement learning. In a fictitious play model, strategies are represented as neural networks and the strategies are updated by selecting the best responses to their opponents' average strategies. This approach is advantageous in the sense that the approach does not rely on abstracting the game, and in theory, the strategy should continually improve as the algorithm iterates more steps. However, these methods do not explicitly take into account the hidden information in a game, and in experiments for games
1

Under review as a conference paper at ICLR 2019
such as Leduc Hold'em, these methods converge slower than tabular based counterfactual regret minimization algorithms.
Thus it remains an open question whether the purely neural-based end-to-end approach can achieve comparable performance to tabular based CFR approach. In the paper, we partially resolve this open question by designing a double neural counterfactual regret minimization algorithm which can match the performance of tabular based counterfactual regret minimization algorithm. We employed two neural networks, one for the cumulative regret, and the other for the average strategy. We show that care algorithm design allows these two networks to track the cumulative regret and average strategy respectively, resulting in a converging neural strategy. Furthermore, in order to improve the convergence of the neural algorithm, we also developed a new sampling technique which has lower variance than the outcome sampling, while being more memory efficient than the external sampling. In experiments with Leduc Hold'em and One-card poker, we showed that the proposed double neural algorithm can converge to comparable results produced by its tabular counterpart while performing much better than deep reinforcement learning method. The current results open up the possibility for a purely neural approach to directly solve large IIG.
2 BACKGROUND
In this section, we will introduce some background on IIG and existing approaches to solve them.
2.1 REPRESENTATION OF EXTENSIVE-FORM GAME
We define the components of an extensive-form game following Osborne & Ariel (1994) (page 200  201). A finite set N = {0, 1, ..., n - 1} of players. Define hiv as the hidden variable of player i in IIG, e.g., in poker game hiv refers to the private cards of player i. H refers to a finite set of histories. Each member h = (hiv)i=0,1,...,n-1(al)l=0,...,L-1 = hv0h1v...hvn-1a0a1...aL-1 of H denotes a possible history (or state), which consists of each player's hidden variable and L actions taken by players including chance. For player i, h also can be denoted as hivhv-ia0a1...aL-1, where hv-i refers to the opponent's hidden variables. The empty sequence  is a member of H. hj h denotes hj is a prefix of h, where hj = (hvi )i=0,1,...,n-1(al)l=1,...,L -1 and 0 < L < L. Z  H denotes the terminal histories and any member z  Z is not a prefix of any other sequences. A(h) = {a : ha  H} is the set of available actions after non-terminal history h  H \ Z. A player function P assigns a member of N  {c} to each non-terminal history, where c denotes the chance player id, which usually is -1. P (h) is the player who takes an action after history h. Ii of a history {h  H : P (h) = i} is an information partition of player i. A set Ii  Ii is an information set of player i and Ii(h) refers to information set Ii at state h. Generally, Ii could only remember the information observed by player i including player i s hidden variable and public actions. Therefore Ii indicates a sequence in IIG, i.e., hiva0a2...aL-1. For Ii  Ii we denote by A(Ii) the set A(h) and by P (Ii) the player P (h) for any h  Ii. For each player i  N a utility function ui(z) define the payoff of the terminal state z. A more detailed explanation of these notations and definitions is presented in section B.
2.2 STRATEGY AND NASH EQUILIBRIUM
The strategy in an extensive-form game contains the following components. A strategy profile  = {i|i  i, i  N } is a collection of strategies for all players, where i is the set of all possible strategies for player i. -i refers to all strategies in  expect i. For play i  N the strategy i(Ii) is a function, which assigns an action distribution over A(Ii) to information set Ii. i(a|h) denotes the probability of action a taken by player i  N  {c} at state h. In IIG, h1, h2  Ii , we have Ii = Ii(h1) = Ii(h2), i(Ii) = i(h1) = i(h2), i(a|Ii) = i(a|h1) = i(a|h2). For iterative method such as CFR, t refers to the strategy profile at t-th iteration. The state reach probability of history h is denoted by (h) if players take actions according to . For an empty sequence () = 1. The reach probability can be decomposed into (h) = iN{c} i(h) = i(h)- i(h) according to each player's contribution, where i(h) =
h a h,P (h )=P (h) i(a|h ) and - i(h) = h a h,P (h )=P (h) -i(a|h ). The information set reach probability of Ii is defined as (Ii) = hIi (h). If h h, the interval state reach probability from state h to h is defined as (h , h), then we have (h , h) = (h)/(h ). i(Ii), - i(Ii), i(h , h), and - i(h , h) are defined similarly.
2

Under review as a conference paper at ICLR 2019

2.3 COUNTERFACTUAL REGRET MINIMIZATION
In large and zero-sum IIG, CFR is proved to be an efficient method to compute Nash equilibrium (Zinkevich et al., 2008; Brown & Sandholm, 2017; Moravk et al., 2017). We present some key ideas of this method as follows.

Lemma 1: The state reach probability of one player is proportional to posterior probability of the opponent's hidden variable, i.e., p(hvi |Ii)  - i(h), where hvi and Ii indicate a particular h. (see the proof in section E.1)

For player i and strategy profile , the counterfactual value (CFV) vi(h) at state h is define as

vi(h) =

- i(h)(h, z)ui(z) =

i(h, z)ui(z).

h z,zZ

h z,zZ

(1)

where ui(z) = - i(z)ui(z) is the expected reward of player i with respective to the approximated posterior distribution of the opponent's hidden variable. The action counterfactual value of taking
action a is vi(a|h) = vi(ha) and the regret of taking this action is ri(a|h) = vi(a|h) - vi(h). Similarly, the CFV of information set Ii is vi(Ii) = hIi vi(h) and the regret is ri(a|Ii) =
zZ,ha z,hIi i(ha, z)ui(z) - zZ,h z,hIi i(h, z)ui(z). Then the cumulative regret of action a after T iterations is

T
RiT (a|Ii) = (vit (a|Ii) - vit (Ii)) = RiT -1(a|Ii) + riT (a|Ii).
t=1

(2)

where Ri0(a|Ii) = 0. Define RiT,+(a|Ii) = max(RiT (a|Ii), 0), the current strategy at T + 1 iteration will be updated by

  iT +1(a|Ii) =

RiT ,+ (a|Ii ) aA(Ii) RiT,+(a|Ii) 1

 |A(Ii)|

if aA(Ii) RiT,+(a|Ii) > 0 otherwise.

(3)

The average strategy ¯iT from iteration 1 to T is defined as:

¯iT (a|Ii) =

T t=1

it (Ii)it(a|Ii)

T t=1

it

(Ii

)

.

(4)

where it (Ii) denotes the information set reach probability of Ii at t-th iteration and is used to weight the corresponding current strategy it(a|Ii). Define sti(a|Ii) = it (Ii)it(a|Ii) as the additional numerator in iteration t, then the cumulative numerator can be defined as

T
ST (a|Ii) = it (Ii)it(a|Ii) = ST -1(a|Ii) + siT (a|Ii).
t=1

(5)

where S0(a|Ii) = 0.

2.4 MONTE CARLO CFR
When solving a game, CFR needs to traverse the entire game tree in each iteration, which will prevent it from handling large games with limited memory. To address this challenge, Lanctot et al. (2009) proposed a Monte Carlo CFR to minimize counterfactual regret. Their method can compute an unbiased estimation of counterfactual value and avoid traversing the entire game tree. Since only subsets of all information sets are visited in each iteration, this approach requires less memory than standard CFR.

Define Q = {Q1, Q2, ..., Qm}, where Qj  Z is a block of sampling terminal histories in each

iteration, such that Qj spans the set Z. Generally, different Qj may have an overlap according to

the specify sampling schema. Specifically, in the external sampling and outcome sampling, each

block Qj  Q is a partition of Z. Define qQj as the probability of considering block Qj, where

m j=1

qQj

=

1.

Define

q(z)

=

j:zQj qQj as the probability of considering a particular terminal

history z. Specifically, vanilla CFR is a special case of MCCFR, where Q = {Z} only contain one

block and qQ1 = 1. In outcome sampling, only one trajectory will be sampled, such that Qj  Q, |Qj| = 1 and |Qj| = |Z|. For information set Ii, a sample estimate of counterfactual value is

v~i(Ii|Qj ) =

hIi,zQj ,h

z

1 q(z)

- i(z)i

(h,

z

)ui

(z).

3

Under review as a conference paper at ICLR 2019

Lemma 2: The sampling counterfactual value in MCCFR is the unbiased estimation of actual counterfactual value in CFR. EjqQj [v~i(Ii|Qj)] = vi(Ii) (see the proof in section E.2)

Define rs as sampling strategy profile, where irs is the sampling strategy for player i and -rsi are the sampling strategies for players expect i. Particularly, for both external sampling and outcome
sampling proposed by (Lanctot et al., 2009), -rsi = -i. The regret of the sampled action a  A(Ii) is defined as

r~i((a|Ii)|Qj) =

i(ha, z)uirs(z) -

i(h, z)uirs(z) , (6)

zQj ,ha z,hIi

zQj ,h z,hIi

where

uri s(z)

=

ui (z ) irs (z)

is

a

new

utility

weighted

by

.1
irs (z)

The

sample

estimate

for

cumulative

regret of action a after T iterations is R~iT ((a|Ii)|Qj) = R~iT -1((a|Ii)|Qj) + r~iT ((a|Ii)|Qj) with

R~i0((a|Ii)|Qj) = 0.

3 DOUBLE NEURAL COUNTERFACTUAL REGRET MINIMIZATION

A

1 1
2 3 4 1 4
7 8 4 5

2
3 5 6 2 3

Regret Matching

-1 (| )

 ((| )| )

+ (|)



all  

-1 (| )

 (| )

+ (|)



Tabular Method

all  

B Regret Matching

1 1
2 3 4 1 4
7 8 4 5

2
3 5 6 2 3



 ((| )| )

RegretSumNetwork

  

gradient descent

+

 (| )

AvgStrategyNetwork

   Neural Method

+

gradient descent

Figure 1: (A) tabular based CRF, and (B) our double neural based CRF framework.

In this section, we will explain our double neural CFR algorithm, where we employ two neural networks, one for the cumulative regret, and the other for the average strategy.
As shown in Figure 1 (A), standard CFR-family methods such as CFR (Zinkevich et al., 2008), outcome-sampling MCCFR, external sampling MCCFR (Lanctot et al., 2009), and CFR+ (Tammelin, 2014) need to use two large tabular-based memories MR and MS to record the cumulative regret and average strategy for all information sets. Such tabular representation makes these methods difficult to apply to large extensive-form games with limited time and space (Burch, 2017).
In contrast, we will use two deep neural networks to compute approximate Nash equilibrium of IIG as shown in Figure 1 (B). Different from NFSP, our method is based on the theory of CFR, where the first network is used to learn the cumulative regret and the other is to learn the cumulative numerator of the average strategy profile. With the help of these two networks, we do not need to use a large memory to save the key information of the entire game tree. In practice, the proposed double neural method can achieve a lower exploitability with fewer iterations than NFSP. In addition, we present experimentally that our double neural CFR can also continually improve after initialization from a poor tabular strategy.

3.1 OVERALL FRAMEWORK
An algorithm in the CFR framework needs to be able to answer two queries:
1. what is the current strategy t+1(a|Ii) for iteration t + 1; 2. and what is the average strategy ¯it(a|Ii) after t iterations;
i  N, Ii  Ii, a  A(Ii), t  [1, T ]. Thus, our neural networks are designed to address the needs for these two queries respectively.
For the first query. According to Eq. (3), current strategy t+1(a|Ii) is computed by the cumulative regret Rt(a|Ii). Given information set Ii and action a, we design a neural network RegretSumNetwork(RSN) R(a, Ii|Rt ) to learn Rt(a|Ii), where Rt is the parameter in the network at t-th iteration. As shown Figure 1 (b), define memory MR = {(Ii, r~it ((a|Ii)|Qj))|i  N, a  A(Ii), h  Ii, h z, z  Qj}. Each member of MR is the visited information set Ii and the corresponding regret r~it ((a|Ii)|Qj), where Qj is the sampled block in t-th iteration. According

4

Under review as a conference paper at ICLR 2019

to Eq. (2), we can estimate R(a, Ii|Rt+1) using the following optimization:

Rt+1  argmin

R(a, Ii|Rt ) + r~it ((a|Ii)|Qj ) - R(a, Ii|Rt+1)

Rt+1 (Ii,r~it ((a|Ii)|Qj ))MR

2
.

(7)

For the second query. According to Eq. (4), the approximate Nash equilibrium is the weighted average of all previous strategies over T iterations. We only need to track the numerator in Eq. (4)
since the denominator is used to normalize the summation. Similar to the cumulative regret, we employ another deep neural network AvgStrategyNetwork(ASN) to learn the cumulative numerator of the average strategy. Define MS = {(Ii, it (Ii)it(a|Ii))|i  N, a  A(Ii), h  Ii, h z, z  Qj}. Each member of MS is the visited information set Ii and the value of it (Ii)it(a|Ii), where Qj is the sampled block in t-th iteration. Then the parameter St+1 can estimated by the following optimization:

St+1  argmin
St+1 (Ii,sit(a|Ii))MS

2
S(a, Ii|St ) + sti(a|Ii) - S(a, Ii|St+1) .

(8)

Relation between CFR, MCCFR and our double neural method. As shown in Figure 1, these three methods are based on the CFR framework. The CFR computes counterfactual value and regret by traversing the entire tree in each iteration, which makes it computationally intensive to be applied to large games directly. MCCFR samples a subset of information sets and will need less computation than CFR in each iteration. However, both CFR and MCCFR need two huge memories to save the cumulative regrets and the numerators of average strategy for all information sets after multiple iterations, which prevents these two methods to be used in large games directly. The proposed neural method keeps the benefit of MCCFR yet without the need for a large memory.

3.2

RECURRENT NEURAL NETWORK REPRESENTATION FOR INFORMATION SET

A 6

Sequential Representation

Value Network
+

[1, 2, ... , 6]

1 2 3 4 5 6

Attention Network

B player 0
player 1
chance
Mini-batch Robust Sampling

...

e1 e2 e3 e4 e5 e6

c1 c2 c3 c4 c5 c6

a1 a2 a3 a4 a5 a6

10 10

20 50

Average Strategy Deep Network

Regret Sum Deep Network

Figure 2: (A) the key architecture of the sequential neural networks. (B) an overview of the novel double neural counterfactual regret minimization method.
In order to define our R and S network, we need to represent the information set Ii  I in extensiveform games. In such games, players take action in alternating fashion and each player makes a decision according to the observed history. In this paper, we model the behavior sequence as a recurrent neural network and each action in the sequence corresponds to a cell in RNN. Figure 2 (A) provides an illustration of the proposed deep sequential neural network representation for information sets.
In standard RNN, the recurrent cell will have a very simple structure, such as a single tanh or sigmoid layer. Hochreiter & Schmidhuber (1997) proposed a long short-term memory method (LSTM) with the gating mechanism, which outperforms the standard version and is capable of learning long-term dependencies. Thus we will use LSTM for the representation. Furthermore, different position in the sequence may contribute differently to the decision making, we will add an attention mechanism (Desimone & Duncan, 1995; Cho et al., 2015) to the LSTM architecture to enhance the representation. For example, the player may need to take a more aggressive strategy after beneficial public cards are revealed. Thus the information, after the public cards are revealed may be more important.

5

Under review as a conference paper at ICLR 2019

More specifically, for l-th cell, define xl as the input vector (which can be either player or chance

actions), el as the hidden layer embedding,  as a general nonlinear function. Each action is

represented by a LSTM cell, which has the ability to remove or add information to the cell state with

three different gates. Define the notation · as element-wise product. The first forgetting gate layer is

defined as glf = f (wf · [xl, el-1]), where [xl, el-1] denotes the concatenation of xl and el-1. The second input gate layer decides which values to update and is defined as gli = i(wi · [xl, el-1]). A nonlinear layer output a vector of new candidate values C~l = c(wl · [xl, el-1]) to decide what

can be added to the state. After the forgetting gate and the input gate, the new cell state is updated

by Cl = glf the updated

· Cl-1 + gli · C~l. The third hidden embedding is el =

output gate glo · e(Cl).

is defined As shown

as in

glo = o(wo · [xl, el-1]). Finally, Figure 2 (A), for each LSTM cell

j, the vector of attention weight is learned by an attention network. Each member in this vector is

a scalar j = a(wa · ej). The attention embedding of l-th cell is then defined as ela =

l j=1

j

ej

,

which is the summation of the hidden embedding ej and the learned attention weight j. The final

output of the network is predicted by a value network, which is defined as


l
y~l := f (a, Ii|) = wy · v(ela) = wy · v  a(wa · ej) · ej ,
j=1

(9)

where  is the parameters in the defined sequential neural networks. Specifically, f , i, o are sig-
moid functions. c and e are hyperbolic tangent functions. a and v are rectified linear functions.
The proposed RSN and ASN share the same neural architecture, but use different parameters. That is R(a, Ii|Rt ) = f (a, Ii|Rt ) and S(a, Ii|St ) = f (a, Ii|St ). R(·, Ii|Rt ) and S(·, Ii|St ) denote two vectors of inference value for all a  A(Ii).

3.3 CONTINUAL IMPROVEMENT

With the proposed framework of double neural CFR, it is easy to initialize the neural networks
from an existing strategy profile based on the tabular representation or neural representation. For information set Ii and action a, in an existing strategy profile, define Ri(a|Ii) as the cumulative regret and S (a|Ii) as the cumulative numerator of average strategy. We can clone the cumulative regret for all information sets and actions by optimizing

2

R  argmin

R(a, Ii|R) - R (a|Ii) .

R iN,IiIi,aA(Ii)

(10)

Similarly, timized in

the the

parameters same way.

S for Based

cloning on the

the cumulative learned R and

numerator of average strategy can S , we can warm start the double

be opneural

networks and continually improve beyond the tabular strategy profile.

3.4 OVERALL ALGORITHM

Algorithm 1: Counterfactual Regret Minimization with Two Deep Neural Networks

1 Function Agent(T , b):

2 For t = 1 to T do

3 if t = 1 and using warm starting then 4 initialize Rt and St from an existing checkpoint 5 tt+1

skip cold starting

6 else

7 initialize Rt and St randomly. 8 MR, MS  sampling methods for CFV and average strategy.

such as Algorithm3

9

sum aggregate the value in MR by information set.

according to the Lemma 5 and Equation 12

10 remove duplicated records in MS.

11 Rt  NeuralAgent(R(·|Rt-1), MR, Rt-1, R ) 12 St  NeuralAgent(S(·|St-1), MS, St-1, S )

update Rt using Algorithm2 update St using Algorithm2

13 return Rt , St

Algorithm 1 provides a summary of the proposed double neural counterfactual regret minimization algorithm. In the first iteration, if the system warm starts from tabular based CFR or MCCFR methods, the techniques in section 3.3 will be used to clone the cumulative regrets and strategy. If there

6

Under review as a conference paper at ICLR 2019

is no warm start initialization, we can start our algorithm by randomly initializing the parameters in RSN and ASN at iteration t = 1. Then sampling methods will return the counterfactual regret and the numerator of average strategy for the sampled information sets in this iteration, and they will be saved in memories MR and MS respectively. Then these samples will be used by the NeuralAgent algorithm from Algorithm2 to optimize RSN and ASN. Further details for the sampling methods and the NeuralAgent fitting algorithm will be discussed in the next section.

4 EFFICIENT TRAINING
In this section, we will propose three techniques to improve the efficiency of the double neural method. These algorithms can also be used separately in other CFR-family methods.

4.1 ROBUST SAMPLING TECHNIQUES

Theoretically, outcome sampling is more memory efficient than the external sampling, since in out-

come sampling only one trajectory is sampled according to strategy profile while in the external

sampling, player i will traverse all actions for all information set Ii  Ii and the opponent players

including chance sample one action. Therefore many information sets will be visited in a sampling

process and block Qi  Q will contains many terminal nodes in external sampling. In outcome

sampling method,

the

weighted utility

uri s(z)

=

ui (z ) i (z)

for

terminal

node

depends on

the

concrete

reach probability i(z) in each iteration, therefore it will lead to a high variance and slow down the

convergence of the resulting strategy profile.

In this paper, we proposed a new and robust sampling technique which has lower variance than
outcome sampling, while being more memory efficient than the external sampling. In this robust
sampling method, the sampling profile is defined as rs(k) = (irs(k), -i), where player i will randomly select k actions according to sampling strategy irs(k)(Ii) for each information set Ii and other players will randomly select one action according to strategy -i.

Specifically, if player i randomly selects min(k, |A(Ii)|) actions according to discrete uniform dis-

tribution

unif (0, |A(Ii)|)

at

information

set

Ii,

i.e.,

irs(k)(a|Ii)

=

min(k,|A(Ii |A(Ii )|

)|)

,

then

rs(k)
i

(Ii

)

=

hIi ,h

min(k, |A(Ii)|)

h,h a h,h Ii

|A(Ii )|

(11)

and the weighted utility uri s(k)(z) will be a constant number in each iteration, which has a low variance. In addition, because the weighted utility no longer requires explicit knowledge of the
opponent's strategy, we can use this sampling method for online regret minimization. For simplicity, k = max refers to k = maxIiI|A(Ii)| in the following sections.

Lemma 3: If k = max and i  N, Ii  Ii, a  A(Ii), irs(k)(a|Ii)  unif (0, |A(Ii)|), then robust sampling is the same as external sampling.

Lemma 4: If k = 1 and irs(k) = i, then robust sampling is the same as outcome sampling.
Lemma 3 and Lemma 4 provide the relationship between outcome sampling, external sampling, and the proposed robust sampling algorithm. The detailed theoretical analysis are presented in Appendix E.3.

4.2 MINI-BATCH TECHNIQUES

Mini-batch MCCFR: Traditional outcome sampling and external sampling only sample one block
in an iteration and provide an unbiased estimation of origin CFV according to Lemma 2. In this
paper, we present a mini-batch Monte Carlo technique and randomly sample b blocks in one iterations. Let Qj denote a block of terminals sampled according to the scheme in section 4.1 at j-th
time, then mini-batch CFV with b mini-batches for information set Ii can be defined as



v~i (Ii |b)

=

1 b

b



- i(z)i(h, z)ui(z)  = b v~i(Ii|Qj) . q(z) b

j=1 hIi,zQj ,h z

j=1

(12)

Furthermore, we can show that v~i(Ii|b) is an unbiased estimator of the counterfactual value of Ii: Lemma 5: EQjRobust Sampling[v~i(Ii|b)] = vi(Ii). (see the proof in section E.4) Similarly, the

7

Under review as a conference paper at ICLR 2019

cumulative mini-batch regret of action a is

R~iT ((a|Ii)|b) = R~iT -1((a|Ii)|b) + v~iT ((a|Ii)|b) - v~iT (Ii|b)

, (13)

where R~i0((a|Ii)|b) = 0. In practice, mini-batch technique can sample b blocks in parallel and help MCCFR to converge faster.

Mini-Batch MCCFR+: When optimizing counterfactual regret, CFR+ (Tammelin, 2014) substitutes the regret-matching algorithm (Hart & Mas-Colell, 2000) with regret-matching+ and can converge faster than CFR. However, Burch (2017) showed that MCCFR+ actually converge slower than MCCFR when mini-batch is not used. In our paper, we derive mini-batch version of MCCFR+ which updates cumulative mini-batch regret R~T,+((a|Ii)|b) up to iteration T by

R~T,+((a|Ii)|b) =

v~iT ((a|Ii)|b) - v~iT (Ii|b) + R~iT -1,+((a|Ii)|b) + v~iT ((a|Ii)|b) - v~iT (Ii|b) +

if T = 0 , if T > 0

(14)

where (x)+ = max(x, 0). In practice, we find that mini-batch MCCFR+ converges faster than mini-batch MCCFR when specifying a suitable mini-batch size.

4.3 NEURAL AGENT FOR OPTIMIZING NEURAL REPRESENTATION

Algorithm 2: Optimization of Deep Neural Network

1 Function NeuralAgent(f (·|T -1), M, T -1, ):

2 initialize optimizer, scheduler

gradient descent optimizer and learning rate scheduler

3 T  T -1, lbest  , tbest  0

warm starting from the checkpoint of the last iteration

4 For t = 1 to epoch do 5 loss  []

initialize loss as an empty list

6 For each training epoch do

7 {x(i), y(i)}im=1  M

sampling a mini-batch from M

8

batch

loss



1 m

m i=1

(f

(x(i)

|T

-1)

+

y(i)

-

f (x(i)|T

))2

9 back propagation batch loss with learning rate lr

10 clip gradient of T to [- , ]d

d is the dimension of T

11 optimizer(batch loss)

12 loss.append(batch loss)

13 lr  sheduler(lr)

reduce learning rate adaptively when loss has stopped improving

14 if avg(loss) < loss then

15 bTest  T , early stopping.

if loss is small enough, using early stopping mechanism.

16 else if avg(loss) < lbest then

17 lbest = avg(loss), tbest  t, bTest  T

18 if t - tbest > re then

19 lr  lr

reset learning rate to escape from potential saddle point or local minima.

20 return T

Define epoch as training epoch, lr as learning rate, loss as the criteria for early stopping, re as the upper bound for the number of iterations from getting the minimal loss last time, t-1 as the parameter to optimize, f (·|t-1) as the neural network, M as the training sample consisting information set and the corresponding target. To simplify notations, we use  to denote the set of hyperparameters in the proposed deep neural networks. R and S refer to the sets of hyperparameters in RSN and ASN respectively. Algorithm 2 presents the details of how to optimize the proposed neural networks.
Both R(a, Ii|Rt+1) and S(a, Ii|St ) are optimized by mini-batch stochastic gradient descent method. In this paper, we use Adam optimizer (Kingma & Ba, 2014) with both momentum and adaptive learning rate. Some other optimizers such as Nadam, RMSprop, Nadam from (Ruder, 2017) are also tried in our experiments, however, they do not achieve better experimental results. In practice, existing optimizers may not return a relatively low enough loss because of potential saddle point or local minima. To obtain a relatively higher accuracy and lower optimization loss, we use a carefully designed scheduler to reduce the learning rate when the loss has stopped decrease. Specifically, the
8

Under review as a conference paper at ICLR 2019

scheduler reads a metrics quantity, e.g, mean squared error, and if no improvement is seen for a number of epochs, the learning rate is reduced by a factor. In addition, we will reset the learning rate in both optimizer and scheduler once loss stops decrease in re epochs. Gradient clipping mechanism is used to limit the magnitude of the parameter gradient and make optimizer behave better in the vicinity of steep cliffs. After each epoch, the best parameter will be updated. Early stopping mechanism is used once the lowest loss is less than the specified criteria loss.

5 EXPERIMENT
The proposed double neural CFR algorithm will be evaluated in No-Limit Leduc Hold'em with stack size 5 and One-Card-Poker game with 5 cards. We will compare it with tabular CFR and deep reinforcement learning based method such as NFSP. The experiments show that the proposed double neural algorithm can converge to comparable results produced by its tabular counterpart while performing much better than deep reinforcement learning method. The current results open up the possibility for a purely neural approach to directly solve large IIG. Due to space limit, we present experimental results for One-Card-Poker and the analysis in section C.

Settings. To simplify the expression, the abbreviations of different methods are defined as follows. XFP refers to the full-width extensive-form fictitious play method in (Heinrich et al., 2015), NFSP refers to the reinforcement learning based fictitious self-play method in (Heinrich & Silver, 2016). RS-MCCFR refers to the proposed robust sampling MCCFR. This method with regret matching+ acceleration technique is denoted by RS-MCCFR+. To evaluate the contribution of each neural agent, we replace the tabular based cumulative regret and numerator with RSN and ANS separately. These methods only containing one neural network are denoted by RS-MCCFR+-RSN and RS-MCCFR+-ASN respectively. RS-MCCFR+-RSN-ASN refers to the proposed double neural MCCFR. According to Lemma 3, if k = max, ES-MCCFR is the same with RS-MCCFR. More specifically, we investigated the following questions.

AB

C

Figure 3: Comparison of different CFR-family methods in Leduc Hold'em. (A) Performance of robust sampling with different batch size. (B) Performance of robust sampling with different parameter k by iteration. (C) Performance by the number of touched node.
Is mini-batch sampling helpful? Figure 3(A) presents the convergence curves of the proposed robust sampling method with k = max under different mini-batch sizes (b=1, 1000, 5000, 10000 respectively). The experimental results show that larger batch sizes generally lead to better strategy profiles. Furthermore, the convergence for b = 5000 is as good as b = 10000. Thus in the later experiments, we set the mini-batch size equal to 5000.
Is robust sampling helpful? Figure 3 (B) and (C) presents convergence curves for outcome sampling, external sampling(k = max) and the proposed robust sampling method under the different number of sampled actions. The outcome sampling cannot converge to a low exploitability smaller than 0.1 after 1000 iterations (touch more than 107 nodes as shown in Figure 3(C) because of the high variance. The proposed robust sampling algorithm with k = 1, which only samples one trajectory like the outcome sampling, can achieve a better strategy profile after the same number of iterations. With an increasing k, the robust sampling method achieves an even better convergence rate. Experiment results show k = 3 and 5 have a similar trend with k = max, which demonstrates that the proposed robust sampling achieves similar strategy profile but requires less memory than
9

Under review as a conference paper at ICLR 2019
the external sampling. We choose k = 3 for the later experiments in Leduc Hold'em Poker. Figure 3 (C) presents the results in a different way and displays the relation between exploitability and the cumulative number of touched nodes. The robust sampling with small k is just as good as the external sampling while being more memory efficient on the condition that each algorithm touches the same number of nodes.
ABC

Figure 4: Performance of different methods in Leduc Hold'em. (A) comparison of NSFP, XFP and the proposed double neural method. (B) each contribution of RSN and ASN. (C) continue improvement from tabular based CFR and RS-MCCFR+

How does double neural CRF compared to tabular counterpart, XFP and NFSP? To obtain an approximation of Nash equilibrium, Figure 4(A) demonstrates that NFSP needs 106 iterations to reach a 0.06-Nash equilibrium, and requires 2 × 105 state-action pair samples and 2 × 106 samples for supervised learning respectively. The XFP needs 103 iterations to obtain the same exploitability,
however, this method is the precursor of NFSP and updated by a tabular based full-width fictitious
play. Our proposed neural method only needs 200 iterations to achieve the same performance which
shows that the proposed double neural algorithm converges significantly better than the reinforce-
ment learning counterpart. In practice, our double neural method can achieve an exploitability of
0.02 after 1000 iterations, which is similar to the tabular method.

What is the individual effect of RSN and ASN? Figure 4(B) presents ablation study of the ef-

fects of RSN and ASN network respectively. Both MCCFR+-RSN and MCCFR+-ASN, which only

employ one neural network, perform only slightly better than the double neural method. All the

proposed neural methods can match the performance of the tabular based method. For RSN, we

set the hyperparameters as follows: neural batch size is 256, hidden size is 128 and learning rate

lr = 0.001. A scheduler, who will reduce the learning rate based on the number of epochs and the

convergence rate of loss, help the neural agent to obtain a high accuracy. The learning rate will be

reduced by 0.5 when loss has stopped improving after 10 epochs. The lower bound on the learning rate of all parameters in this scheduler is 10-6. To avoid the algorithm converging to potential local

minima or saddle point, we will reset the learning rate to 0.001 and help the optimizer to learn a

better performance. bTest is loss for epoch t is less than

the the

best parameters to achieve the specified criteria loss=10-4,

lowest loss after T we will early stop

epochs. If average the optimizer. We

set epoch = 2000 and update the optimizer 2000 maximum epochs. For ASN, we set the hidden size as 256, the loss of early stopping criteria as 10-5. The learning rate will be reduced by 0.7 when

loss has stopped improving after 15 epochs. Other hyperparameters in ASN are similar to RSN.

How well does continual improvement work? In practice, we usually want to continually improve our strategy profile from an existing checkpoint (Brown & Sandholm, 2016). In the framework of the proposed neural counterfactual regret minimization algorithm, warm starting is easy and friendly. Firstly, we employ two neural networks to clone the existing tabular based cumulative regret and the numerator of average strategy by optimizing Eq. (10). Then the double neural methods can continually improve the tabular based methods. As shown in Figure 4(C), warm start from either full-width based or sampling based CFR the existing can lead to continual improvements. Specifically, the first 10 iterations are learned by tabular based CFR and RS-MCCFR+. The remaining iterations are continually improved by the double neural method, where b = 5000, k = max.

10

Under review as a conference paper at ICLR 2019
REFERENCES
Noam Brown and Tuomas Sandholm. Strategy-based warm starting for regret minimization in games. pp. 432­438. AAAI, 2016.
Noam Brown and Tuomas Sandholm. Superhuman ai for heads-up no-limit poker: Libratus beats top professionals. Science, pp. eaao1733, 2017.
Neil Burch. Time and space: Why imperfect information games are hard. PhD thesis, 2017. Kyunghyun Cho, Aaron Courville, and Yoshua Bengio. Describing multimedia content using
attention-based encoderdecoder networks. arXiv preprint arXiv:1507.01053, 2015. Robert Desimone and John Duncan. Neural mechanisms of selective visual attention. Number 18,
pp. 193­222. Annual review of neuroscience, 1995. Geoffrey J. Gordon. No-regret algorithms for structured prediction problems. Number CMU-
CALD-05-112. CARNEGIE-MELLON UNIV PITTSBURGH PA SCHOOL OF COMPUTER SCIENCE, 2005. David Harris and Sarah Harris. Digital design and computer architecture (2nd ed.), volume ISBN 978-0-12-394424-5. San Francisco, Calif.: Morgan Kaufmann. Sergiu Hart and Andreu Mas-Colell. A simple adaptive procedure leading to correlated equilibrium. Econometrica, (65(5)):1127­1150, 2000. Johannes Heinrich and David Silver. Deep reinforcement learning from self-play in imperfectinformation games. arXiv preprint arXiv:1603.01121, 2016. Johannes Heinrich, Marc Lanctot, and David Silver. Fictitious self-play in extensive-form games. pp. 805­813. International Conference on Machine Learning, 2015. Sepp Hochreiter and Jrgen Schmidhuber. Long short-term memory. Number 8, pp. 1735­1780. Neural computation, 1997. Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. Marc Lanctot, Waugh Kevin, Zinkevich Martin, and Michael Bowling. Monte carlo sampling for regret minimization in extensive games. In Advances in neural information processing systems, 2009. Matej Moravk, Schmid Martin, Burch Neil, Lis Viliam, Dustin Morrill, Nolan Bard, Trevor Davis, Kevin Waugh, Michael Johanson, and Michael Bowling. Deepstack: Expert-level artificial intelligence in heads-up no-limit poker. Science, (6337):508­513, 2017. Martin J. Osborne and Rubinstein Ariel. A course in game theory, volume 1. MIT Press, 1994. Sebastian Ruder. An overview of gradient descent optimization algorithms. arXiv preprint arXiv:1609.04747, 2017. Finnegan Southey, Michael P. Bowling, Bryce Larson, Carmelo Piccione, Neil Burch, Darse Billings, and Chris Rayner. Bayes' bluff: Opponent modelling in poker. arXiv preprint arXiv:1207.1411, 2012. Oskari Tammelin. Solving large imperfect information games using cfr+. arXiv preprint, 2014. Martin Zinkevich, Johanson Michael, Bowling Michael, and Carmelo Piccione. Regret minimization in games with incomplete information. Advances in neural information processing systems, 2008.
11

Under review as a conference paper at ICLR 2019

APPENDIX A GAME RULES
Leduc Hold'em a two-players IIG of poker, which was first introduced in (Southey et al., 2012). In Leduc Hold'em, there is a deck of 6 cards comprising two suits of three ranks. The cards are often denoted by king, queen, and jack. In No-Limit Leduc Hold'em, the player may wager any amount of chips up to a maximum of that player's remaining stack. There is also no limit on the number of raises and bets in each betting round. There are two rounds. In the first betting round, each player is dealt one card from a deck of 6 cards. In the second betting round, a community (or public) card is revealed from a deck of the remaining 4 cards.
One-Card Poker is a two-players IIG of poker described by (Gordon, 2005). The game rules are defined as follows. Each player is dealt one card from a deck of X cards. The first player can pass or bet, If the first player bet, the second player can call or fold. If the first player pass, the second player can pass or bet. If second player bet, the first player can fold or call. The game ends with two pass, call, fold. The fold player will lose 1 chips. If the game ended with two passes, the player with higher card win 1 chips, If the game end with call, the player with higher card win 2 chips.
APPENDIX B DEFINITION OF EXTENSIVE-FORM GAMES

player 0 player 1 chance






PB

3
PB
 

4
FC
 

FC
 


PB

 

PB

F

 



FC
 

C



PB

 

PB
 

FC
 

FC
 


PB

 

PB

F

  

FC

 

C


Figure 5: Illustration of extensive-form game. The left and right denote two different kinds of dealt private cards. We use same color other than gray for each state in the same information set. F, C, P, B refer to fold, call, pass, bet respectively.

B.1 ADDITIONAL DEFINITIONS

For player i, the expected game utility ui = zZ (z)ui(z) of  is the expected payoff of all

fopoforspmsliabgylaeemrtieertshm,aaitnNaacalhsniheovdeeeqssu.milGaibxivrimeiunimzaefiipsxaaeydsotfrsfatrataegtgaeyignypsrtporfio-lfiei liesa=-bie,(stan0rye, ssp1tor)anstseuegc.yhFtohiratt=weoamcphalapxylaeiyrse'rie'sxusti(etrnia,stiev-geiy)-

is a best response to the opponent. An -Nash equilibrium is an approximation of a Nash equilib-

rolaifurgames,ttwrwahoteogpsyleaysteirraiszteedgreoyfi-pnsureomdfialgseami(essias)tui=csfihuepsio:ke-ir,uui(iNi,,is-uiiin)i.trAa+cstatrbalteemgtoyacixsomuinpeuxitpeu.l(ioHiitoa,wb-leeiv)i.efrE, iixf(ptlhio)eit=palba0iy.leiItrnys

alternate their positions, the value of exploitability of strategy profile  as

a(pa)i=r o(fug(1am0,e1s)is+zuer0(o0s,,i1.)e)./, 2u.0 

+

u1

=

0

.

We

define

the

B.2 EXPLANATION BY EXAMPLE

To provide a more detailed explanation, Figure 5 presents an illustration of a partial game tree in
One-Card Poker. In the first tree, two players are dealt (queen, jack) as shown in the left subtree and
(queen, king) as shown in the right subtree. zi denotes terminal node and hi denotes non-terminal node. There are 19 distinct nodes, corresponding 9 non-terminal nodes including chance h0 and 10 terminal nodes in the left tree. The trajectory from the root to each node is a history of actions. In
an extensive-form game, hi refers to this history. For example, h3 consists of actions 0:Q, 1:J and P. h7 consists of actions 0:Q, 1:J, P and B. h8 consists of actions 0:Q, 1:K, P and B. We have h3 h7, A(h7) = {P, B} and P (h3) = 1

In IIG, the private card of player 1 is invisible to player 0, therefore h7 and h8 are actually the same for player 0. We use information set to denote the set of these undistinguished states. Similarly, h1

12

Under review as a conference paper at ICLR 2019
and h2 are in the same information set. For the right tree of Figure 5, h3 and h5 are in the same information set. h4 and h6 are in the same information set. Generally, any Ii  I could only remember the information observed by player i including player i s hidden variable and public actions. For example, the information set of h7 and h8 indicates a sequence of 0:Q, P, and B. Because h7 and h8 are undistinguished by player 0 in IIG, all the states have a same strategy. For example, I0 is the information set of h7 and h8, we have I0 = I0(h7) = I0(h8), 0(I0) = 0(h7) = 0(h8), 0(a|I0) = 0(a|h7) = 0(a|h8).
APPENDIX C ADDITIONAL EXPERIMENT DETAILS
C.1 FEATURE ENCODING OF POKER GAMES The feature is encoded as following. As shown in the figure 2 (A), for a history h and player P (h), we use one-hot encoding (Harris & Harris) to represent the observed actions including chance player. For example, the input feature xl for l-th cell is the concatenation of three one-hot features including the given private cards, the revealed public cards and current action a. Both the private cards and public cards are encoded by one-hot technique, where the value in the existing position is 1 and the others are 0. If there are no public cards, the respective position will be filled with 0. Because the action taking by chance is also a cell in the proposed sequential model. Thus in a NoLimit poker, such as Leduc Hold'em, action a could be any element in {fold, cumulative spent}  {public cards}, where cumulative spent denotes the total chips after making a call or raise. The length of the encoding vector of action a is the quantities of public cards plus 2, where cumulative spent is normalized by the stack size. C.2 ADDITIONAL EXPERIMENT RESULTS
ABC
Figure 6: Comparison of different CFR-family methods and neural network methods in One-CardPoker. (A) Comparison of the robust sampling with different mini-batch size. (B) Comparison of the outcome sampling and the robust sampling with different sample actions k. (C) Comparison of tabular based RS-MCCFR+ and the double neural method.
Figure 6 (A) presents the convergence rate for the proposed robust sampling method of different mini-batch size b = (1, 100, 500, 1000). The experimental results are similar to Leduc Hold'em poker, larger mini-batch size indicates a better exploitability. Figure 6 (B) demonstrates that the convergence rate for different sampling methods including outcome sampling and robust sampling under k = 1, 2. The conclusion is that RS-MCCFR+ converges significantly faster than OS-MCCFR+ after touching the same number of nodes. Experiment results show that k = 1 has a similar trend with k = 2 (external sampling). Because only one trajectory is sampled, the proposed RS-MCCFR+ will require less memory than the external sampling.
13

Under review as a conference paper at ICLR 2019

Figure 6 (C) compares the performance between the tabular method and the double neural method. Experimental results demonstrate that RS-MCCFR+-RSN-ASN can achieve an exploitability of less than 0.0004 in One-Card Poker, which matches the performance of the tabular method. For RSN and ASN, we set neural batch size 4, hidden size 32 and learning rate 0.001.

APPENDIX D OPTIMIZE COUNTERFACTUAL REGRET MINIMIZATION WITH TWO DEEP NEURAL NETWORKS

Algorithm 3: Mini-Batch RS-MCCFR with Double Neural Networks
1 Function Mini-Batch-MCCFR-NN(t): 2 MR  , MS   3 For all i = 1 to b do in parallel then 4 MCCFR-NN(t, , 0, 1, 1) 5 MCCFR-NN(t, , 1, 1, 1)
6 return MR, MS

7

8 Function MCCFR-NN(t, h, i, i, irs(k)): 9 Ii  Ii(h)

10 if h  Z then

11

return

ui (h) irs(k)

12 else if P (h) = -1 then

13 a  -i(Ii)

14 return MCCFR-NN(t, ha, i, i, irs(k))

15 16

else if P (h) R^i(·|Ii)

= 

i then R(·, Ii|Rt )

if

t

>

1

else

- 0

17 i(Ii) CalculateStrategy(R^i(·|Ii), Ii)

information set at state h return game payoff
Sample an action from -i(h)
inference the vector of cumulative regret a  A(Ii) calculate current strategy

18 vi(h)  0, ri(·|Ii)  0, si(·|Ii)  0

ri(·|Ii) and i(·|Ii) are two vectors over A(Ii)

19 Ars(k)(Ii)  sampling k different actions according to irs(k)

20 For a  Ars(k)(Ii) do

21 vi(a|h) MCCFR-NN(t, ha, i, ii(a|Ii), irsirs(k)(a|Ii))

22 vi(h)  vi(h) + vi(a|h)i(a|Ii)

update counterfactual value

23 For a  Ars(k)(Ii) do 24 ri(a|Ii)  vi(a|h) - vi(h) 25 si(a|Ii)  i(Ii)i(a|Ii)

update cumulative regret update average strategy numerator

26 Store updated cumulative regret tuple (Ii, ri(·|Ii)) in MR

27 Store updated current strategy dictionary (Ii, si(·|Ii)) in MS

28 29

else R^-i(·|Ii)  R(·, Ii|Rt ) if t > 1 else -0

30 -i(Ii) CalculateStrategy(R^-i(·|Ii), Ii)

inference cumulative regret calculate current strategy

31 a  -i(Ii)

Sample an action from -i(Ii)

32 return MCCFR-NN(t, ha, i, i, irs(k))

33

34 Function CalculateStrategy(Ri(·|Ii), Ii):

35 sum  aA(Ii) max(Ri(a|Ii), 0)

36 For a  A(Ii) do

37

i(a|Ii)

=

max(Ri (a|Ii ),0) sum

if

sum

>

0

else

1 |A(Ii )|

38 return i(Ii)

39

Algorithm 3 presents one application scenario of the proposed double neural method, which is based on the proposed mini-batch robust sampling method. The function MCCFR-NN will traverse the

14

Under review as a conference paper at ICLR 2019

game tree like tabular MCCFR, which starts from the root history h = . Define Ii as the information set of h. Suppose that player i will sample k actions according to the robust sampling. Then the function can be defined as follows. (1) If the history is terminal, the function returns the weighted utility. (2) If the history is the chance player, one action a  A(Ii) will be sampled according to the strategy -i(Ii). Then this action will be added to the history, i.e., h  ha. (3) If P (Ii) = i, the current strategy can be updated by the cumulative regret predicted by RSN. Then we sample k actions according the specified sampling strategy profile irs(k). After a recursive updating, we can obtain the counterfactual value and regret of each action at Ii. For the visited node, their counterfactual regrets and numerators of the corresponding average strategy will be stored in MR and MS respectively. (4) If P (Ii) is the opponent, only one action will be sampled according the strategy -i(Ii).
The function Mini-Batch-MCCFR-NN presents a mini-batch sampling method, where b blocks will be sampled in parallel. This mini-batch method can help the MCCFR to achieve a more accurate estimation of CFV. The parallel sampling makes this method efficient in practice.
APPENDIX E THEORETICAL ANALYSIS

E.1 REACH PROBABILITY AND POSTERIOR PROBABILITY

Lemma 1: The state reach probability of one player is proportional to posterior probability of the opponent's hidden variable, i.e., p(hvi |Ii)  - i(h).

Proof: For player i at information set Ii and fixed i s strategy profile i, i.e., h  Ii, i(h) is constant. Based on the defination of extensive-form game in Section 2.1, the cominbation of Ii and opponent's hidden state h-v i can indicate a particular history h = hvi hv-ia0a1...aL-1. With Bayes'
Theorem, we can inference the posterior probability of opponent's private cards with Equation15

p(hiv |Ii )

=

p(hiv, Ii) p(Ii)

=

p(h) p(Ii)



p(h)

L
 p(hiv)p(hv-i) P (hivhv-ia0a1...al-1)(ak|hvi hv-ia0a1...al-1)
l=1
 (h) = i(h)- i(h)  - i(h)

(15)

E.2 MCCFR PRESENTS AN UNBIASED ESTIMATION OF COUNTERFACTUAL VALUE

Lemma 2: EjqQj [v~i(Ii|Qj )] = vi(Ii)

Proof:

m

EjqQj [v~i(Ii|Qj )] =

qQj vi(Ii|Qj )

j=1

m
=

qQj q(z)

- i

(z

)i

(h,

z)ui

(z

)

j=1 hIi,zQj ,h z

=
hIi,zZ,h z

j:zQj
q(z)

qQj

- i(z)i(h,

z)ui(z)

= - i(z)i(h, z)ui(z)
hIi,zZ,h z
= vi(Ii)

(16)

E.3 ROBUST SAMPLING, OUTCOME SAMPLING AND EXTERNAL SAMPLING
Lemma 3: If k = maxIiI |A(Ii)| and i  N, Ii  Ii, a  A(Ii), irs(k)(a|Ii)  unif (0, |A(Ii)|), then robust sampling is same with external sampling. (see the proof in section E.3)

15

Under review as a conference paper at ICLR 2019

Lemma 4: If k = 1 and irs(k) = i, then robust sampling is same with outcome sampling. (see the proof in section E.3)

For robust sampling, given strategy profile  and the sampled block Qj according to sampling profile

rs(k)

=

(irs(k),

-i),

then

q(z)

=

rs(k)
i

(z)- i

(z

),

and

the

regret

of

action

a



Ars(k)(Ii)

is

r~i((a|Ii)|Qj) = v~i((a|Ii)|Qj) - v~i(Ii|Qj)

=

1 q(z)

- i(z

)i

(ha,

z)ui(z)

-

1 q(z)

- i(z)i

(h,

z

)ui

(z)

zQj ,ha z,hIi

zQj ,h z

=
zQj ,ha

z,hIi

ui(z rs(k)
i

) (z

)

i

(ha,

z

)

-

zQj ,h

z,hIi

ui(z)

rs(k)
i

(z)

i

(h,

z)

=

i(ha, z)uri s(z) -

i(h, z)uirs(z),

zQj ,ha z,hIi

zQj ,h z,hIi

(17)

where

uri s(z)

=

ui (z ) irs(k) (z)

is

the weighted

utility

according to reach

probability

rs(k)
i

(z).

Because

the weighted utility no long requires explicit knowledge of the opponent's strategy, we can use this

sampling method for online regret minimization.

Generally, if player i randomly selects min(k, |A(Ii)|) actions according to discrete uniform distri-

bution

unif (0, |A(Ii)|)

at

information

set

Ii,

i.e.,

irs(k)(a|Ii)

=

min(k,|A(Ii |A(Ii )|

)|)

,

then

rs(k)
i

(Ii

)

=

hIi ,h

min(k, |A(Ii)|)

h,h a h,h Ii

|A(Ii )|

and uirs(z) is a constant number when given the sampling profile rs(k). Specifically,

(18)

· if k = maxIiI |A(Ii)|, then irs(k)(Ii) = 1, uirs(k)(z) = ui(z), and

r~i((a|Ii)|Qj) =

ui(z)(i(ha, z) - i(h, z))

zQj ,h z,hIi

(19)

Therefore, robust sampling is same with external sampling when k = maxIiI |A(Ii)|.

· if k = 1 and irs(k) = i, only one history z is sampled in this case,then uri s(k)(z) =

,ui(z)
ii (z)

h



Ii,

for

a



Ars(k)(Ii)

r~i((a|Ii)|Qj) = r~i((a|h)|Qj)

=

i(ha, z)uri s(z) -

i(h, z)uri s(z)

zQj ,ha z

zQj ,h z

=

(1

- i(a|h))ui(z) i (ha)

(20)

For a  Ars(k)(Ii), the regret will be r~i((a|h)|j) = 0 - v~i(h|j). Therefore, robust sampling is same with outcome sampling when k = 1 and irs(k) = i.

· if k = 1, and player i randomly selects one action according to discrete uniform distribution

unif (0, |A(Ii)|)

at

information

set

Ii,

then

uirs(1)(z)

=

ui (z ) irs(k) (z)

is

a

constant,

h



Ii,

16

Under review as a conference paper at ICLR 2019

for a  Ars(k)(Ii)

r~i((a|Ii)|Qj) =

i(ha, z)uirs(z) -

i(h, z)uri s(z)

zQj ,ha z,hIi

zQj ,h z,hIi

(21)

= (1 - i(a|h))i(ha, z)uri s(1)(z)

if action a is not sampled at state h, the regret is r~i((a|h)|j) = 0 - v~i(h|j). Compared to outcome sampling, the robust sampling in that case have a lower variance because
of the constant uri s(1)(z).

E.4 MINI-BATCH MCCFR GIVES AN UNBIASED ESTIMATION OF COUNTERFACTUAL VALUE

Lemma 5: EQjRobust Sampling[v~i(Ii|b)] = vi(Ii).

Proof:

EQjRobust Sampling[v~i(Ii|b)] = Eb unif(0,b)[v~i(Ii|b )]



b
= Eb unif(0, b) 

- i(z)i(h, z)ui(z)

q(z)b



j=1 hIi,zQj ,h z



1 = Eb unif(0, b)  b

b

v~i(Ii|Qj )

j=1



1 =
b

b

1 b

b

v~i(Ii|Qj )

b =1

j=1



1 =
b

b

1 b

b

E (v~i (Ii |Qj ))

b =1

j=1



1 =
b

b

1 b

b

vi (Ii )

b =1

j=1

= vi(Ii)

(22)

17


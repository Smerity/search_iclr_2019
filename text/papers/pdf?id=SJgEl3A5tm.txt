Under review as a conference paper at ICLR 2019
CAMOU: LEARNING A VEHICLE CAMOUFLAGE FOR PHYSICAL ADVERSARIAL ATTACK ON OBJECT DETECTORS IN THE WILD
Anonymous authors Paper under double-blind review
ABSTRACT
In this paper, we conduct an interesting experimental study about the physical adversarial attack on object detectors in the wild. In particular, we learn a camouflage pattern to hide vehicles from being detected by state-of-the-art convolutional neural network based detectors. Our approach alternates between two threads. In the first, we train a neural approximation function to imitate how a simulator applies camouflages to vehicles and how a vehicle detector performs given an image generated by the simulator. In the second, we minimize the approximated detection score by searching for the optimal camouflage. Experiments show that the learned camouflage can not only hide a vehicle from the image-based detectors under many cases, but also generalizes to different environments, vehicles, and object detectors.
1 INTRODUCTION
Is it possible to paint a unique pattern on a vehicle's body and hence hide it from being detected by surveillance cameras? We conjecture that the answer is affirmative mainly for two reasons. First, deep neural networks will be widely used in modern surveillance and autonomous driving systems for automatic vehicle detection. Second, unfortunately, these neural networks are intriguingly vulnerable to adversarial examples Akhtar & Mian (2018). The authors in Szegedy et al. (2013) found that adding imperceptible perturbations to clean images can result in the failure of neural networks trained for image classification. This motivates a rich line of work on developing defense techniques for the neural networks Akhtar & Mian (2018) and powerful attack methods to defeat those defenses (Athalye et al., 2018a). Moreover, the adversarial attack has been extended to other tasks, such as semantic segmentation (Arnab et al., 2018), objection detection (Xie et al., 2017), image captioning (Chen et al., 2018a), etc. It is worth noting that the adversarial examples in the works mentioned above are not physical, i.e. the adversary directly manipulates image pixels. Although it is arguably more challenging to create physical adversarial objects than producing adversarial images, some existing works have shown promising results with adversarial patches (Brown et al., 2017), stop signs (Eykholt et al., 2018; Chen et al., 2018b), and small objects like baseballs and 3D turtle models (Athalye et al., 2018b).
Figure 1: A Toyota Camry XLE in the center of the image fools the Mask R-CNN object detector after we apply the learned camouflage to it (on the right), whereas neither plain colors (on the left) or a random camouflage (in the middle) is able to escape the Camry from being detected.
1

Under review as a conference paper at ICLR 2019
To this end, we are reasonably optimistic about designing a special pattern to camouflage a 3D car, in order to make it difficult to detect with deep learning-based vehicle detectors.
It is undoubtedly challenging to run experiments in the real world considering financial and time constraints. In this paper, we instead demonstrate results using a simulation Unreal engine with a high-fidelity 3D sedan model and a 3D SUV. Fig.1 shows that the vehicle in the simulation is realistic enough that even covered with random camouflage, it still would be detected by the COCO trained detector.
The simulation engine enables us to test the generated adversarial cars under a considerable spectrum of environmental conditions: lighting, backgrounds, camera-to-object distances, viewing angles, occlusions, etc. In contrast, existing experiments on physical adversarial attacks are all executed in simplified scenarios. Eykholt et al. (2018) and Chen et al. (2018b) respectively attack neural classifiers and detectors using the front views of a stop sign. Athalye et al. (2018b) synthesize objects (e.g., baseball, turtle, etc.) which are adversarial within a small range of camera-to-object distances and view angles.
Given a 3D vehicle model in the simulation engine, we learn a camouflage for it by following the expectation over transformation (EoT) principle of Athalye et al. (2018b). The main idea is to consider a variety of transformations under which the camouflage can consistently hide the vehicle from a neural detector. A transformation imitates the imaging procedure and produces an image of the 3D vehicle model in the simulated environment. If a camouflage works under many transformations seen in the training phase, it is expected to also generalize to unseen transformations in the test phase.
One of the challenges in our context is that the simulator's image generation procedure is nondifferentiable. A seemingly plausible solution is to train a neural network to approximate this procedure. The network takes as input the environment, a camouflage pattern, the 3D vehicle model and outputs an image as close as possible to the one rendered by the simulator. Although this approach is viable, it is extremely difficult to generate high-resolution images. E.g. the current state-of-the-art RenderNet (Nguyen-Phuoc et al., 2018) is only able to generate single simple 3D object without background.
We tackle the above challenge by drawing the following observation. In EoT (Athalye et al., 2018b; Chen et al., 2018b), the gradients propagate back to the physical object from the detector/classifier's decision values. If we jointly consider the object detector and the imaging procedure of the simulator as a whole black box, it is easier to learn a function to approximate this black box's behavior than training the image generation neural network. As a result, we learn a substitute neural network which takes as input a camouflage, a vehicle model, and the environment and outputs the vehicle detector's decision values. Equipped with this substitute network, we can readily run the EoT algorithm (Athalye et al., 2018b) over our simulator in order to infer an adversarial camouflage for our vehicles.
Finally, we make some remarks about the significance and potential impact of this work. In the real world, multiclass visual object detection neural networks He et al. (2017); Redmon & Farhadi (2018) have become the cornerstone of multiple industrial applications such as surveillance systems (Nair et al. (2018)), autonomous driving (Huval et al. (2015)) and military systems (Pellerin (2017)). Among these applications, the car is one of the most crucial objects. Attacking vehicle detectors in the physical world will be enormously valuable and impactful from the perspective of the malicious adversaries. Compared with the stop sign, it is legal in the United States to paint a car while defacing a stop sign is criminal. This poses a more significant threat to autonomous driving systems since anyone has access to perturb public machine learning based system legally. This observation motivates us to set our goal as the car. We limit our camouflage within the legally paintable car body parts, which means that we will leave crucial visual cues, such as the tires, windows, grille, lights, etc, for the detector. And this will make our task further challenging.
2

Under review as a conference paper at ICLR 2019
2 RELATED WORK
2.1 ADVERSARIAL ATTACK AND ITS GENERALIZATION
Currently, the adversarial attack is powerful enough to attack image classification (MoosaviDezfooli et al. (2017)), object detection (Arnab et al. (2018)), semantic segmentation (Xie et al. (2017)), audio recognition (Carlini & Wagner (2018)) and even bypass most of the defense mechanism (Athalye et al. (2018a)). The mainstream adversarial machine learning research focuses on the in silico ones or the generalization within in silico (Liu et al. (2017)). Such learned perturbations are practically unusable in the real world as shown in the experiment by Lu et al. (2017b) who found that almost all perturbation methods failed to prevent a detector from detecting real stop signs.
The first physical world adversarial attack by Kurakin et al. (2016) found perturbations remain effective on the printed paper. Eykholt et al. (2018) found a way to train perturbations that remain effective against a classifier on real stop signs for different view angles and such. Athalye et al. (2018b) trained another perturbation that successfully attacks an image classifier on 3D-printed objects. However Lu et al. (2017c) found that Eykholt et al. (2018)'s perturbation does not fool the object detectors (YOLO9000 (Redmon & Farhadi, 2017) and Faster RCNN (Ren et al., 2015)). They argue that fooling an image classifier is different and easier than fooling an object detector because the detector is able to propose object bounding boxes on its own. In the meanwhile, Lu et al. (2017a)'s and Chen et al. (2018b)'s work could be generalized to attack the stop sign detector in the physical world more effectively. However, all the above methods aim to perturb detecting the stop signs.
Blackbox attack is another relevant topic. Among the current blackbox attack literature, Papernot et al. (2017) trained a target model substitution based on the assumption that the gradient between the image and the perturbation is available. We do not have the gradient since the simulation is nondifferentiable. Chen et al. (2017) proposed a coordinate descent to attack the model. However we found that our optimization problem is time-consuming, noisy (see Sec. E.2), empirically nonlinear and non-convex (see Sec. F). Time constraints made this approach unavailable to us since it requires extensive evaluations during coordinate descent. Besides, coordinate descent generally requires precise evaluation at each data-point.
2.2 SIMULATION AIDED MACHINE LEARNING
Since the dawn of deep learning, data collection has always been of fundamental importance as deep learning model performance is generally correlated with the amount of training data used. Given the sometimes unrealistically expensive annotation costs, some machine learning researchers use synthetic data to train their models. This is especially true in computer vision applications since state-of-the-art computer-generated graphics is virtually photo-realistic. Ros et al. (2016); Richter et al. (2017) proposed synthetic datasets for semantic segmentation. Gaidon et al. (2016) proposed virtual KITTI as a synthetic replica of the famous KITTI dataset (Geiger et al., 2012) for tracking, segmentation etc. And Varol et al. (2017) proposed using synthetic data for human action learning. Zhang et al. (2017); Bousmalis et al. (2017) adapt RL-trained robot grasping models from the synthetic environment to the real environment. Tremblay et al. (2018) trained a detection network using the same Unreal engine that we are using.
3 PROBLEM STATEMENT
In this paper, we investigate physical adversarial attack on state-of-the-art neural network based object detectors. The objective is to find a camouflage pattern such that, when it is painted to the body of a vehicle, the Mask R-CNN (He et al., 2017) or YOLO (Redmon & Farhadi, 2018) detectors fail to detect the vehicle under a considerable spectrum of variations (e.g., locations, lighting conditions, view angles, etc.). We learn the camouflage in a black-box fashion, without the need of accessing the detectors' network architecture or weights.
Expectation-over-transformation (EoT). We formalize the physical adversarial attack problem by the EoT framework (Athalye et al., 2018b). Denote by t a transformation which converts a camouflage pattern c to a real photo. Such a transformation actually represents a procedure: paint the
3

Under review as a conference paper at ICLR 2019

Figure 2: Example procedure of a transformation t. From left to right: a 16 × 16 camouflage, a highfidelity vehicle, a location in our simulated environment, and an image due to this transformation over the camouflage.

pattern to a vehicle's body, drive the vehicle to a location, and take a picture of the vehicle. Moreover, the transformation conveniently abstracts away enormous factors (e.g., quality of painting, camera, etc.), each of which could play a role in this procedure. Denote by Vt(c) the detection score (e.g., by Mask-RCNN) over an image which is a result of a transformation t and a camouflage c. The physical adversarial attack problem is then described by

min
c

EtT Vt(c)

(1)

where T is a distribution over all the possible transformations {t}. In other words, we search for a camouflage c that minimizes the vehicle detection score in expectation.

Transformation in simulation. Due to financial and time constraints, we conduct our study with the photo-realistic Unreal 4 game engine. It supplies us sufficient configuration parameters, such as the resolution and pattern of the camouflage, 3D models of vehicles, parameters of cameras and environments, etc. Figure 2 shows a camouflage pattern, a high-fidelity 3D model of Toyota Camry, a corner of the virtual city used in this paper, and finally the picture taken by a camera after we apply the camouflage to the Camry and drive it to that corner -- in other words, the rightmost image is the result of a certain transformation t acting on the three left images of Figure 2.

4 APPROACH
In this section, we present two key techniques for solving the problem minc Et Vt(c) (cf. eq. (1) and the text there). One is to estimate the expectation Et by an empirical mean over many transformations. The other is to train a neural network to clone the joint behavior Vt(c) of the black-box detector and the nondifferentiable simulator.
4.1 SAMPLING TRANSFORMATIONS TO ESTIMATE Et
Recall that a transformation t specifies a particular procedure from applying the camouflage c to a vehicle until the corresponding image captured by a camera. In the context of the simulation engine, the camouflage c is first programmed to textures, which are then warped onto the 3D model of an object. The simulator can teleport the object to different locations in the environment. The simulator also has multiple cameras to photograph the teleported object from various distances and view angles.
We identify some key factors involved in this procedure, including vehicle, location, camera-toobject distance, and view angle. The combinations of them also give rise to variations along other dimensions. For instance, the lighting condition changes from one location to another. The vehicle of interest is occluded to different degrees in the images captured by the cameras. Denote by TS all the sampled transformations that are used for learning the camouflage.
Fig. 3 illustrates the positions where the cameras are placed. We can see how the view angles and camera-to-object distances vary. The ones shown in the green color are used in the training. We left out some cameras shown in the red color for the testing. Since utilizing the camera is computationally expensive, we arrange the cameras in different heights and distances to create as much variance as possible instead of permuting all possible height/ distance combination.

4

Under review as a conference paper at ICLR 2019

L: 20m H: 4m

......

L: 20m H: 4m

L: 21m
H: 4m
II

L: 13m H: 2m

I L: 6m H: 1.5m

L: 5.5m H: 1.5m

L: 5.5m H: 1.5m

L: 21m H: 4m
III
L: 13m H: 2m

L: 5.5m H: 1.5m
... L: 6m H: 1.5m
L: 5.5m H: 1.5m

L: 5.5m H: 1.5m

IV
L: 6m H: 1.5m
L: 5.5m H: 1.5m

...

L: 20m H: 4m

L: 13m H: 2m

L: 5.5m H: 1.5m

L: 21m H: 4m

L: 5.5m H: 1.5m
L: 6m H: 1.5m

L: 13m H: 2m
L: 21m H: 4m

I II III IV

L: 20m H: 4m
Figure 3: The camera setup as well as some exemplar locations. The cameras depicted in Green color are used to learn the camouflage while those in the red ones are unseen cameras used for testing the camouflage's transferability. (H: camera height, L: vehicle-to-camera distance.

4.2 LEARNING A CLONE NETWORK V(c, t) TO APPROXIMATE Vt(c)

If we unroll the detection score Vt(c), it contains at least two major components. The first renders an image based on the camouflage c by following the procedure specified by the transformation t. The second is to obtain the detection score of a vehicle detector (e.g., Mask-RCNN) over the rendered image. The first component is nondifferentiable while the second could be a black box in practice. Therefore, we propose to consider them jointly as a black box. Furthermore, we learn a clone neural network V(c, t) to imitate the input-output behavior of this extended black box. As the transformation t itself is involved and hard to represent, we instead input its consequence to the network: a background image and the cropped foreground of a white vehicle.

Figure 4b shows the network architecture. It takes as input a camouflage pattern c, the background
image due to a sampled transformation t, and the cropped foreground. The network V(c, t) has a single output to approximate the detection score Vt(c).

To this end, we are ready to write down an approximate form of problem (1),

1

min
c

|TS | tTS V(c, t).

(2)

Thanks to the differentiable network V(c, t) with respect to the camouflage c, we can solve the problem above by standard (stochastic) gradient descent.

It is important to note that the fidelity of problem (2) depends on the size and diversity of the sampled set of transformations TS as well as the quality of the clone network V(c, t). It is straightforward to generate a large training set for the clone network by randomizing the camouflages and transformations. However, if the optimal camouflage is unfortunately not covered by this training set, a discrepancy would occur when we solve problem 2. In other words, the network may fail to well approximate the detector scores at the region around the optimal camouflage. We address this discrepancy by an alternative learning algorithm as follows.

4.3 JOINTLY LEARNING THE CLONE NETWORK AND THE OPTIMAL CAMOUFLAGE
We alternatively learn the clone network V(c, t) and solve the problem (2). Once a new camouflage pattern is found due to optimizing problem (2), it is converted to multiple images by the training

5

Under review as a conference paper at ICLR 2019



argmin


1  

  [, 
 

, 

] + 



2

 =   {}

argmin


 (, ) 



(a) Our iterative camouflage optimization framework.

Conv Max

=

3 × 3 Conv2D
Output 4 channel

Max Pooling

Conv Max

...

Conv Max

 ×  

log2() - 2 ConvMax

64 × 64 background image

Conv

Conv

Conv

Max Max Max

Conv Max

64 × 64 foreground image

Conv

Conv

Conv

Max Max Max

Conv Max

(b) Architecture of V.

Figure 4: Overview of our optimization pipeline and the clone network V


Linear & Sigmoid 16  1
Conv2D 4
Channel Concat

transformations TS. We obtain the detection score for each of the images by querying a detector. The camouflage pattern, along with the detection scores are then added to the training set for learning the clone network V(c, t). Figure 4 illustrates this process.

Implementation details. Denote by H[p, q] := -p log q - (1 - p) log(1 - q) the cross-entropy loss. We alternately optimize the following two problems,

1

min


H |C||TS | cC tTS

s, V(c, t)

+



2,

1

min c |T |

H 0, V(c, t)

tTS

(3)

where C is the collection of all camouflages in the training set for learning the clone network V(c, t). s is the stored Vt(c).

The 2 regularization   over the network weights is essential. Without this term, the two loss functions may cause oscillations. We set  = 10 in the experiments. As the approximation accuracy of the clone network near the optimal camouflage is more important than the other regions, we weigh the samples newly added to the training set 10 times the old ones at every iteration. Algorithm 1 in the Appendix gives more details.

5 EXPERIMENTS
Since the primary objective of this paper is to learn camouflage patterns that deceive vehicle detectors, we introduce two baseline camouflage patterns in the experiments: 6 most popular car colors and 800 random camouflages in different resolutions. We then analyze how the resolution of the camouflage affects the detection performance. For the vehicles, we employ two 3D models: a 2015 Toyota Camry and a virtual SUV. In addition to the main comparison results, we also test the transferability of the learned camouflage across different car models, environments, camera positions, and object detectors.
5.1 EXPERIMENT SETUP
We describe the detailed experimental setup in this section, including the simulator, vehicle detector, evaluation metrics, and the baseline camouflage patterns.
5.1.1 THE SIMULATORS
As shown in Fig. 5a, we used the Unreal engine to build our first simulation environment from the photo-realistic DownTown environment. It is modeled after downtown Manhattan in New York. There are the skyscrapers, cars, traffic signs, a park, and roads, resembling a typical urban environment. We sample 32 different locations along the streets. Eight cameras perch at each location, each taking pictures of the size 720 × 360. The relative position of a camera is indexed by the view angle and camera-to-object distance, as shown in Fig. 3. In addition, we install another 16 cameras to test the transferability of camouflages across view angles, distances, etc. In total, we use 18 locations

6

Under review as a conference paper at ICLR 2019
(a) An urban environment.
(b) A mountain environment. Figure 5: Two of our environments we built to train and test our camouflage. Zoom in for more details. These images are of the higher resolution but the same quality the detector perceived. for training and another 18 for testing. Note that the vehicles are invisible to some cameras due to occlusion. Our second environment is based on a totally different countryside scene Landscape Mountains, shown in Fig. 5b. The roads lie on high-altitude mountains and cross bridges, forest, snow, and a lake. We use this scene to test the generalization ability of our camouflage across different environments; this scene is not used in the training. Like the DownTown environment, we sample 18 locations along the roads for the purpose of testing. The two vehicles used in the experiments are shown in Figure 6. One is a 2015 Toyota Camry XLE sold in the European Union. The other is a virtual SUV from AirSim (Shah et al. (2017)). It is worth noting that the Toyota sedan appears multiple times in the MS-COCO dataset Lin et al. (2014) (cf. some examples in Fig.10). Since the object detectors are trained on MS-COCO, it is more challenging to hide the Camry than the virtual SUV. 5.1.2 VEHICLE DETECTOR We study two state-of-the-art detectors: Mask R-CNN (He et al., 2017) and YOLOv3-SPP Redmon & Farhadi (2018). Mask R-CNN is one of the most powerful publicly available object detectors; it currently ranks 4th place in the MS COCO detection leaderboard. Both detectors are pre-trained
7

Under review as a conference paper at ICLR 2019

Figure 6: Orthogonal views of the Toyota Carmy XLE 2015 (second row) and the virtual SUV (first row) used in our simulation.

on MS COCO. For Mask R-CNN, we use the one implemented by Abdulla (2017). Its backbone network is ResNet-101 (He et al., 2016). YOLOv3 has comparable performance with Mask R-CNN. Its network architecture is very different from Mask R-CNN's, causing challenges to the transfer of the camouflage between the two detectors. We use the spatial pyramid pooling (SPP) variant of YOLOv3 in the experiments.
In the rest of the paper, we experiment with Mask R-CNN except the transfer experiments in Sec. 5.4.

5.1.3 EVALUATION METRICS

We adopt two metrics to evaluate the detection performance. The first one is a variation of the

mean Intersection over Union (mIoU) proposed by Everingham et al. (2015). The IoU between a

predicted

box

and

the

groundtruth

bounding

box

is

defined

as

IoU (A, B)

=

AB AB

.

Since

IoU

was

originally proposed to evaluate the results of multi-object detection, as opposed to the single vehicle

detection in our context, we modify it to the following to better capture the vehicle of interest:

maxpP IoU (p, GT ), where P is the set of all detection proposals in an image. We average this

quantity across all the test images and denote the average by mIoU.

Our second metric is precision at 0.5 or P@0.5. Everingham et al. (2015) set a 50% threshold for the detection IoU in the PASCAL VOC detection challenge to determine whether a detection is a hit or miss. We report the percentage of the hit detections across all observations as our P@0.5. We also report the relative performance drop over baseline colors whenever possible.

5.1.4 BASELINES
Our first baseline is the detector's performance when a vehicle is colored in popular real car colors. We select 6 basic car colors (red, black, silver, grey, blue, and white) which cover over 90% of the car colors worldwide (Axalta). We obtain their RGB values according to the X11 Color Names.
One may concerned that the learned camouflage successfully attacks the detector not because it exploits the CNN's structural weakness, but because it takes advantage of the domain gap between the real data used to train the detector and our simulated data used to test the detector.
To address this concern, we generate 800 random camouflages in different resolutions ranging in {2i × 2i|i  [1..8]}. Since we find that camouflages with strong contrasts work better, we generate half of the camouflages using RGB values  [0, 255] and the other half of them using RGB values  {0, 255}. After we obtain these camouflages and the corresponding detection scores, we use those with proper resolutions to initialize the training set for the clone network V(c, t).

5.2 RESOLUTION OF THE CAMOUFLAGES
We first report the random camouflages' performance on hiding the Camry from the detectors in the Downtown environment. Fig.7 shows the results. The first observation is that, although ran-

8

Detection mIoU Detection Precision@0.5

Under review as a conference paper at ICLR 2019

76%

mean mIoU 86%

mean Precision

75% 84% 74% 73% 82%

72% 80% 71% 70% 78%

69% 76%

2 4 8 16 32 64 128 256 Camou Resolution
(a) mIoU.

2 4 8 16 32 64 128 256 Camou Resolution
(b) Precision.

Figure 7: The mIoU and P@0.5 of the 800 random camouflages in different resolutions on Camry in the Downtown environment. There are 100 camouflages per resolution.

Figure 8: Visualization of three random camouflages of the resolutions 4×4, 16×16, and 256×256, respectively, as well as the resulting images by the same camera.
dom camouflages are visually very different from conventional car paintings (cf. Fig.8), the Mask R-CNN detector is still able to detect them without any challenge. Another slightly counter-intuitive yet interesting observation is that the detector's performance does not always decrease as the camouflage's resolution increases. This is probably because some fine-grained patterns displayed by the high-resolution camouflage become harder to observe from a distance. Since the high-resolution camouflage does not bring any additional benefits beyond the 16×16 resolution, we use camouflages of size 16 × 16 in the remaining experiments.
5.3 TOYOTA CAMRY IN URBAN AREA
Here we report the detection results on the Camry. Table 1 summarizes the results of the Camry with baseline colors, random camouflages, and our learned camouflage in the urban environment. We can see from the table that the Mask R-CNN is surprisingly robust against different types of random camouflages. The random camouflages' detection scores are close to the baseline colors'. Moreover, the standard deviation of the random camouflages' scores is also very low, indicating that the difference in the random camouflages does not really impact the performance. Note that we apply the camouflage to the body of the Camry, leaving tires, windows, grilles, etc., as informative visual cues to the detector. Despite that, our learned camouflage reduces the precision by around 30% over baseline colors in both training and testing scenes in the urban environment.
Virtual SUV. Appendix A presents results on the virtual SUV. The learned camouflage decreases Mask R-CNN's detection precision by around 40% relative to the precision with baseline colors.
5.4 TRANSFERABILITY EXPERIMENTS
We show that the camouflage learned to attack Mask R-CNN can actually also defeat YOLOv3 to a certain degree. The results are reported in Table 2. Similarly, appendices A to D report the transferabilities of the learned camouflage across different environments, vehicles, camera view angles and distances to the object.
9

Under review as a conference paper at ICLR 2019

Camouflages
Baseline Colors Random Camou Ours Relative Performance Drop

Training Scenes

mIoU (%) P@0.5 (%)

76.14

84.40

73.48± 0.80 82.17± 1.20

57.69

62.14

24.23%

26.37%

Testing Scenes

mIoU (%) P@0.5 (%)

72.88

77.57

67.79± 0.79 71.42± 1.20

53.64

52.17

26.39%

32.74%

Table 1: Mask R-CNN detection performance on the Camry in an urban environment.

Camouflages
Baseline colors Random Camou Ours (YOLO trained) Ours (Mask R-CNN trained)
Relative Performance Drop

Training Scenes

mIoU (%) P@0.5 (%)

76.79 73.19±0.75 65.83 65.43

85.34 83.30±0.89 72.53 70.42

14.79%

17.48%

Testing Scenes

mIoU (%) P@0.5 (%)

73.00 68.76±0.81 64.03 61.79

78.50 73.92±1.00 69.56 65.21

15.35%

16.92%

Table 2: YOLOv3-SPP detection performance on the Camry in an urban environment. In addition to the camouflage trained for YOLO, we also include the YOLO detection results on the camouflage learned for Mask R-CNN.

5.5 QUALITATIVE RESULTS
We present some of our detection results on the baseline color, random camouflaged, and learned camouflaged Camry in Fig.9.
We can draw a lot of interesting observations from the qualitative results that we cannot from the quantitative ones. We discover that there are 3 types of successful attacks: (1) Camouflages lower the objectiveness of the car and the car is not detected or only partially detected; (2) The car is successfully detected but misclassified (i.e., kite, cake, truck, potted plant as shown in the examples) or the classification score is too low to pass the detection classification score threshold. (3) The car is successfully detected and classified, but the camouflage resulted in another incorrect overlapped detection (the car in the 5th row, for example, is being detected as both a car and a boat for some reason).
One can see that the context or background plays a vital role in object detection. Although some cars have the same pose, the detector makes completely different predictions for them. Also, these qualitative results imply that the detector works in a way different from human vision. In the landscape scenarios, our learned camouflage has the strongest contrast to the background compared to other tested patterns. However, the detector is still sometimes not able to detect these cars.
6 CONCLUSION
In this paper, we investigate whether it is possible to camouflage objects with complex shapes, i.e., vehicles, to hide them from state-of-the-art detectors such as Mask R-CNN. We found this to be feasible. We proposed a clone network to mimic the simulation and the detector. Then a camouflage could be proposed by minimizing the output of the clone network. Our learned camouflage significantly reduced the detectability of these vehicles and we further showed that the camouflage is transferable across different environments. We plan to look into possible ways to white-box the entire process to propose a stronger camouflage in our future work.

10

Under review as a conference paper at ICLR 2019
REFERENCES
Waleed Abdulla. Mask r-cnn for object detection and instance segmentation on keras and tensorflow. https://github.com/matterport/Mask_RCNN, 2017.
Naveed Akhtar and Ajmal Mian. Threat of adversarial attacks on deep learning in computer vision: A survey. arXiv preprint arXiv:1801.00553, 2018.
Anurag Arnab, Ondrej Miksik, and Philip H. S. Torr. On the robustness of semantic segmentation models to adversarial attacks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2018.
Anish Athalye, Nicholas Carlini, and David Wagner. Obfuscated gradients give a false sense of security: Circumventing defenses to adversarial examples. In Proceedings of the 35th International Conference on Machine Learning, July 2018a.
Anish Athalye, Logan Engstrom, Andrew Ilyas, and Kevin Kwok. Synthesizing robust adversarial examples. In Proceedings of the 35th International Conference on Machine Learning, volume 80, pp. 284­293, 2018b.
Axalta. Global automotive 2017 color popularity report, 2017.
Konstantinos Bousmalis, Alex Irpan, Paul Wohlhart, Yunfei Bai, Matthew Kelcey, Mrinal Kalakrishnan, Laura Downs, Julian Ibarz, Peter Pastor, Kurt Konolige, et al. Using simulation and domain adaptation to improve efficiency of deep robotic grasping. arXiv preprint arXiv:1709.07857, 2017.
Tom B Brown, Dandelion Mane´, Aurko Roy, Mart´in Abadi, and Justin Gilmer. Adversarial patch. arXiv preprint arXiv:1712.09665, 2017.
Nicholas Carlini and David Wagner. Audio adversarial examples: Targeted attacks on speech-totext. arXiv preprint arXiv:1801.01944, 2018.
Hongge Chen, Huan Zhang, Pin-Yu Chen, Jinfeng Yi, and Cho-Jui Hsieh. Attacking visual language grounding with adversarial examples: A case study on neural image captioning. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, volume 1, pp. 2587­ 2597, 2018a.
Pin-Yu Chen, Huan Zhang, Yash Sharma, Jinfeng Yi, and Cho-Jui Hsieh. Zoo: Zeroth order optimization based black-box attacks to deep neural networks without training substitute models. In Proceedings of the 10th ACM Workshop on Artificial Intelligence and Security, pp. 15­26. ACM, 2017.
Shang-Tse Chen, Cory Cornelius, Jason Martin, and Duen Horng Chau. Shapeshifter: Robust physical adversarial attack on faster r-cnn object detector. arXiv preprint arXiv:1804.05810, 2018b.
DownTown, 2017. URL https://www.unrealengine.com/marketplace/downtown.
M. Everingham, S. M. A. Eslami, L. Van Gool, C. K. I. Williams, J. Winn, and A. Zisserman. The pascal visual object classes challenge: A retrospective. International Journal of Computer Vision, 111(1):98­136, January 2015.
Kevin Eykholt, Ivan Evtimov, Earlence Fernandes, Bo Li, Amir Rahmati, Chaowei Xiao, Atul Prakash, Tadayoshi Kohno, and Dawn Song. Robust physical-world attacks on deep learning visual classification. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 1625­1634, 2018.
Adrien Gaidon, Qiao Wang, Yohann Cabon, and Eleonora Vig. Virtual worlds as proxy for multiobject tracking analysis. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 4340­4349, 2016.
Andreas Geiger, Philip Lenz, and Raquel Urtasun. Are we ready for autonomous driving? the kitti vision benchmark suite. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2012.
11

Under review as a conference paper at ICLR 2019

Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 770­778, 2016.

Kaiming He, Georgia Gkioxari, Piotr Dolla´r, and Ross Girshick. Mask r-cnn. In International Conference on Computer Vision, pp. 2980­2988. IEEE, 2017.

Brody Huval, Tao Wang, Sameep Tandon, Jeff Kiske, Will Song, Joel Pazhayampallil, Mykhaylo Andriluka, Pranav Rajpurkar, Toki Migimatsu, Royce Cheng-Yue, Fernando Mujica, Adam Coates, and Andrew Y. Ng. An empirical evaluation of deep learning on highway driving. arXiv preprint arXiv:1504.01716, 2015.

Alexey Kurakin, Ian Goodfellow, and Samy Bengio. Adversarial examples in the physical world. arXiv preprint arXiv:1607.02533, 2016.

Landscape Mountains, 2014.

URL https://www.unrealengine.com/blog/

new-on-marketplace-landscape-mountains.

Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dolla´r, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In European conference on computer vision, pp. 740­755. Springer, 2014.

Yanpei Liu, Xinyun Chen, Chang Liu, and Dawn Song. Delving into transferable adversarial examples and black-box attacks. In Proceedings of 5th International Conference on Learning Representations, 2017.

Jiajun Lu, Hussein Sibai, and Evan Fabry. Adversarial examples that fool detectors. arXiv preprint arXiv:1712.02494, 2017a.

Jiajun Lu, Hussein Sibai, Evan Fabry, and David Forsyth. No need to worry about adversarial examples in object detection in autonomous vehicles. arXiv preprint arXiv:1707.03501, 2017b.

Jiajun Lu, Hussein Sibai, Evan Fabry, and David Forsyth. Standard detectors aren't (currently) fooled by physical adversarial stop signs. arXiv preprint arXiv:1710.03337, 2017c.

Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, Omar Fawzi, and Pascal Frossard. Universal adversarial perturbations. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 1765­1773, 2017.

Malavika Nair, Mathew Gillroy, Neethu Jose, and Jasmy Davies. i-surveillance crime monitoring and prevention using neural networks. International Research Journal of Engineering and Technology, 5, March 2018.

Thu Nguyen-Phuoc, Chuan Li, Stephen Balaban, and Yongliang Yang. Rendernet: A deep convolutional network for differentiable rendering from 3d shapes. arXiv preprint arXiv:1806.06575, 2018.

Nicolas Papernot, Patrick McDaniel, Ian Goodfellow, Somesh Jha, Z Berkay Celik, and Ananthram Swami. Practical black-box attacks against machine learning. In Proceedings of the 2017 ACM on Asia Conference on Computer and Communications Security, pp. 506­519. ACM, 2017.

Cheryl Pellerin. Project maven to deploy computer algorithms to war zone by year's end, July 2017. U.S. Department of Defense.

Joseph Redmon and Ali Farhadi. Yolo9000: Better, faster, stronger. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 6517­6525. IEEE, 2017.

Joseph Redmon and Ali Farhadi. Yolov3: An incremental improvement. arXiv preprint arXiv:1804.02767, 2018.

Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object detection with region proposal networks. In Advances in neural information processing systems, pp. 91­99, 2015.

12

Under review as a conference paper at ICLR 2019
Stephan R Richter, Zeeshan Hayder, and Vladlen Koltun. Playing for benchmarks. In International conference on computer vision, volume 2, 2017.
German Ros, Laura Sellart, Joanna Materzynska, David Vazquez, and Antonio M Lopez. The synthia dataset: A large collection of synthetic images for semantic segmentation of urban scenes. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 3234­ 3243, 2016.
Shital Shah, Debadeepta Dey, Chris Lovett, and Ashish Kapoor. Airsim: High-fidelity visual and physical simulation for autonomous vehicles. In Field and Service Robotics, 2017. URL https: //arxiv.org/abs/1705.05065.
Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus. Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199, 2013.
Jonathan Tremblay, Aayush Prakash, David Acuna, Mark Brophy, Varun Jampani, Cem Anil, Thang To, Eric Cameracci, Shaad Boochoon, and Stan Birchfield. Training deep networks with synthetic data: Bridging the reality gap by domain randomization. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, pp. 969­977, 2018.
Unreal, 1998. URL https://www.unrealengine.com. Gu¨l Varol, Javier Romero, Xavier Martin, Naureen Mahmood, Michael J Black, Ivan Laptev, and
Cordelia Schmid. Learning from synthetic humans. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 4627­4635. IEEE, 2017. X11 Color Names. URL https://cgit.freedesktop.org/xorg/app/rgb/tree/ rgb.txt. Cihang Xie, Jianyu Wang, Zhishuai Zhang, Yuyin Zhou, Lingxi Xie, and Alan Yuille. Adversarial examples for semantic segmentation and object detection. In International Conference on Computer Vision. IEEE, 2017. Fangyi Zhang, Ju¨rgen Leitner, Michael Milford, and Peter Corke. Sim-to-real transfer of visuomotor policies for reaching in clutter: Domain randomization and adaptation with modular networks. arXiv preprint arXiv:1709.05746, 2017.
13

Under review as a conference paper at ICLR 2019

Camouflages
Baseline Colors Random Camou Ours Relative Performance Drop

Training Scenes

mIoU (%) P@0.5 (%)

82.35

91.07

83.53±3.26 93.21±3.48

53.27

55.79

35.31%

38.79%

Testing Scenes

mIoU (%) P@0.5 (%)

81.06

89.22

78.02±2.53 84.79±2.81

48.91

50.36

39.66%

43.55%

Table 3: Detection performance of camouflages on SUV in urban environment.

Camouflages
Baseline Colors Random Camou Ours - Transferred Relative Performance Drop

Testing Scenes

mIoU (%) P@0.5 (%)

87.72

97.28

86.10±1.97 94.14±2.28

71.59

77.27

22.53%

20.56%

Table 4: Detection performance of camouflages on Camry in countryside environment. note this camouflage is pretrained in urban environment and then transferred without any finetuning.

A VIRTUAL SUV IN URBAN AREA
We then report the camouflage performance on the newly modeled SUV in the urban environment in Table.3.
Judging by the results, it is clear that the Mask R-CNN is a very generalized detector. The SUV's baseline color and random camouflage detection scores are even higher than the Camry correspondence although the detector has never seen it before. This might because the SUV has much less polygon and is built to resemble the shape of a general SUV as shown in Fig.6.
However, the price for not seeing this vehicle during training is that Mask R-CNN is more likely to be affected by the camouflage. The std of the random camouflage mIoU/ precision is higher (3.26/3.48 vs. 0.8/1.2) than Camry. And our camouflage achieves lower detection precision despite both baseline colors and random camouflage's detection scores are higher than Camry's. The detectability reduced is almost 1.5 times of Camry's results in this case. This experiment shows that Mask R-CNN might well generalized to unseen vehicles, but it is easier to get attacked.
B TRANSFERABILITY ACROSS ENVIRONMENTS
Another important question is that what if we transfer the camouflage to a not only previously unseen but a totally different environment? To quantitatively answer this question, we build a countryside environment (Fig.5b) to test our camouflages trained in urban environment (Fig.5a) using the Camry vehicle. The results are reported in Table.4. Some qualitative results are shown in Fig.9.
Given the barren landscape with few objects in sight, the detector detects the car much better than it did in the urban environment for both baseline colors (97.28 vs. 77.57) and random camouflages (94.14 vs. 77.57) with almost perfect precision possibly due to the absence of distractions. However, our directly transferred camouflage is still able to beat both of them by more than 20% regarding both detection mIoU and precision without any fine-tuning.
It is interesting to notice that the Camry with grey color looks almost identical to the background in this environment (Fig.9). However, it still could be perfectly detected. Meanwhile, our leaned camouflage results better stealth despite it has a sharp contrast to the background.
14

Under review as a conference paper at ICLR 2019

SUV Camry

Test SUV Camry
50.36 47.44 58.39 52.17

Train

Table 5: Camouflage transferability across vehicle reported in testing P@0.5 in urban environment.

Camouflages
Baseline Colors Random Camou Ours - Transferred Relative Performance Drop

Testing Scenes

mIoU (%) P@0.5 (%)

78.43

86.16

77.26±0.96 84.61±1.06

67.74

73.14

13.62%

15.11%

Table 6: Detection performance of pretrained camouflages on Camry with urban environment in 16 unseen cameras.

C TRANSFERABILITY ACROSS VEHICLES
It will be impractical to retrain a specific camouflage for each vehicle whenever we need it. Hence it would be interesting to look into the transferability between vehicles in this scenario. We swap the camouflages of SUV and Camry and see how they would perform in the urban environment. We present the testing precision after swapping in Table.5. First, both camouflages are definitely transferable. It is also interesting to see that the Camry learned camouflage is not as good as the SUV learned camouflage even when being applied on the Camry itself. This might due to that the SUV resembles more generic car feature and hence learning camouflage on SUV is less likely to encounter local minima during optimization.
D TRANSFERABILITY ACROSS VIEW POSITION
One of the probabilities most concerned question is that whether this camouflage is robust to the change of the camera position. To answer this question, we set up another 16 new cameras, as shown in Fig.3, surrounding the vehicle in different relative locations. We then test the learned camouflage's performance on these new cameras. The results are shown in Table.6. Our camouflage performance drop has slightly decrease 5% from Table.1. This indicates that the change of the camera locations would impact the performance, but the performance drop is still way beyond the stand deviation of random camouflages' scores. Given that the new cameras' views cover more perspectives as shown in Fig.3, this result is reasonable.
E SIMULATION SETUP
E.1 SIMULATION IMPLEMENTATION
The iterative optimization framework works on two Nvidia GTX 1080 Ti in our experiment. We use one to run the detector and another one to run the simulation and training/ prediction of evaluation network. The simulation was implemented using AirSim by Shah et al. (2017) and UnrealEnginePython. All submodules are implemented asynchronously to run in parallel and are communicating with each other using RPC/ RPyC. All the camouflages and the baseline colors are implemented using or based on the official Unreal Automotive Material. Each evaluation in the simulation, which is the most time-consuming part, takes around 15 to 20 second.
15

Under review as a conference paper at ICLR 2019

E.2 SIMULATION ERROR
Despite our best effort, we observe Vt(c) come with a standard deviation of 0.008 due to the inherent and mostly necessary random processes in the rendering (i.e., Monte Carlo in path tracing, etc.). This unfortunately makes Vt(c) a noisy function. We reduce this error by repeat sampling Vt(c) 5 times whenever use it.

F NON-LINEARITY AND NON-CONVEXITY

Since this is a blackbox optimization problem, it is important to exam some important features of the

Et Vt(·). We first verify its convexity via the convexity definition. We test the convexity of Et Vt(·)

by

test

the

convexity

of

subsampled

correspondence

1 |TS |

tTS Vt(·) via:

c1, c2  C :

1 |TS |

tTS

Vt( c1

+ c2 ) 2



1 |TS |

Vt

VTS

(c1)

+ 2

VTS

(c2)

tTS

(4)

where C is a set of camouflages. We sampled 1000 pairs of random camouflages from C and half

of

them

does

not

meet

the

equation.

Hence

1 |TS |

tTS Vt(·) is nonconvex.

Besides,

we

find

a

simple

linear

MLP

is

insufficient

to

approximate

1 |TS |

tTS Vt(·), which empir-

ically shows it is nonlinear.

G SUPPLEMENTAL FIGURES

Algorithm 1: Iterative Object Camouflage Learning

Input : Clone network parameter V(·); Simulation and detection VTS (·); Transformation TS which are parameterized as rendered background and foreground images; Regularization

tradeoff ; Random camouflage set CR. 1 Initialize V with random weights  2 Set score record s  +

3 C  CR

4 repeat

5

  arg min

1 |C||TS |

cC

tTS H s, V(c, t) +   2

6

c



arg

minc

1 |T |

tTS H 0, V(c, t)

7 s  {Vt(c)|t  T }

8 C  C  {c }

9

if

s |s

|

<

s

then

10

s



s |s

|

11 c  c

12 until Reach maximum training steps output: Best learned camouflage c

16

Under review as a conference paper at ICLR 2019

Not detected

"truck"

"potted plant" Partially detected "boat" & "car"

"cake" & "car"

Not detected

"truck"

Not detected

"kite"

Not detected

Grey

Random Camou

Ours

Figure 9: Qualitative comparison of the Mask R-CNN detections results of the grey baseline color,

random camouflages and our learned camouflages in different transformations. Zoom in for more

details.

17

Under review as a conference paper at ICLR 2019
Figure 10: A fraction of different Toyota sedan appearances in MS COCO dataset. 18

